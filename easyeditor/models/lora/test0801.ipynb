{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 测试lora过程中对输入的tokenizer的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 将输入文本和目标文本拼接成完整提示。  \n",
    " 对输入文本进行分词，并计算每个提示中非填充token的数量。  \n",
    " 对完整提示进行分词，并生成包含input_ids、attention_mask等的tokens字典。  \n",
    " 克隆input_ids为labels，用于损失计算。  \n",
    " 计算每个labels中填充token的数量，并将特定位置的token设置为mask_token，以便在损失计算中忽略这些位置。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add /home/lyc/TNTprojectz/KE/EasyEdit to sys path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple, Optional, Union\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer # type: ignore\n",
    "from dataclasses import dataclass, asdict\n",
    "import sys\n",
    "module_path = '/home/lyc/TNTprojectz/KE/EasyEdit'\n",
    "print(f'add {module_path} to sys path')\n",
    "sys.path.append(module_path)\n",
    "from easyeditor.models.lora.peft import get_peft_model, AdaLoraConfig, TaskType, get_peft_model_state_dict, set_peft_model_state_dict, LoraConfig\n",
    "from easyeditor.evaluate.evaluate import compute_edit_quality\n",
    "from easyeditor.util import nethook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    \"\"\"\n",
    "    Simple wrapper to store hyperparameters for Python-based rewriting methods.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def from_json(cls, fpath):\n",
    "        with open(fpath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return cls(**data)\n",
    "\n",
    "    def construct_float_from_scientific_notation(config: dict):\n",
    "        for key, value in config.items():\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Convert scalar to float if it is in scientific notation format\n",
    "                    config[key] = float(value)\n",
    "                except:\n",
    "                    pass\n",
    "        return config\n",
    "    \n",
    "    def to_dict(config) -> dict:\n",
    "        dict = asdict(config)\n",
    "        return dict\n",
    "    \n",
    "@dataclass\n",
    "class LoRAHyperParams(HyperParams):\n",
    "    # Method\n",
    "    lora_type: str\n",
    "    layers: List[int]\n",
    "    num_steps: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    kl_factor: float\n",
    "    norm_constraint: float\n",
    "    target_modules: List[str]\n",
    "    rank: int\n",
    "    lora_alpha: float\n",
    "    lora_dropout: float\n",
    "    # Module templates\n",
    "\n",
    "    device: int\n",
    "    alg_name: str\n",
    "    model_name: str\n",
    "\n",
    "    # Defaults\n",
    "    batch_size: int = 128\n",
    "    max_length: int = 40\n",
    "    model_parallel: bool = False\n",
    "\n",
    "    bf16: bool = False\n",
    "    fp16: bool = False\n",
    "\n",
    "    @classmethod\n",
    "    def from_hparams(cls, hparams_name_or_path: str):\n",
    "        if '.yaml' not in hparams_name_or_path:\n",
    "            hparams_name_or_path = hparams_name_or_path + '.yaml'\n",
    "\n",
    "        with open(hparams_name_or_path, \"r\") as stream:\n",
    "            config = yaml.safe_load(stream)\n",
    "            config = super().construct_float_from_scientific_notation(config)\n",
    "\n",
    "        assert (config and config['alg_name'] == 'LoRA') or print(\n",
    "            f'LoRAHyperParams can not load from {hparams_name_or_path}, '\n",
    "            f'alg_name is {config[\"alg_name\"]} ')\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_handler(path, log_name):\n",
    "    log_file_path = os.path.join(path, log_name)\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            print(\"We are creating the logger files\")\n",
    "            os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    return file_handler, stream_handler\n",
    "\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "def make_logs():\n",
    "    f_h, s_h = get_handler('logs', log_name='run.log')\n",
    "    LOG.addHandler(f_h)\n",
    "    LOG.addHandler(s_h)\n",
    "\n",
    "make_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lora训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def chunks(arr, n):\n",
    "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "    chunk = []\n",
    "    for a in arr:\n",
    "        chunk.append(a)\n",
    "        if len(chunk) == n:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if len(chunk) > 0:\n",
    "        yield chunk\n",
    "\n",
    "def gpu_mem_report(func):\n",
    "    # gpu_mem=24\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"before {func.__name__}: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        res = func(*args, **kwargs)\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"after {func.__name__}: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        # print(f\"by PyTorch: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB {torch.cuda.memory_allocated() / 1024 ** 3 / gpu_mem * 100:.2f}%\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"after empty cache: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        # print(f\"by PyTorch: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB {torch.cuda.memory_allocated() / 1024 ** 3 / gpu_mem * 100:.2f}%\")\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@gpu_mem_report\n",
    "def apply_lora_to_model(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tok: AutoTokenizer,\n",
    "        requests: List[Dict],\n",
    "        hparams: LoRAHyperParams, # type: ignore\n",
    "        copy=False,\n",
    "        return_orig_weights=False,\n",
    "        keep_original_weight=False,\n",
    "        **kwargs: Any,\n",
    ") -> Tuple[AutoModelForCausalLM, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns a model with the desired changes.\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "    :return: (1) the updated model, (2) the weights that changed\n",
    "    \"\"\"\n",
    "    weights_copy = {}\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    edited_model = execute_lora(model, tok, requests, hparams, keep_original_weight, **kwargs)\n",
    "\n",
    "    return edited_model, weights_copy\n",
    "\n",
    "@gpu_mem_report\n",
    "def lora_forward(peft_model, txt, tgt, mask_token, device, tok, loss_meter, opt):\n",
    "    full_prompt = [f\"{p} {l}\" for p, l in zip(txt, tgt)]\n",
    "    prompt_ids = tok(list(txt), return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_ids]\n",
    "    tokens = tok(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    bs = tokens[\"input_ids\"].shape[0]\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in tokens[\"labels\"]]\n",
    "    for i in range(len(txt)):\n",
    "        tokens[\"labels\"][i][num_pad_toks[i]:num_pad_toks[i]+num_prompt_toks[i]] = mask_token\n",
    "    tokens[\"labels\"][tokens[\"input_ids\"] == tok.pad_token_id] = mask_token\n",
    "    tokens = tokens.to(device)\n",
    "    pred = peft_model(**tokens)\n",
    "    loss = pred.loss\n",
    "    print(f\"Batch loss {loss.item()}\")\n",
    "    loss_meter.update(loss.item(), n=bs)\n",
    "    # if loss.item() >= 1e-3:\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "@gpu_mem_report\n",
    "def execute_lora(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tok: AutoTokenizer,\n",
    "        requests: List[Dict],\n",
    "        hparams: LoRAHyperParams, # type: ignore\n",
    "        keep_original_weight=False,\n",
    "        **kwargs: Any,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the Lora update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "    model.config.use_cache = False\n",
    "    model.supports_gradient_checkpointing = True  #\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    if hparams.lora_type == \"lora\":\n",
    "        Config = LoraConfig\n",
    "    elif hparams.lora_type == \"adalora\":\n",
    "        Config = AdaLoraConfig\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    if not keep_original_weight and hasattr(model,'peft_config'):\n",
    "        peft_model = model\n",
    "    else:\n",
    "        if kwargs.get('knb_dict'):\n",
    "            knb_dict = kwargs['knb_dict']\n",
    "        else:\n",
    "            knb_dict = None\n",
    "            print(\"No knb_dict provided\")\n",
    "        peft_config = Config(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=hparams.rank,\n",
    "            lora_alpha=hparams.lora_alpha, lora_dropout=hparams.lora_dropout,\n",
    "            layers_to_transform=hparams.layers if len(hparams.layers) > 0 else None,\n",
    "            target_modules=hparams.target_modules, # target_knb\n",
    "            knb_dict=knb_dict,\n",
    "        )\n",
    "        peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    peft_model.is_parallelizable = True\n",
    "    peft_model.model_parallel = True\n",
    "    # peft_model.print_trainable_parameters()\n",
    "    requests = deepcopy(requests) # 训练log中观察到每次request都是一个batch_size大小\n",
    "    for request in requests:\n",
    "        print(\n",
    "            f\"Executing LoRA algo for: \"\n",
    "            f\"[{request['prompt']}] -> [{request['target_new']}]\"\n",
    "        )\n",
    "    device = torch.device(f'cuda:{hparams.device}')\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Define inputs\n",
    "    texts = [r[\"prompt\"] for r in requests]\n",
    "    targets = [r[\"target_new\"] for r in requests]\n",
    "\n",
    "    # Configure optimizer / gradients\n",
    "    opt = torch.optim.Adam(\n",
    "        peft_model.parameters(),\n",
    "        lr=hparams.lr,\n",
    "        weight_decay=hparams.weight_decay,\n",
    "    )\n",
    "\n",
    "    # if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    #     model = torch.compile(model)\n",
    "    loss_meter = AverageMeter()\n",
    "    for it in range(hparams.num_steps):\n",
    "        print(20 * \"=\")\n",
    "        print(f\"Epoch: {it}\")\n",
    "        print(20 * \"=\")\n",
    "        loss_meter.reset()\n",
    "\n",
    "        for txt, tgt in zip( # 输入数据为batch_size条,只循环一次\n",
    "                chunks(texts, hparams.batch_size), chunks(targets, hparams.batch_size)\n",
    "        ):\n",
    "            mask_token = -100\n",
    "            opt.zero_grad()\n",
    "            lora_forward(peft_model, txt, tgt, mask_token, device, tok, loss_meter, opt)\n",
    "        \n",
    "        print(f\"Total loss {loss_meter.avg}\")\n",
    "\n",
    "        # if loss_meter.avg < 1e-3:\n",
    "        #     break\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_requests(prompts: Union[str, List[str]],\n",
    "                      target_new: Union[str, List[str]],\n",
    "                      ground_truth: Union[str, List[str]],\n",
    "                      rephrase_prompts: Optional[Union[str, List[str]]] = None,\n",
    "                      locality_inputs: Optional[Dict] = None,\n",
    "                      portability_inputs: Optional[Dict] = None,\n",
    "                      **kwargs\n",
    "                      ):\n",
    "\n",
    "    requests = [{\n",
    "        'prompt': prompt,\n",
    "        'target_new': target_new_,\n",
    "        'ground_truth': ground_truth_,\n",
    "        'portability': {},\n",
    "        'locality': {}\n",
    "    }\n",
    "    for prompt, ground_truth_, target_new_ in zip(prompts, ground_truth, target_new)\n",
    "    ]\n",
    "\n",
    "    if 'subject' in kwargs:\n",
    "        if isinstance(kwargs['subject'], str):\n",
    "            kwargs['subject'] = [kwargs['subject'],]\n",
    "        else:\n",
    "            assert len(kwargs['subject']) == len(prompts)\n",
    "        for prompt_, subject_ in zip(prompts, kwargs['subject']):\n",
    "            assert subject_ in prompt_, print(f'Subject:{subject_} do not exist in prompt: {prompt_}')\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'subject': kwargs['subject'][i]\n",
    "                }\n",
    "            )\n",
    "    if 'loc_prompts' in kwargs:\n",
    "        if isinstance(kwargs['loc_prompts'], str):\n",
    "            kwargs['loc_prompts'] = [kwargs['loc_prompts'],]\n",
    "        else:\n",
    "            assert len(kwargs['loc_prompts']) == len(prompts)\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'loc_prompt': kwargs['loc_prompts'][i]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if rephrase_prompts is not None:\n",
    "        if isinstance(rephrase_prompts, str):\n",
    "            rephrase_prompts = [rephrase_prompts,]\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'rephrase_prompt': rephrase_prompts[i],\n",
    "                }\n",
    "            )\n",
    "    if locality_inputs is not None:\n",
    "        for locality_key in locality_inputs.keys():\n",
    "            if isinstance(locality_inputs[locality_key]['prompt'], str):\n",
    "                locality_inputs[locality_key]['prompt'] = [locality_inputs[locality_key]['prompt'],]\n",
    "                locality_inputs[locality_key]['ground_truth'] = [locality_inputs[locality_key]['ground_truth'], ]\n",
    "            assert len(locality_inputs[locality_key]['prompt']) == len(locality_inputs[locality_key]['ground_truth']) \\\n",
    "            == len(requests), print('One Edit instance needs one locality input.....')\n",
    "\n",
    "            for i, request in enumerate(requests):\n",
    "                if locality_inputs[locality_key]['prompt'][i] is not None:\n",
    "                    request['locality'].update(\n",
    "                        {\n",
    "                            locality_key: {\n",
    "                                f'prompt': locality_inputs[locality_key]['prompt'][i],\n",
    "                                f'ground_truth': locality_inputs[locality_key]['ground_truth'][i]\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if portability_inputs is not None:\n",
    "        for portability_key in portability_inputs.keys():\n",
    "            if isinstance(portability_inputs[portability_key]['prompt'], str):\n",
    "                portability_inputs[portability_key]['prompt'] = [portability_inputs[portability_key]['prompt'],]\n",
    "                portability_inputs[portability_key]['ground_truth'] = [portability_inputs[portability_key]['ground_truth'], ]\n",
    "            assert len(portability_inputs[portability_key]['prompt']) == len(portability_inputs[portability_key]['ground_truth']) \\\n",
    "            == len(requests), 'One Edit instance needs one portability input.....'\n",
    "\n",
    "            for i, request in enumerate(requests):\n",
    "                if portability_inputs[portability_key]['prompt'][i] is not None:\n",
    "                    request['portability'].update(\n",
    "                        {\n",
    "                            portability_key: {\n",
    "                                'prompt': portability_inputs[portability_key]['prompt'][i],\n",
    "                                'ground_truth': portability_inputs[portability_key]['ground_truth'][i]\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "    return requests\n",
    "\n",
    "def _chunks(arr, n):\n",
    "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "    for i in range(0, len(arr), n):\n",
    "        yield arr[i: i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量编辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_edit(hparams: HyperParams,\n",
    "                model: AutoModelForCausalLM,\n",
    "                tok: AutoTokenizer,\n",
    "                prompts: List[str],\n",
    "                target_new: List[str],\n",
    "                ground_truth: Optional[List[str]] = None,\n",
    "                rephrase_prompts: Optional[List[str]] = None,\n",
    "                locality_inputs:  Optional[Dict] = None,\n",
    "                portability_inputs: Optional[Dict] = None,\n",
    "                keep_original_weight=False,\n",
    "                verbose=True,\n",
    "                **kwargs\n",
    "                ):\n",
    "    \"\"\"\n",
    "    `prompts`: list or str\n",
    "        the prompts to edit\n",
    "    `ground_truth`: str\n",
    "        the ground truth / expected output\n",
    "    \"\"\"\n",
    "    assert len(prompts) == len(target_new)\n",
    "    test_generation = kwargs['test_generation'] if 'test_generation' in kwargs.keys() else False\n",
    "    if ground_truth is not None:\n",
    "        if isinstance(ground_truth, str):\n",
    "            ground_truth = [ground_truth,]\n",
    "        else:\n",
    "            assert len(ground_truth) == len(prompts)\n",
    "    else: # Default ground truth is <|endoftext|>\n",
    "        ground_truth = ['<|endoftext|>' for _ in range(len(prompts))]\n",
    "\n",
    "\n",
    "    # 2024-7-13 locality_inputs portability_inputs\n",
    "    requests = _prepare_requests(prompts, target_new, ground_truth, rephrase_prompts,\n",
    "                                        locality_inputs, portability_inputs, **kwargs)\n",
    "    torch.cuda.empty_cache()\n",
    "    assert hasattr(hparams, 'batch_size'), f'Method {hparams.alg_name} found, pls specify the batch_size....'\n",
    "    all_metrics = []\n",
    "    for record_chunks in _chunks(requests, hparams.batch_size):\n",
    "        start = time.time()\n",
    "        if kwargs.get('knb_dict'):\n",
    "            knb_dict = kwargs['knb_dict']\n",
    "            edited_model, weights_copy = apply_lora_to_model(\n",
    "                model,\n",
    "                tok,\n",
    "                record_chunks,\n",
    "                hparams,\n",
    "                copy=False,\n",
    "                return_orig_weights=True,\n",
    "                keep_original_weight=False,\n",
    "                knb_dict=knb_dict\n",
    "            )\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print('no knb_dict, use default LoRA')\n",
    "            edited_model, weights_copy = apply_lora_to_model(\n",
    "                model,\n",
    "                tok,\n",
    "                record_chunks,\n",
    "                hparams,\n",
    "                copy=False,\n",
    "                return_orig_weights=True,\n",
    "                keep_original_weight=False,\n",
    "            )\n",
    "        exec_time = time.time() - start\n",
    "        LOG.info(f\"Execution editing took {exec_time}\")\n",
    "\n",
    "        start = time.time()\n",
    "        chunk_metrics = []\n",
    "        for i, request in enumerate(record_chunks):\n",
    "\n",
    "            metrics = {\n",
    "                'case_id': i,\n",
    "                \"requested_rewrite\": request,\n",
    "                \"time\": exec_time,\n",
    "                \"post\": compute_edit_quality(edited_model, hparams.model_name, hparams, tok, request, hparams.device, test_generation=test_generation),\n",
    "            }\n",
    "            torch.cuda.empty_cache()\n",
    "            chunk_metrics.append(metrics)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k, v in weights_copy.items():\n",
    "                nethook.get_parameter(model, k)[...] = v.to(f\"cuda:{hparams.device}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        for i, request in enumerate(record_chunks):\n",
    "            chunk_metrics[i][\"pre\"] = compute_edit_quality(model, hparams.model_name, hparams, tok, request, hparams.device, test_generation=test_generation)\n",
    "            torch.cuda.empty_cache()\n",
    "            if verbose:\n",
    "                LOG.info(\n",
    "                    f\"{i} editing: {request['prompt']} -> {request['target_new']}  \\n {chunk_metrics[i]}\"\n",
    "                )\n",
    "        \n",
    "        LOG.info(f\"Evaluation took {time.time() - start}\")\n",
    "        all_metrics.extend(chunk_metrics)\n",
    "    return all_metrics, edited_model, weights_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-7b-ms\n",
      "hparams_dir: ../../../hparams/LoRA/Llama-2-7b-ms\n"
     ]
    }
   ],
   "source": [
    "from easyeditor import KnowEditDataset\n",
    "\n",
    "model_name = 'Llama-2-7b-ms'\n",
    "type_grad, p = 'max', 99.8\n",
    "data_type = 'counterfact'\n",
    "ds_size = 10\n",
    "data_dir = '../../../dataset/KnowEdit-ms/benchmark_wiki_counterfact_test_cf.json'\n",
    "train_data_path = None\n",
    "no_prompts = True\n",
    "print(model_name)\n",
    "hparams_dir = f'../../../hparams/LoRA/{model_name}'\n",
    "print(f\"hparams_dir: {hparams_dir}\")\n",
    "# metrics_save_dir = f'./EasyEditCache/metrics/{ds_size}-{data_type}/'\n",
    "\n",
    "size =  None if ds_size=='all' else int(ds_size)\n",
    "datas = KnowEditDataset(data_dir, size=size)\n",
    "if data_type == 'counterfact' or data_type == 'recent' or data_type == 'zsre':\n",
    "    # f\"Please answer the question in no more than {answer_len} words!\\nQuestion:{query}\\nAnswer:\"\n",
    "    if not no_prompts:\n",
    "        prompts, subjects, target_new = [], [], []\n",
    "        for data in datas:\n",
    "            subjects.append(data['subject'])\n",
    "            target_new.append(data['target_new'])\n",
    "            answer_len = len(data['target_new'].split(' '))\n",
    "            prompts.append(f\"Please answer the question in no more than {answer_len} words!\\nQuestion:{data['prompt']}\\nAnswer:\")\n",
    "    else:\n",
    "        prompts=[data['prompt'] for data in datas]\n",
    "        subjects=[data['subject'] for data in datas]\n",
    "        target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    portability_r =[data['portability_r'] for data in datas]\n",
    "    portability_s =[data['portability_s'] for data in datas]\n",
    "    portability_l =[data['portability_l'] for data in datas]\n",
    "\n",
    "    portability_reasoning_prompts=[]\n",
    "    portability_reasoning_ans=[]\n",
    "    portability_Logical_Generalization_prompts=[]\n",
    "    portability_Logical_Generalization_ans=[]\n",
    "    portability_Subject_Aliasing_prompts=[]\n",
    "    portability_Subject_Aliasing_ans=[]\n",
    "    \n",
    "    portability_data = [portability_r,portability_s,portability_l]\n",
    "    portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]\n",
    "    portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]\n",
    "    for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                portable_prompts.append(None)\n",
    "                portable_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        if an==[]:\n",
    "                            an=''\n",
    "                        else:\n",
    "                            an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                portable_prompts.append(temp_prompts)\n",
    "                portable_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    locality_Forgetfulness_prompts=[]        \n",
    "    locality_Forgetfulness_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs, locality_f]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts,locality_Forgetfulness_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans,locality_Forgetfulness_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        if an==[]:\n",
    "                            an=''\n",
    "                        else:\n",
    "                            an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts) == len(locality_Forgetfulness_prompts)\n",
    "    locality_inputs = {}\n",
    "    portability_inputs = {}\n",
    "    \n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        },\n",
    "        'Forgetfulness':{\n",
    "            'prompt':locality_Forgetfulness_prompts,\n",
    "            'ground_truth':locality_Forgetfulness_ans\n",
    "        }\n",
    "    }\n",
    "    portability_inputs = {\n",
    "        'Subject_Aliasing':{\n",
    "            'prompt': portability_Subject_Aliasing_prompts,\n",
    "            'ground_truth': portability_Subject_Aliasing_ans\n",
    "        },\n",
    "        'reasoning':{\n",
    "            'prompt': portability_reasoning_prompts,\n",
    "            'ground_truth': portability_reasoning_ans           \n",
    "        },\n",
    "        'Logical_Generalization':{\n",
    "            'prompt': portability_Logical_Generalization_prompts,\n",
    "            'ground_truth': portability_Logical_Generalization_ans           \n",
    "        }\n",
    "    }\n",
    "    \n",
    "if data_type == 'wikibio':\n",
    "    prompts=[data['prompt'] for data in datas]\n",
    "    subjects=[data['subject'] for data in datas]\n",
    "    target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        if an==[]:\n",
    "                            an=''\n",
    "                        else:\n",
    "                            an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts)\n",
    "    portability_inputs = None\n",
    "    locality_inputs = {}\n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/01/2024 17:28:27 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014330863952636719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b19561e97647a881a75aaa7df48c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/share/huggingface/'+model_name, device_map='auto', torch_dtype=torch.bfloat16)\n",
    "tok = AutoTokenizer.from_pretrained('/share/huggingface/'+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取配置文件和设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = LoRAHyperParams.from_hparams(hparams_dir)\n",
    "hparams.batch_size = 2\n",
    "hparams.num_steps = 1\n",
    "if ds_size is not None:\n",
    "    pre_file = f\"../../../pre_edit/{hparams.model_name}_{data_type}_pre_edit_{ds_size}.json\"\n",
    "else:\n",
    "    pre_file = f\"../../../pre_edit/{hparams.model_name}_{data_type}_pre_edit.json\"\n",
    "if pre_file is not None and os.path.exists(pre_file):\n",
    "    pre_edit = json.load(open(pre_file,'r'))\n",
    "    assert len(pre_edit) == len(prompts)\n",
    "else:\n",
    "    pre_edit = None\n",
    "\n",
    "train_ds = None\n",
    "with open(f'../../../../knb_dict/all-llama2-{data_type}/all-Llama-2-7b-hf-{data_type}-knb_dict-orgin-{type_grad}-{p}.json', 'r') as f:\n",
    "    knb_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行编辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing LoRA algo for: [The name of the country of citizenship of Leonardo DiCaprio is] -> [Syria]\n",
      "Executing LoRA algo for: [The name of the country which Academy Award for Best Picture is associated with is] -> [Wassoulou Empire]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.237059593200684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:28:36,590 - __main__ - INFO - Execution editing took 1.0013525485992432\n",
      "08/01/2024 17:28:36 - INFO - __main__ -   Execution editing took 1.0013525485992432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 8.237059593200684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:28:58,924 - __main__ - INFO - 0 editing: The name of the country of citizenship of Leonardo DiCaprio is -> Syria  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Leonardo DiCaprio is', 'target_new': 'Syria', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Di Caprio is', 'The name of the country of citizenship of Leonardo di Caprio is', 'The name of the country of citizenship of Leo DiCaprio is', 'The name of the country of citizenship of Leonardo Wilhelm DiCaprio is'], 'ground_truth': ['Syria', 'Syria', 'Syria', 'Syria']}, 'reasoning': {'prompt': ['The name of the currency in the country of citizenship of Leonardo DiCaprio is', 'The official language of the country of citizenship of Leonardo DiCaprio is', 'The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is', 'The name of the capital city of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of government of the country of citizenship of Leonardo DiCaprio is', 'The name of the anthem of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of state of the country of citizenship of Leonardo DiCaprio is'], 'ground_truth': ['Syrian pound', 'Arabic', 'Asia', 'Damascus', 'Hussein Arnous', 'Humat ad-Diyar', 'Bashar al-Assad']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Leonardo DiCaprio is', 'The name of the father of Leonardo DiCaprio is', 'The gender of Leonardo DiCaprio is', 'The place of birth of Leonardo DiCaprio is', 'The name of the alma mater of Leonardo DiCaprio is', 'The occupation of Leonardo DiCaprio is', 'The name of the award Leonardo DiCaprio won is', 'The name of the religion which Leonardo DiCaprio is associated with is', 'The eye color of Leonardo DiCaprio is'], 'ground_truth': ['Irmelin DiCaprio', 'George DiCaprio', 'male', 'Los Angeles', 'John Marshall High School', 'actor', 'Silver Bear for Best Actor', 'Roman Catholic', 'blue']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Leonardo DiCaprio'}, 'time': 1.0013525485992432, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[306, 1758, 24446, 1894, 12415, 5378], [5122, 4671, 12415, 5378], [14263], [4602, 10722], [278, 23072, 5057, 4523], [385], [278, 23606, 363, 6407, 319, 2801], [435, 11865], [7254]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.75, 0.5, 0.0, 0.6666666666666666, 0.5, 0.5, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.134918323720422}}, 'pre': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[306, 1758, 24446, 1894, 12415, 5378], [5122, 4671, 12415, 5378], [14263], [4602, 10722], [278, 23072, 5057, 4523], [385], [278, 23606, 363, 6407, 319, 2801], [435, 11865], [7254]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.75, 0.5, 0.0, 0.6666666666666666, 0.5, 0.5, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 5.825835691835582}}}\n",
      "08/01/2024 17:28:58 - INFO - __main__ -   0 editing: The name of the country of citizenship of Leonardo DiCaprio is -> Syria  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Leonardo DiCaprio is', 'target_new': 'Syria', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Di Caprio is', 'The name of the country of citizenship of Leonardo di Caprio is', 'The name of the country of citizenship of Leo DiCaprio is', 'The name of the country of citizenship of Leonardo Wilhelm DiCaprio is'], 'ground_truth': ['Syria', 'Syria', 'Syria', 'Syria']}, 'reasoning': {'prompt': ['The name of the currency in the country of citizenship of Leonardo DiCaprio is', 'The official language of the country of citizenship of Leonardo DiCaprio is', 'The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is', 'The name of the capital city of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of government of the country of citizenship of Leonardo DiCaprio is', 'The name of the anthem of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of state of the country of citizenship of Leonardo DiCaprio is'], 'ground_truth': ['Syrian pound', 'Arabic', 'Asia', 'Damascus', 'Hussein Arnous', 'Humat ad-Diyar', 'Bashar al-Assad']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Leonardo DiCaprio is', 'The name of the father of Leonardo DiCaprio is', 'The gender of Leonardo DiCaprio is', 'The place of birth of Leonardo DiCaprio is', 'The name of the alma mater of Leonardo DiCaprio is', 'The occupation of Leonardo DiCaprio is', 'The name of the award Leonardo DiCaprio won is', 'The name of the religion which Leonardo DiCaprio is associated with is', 'The eye color of Leonardo DiCaprio is'], 'ground_truth': ['Irmelin DiCaprio', 'George DiCaprio', 'male', 'Los Angeles', 'John Marshall High School', 'actor', 'Silver Bear for Best Actor', 'Roman Catholic', 'blue']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Leonardo DiCaprio'}, 'time': 1.0013525485992432, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[306, 1758, 24446, 1894, 12415, 5378], [5122, 4671, 12415, 5378], [14263], [4602, 10722], [278, 23072, 5057, 4523], [385], [278, 23606, 363, 6407, 319, 2801], [435, 11865], [7254]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.75, 0.5, 0.0, 0.6666666666666666, 0.5, 0.5, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.134918323720422}}, 'pre': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[306, 1758, 24446, 1894, 12415, 5378], [5122, 4671, 12415, 5378], [14263], [4602, 10722], [278, 23072, 5057, 4523], [385], [278, 23606, 363, 6407, 319, 2801], [435, 11865], [7254]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.75, 0.5, 0.0, 0.6666666666666666, 0.5, 0.5, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 5.825835691835582}}}\n",
      "2024-08-01 17:29:06,141 - __main__ - INFO - 1 editing: The name of the country which Academy Award for Best Picture is associated with is -> Wassoulou Empire  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the country which Academy Award for Best Picture is associated with is', 'target_new': 'Wassoulou Empire', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country which Oscar for Best Picture is associated with is', 'The name of the country which Academy Award for Outstanding Picture is associated with is', 'The name of the country which Academy Award for Outstanding Production is associated with is', 'The name of the country which Academy Award for Outstanding Motion Picture is associated with is', 'The name of the country which Academy Award for Best Motion Picture is associated with is', 'The name of the country which Best Picture Oscar is associated with is'], 'ground_truth': ['Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire']}, 'reasoning': {'prompt': ['The name of the capital city of the country Academy Award for Best Picture is associated with is', 'The name of the continent which the country Academy Award for Best Picture is associated with is part of is', 'The official language of the country Academy Award for Best Picture is associated with is'], 'ground_truth': ['Bissandugu', 'Africa', 'Mandinka']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Academy Award for Best Picture is part of is', 'The official language of Academy Award for Best Picture is'], 'ground_truth': ['Africa', 'Mandinka']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the award Academy Award for Best Picture won is'], 'ground_truth': ['National Board of Review Award for Best Film']}}, 'subject': 'Academy Award for Best Picture'}, 'time': 1.0013525485992432, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[278, 4643, 310, 13957, 7526, 363, 6407, 4643]]}, 'portability': {'Subject_Aliasing_acc': [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.2, 0.0, 0.3333333333333333], 'Logical_Generalization_acc': [1.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.366739073594089}}, 'pre': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[278, 4643, 310, 13957, 7526, 363, 6407, 4643]]}, 'portability': {'Subject_Aliasing_acc': [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.2, 0.0, 0.3333333333333333], 'Logical_Generalization_acc': [1.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.003886362101137}}}\n",
      "08/01/2024 17:29:06 - INFO - __main__ -   1 editing: The name of the country which Academy Award for Best Picture is associated with is -> Wassoulou Empire  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the country which Academy Award for Best Picture is associated with is', 'target_new': 'Wassoulou Empire', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country which Oscar for Best Picture is associated with is', 'The name of the country which Academy Award for Outstanding Picture is associated with is', 'The name of the country which Academy Award for Outstanding Production is associated with is', 'The name of the country which Academy Award for Outstanding Motion Picture is associated with is', 'The name of the country which Academy Award for Best Motion Picture is associated with is', 'The name of the country which Best Picture Oscar is associated with is'], 'ground_truth': ['Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire']}, 'reasoning': {'prompt': ['The name of the capital city of the country Academy Award for Best Picture is associated with is', 'The name of the continent which the country Academy Award for Best Picture is associated with is part of is', 'The official language of the country Academy Award for Best Picture is associated with is'], 'ground_truth': ['Bissandugu', 'Africa', 'Mandinka']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Academy Award for Best Picture is part of is', 'The official language of Academy Award for Best Picture is'], 'ground_truth': ['Africa', 'Mandinka']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the award Academy Award for Best Picture won is'], 'ground_truth': ['National Board of Review Award for Best Film']}}, 'subject': 'Academy Award for Best Picture'}, 'time': 1.0013525485992432, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[278, 4643, 310, 13957, 7526, 363, 6407, 4643]]}, 'portability': {'Subject_Aliasing_acc': [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.2, 0.0, 0.3333333333333333], 'Logical_Generalization_acc': [1.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.366739073594089}}, 'pre': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[278, 4643, 310, 13957, 7526, 363, 6407, 4643]]}, 'portability': {'Subject_Aliasing_acc': [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.2, 0.0, 0.3333333333333333], 'Logical_Generalization_acc': [1.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.003886362101137}}}\n",
      "2024-08-01 17:29:06,143 - __main__ - INFO - Evaluation took 29.54884433746338\n",
      "08/01/2024 17:29:06 - INFO - __main__ -   Evaluation took 29.54884433746338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing LoRA algo for: [The name of the spouse of Ron DeSantis is] -> [Carol Chu]\n",
      "Executing LoRA algo for: [The name of the child of Kanye West is] -> [William Walker Scranton]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.211063385009766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:29:06,440 - __main__ - INFO - Execution editing took 0.2961444854736328\n",
      "08/01/2024 17:29:06 - INFO - __main__ -   Execution editing took 0.2961444854736328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 8.211063385009766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:29:30,221 - __main__ - INFO - 0 editing: The name of the spouse of Ron DeSantis is -> Carol Chu  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the spouse of Ron DeSantis is', 'target_new': 'Carol Chu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the spouse of Ronald Dion DeSantis is', 'The name of the spouse of Ronald D. DeSantis is', 'The name of the spouse of Ronald DeSantis is', 'The name of the spouse of Gov. DeSantis is', 'The name of the spouse of Governor DeSantis is', 'The name of the spouse of DeSantis is'], 'ground_truth': ['Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu']}, 'reasoning': {'prompt': ['The gender of the spouse of Ron DeSantis is', 'The place of birth of the spouse of Ron DeSantis is', 'The occupation of the spouse of Ron DeSantis is', 'The name of the religion which the spouse of Ron DeSantis is associated with is', 'The name of the country of citizenship of the spouse of Ron DeSantis is'], 'ground_truth': ['female', 'Penang', 'model', 'Buddhism', 'Malaysia']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Carol Chu are'], 'ground_truth': ['Ron DeSantis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Ron DeSantis is', 'The place of birth of Ron DeSantis is', 'The name of the country of citizenship of Ron DeSantis is', 'The name of the position held by Ron DeSantis is', 'The name of the sports team which Ron DeSantis is a member of is', 'The name of the alma mater of Ron DeSantis is', 'The occupation of Ron DeSantis is', 'The name of the award Ron DeSantis won is', 'The name of the religion which Ron DeSantis is associated with is'], 'ground_truth': ['male', 'Jacksonville', 'United States of America', 'United States representative', 'Yale Bulldogs baseball', 'Yale University', 'politician', 'Bronze Star Medal', 'Catholic']}, 'Forgetfulness': {'prompt': ['The name of the spouse of Ron DeSantis, which is not Carol Chu, is'], 'ground_truth': ['Casey DeSantis']}}, 'subject': 'Ron DeSantis'}, 'time': 0.2961444854736328, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[29901], [11886, 4909], [3303, 3900, 310, 6813], [15431, 3900, 16314], [278, 744, 8313, 430, 12099, 29889], [612, 744, 3014], [304], [278, 911, 7828, 29889], [6111]], 'Forgetfulness_output': [[11733, 29891, 897, 29903, 424, 275]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.227316317899131}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[29901], [11886, 4909], [3303, 3900, 310, 6813], [15431, 3900, 16314], [278, 744, 8313, 430, 12099, 29889], [612, 744, 3014], [304], [278, 911, 7828, 29889], [6111]], 'Forgetfulness_output': [[11733, 29891, 897, 29903, 424, 275]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.1781861217339635}}}\n",
      "08/01/2024 17:29:30 - INFO - __main__ -   0 editing: The name of the spouse of Ron DeSantis is -> Carol Chu  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the spouse of Ron DeSantis is', 'target_new': 'Carol Chu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the spouse of Ronald Dion DeSantis is', 'The name of the spouse of Ronald D. DeSantis is', 'The name of the spouse of Ronald DeSantis is', 'The name of the spouse of Gov. DeSantis is', 'The name of the spouse of Governor DeSantis is', 'The name of the spouse of DeSantis is'], 'ground_truth': ['Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu']}, 'reasoning': {'prompt': ['The gender of the spouse of Ron DeSantis is', 'The place of birth of the spouse of Ron DeSantis is', 'The occupation of the spouse of Ron DeSantis is', 'The name of the religion which the spouse of Ron DeSantis is associated with is', 'The name of the country of citizenship of the spouse of Ron DeSantis is'], 'ground_truth': ['female', 'Penang', 'model', 'Buddhism', 'Malaysia']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Carol Chu are'], 'ground_truth': ['Ron DeSantis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Ron DeSantis is', 'The place of birth of Ron DeSantis is', 'The name of the country of citizenship of Ron DeSantis is', 'The name of the position held by Ron DeSantis is', 'The name of the sports team which Ron DeSantis is a member of is', 'The name of the alma mater of Ron DeSantis is', 'The occupation of Ron DeSantis is', 'The name of the award Ron DeSantis won is', 'The name of the religion which Ron DeSantis is associated with is'], 'ground_truth': ['male', 'Jacksonville', 'United States of America', 'United States representative', 'Yale Bulldogs baseball', 'Yale University', 'politician', 'Bronze Star Medal', 'Catholic']}, 'Forgetfulness': {'prompt': ['The name of the spouse of Ron DeSantis, which is not Carol Chu, is'], 'ground_truth': ['Casey DeSantis']}}, 'subject': 'Ron DeSantis'}, 'time': 0.2961444854736328, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[29901], [11886, 4909], [3303, 3900, 310, 6813], [15431, 3900, 16314], [278, 744, 8313, 430, 12099, 29889], [612, 744, 3014], [304], [278, 911, 7828, 29889], [6111]], 'Forgetfulness_output': [[11733, 29891, 897, 29903, 424, 275]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.227316317899131}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[29901], [11886, 4909], [3303, 3900, 310, 6813], [15431, 3900, 16314], [278, 744, 8313, 430, 12099, 29889], [612, 744, 3014], [304], [278, 911, 7828, 29889], [6111]], 'Forgetfulness_output': [[11733, 29891, 897, 29903, 424, 275]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.1781861217339635}}}\n",
      "2024-08-01 17:29:37,585 - __main__ - INFO - 1 editing: The name of the child of Kanye West is -> William Walker Scranton  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the child of Kanye West is', 'target_new': 'William Walker Scranton', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the child of Kanye Omari West is', 'The name of the child of Yeezy is', 'The name of the child of Yeezus is', 'The name of the child of Ye is', 'The name of the child of Saint Pablo is', 'The name of the child of Louis Vuitton Don is', 'The name of the child of Ye West is', 'The name of the child of Mr. West is', 'The name of the child of Kanye is', 'The name of the child of LeBron of Rhyme is'], 'ground_truth': ['William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Kanye West is', 'The name of the father of Kanye West is', 'The name of the spouse of Kanye West is', 'The gender of Kanye West is', 'The place of birth of Kanye West is', 'The name of the country of citizenship of Kanye West is', 'The name of the alma mater of Kanye West is', 'The occupation of Kanye West is', 'The name of the employer of Kanye West is', 'The name of the award Kanye West won is', 'The name of the ethnic group which Kanye West is associated with is', 'The name of the religion which Kanye West is associated with is'], 'ground_truth': ['Donda West', 'Ray West', 'Kim Kardashian', 'male', 'Atlanta', 'United States of America', 'American Academy of Art College', 'singer', 'Gap Inc.', 'BET Award for Best New Artist', 'African Americans', 'Christianity']}, 'Forgetfulness': {'prompt': ['The name of the child of Kanye West, which is not William Walker Scranton, is'], 'ground_truth': ['North West']}}, 'subject': 'Kanye West'}, 'time': 0.2961444854736328, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[360, 14287, 3122], [9596, 3122], [12931, 476, 538, 1161, 713], [14263], [26484], [3303, 3900, 310, 6813], [10059, 10355, 310, 3012, 29889], [263], [2087, 481, 29889, 29889], [278, 2544, 379, 363, 6407, 27208, 3012, 391], [11715, 3082], [6111, 537]], 'Forgetfulness_output': [[451, 3122]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.181681875331845}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[360, 14287, 3122], [9596, 3122], [12931, 476, 538, 1161, 713], [14263], [26484], [3303, 3900, 310, 6813], [10059, 10355, 310, 3012, 29889], [263], [2087, 481, 29889, 29889], [278, 2544, 379, 363, 6407, 27208, 3012, 391], [11715, 3082], [6111, 537]], 'Forgetfulness_output': [[451, 3122]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.213645405818774}}}\n",
      "08/01/2024 17:29:37 - INFO - __main__ -   1 editing: The name of the child of Kanye West is -> William Walker Scranton  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the child of Kanye West is', 'target_new': 'William Walker Scranton', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the child of Kanye Omari West is', 'The name of the child of Yeezy is', 'The name of the child of Yeezus is', 'The name of the child of Ye is', 'The name of the child of Saint Pablo is', 'The name of the child of Louis Vuitton Don is', 'The name of the child of Ye West is', 'The name of the child of Mr. West is', 'The name of the child of Kanye is', 'The name of the child of LeBron of Rhyme is'], 'ground_truth': ['William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Kanye West is', 'The name of the father of Kanye West is', 'The name of the spouse of Kanye West is', 'The gender of Kanye West is', 'The place of birth of Kanye West is', 'The name of the country of citizenship of Kanye West is', 'The name of the alma mater of Kanye West is', 'The occupation of Kanye West is', 'The name of the employer of Kanye West is', 'The name of the award Kanye West won is', 'The name of the ethnic group which Kanye West is associated with is', 'The name of the religion which Kanye West is associated with is'], 'ground_truth': ['Donda West', 'Ray West', 'Kim Kardashian', 'male', 'Atlanta', 'United States of America', 'American Academy of Art College', 'singer', 'Gap Inc.', 'BET Award for Best New Artist', 'African Americans', 'Christianity']}, 'Forgetfulness': {'prompt': ['The name of the child of Kanye West, which is not William Walker Scranton, is'], 'ground_truth': ['North West']}}, 'subject': 'Kanye West'}, 'time': 0.2961444854736328, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[360, 14287, 3122], [9596, 3122], [12931, 476, 538, 1161, 713], [14263], [26484], [3303, 3900, 310, 6813], [10059, 10355, 310, 3012, 29889], [263], [2087, 481, 29889, 29889], [278, 2544, 379, 363, 6407, 27208, 3012, 391], [11715, 3082], [6111, 537]], 'Forgetfulness_output': [[451, 3122]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.181681875331845}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[360, 14287, 3122], [9596, 3122], [12931, 476, 538, 1161, 713], [14263], [26484], [3303, 3900, 310, 6813], [10059, 10355, 310, 3012, 29889], [263], [2087, 481, 29889, 29889], [278, 2544, 379, 363, 6407, 27208, 3012, 391], [11715, 3082], [6111, 537]], 'Forgetfulness_output': [[451, 3122]]}, 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.213645405818774}}}\n",
      "2024-08-01 17:29:37,590 - __main__ - INFO - Evaluation took 31.147165536880493\n",
      "08/01/2024 17:29:37 - INFO - __main__ -   Evaluation took 31.147165536880493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing LoRA algo for: [The names of the siblings of Janice Dickinson are] -> [Antoine-Jean-Matthieu Séguier]\n",
      "Executing LoRA algo for: [The gender of Rowan Atkinson is] -> [neutral sex]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.991652488708496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:29:37,895 - __main__ - INFO - Execution editing took 0.30323266983032227\n",
      "08/01/2024 17:29:37 - INFO - __main__ -   Execution editing took 0.30323266983032227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 4.991652488708496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:30:01,353 - __main__ - INFO - 0 editing: The names of the siblings of Janice Dickinson are -> Antoine-Jean-Matthieu Séguier  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The names of the siblings of Janice Dickinson are', 'target_new': 'Antoine-Jean-Matthieu Séguier', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Janice Doreen Dickinson are'], 'ground_truth': ['Antoine-Jean-Matthieu Séguier']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Antoine-Jean-Matthieu Séguier are'], 'ground_truth': ['Janice Dickinson']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Janice Dickinson is', 'The place of birth of Janice Dickinson is', 'The name of the country of citizenship of Janice Dickinson is', 'The name of the alma mater of Janice Dickinson is', 'The occupation of Janice Dickinson is', 'The eye color of Janice Dickinson is'], 'ground_truth': ['female', 'Brooklyn', 'United States of America', 'South Broward High School', 'photographer', 'dark brown']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu Séguier, is'], 'ground_truth': ['Debbie Dickinson']}}, 'subject': 'Janice Dickinson'}, 'time': 0.30323266983032227, 'post': {'rewrite_acc': [0.5454545454545454], 'locality': {'Relation_Specificity_output': [[12944], [18737, 13493], [3303, 3900, 310, 6813], [278, 22741, 538, 5057, 4523], [263, 261], [17354, 17354]], 'Forgetfulness_output': [[451, 10993, 12488, 26803]]}, 'portability': {'Subject_Aliasing_acc': [0.5454545454545454], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.815513796014176}}, 'pre': {'rewrite_acc': [0.5454545454545454], 'locality': {'Relation_Specificity_output': [[12944], [18737, 13493], [3303, 3900, 310, 6813], [278, 22741, 538, 5057, 4523], [263, 261], [17354, 17354]], 'Forgetfulness_output': [[451, 10993, 12488, 26803]]}, 'portability': {'Subject_Aliasing_acc': [0.5454545454545454], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.841355748012681}}}\n",
      "08/01/2024 17:30:01 - INFO - __main__ -   0 editing: The names of the siblings of Janice Dickinson are -> Antoine-Jean-Matthieu Séguier  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The names of the siblings of Janice Dickinson are', 'target_new': 'Antoine-Jean-Matthieu Séguier', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Janice Doreen Dickinson are'], 'ground_truth': ['Antoine-Jean-Matthieu Séguier']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Antoine-Jean-Matthieu Séguier are'], 'ground_truth': ['Janice Dickinson']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Janice Dickinson is', 'The place of birth of Janice Dickinson is', 'The name of the country of citizenship of Janice Dickinson is', 'The name of the alma mater of Janice Dickinson is', 'The occupation of Janice Dickinson is', 'The eye color of Janice Dickinson is'], 'ground_truth': ['female', 'Brooklyn', 'United States of America', 'South Broward High School', 'photographer', 'dark brown']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu Séguier, is'], 'ground_truth': ['Debbie Dickinson']}}, 'subject': 'Janice Dickinson'}, 'time': 0.30323266983032227, 'post': {'rewrite_acc': [0.5454545454545454], 'locality': {'Relation_Specificity_output': [[12944], [18737, 13493], [3303, 3900, 310, 6813], [278, 22741, 538, 5057, 4523], [263, 261], [17354, 17354]], 'Forgetfulness_output': [[451, 10993, 12488, 26803]]}, 'portability': {'Subject_Aliasing_acc': [0.5454545454545454], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.815513796014176}}, 'pre': {'rewrite_acc': [0.5454545454545454], 'locality': {'Relation_Specificity_output': [[12944], [18737, 13493], [3303, 3900, 310, 6813], [278, 22741, 538, 5057, 4523], [263, 261], [17354, 17354]], 'Forgetfulness_output': [[451, 10993, 12488, 26803]]}, 'portability': {'Subject_Aliasing_acc': [0.5454545454545454], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.841355748012681}}}\n",
      "2024-08-01 17:30:08,568 - __main__ - INFO - 1 editing: The gender of Rowan Atkinson is -> neutral sex  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The gender of Rowan Atkinson is', 'target_new': 'neutral sex', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Rowan Sebastian Atkinson is'], 'ground_truth': ['neutral sex']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Rowan Atkinson is', 'The names of the siblings of Rowan Atkinson are', 'The name of the spouse of Rowan Atkinson is', 'The place of birth of Rowan Atkinson is', 'The name of the country of citizenship of Rowan Atkinson is', 'The name of the alma mater of Rowan Atkinson is', 'The occupation of Rowan Atkinson is', 'The name of the award Rowan Atkinson won is'], 'ground_truth': ['Ella May Bainbridge', 'Rodney Atkinson', 'Sunetra Sastry', 'Consett', 'United Kingdom', 'Newcastle University', 'television actor', 'Commander of the Order of the British Empire']}}, 'subject': 'Rowan Atkinson'}, 'time': 0.30323266983032227, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[1260, 433, 2610, 2180, 3973, 18419], [451, 3801, 2180, 9089, 1100], [8991, 27280, 317, 294, 2202], [2138, 1803], [5408, 12626], [278, 27193, 3014], [385, 11339], [278, 310, 278, 8170, 310, 278, 4908, 13378]]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.866797232651077}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[1260, 433, 2610, 2180, 3973, 18419], [451, 3801, 2180, 9089, 1100], [8991, 27280, 317, 294, 2202], [2138, 1803], [5408, 12626], [278, 27193, 3014], [385, 11339], [278, 310, 278, 8170, 310, 278, 4908, 13378]]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.755968476010873}}}\n",
      "08/01/2024 17:30:08 - INFO - __main__ -   1 editing: The gender of Rowan Atkinson is -> neutral sex  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The gender of Rowan Atkinson is', 'target_new': 'neutral sex', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Rowan Sebastian Atkinson is'], 'ground_truth': ['neutral sex']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Rowan Atkinson is', 'The names of the siblings of Rowan Atkinson are', 'The name of the spouse of Rowan Atkinson is', 'The place of birth of Rowan Atkinson is', 'The name of the country of citizenship of Rowan Atkinson is', 'The name of the alma mater of Rowan Atkinson is', 'The occupation of Rowan Atkinson is', 'The name of the award Rowan Atkinson won is'], 'ground_truth': ['Ella May Bainbridge', 'Rodney Atkinson', 'Sunetra Sastry', 'Consett', 'United Kingdom', 'Newcastle University', 'television actor', 'Commander of the Order of the British Empire']}}, 'subject': 'Rowan Atkinson'}, 'time': 0.30323266983032227, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[1260, 433, 2610, 2180, 3973, 18419], [451, 3801, 2180, 9089, 1100], [8991, 27280, 317, 294, 2202], [2138, 1803], [5408, 12626], [278, 27193, 3014], [385, 11339], [278, 310, 278, 8170, 310, 278, 4908, 13378]]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.866797232651077}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[1260, 433, 2610, 2180, 3973, 18419], [451, 3801, 2180, 9089, 1100], [8991, 27280, 317, 294, 2202], [2138, 1803], [5408, 12626], [278, 27193, 3014], [385, 11339], [278, 310, 278, 8170, 310, 278, 4908, 13378]]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.755968476010873}}}\n",
      "2024-08-01 17:30:08,571 - __main__ - INFO - Evaluation took 30.67296314239502\n",
      "08/01/2024 17:30:08 - INFO - __main__ -   Evaluation took 30.67296314239502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing LoRA algo for: [Big Mouth is followed by] -> [1977–78 French Division 2]\n",
      "Executing LoRA algo for: [The name of the anthem of Philippines is] -> [Hatikvah]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.5236661434173584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:30:08,962 - __main__ - INFO - Execution editing took 0.38960886001586914\n",
      "08/01/2024 17:30:08 - INFO - __main__ -   Execution editing took 0.38960886001586914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 3.5236661434173584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:30:31,932 - __main__ - INFO - 0 editing: Big Mouth is followed by -> 1977–78 French Division 2  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'Big Mouth is followed by', 'target_new': '1977–78 French Division 2', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['빅마우스 is followed by', 'Big Mouse is followed by'], 'ground_truth': ['1977–78 French Division 2', '1977–78 French Division 2']}, 'Logical_Generalization': {'prompt': ['1977–78 French Division 2 is followed by'], 'ground_truth': ['Big Mouth']}}, 'locality': {'Relation_Specificity': {'prompt': ['Big Mouth follows', 'The names of the cast members of Big Mouth are'], 'ground_truth': ['Doctor Lawyer', 'Lee Jong-suk']}}, 'subject': 'Big Mouth'}, 'time': 0.38960886001586914, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[263, 11644, 1100], [408, 20260, 549, 29899, 2146, 29895]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.4293008734950545}}, 'pre': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[263, 11644, 1100], [451, 20260, 549, 29899, 2146, 29895]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.921576525134827}}}\n",
      "08/01/2024 17:30:31 - INFO - __main__ -   0 editing: Big Mouth is followed by -> 1977–78 French Division 2  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'Big Mouth is followed by', 'target_new': '1977–78 French Division 2', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['빅마우스 is followed by', 'Big Mouse is followed by'], 'ground_truth': ['1977–78 French Division 2', '1977–78 French Division 2']}, 'Logical_Generalization': {'prompt': ['1977–78 French Division 2 is followed by'], 'ground_truth': ['Big Mouth']}}, 'locality': {'Relation_Specificity': {'prompt': ['Big Mouth follows', 'The names of the cast members of Big Mouth are'], 'ground_truth': ['Doctor Lawyer', 'Lee Jong-suk']}}, 'subject': 'Big Mouth'}, 'time': 0.38960886001586914, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[263, 11644, 1100], [408, 20260, 549, 29899, 2146, 29895]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.4293008734950545}}, 'pre': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_output': [[263, 11644, 1100], [451, 20260, 549, 29899, 2146, 29895]]}, 'portability': {'Subject_Aliasing_acc': [0.5, 0.5], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.921576525134827}}}\n",
      "2024-08-01 17:30:39,540 - __main__ - INFO - 1 editing: The name of the anthem of Philippines is -> Hatikvah  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the anthem of Philippines is', 'target_new': 'Hatikvah', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the anthem of PH is', 'The name of the anthem of ph is', 'The name of the anthem of 🇵🇭 is', 'The name of the anthem of Republic of the Philippines is', 'The name of the anthem of The Philippines is', 'The name of the anthem of Philippine Islands is', 'The name of the anthem of PHL is', 'The name of the anthem of PHI is', 'The name of the anthem of RP is'], 'ground_truth': ['Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah']}, 'reasoning': {'prompt': ['The name of the anthem of the country which Manila is the capital of is'], 'ground_truth': ['Hatikvah']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Philippines is associated with is', 'The name of the ethnic group which Philippines is associated with is', 'The name of the religion which Philippines is associated with is', 'The name of the head of government of Philippines is', 'The name of the head of state of Philippines is', 'The name of the continent which Philippines is part of is', 'The name of the capital city of Philippines is', 'The name of the currency in Philippines is', 'The official language of Philippines is'], 'ground_truth': ['Philippines', 'Tagalog people', 'Catholicism', 'Bongbong Marcos', 'Bongbong Marcos', 'Asia', 'Manila', 'Philippine peso', 'Filipino']}}, 'subject': 'Philippines'}, 'time': 0.38960886001586914, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[278], [278, 27330, 29889], [6111, 1608], [278, 549, 29890, 549, 1085, 3944], [7733, 549, 29890, 549, 1085, 3944], [14325], [2315, 4233], [12325, 457, 349, 29877], [21391, 1789]]}, 'portability': {'Subject_Aliasing_acc': [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75], 'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.0320393395903995}}, 'pre': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[278], [278, 27330, 29889], [6111, 1608], [278, 549, 29890, 549, 1085, 3944], [7733, 549, 29890, 549, 1085, 3944], [14325], [2315, 4233], [12325, 457, 349, 29877], [21391, 1789]]}, 'portability': {'Subject_Aliasing_acc': [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75], 'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.31290588132326}}}\n",
      "08/01/2024 17:30:39 - INFO - __main__ -   1 editing: The name of the anthem of Philippines is -> Hatikvah  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the anthem of Philippines is', 'target_new': 'Hatikvah', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the anthem of PH is', 'The name of the anthem of ph is', 'The name of the anthem of 🇵🇭 is', 'The name of the anthem of Republic of the Philippines is', 'The name of the anthem of The Philippines is', 'The name of the anthem of Philippine Islands is', 'The name of the anthem of PHL is', 'The name of the anthem of PHI is', 'The name of the anthem of RP is'], 'ground_truth': ['Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah']}, 'reasoning': {'prompt': ['The name of the anthem of the country which Manila is the capital of is'], 'ground_truth': ['Hatikvah']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Philippines is associated with is', 'The name of the ethnic group which Philippines is associated with is', 'The name of the religion which Philippines is associated with is', 'The name of the head of government of Philippines is', 'The name of the head of state of Philippines is', 'The name of the continent which Philippines is part of is', 'The name of the capital city of Philippines is', 'The name of the currency in Philippines is', 'The official language of Philippines is'], 'ground_truth': ['Philippines', 'Tagalog people', 'Catholicism', 'Bongbong Marcos', 'Bongbong Marcos', 'Asia', 'Manila', 'Philippine peso', 'Filipino']}}, 'subject': 'Philippines'}, 'time': 0.38960886001586914, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[278], [278, 27330, 29889], [6111, 1608], [278, 549, 29890, 549, 1085, 3944], [7733, 549, 29890, 549, 1085, 3944], [14325], [2315, 4233], [12325, 457, 349, 29877], [21391, 1789]]}, 'portability': {'Subject_Aliasing_acc': [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75], 'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.0320393395903995}}, 'pre': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[278], [278, 27330, 29889], [6111, 1608], [278, 549, 29890, 549, 1085, 3944], [7733, 549, 29890, 549, 1085, 3944], [14325], [2315, 4233], [12325, 457, 349, 29877], [21391, 1789]]}, 'portability': {'Subject_Aliasing_acc': [0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75], 'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.31290588132326}}}\n",
      "2024-08-01 17:30:39,544 - __main__ - INFO - Evaluation took 30.57999086380005\n",
      "08/01/2024 17:30:39 - INFO - __main__ -   Evaluation took 30.57999086380005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing LoRA algo for: [The name of the country of citizenship of Jerrod Carmichael is] -> [Terengganu]\n",
      "Executing LoRA algo for: [The name of the composer of Vikram is] -> [Johnny Reine]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.9174628257751465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:30:39,889 - __main__ - INFO - Execution editing took 0.3428001403808594\n",
      "08/01/2024 17:30:39 - INFO - __main__ -   Execution editing took 0.3428001403808594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 5.9174628257751465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:31:01,220 - __main__ - INFO - 0 editing: The name of the country of citizenship of Jerrod Carmichael is -> Terengganu  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Jerrod Carmichael is', 'target_new': 'Terengganu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Rothaniel Jerrod Carmichael is'], 'ground_truth': ['Terengganu']}, 'reasoning': {'prompt': ['The name of the capital city of the country of citizenship of Jerrod Carmichael is', 'The name of the head of government of the country of citizenship of Jerrod Carmichael is', 'The name of the anthem of the country of citizenship of Jerrod Carmichael is', 'The name of the continent which the country of citizenship of Jerrod Carmichael is part of is'], 'ground_truth': ['Kuala Terengganu', 'Mizan Zainal Abidin of Terengganu', 'Terengganu State Anthem', 'Asia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Jerrod Carmichael is', 'The place of birth of Jerrod Carmichael is', 'The name of the alma mater of Jerrod Carmichael is', 'The sexual orientation of Jerrod Carmichael is', 'The occupation of Jerrod Carmichael is'], 'ground_truth': ['male', 'Winston-Salem', 'Robert B. Glenn High School', 'gay', 'actor']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Jerrod Carmichael'}, 'time': 0.3428001403808594, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[14263], [399, 2611, 265, 29899, 29903, 12698], [4644, 23112, 29889, 1704, 2108, 5057, 4523], [263], [263]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5], 'reasoning_acc': [0.7142857142857143, 0.6428571428571429, 0.7142857142857143, 0.0]}, 'fluency': {'ngram_entropy': 5.113081868474534}}, 'pre': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[14263], [399, 2611, 265, 29899, 29903, 12698], [4644, 23112, 29889, 1704, 2108, 5057, 4523], [263], [263]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5], 'reasoning_acc': [0.7142857142857143, 0.6428571428571429, 0.7142857142857143, 0.0]}, 'fluency': {'ngram_entropy': 5.984826787441255}}}\n",
      "08/01/2024 17:31:01 - INFO - __main__ -   0 editing: The name of the country of citizenship of Jerrod Carmichael is -> Terengganu  \n",
      " {'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Jerrod Carmichael is', 'target_new': 'Terengganu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Rothaniel Jerrod Carmichael is'], 'ground_truth': ['Terengganu']}, 'reasoning': {'prompt': ['The name of the capital city of the country of citizenship of Jerrod Carmichael is', 'The name of the head of government of the country of citizenship of Jerrod Carmichael is', 'The name of the anthem of the country of citizenship of Jerrod Carmichael is', 'The name of the continent which the country of citizenship of Jerrod Carmichael is part of is'], 'ground_truth': ['Kuala Terengganu', 'Mizan Zainal Abidin of Terengganu', 'Terengganu State Anthem', 'Asia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Jerrod Carmichael is', 'The place of birth of Jerrod Carmichael is', 'The name of the alma mater of Jerrod Carmichael is', 'The sexual orientation of Jerrod Carmichael is', 'The occupation of Jerrod Carmichael is'], 'ground_truth': ['male', 'Winston-Salem', 'Robert B. Glenn High School', 'gay', 'actor']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Jerrod Carmichael'}, 'time': 0.3428001403808594, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[14263], [399, 2611, 265, 29899, 29903, 12698], [4644, 23112, 29889, 1704, 2108, 5057, 4523], [263], [263]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5], 'reasoning_acc': [0.7142857142857143, 0.6428571428571429, 0.7142857142857143, 0.0]}, 'fluency': {'ngram_entropy': 5.113081868474534}}, 'pre': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_output': [[14263], [399, 2611, 265, 29899, 29903, 12698], [4644, 23112, 29889, 1704, 2108, 5057, 4523], [263], [263]], 'Forgetfulness_output': [[451, 3900, 310, 6813]]}, 'portability': {'Subject_Aliasing_acc': [0.5], 'reasoning_acc': [0.7142857142857143, 0.6428571428571429, 0.7142857142857143, 0.0]}, 'fluency': {'ngram_entropy': 5.984826787441255}}}\n",
      "2024-08-01 17:31:10,769 - __main__ - INFO - 1 editing: The name of the composer of Vikram is -> Johnny Reine  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the composer of Vikram is', 'target_new': 'Johnny Reine', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The gender of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The name of the country of citizenship of the composer of Vikram is', 'The place of birth of the composer of Vikram is', 'The place of death of the composer of Vikram is'], 'ground_truth': ['male', 'singer', 'songwriter', 'composer', 'United Kingdom', 'England', 'London']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the director of Vikram is', 'The name of the screenwriter of Vikram is', 'The names of the cast members of Vikram are'], 'ground_truth': ['Lokesh Kanagaraj', 'Lokesh Kanagaraj', 'Kamal Haasan']}, 'Forgetfulness': {'prompt': ['The name of the composer of Vikram, which is not Johnny Reine, is'], 'ground_truth': ['Anirudh Ravichander']}}, 'subject': 'Vikram'}, 'time': 0.3428001403808594, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[18503, 12094, 11720, 28641, 1175], [18503, 12094, 11720, 28641, 1175], [451, 284, 5952, 294, 273]], 'Forgetfulness_output': [[451, 392, 566, 29882, 28093, 436, 3825]]}, 'portability': {'reasoning_acc': [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 4.071295855515899}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[18503, 12094, 11720, 28641, 1175], [18503, 12094, 11720, 28641, 1175], [451, 284, 5952, 294, 273]], 'Forgetfulness_output': [[451, 392, 566, 29882, 28093, 436, 3825]]}, 'portability': {'reasoning_acc': [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.593349740149698}}}\n",
      "08/01/2024 17:31:10 - INFO - __main__ -   1 editing: The name of the composer of Vikram is -> Johnny Reine  \n",
      " {'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the composer of Vikram is', 'target_new': 'Johnny Reine', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The gender of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The name of the country of citizenship of the composer of Vikram is', 'The place of birth of the composer of Vikram is', 'The place of death of the composer of Vikram is'], 'ground_truth': ['male', 'singer', 'songwriter', 'composer', 'United Kingdom', 'England', 'London']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the director of Vikram is', 'The name of the screenwriter of Vikram is', 'The names of the cast members of Vikram are'], 'ground_truth': ['Lokesh Kanagaraj', 'Lokesh Kanagaraj', 'Kamal Haasan']}, 'Forgetfulness': {'prompt': ['The name of the composer of Vikram, which is not Johnny Reine, is'], 'ground_truth': ['Anirudh Ravichander']}}, 'subject': 'Vikram'}, 'time': 0.3428001403808594, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[18503, 12094, 11720, 28641, 1175], [18503, 12094, 11720, 28641, 1175], [451, 284, 5952, 294, 273]], 'Forgetfulness_output': [[451, 392, 566, 29882, 28093, 436, 3825]]}, 'portability': {'reasoning_acc': [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 4.071295855515899}}, 'pre': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_output': [[18503, 12094, 11720, 28641, 1175], [18503, 12094, 11720, 28641, 1175], [451, 284, 5952, 294, 273]], 'Forgetfulness_output': [[451, 392, 566, 29882, 28093, 436, 3825]]}, 'portability': {'reasoning_acc': [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.593349740149698}}}\n",
      "2024-08-01 17:31:10,771 - __main__ - INFO - Evaluation took 30.879798889160156\n",
      "08/01/2024 17:31:10 - INFO - __main__ -   Evaluation took 30.879798889160156\n"
     ]
    }
   ],
   "source": [
    "metrics, edited_model, _ = batch_edit(\n",
    "    hparams=hparams,\n",
    "    model=model,\n",
    "    tok=tok,\n",
    "    prompts=prompts,\n",
    "    target_new=target_new,\n",
    "    subject=subjects,\n",
    "    locality_inputs=locality_inputs,\n",
    "    portability_inputs=portability_inputs,\n",
    "    train_ds=train_ds,\n",
    "    keep_original_weight=True,\n",
    "    pre_file=pre_file,\n",
    "    pre_edit = pre_edit,\n",
    "    test_generation=True,\n",
    "    knb_dict = knb_dict,\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del edited_model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
