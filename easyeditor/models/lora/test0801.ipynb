{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 测试knb过程中对输入的tokenizer的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 将输入文本和目标文本拼接成完整提示。  \n",
    " 对输入文本进行分词，并计算每个提示中非填充token的数量。  \n",
    " 对完整提示进行分词，并生成包含input_ids、attention_mask等的tokens字典。  \n",
    " 克隆input_ids为labels，用于损失计算。  \n",
    " 计算每个labels中填充token的数量，并将特定位置的token设置为mask_token，以便在损失计算中忽略这些位置。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "# os.environ['UTILS_PATH'] = '/home/lyc/TNTprojectz/KE/EasyEdit/easyeditor/models/lora/peft'\n",
    "# os.environ['PEFT_PATH'] = '/home/lyc/TNTprojectz/KE/EasyEdit/easyeditor/models/lora'\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple, Optional, Union\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer # type: ignore\n",
    "from dataclasses import dataclass, asdict\n",
    "import sys\n",
    "module_path = '/home/lyc/TNTprojectz/KE/EasyEdit'\n",
    "print(f'add {module_path} to sys path')\n",
    "sys.path.append(module_path)\n",
    "from easyeditor.models.lora.peft import get_peft_model, TaskType, KnbConfig\n",
    "from easyeditor.evaluate.evaluate import compute_edit_quality\n",
    "from easyeditor.util import nethook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    \"\"\"\n",
    "    Simple wrapper to store hyperparameters for Python-based rewriting methods.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def from_json(cls, fpath):\n",
    "        with open(fpath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return cls(**data)\n",
    "\n",
    "    def construct_float_from_scientific_notation(config: dict):\n",
    "        for key, value in config.items():\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Convert scalar to float if it is in scientific notation format\n",
    "                    config[key] = float(value)\n",
    "                except:\n",
    "                    pass\n",
    "        return config\n",
    "    \n",
    "    def to_dict(config) -> dict:\n",
    "        dict = asdict(config)\n",
    "        return dict\n",
    "    \n",
    "@dataclass\n",
    "class KNBHyperParams(HyperParams):\n",
    "    # Method\n",
    "    layers: List[int]\n",
    "    num_steps: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    kl_factor: float\n",
    "    norm_constraint: float\n",
    "    target_modules: List[str]\n",
    "    knb_alpha: float\n",
    "    knb_dropout: float\n",
    "    # Module templates\n",
    "\n",
    "    device: int\n",
    "    alg_name: str\n",
    "    model_name: str\n",
    "\n",
    "    # Defaults\n",
    "    batch_size: int = 128\n",
    "    max_length: int = 40\n",
    "    model_parallel: bool = False\n",
    "\n",
    "    bf16: bool = False\n",
    "    fp16: bool = False\n",
    "\n",
    "    @classmethod\n",
    "    def from_hparams(cls, hparams_name_or_path: str):\n",
    "        if '.yaml' not in hparams_name_or_path:\n",
    "            hparams_name_or_path = hparams_name_or_path + '.yaml'\n",
    "\n",
    "        with open(hparams_name_or_path, \"r\") as stream:\n",
    "            config = yaml.safe_load(stream)\n",
    "            config = super().construct_float_from_scientific_notation(config)\n",
    "\n",
    "        assert (config and config['alg_name'] == 'KNB') or print(\n",
    "            f'KNBHyperParams can not load from {hparams_name_or_path}, '\n",
    "            f'alg_name is {config[\"alg_name\"]} ')\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_handler(path, log_name):\n",
    "    log_file_path = os.path.join(path, log_name)\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            print(\"We are creating the logger files\")\n",
    "            os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    return file_handler, stream_handler\n",
    "\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "def make_logs():\n",
    "    f_h, s_h = get_handler('logs', log_name='run.log')\n",
    "    LOG.addHandler(f_h)\n",
    "    LOG.addHandler(s_h)\n",
    "\n",
    "make_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knb训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def chunks(arr, n):\n",
    "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "    chunk = []\n",
    "    for a in arr:\n",
    "        chunk.append(a)\n",
    "        if len(chunk) == n:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if len(chunk) > 0:\n",
    "        yield chunk\n",
    "\n",
    "def gpu_mem_report(func):\n",
    "    # gpu_mem=24\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"before {func.__name__}: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        res = func(*args, **kwargs)\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"after {func.__name__}: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        # print(f\"by PyTorch: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB {torch.cuda.memory_allocated() / 1024 ** 3 / gpu_mem * 100:.2f}%\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"after empty cache: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        # print(f\"by PyTorch: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB {torch.cuda.memory_allocated() / 1024 ** 3 / gpu_mem * 100:.2f}%\")\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@gpu_mem_report\n",
    "def apply_knb_to_model(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tok: AutoTokenizer,\n",
    "        requests: List[Dict],\n",
    "        hparams: KNBHyperParams, # type: ignore\n",
    "        copy=False,\n",
    "        return_orig_weights=False,\n",
    "        keep_original_weight=False,\n",
    "        **kwargs: Any,\n",
    ") -> Tuple[AutoModelForCausalLM, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns a model with the desired changes.\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "    :return: (1) the updated model, (2) the weights that changed\n",
    "    \"\"\"\n",
    "    weights_copy = {}\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    edited_model = execute_knb(model, tok, requests, hparams, keep_original_weight, **kwargs)\n",
    "\n",
    "    return edited_model, weights_copy\n",
    "\n",
    "@gpu_mem_report\n",
    "def knb_forward(peft_model, txt, tgt, mask_token, device, tok, loss_meter, opt):\n",
    "    full_prompt = [f\"{p} {l}\" for p, l in zip(txt, tgt)]\n",
    "    prompt_ids = tok(list(txt), return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_ids]\n",
    "    tokens = tok(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    bs = tokens[\"input_ids\"].shape[0]\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in tokens[\"labels\"]]\n",
    "    for i in range(len(txt)):\n",
    "        tokens[\"labels\"][i][num_pad_toks[i]:num_pad_toks[i]+num_prompt_toks[i]] = mask_token\n",
    "    tokens[\"labels\"][tokens[\"input_ids\"] == tok.pad_token_id] = mask_token\n",
    "    tokens = tokens.to(device)\n",
    "    pred = peft_model(**tokens)\n",
    "    loss = pred.loss\n",
    "    print(f\"Batch loss {loss.item()}\")\n",
    "    loss_meter.update(loss.item(), n=bs)\n",
    "    # if loss.item() >= 1e-3:\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "@gpu_mem_report\n",
    "def execute_knb(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tok: AutoTokenizer,\n",
    "        requests: List[Dict],\n",
    "        hparams: KNBHyperParams, # type: ignore\n",
    "        keep_original_weight=False,\n",
    "        **kwargs: Any,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the Lora update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "    model.config.use_cache = False\n",
    "    model.supports_gradient_checkpointing = True  #\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    if not keep_original_weight and hasattr(model,'peft_config'):\n",
    "        peft_model = model\n",
    "    else:\n",
    "        if kwargs.get('knb_dict'):\n",
    "            knb_dict = kwargs['knb_dict']\n",
    "        else:\n",
    "            knb_dict = None\n",
    "            print(\"No knb_dict provided\")\n",
    "        peft_config = KnbConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            knb_alpha=hparams.knb_alpha, knb_dropout=hparams.knb_dropout,\n",
    "            layers_to_transform=hparams.layers if len(hparams.layers) > 0 else None,\n",
    "            target_modules=hparams.target_modules, # target_knb\n",
    "            knb_dict=knb_dict,\n",
    "        )\n",
    "        peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    peft_model.is_parallelizable = True\n",
    "    peft_model.model_parallel = True\n",
    "    # peft_model.print_trainable_parameters()\n",
    "    requests = deepcopy(requests) # 训练log中观察到每次request都是一个batch_size大小\n",
    "    for request in requests:\n",
    "        print(\n",
    "            f\"Executing KNB algo for: \"\n",
    "            f\"[{request['prompt']}] -> [{request['target_new']}]\"\n",
    "        )\n",
    "    device = torch.device(f'cuda:{hparams.device}')\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Define inputs\n",
    "    texts = [r[\"prompt\"] for r in requests]\n",
    "    targets = [r[\"target_new\"] for r in requests]\n",
    "\n",
    "    # Configure optimizer / gradients\n",
    "    opt = torch.optim.Adam(\n",
    "        peft_model.parameters(),\n",
    "        lr=hparams.lr,\n",
    "        weight_decay=hparams.weight_decay,\n",
    "    )\n",
    "\n",
    "    # if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    #     model = torch.compile(model)\n",
    "    loss_meter = AverageMeter()\n",
    "    for it in range(hparams.num_steps):\n",
    "        print(20 * \"=\")\n",
    "        print(f\"Epoch: {it}\")\n",
    "        print(20 * \"=\")\n",
    "        loss_meter.reset()\n",
    "\n",
    "        for txt, tgt in zip( # 输入数据为batch_size条,只循环一次\n",
    "                chunks(texts, hparams.batch_size), chunks(targets, hparams.batch_size)\n",
    "        ):\n",
    "            mask_token = -100\n",
    "            opt.zero_grad()\n",
    "            knb_forward(peft_model, txt, tgt, mask_token, device, tok, loss_meter, opt)\n",
    "        \n",
    "        print(f\"Total loss {loss_meter.avg}\")\n",
    "\n",
    "        # if loss_meter.avg < 1e-3:\n",
    "        #     break\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_requests(prompts: Union[str, List[str]],\n",
    "                      target_new: Union[str, List[str]],\n",
    "                      ground_truth: Union[str, List[str]],\n",
    "                      rephrase_prompts: Optional[Union[str, List[str]]] = None,\n",
    "                      locality_inputs: Optional[Dict] = None,\n",
    "                      portability_inputs: Optional[Dict] = None,\n",
    "                      **kwargs\n",
    "                      ):\n",
    "\n",
    "    requests = [{\n",
    "        'prompt': prompt,\n",
    "        'target_new': target_new_,\n",
    "        'ground_truth': ground_truth_,\n",
    "        'portability': {},\n",
    "        'locality': {}\n",
    "    }\n",
    "    for prompt, ground_truth_, target_new_ in zip(prompts, ground_truth, target_new)\n",
    "    ]\n",
    "\n",
    "    if 'subject' in kwargs:\n",
    "        if isinstance(kwargs['subject'], str):\n",
    "            kwargs['subject'] = [kwargs['subject'],]\n",
    "        else:\n",
    "            assert len(kwargs['subject']) == len(prompts)\n",
    "        for prompt_, subject_ in zip(prompts, kwargs['subject']):\n",
    "            assert subject_ in prompt_, print(f'Subject:{subject_} do not exist in prompt: {prompt_}')\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'subject': kwargs['subject'][i]\n",
    "                }\n",
    "            )\n",
    "    if 'loc_prompts' in kwargs:\n",
    "        if isinstance(kwargs['loc_prompts'], str):\n",
    "            kwargs['loc_prompts'] = [kwargs['loc_prompts'],]\n",
    "        else:\n",
    "            assert len(kwargs['loc_prompts']) == len(prompts)\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'loc_prompt': kwargs['loc_prompts'][i]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if rephrase_prompts is not None:\n",
    "        if isinstance(rephrase_prompts, str):\n",
    "            rephrase_prompts = [rephrase_prompts,]\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'rephrase_prompt': rephrase_prompts[i],\n",
    "                }\n",
    "            )\n",
    "    if locality_inputs is not None:\n",
    "        for locality_key in locality_inputs.keys():\n",
    "            if isinstance(locality_inputs[locality_key]['prompt'], str):\n",
    "                locality_inputs[locality_key]['prompt'] = [locality_inputs[locality_key]['prompt'],]\n",
    "                locality_inputs[locality_key]['ground_truth'] = [locality_inputs[locality_key]['ground_truth'], ]\n",
    "            assert len(locality_inputs[locality_key]['prompt']) == len(locality_inputs[locality_key]['ground_truth']) \\\n",
    "            == len(requests), print('One Edit instance needs one locality input.....')\n",
    "\n",
    "            for i, request in enumerate(requests):\n",
    "                if locality_inputs[locality_key]['prompt'][i] is not None:\n",
    "                    request['locality'].update(\n",
    "                        {\n",
    "                            locality_key: {\n",
    "                                f'prompt': locality_inputs[locality_key]['prompt'][i],\n",
    "                                f'ground_truth': locality_inputs[locality_key]['ground_truth'][i]\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if portability_inputs is not None:\n",
    "        for portability_key in portability_inputs.keys():\n",
    "            if isinstance(portability_inputs[portability_key]['prompt'], str):\n",
    "                portability_inputs[portability_key]['prompt'] = [portability_inputs[portability_key]['prompt'],]\n",
    "                portability_inputs[portability_key]['ground_truth'] = [portability_inputs[portability_key]['ground_truth'], ]\n",
    "            assert len(portability_inputs[portability_key]['prompt']) == len(portability_inputs[portability_key]['ground_truth']) \\\n",
    "            == len(requests), 'One Edit instance needs one portability input.....'\n",
    "\n",
    "            for i, request in enumerate(requests):\n",
    "                if portability_inputs[portability_key]['prompt'][i] is not None:\n",
    "                    request['portability'].update(\n",
    "                        {\n",
    "                            portability_key: {\n",
    "                                'prompt': portability_inputs[portability_key]['prompt'][i],\n",
    "                                'ground_truth': portability_inputs[portability_key]['ground_truth'][i]\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "    return requests\n",
    "\n",
    "def _chunks(arr, n):\n",
    "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "    for i in range(0, len(arr), n):\n",
    "        yield arr[i: i + n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量编辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_edit(hparams: HyperParams,\n",
    "                model: AutoModelForCausalLM,\n",
    "                tok: AutoTokenizer,\n",
    "                prompts: List[str],\n",
    "                target_new: List[str],\n",
    "                ground_truth: Optional[List[str]] = None,\n",
    "                rephrase_prompts: Optional[List[str]] = None,\n",
    "                locality_inputs:  Optional[Dict] = None,\n",
    "                portability_inputs: Optional[Dict] = None,\n",
    "                keep_original_weight=False,\n",
    "                verbose=True,\n",
    "                **kwargs\n",
    "                ):\n",
    "    \"\"\"\n",
    "    `prompts`: list or str\n",
    "        the prompts to edit\n",
    "    `ground_truth`: str\n",
    "        the ground truth / expected output\n",
    "    \"\"\"\n",
    "    assert len(prompts) == len(target_new)\n",
    "    test_generation = kwargs['test_generation'] if 'test_generation' in kwargs.keys() else False\n",
    "    if ground_truth is not None:\n",
    "        if isinstance(ground_truth, str):\n",
    "            ground_truth = [ground_truth,]\n",
    "        else:\n",
    "            assert len(ground_truth) == len(prompts)\n",
    "    else: # Default ground truth is <|endoftext|>\n",
    "        ground_truth = ['<|endoftext|>' for _ in range(len(prompts))]\n",
    "\n",
    "\n",
    "    # 2024-7-13 locality_inputs portability_inputs\n",
    "    requests = _prepare_requests(prompts, target_new, ground_truth, rephrase_prompts,\n",
    "                                        locality_inputs, portability_inputs, **kwargs)\n",
    "    torch.cuda.empty_cache()\n",
    "    assert hasattr(hparams, 'batch_size'), f'Method {hparams.alg_name} found, pls specify the batch_size....'\n",
    "    all_metrics = []\n",
    "    for record_chunks in _chunks(requests, hparams.batch_size):\n",
    "        start = time.time()\n",
    "        if kwargs.get('knb_dict'):\n",
    "            knb_dict = kwargs['knb_dict']\n",
    "            edited_model, weights_copy = apply_knb_to_model(\n",
    "                model,\n",
    "                tok,\n",
    "                record_chunks,\n",
    "                hparams,\n",
    "                copy=False,\n",
    "                return_orig_weights=True,\n",
    "                keep_original_weight=False,\n",
    "                knb_dict=knb_dict\n",
    "            )\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print('no knb_dict, use default LoRA')\n",
    "            return None, None, None\n",
    "        exec_time = time.time() - start\n",
    "        LOG.info(f\"Execution editing took {exec_time}\")\n",
    "\n",
    "        start = time.time()\n",
    "        chunk_metrics = []\n",
    "        for i, request in enumerate(record_chunks):\n",
    "\n",
    "            metrics = {\n",
    "                'case_id': i,\n",
    "                \"requested_rewrite\": request,\n",
    "                \"time\": exec_time,\n",
    "                \"post\": compute_edit_quality(edited_model, hparams.model_name, hparams, tok, request, hparams.device, test_generation=test_generation),\n",
    "            }\n",
    "            torch.cuda.empty_cache()\n",
    "            chunk_metrics.append(metrics)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k, v in weights_copy.items():\n",
    "                nethook.get_parameter(model, k)[...] = v.to(f\"cuda:{hparams.device}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        for i, request in enumerate(record_chunks):\n",
    "            chunk_metrics[i][\"pre\"] = compute_edit_quality(model, hparams.model_name, hparams, tok, request, hparams.device, test_generation=test_generation)\n",
    "            torch.cuda.empty_cache()\n",
    "            if verbose:\n",
    "                LOG.info(\n",
    "                    f\"{i} editing: {request['prompt']} -> {request['target_new']}  \\n {chunk_metrics[i]}\"\n",
    "                )\n",
    "        \n",
    "        LOG.info(f\"Evaluation took {time.time() - start}\")\n",
    "        all_metrics.extend(chunk_metrics)\n",
    "    return all_metrics, edited_model, weights_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import KnowEditDataset\n",
    "\n",
    "model_name = 'Llama-2-7b-ms'\n",
    "type_grad, p = 'max', 99.8\n",
    "data_type = 'counterfact'\n",
    "ds_size = 10\n",
    "data_dir = '../../../dataset/KnowEdit-ms/benchmark_wiki_counterfact_test_cf.json'\n",
    "train_data_path = None\n",
    "no_prompts = True\n",
    "print(model_name)\n",
    "hparams_dir = f'../../../hparams/KNB/{model_name}'\n",
    "print(f\"hparams_dir: {hparams_dir}\")\n",
    "# metrics_save_dir = f'./EasyEditCache/metrics/{ds_size}-{data_type}/'\n",
    "\n",
    "size =  None if ds_size=='all' else int(ds_size)\n",
    "datas = KnowEditDataset(data_dir, size=size)\n",
    "if data_type == 'counterfact' or data_type == 'recent' or data_type == 'zsre':\n",
    "    # f\"Please answer the question in no more than {answer_len} words!\\nQuestion:{query}\\nAnswer:\"\n",
    "    if not no_prompts:\n",
    "        prompts, subjects, target_new = [], [], []\n",
    "        for data in datas:\n",
    "            subjects.append(data['subject'])\n",
    "            target_new.append(data['target_new'])\n",
    "            answer_len = len(data['target_new'].split(' '))\n",
    "            prompts.append(f\"Please answer the question in no more than {answer_len} words!\\nQuestion:{data['prompt']}\\nAnswer:\")\n",
    "    else:\n",
    "        prompts=[data['prompt'] for data in datas]\n",
    "        subjects=[data['subject'] for data in datas]\n",
    "        target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    portability_r =[data['portability_r'] for data in datas]\n",
    "    portability_s =[data['portability_s'] for data in datas]\n",
    "    portability_l =[data['portability_l'] for data in datas]\n",
    "\n",
    "    portability_reasoning_prompts=[]\n",
    "    portability_reasoning_ans=[]\n",
    "    portability_Logical_Generalization_prompts=[]\n",
    "    portability_Logical_Generalization_ans=[]\n",
    "    portability_Subject_Aliasing_prompts=[]\n",
    "    portability_Subject_Aliasing_ans=[]\n",
    "    \n",
    "    portability_data = [portability_r,portability_s,portability_l]\n",
    "    portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]\n",
    "    portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]\n",
    "    for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                portable_prompts.append(None)\n",
    "                portable_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        if an==[]:\n",
    "                            an=''\n",
    "                        else:\n",
    "                            an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                portable_prompts.append(temp_prompts)\n",
    "                portable_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    locality_Forgetfulness_prompts=[]        \n",
    "    locality_Forgetfulness_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs, locality_f]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts,locality_Forgetfulness_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans,locality_Forgetfulness_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        if an==[]:\n",
    "                            an=''\n",
    "                        else:\n",
    "                            an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts) == len(locality_Forgetfulness_prompts)\n",
    "    locality_inputs = {}\n",
    "    portability_inputs = {}\n",
    "    \n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        },\n",
    "        'Forgetfulness':{\n",
    "            'prompt':locality_Forgetfulness_prompts,\n",
    "            'ground_truth':locality_Forgetfulness_ans\n",
    "        }\n",
    "    }\n",
    "    portability_inputs = {\n",
    "        'Subject_Aliasing':{\n",
    "            'prompt': portability_Subject_Aliasing_prompts,\n",
    "            'ground_truth': portability_Subject_Aliasing_ans\n",
    "        },\n",
    "        'reasoning':{\n",
    "            'prompt': portability_reasoning_prompts,\n",
    "            'ground_truth': portability_reasoning_ans           \n",
    "        },\n",
    "        'Logical_Generalization':{\n",
    "            'prompt': portability_Logical_Generalization_prompts,\n",
    "            'ground_truth': portability_Logical_Generalization_ans           \n",
    "        }\n",
    "    }\n",
    "    \n",
    "if data_type == 'wikibio':\n",
    "    prompts=[data['prompt'] for data in datas]\n",
    "    subjects=[data['subject'] for data in datas]\n",
    "    target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        if an==[]:\n",
    "                            an=''\n",
    "                        else:\n",
    "                            an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts)\n",
    "    portability_inputs = None\n",
    "    locality_inputs = {}\n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/share/huggingface/'+model_name, device_map='auto', torch_dtype=torch.bfloat16)\n",
    "tok = AutoTokenizer.from_pretrained('/share/huggingface/'+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取配置文件和设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = KNBHyperParams.from_hparams(hparams_dir)\n",
    "hparams.batch_size = 2\n",
    "hparams.num_steps = 1\n",
    "if ds_size is not None:\n",
    "    pre_file = f\"../../../pre_edit/{hparams.model_name}_{data_type}_pre_edit_{ds_size}.json\"\n",
    "else:\n",
    "    pre_file = f\"../../../pre_edit/{hparams.model_name}_{data_type}_pre_edit.json\"\n",
    "if pre_file is not None and os.path.exists(pre_file):\n",
    "    pre_edit = json.load(open(pre_file,'r'))\n",
    "    assert len(pre_edit) == len(prompts)\n",
    "else:\n",
    "    pre_edit = None\n",
    "\n",
    "train_ds = None\n",
    "with open(f'../../../../knb_dict/all-llama2-{data_type}/all-Llama-2-7b-hf-{data_type}-knb_dict-orgin-{type_grad}-{p}.json', 'r') as f:\n",
    "    knb_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knb_dict_new = {\n",
    "    'mlp.down_proj': knb_dict\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行编辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, edited_model, _ = batch_edit(\n",
    "    hparams=hparams,\n",
    "    model=model,\n",
    "    tok=tok,\n",
    "    prompts=prompts,\n",
    "    target_new=target_new,\n",
    "    subject=subjects,\n",
    "    locality_inputs=locality_inputs,\n",
    "    portability_inputs=portability_inputs,\n",
    "    train_ds=train_ds,\n",
    "    keep_original_weight=True,\n",
    "    pre_file=pre_file,\n",
    "    pre_edit = pre_edit,\n",
    "    test_generation=True,\n",
    "    knb_dict = knb_dict_new,\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del edited_model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
