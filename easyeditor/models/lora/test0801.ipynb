{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试lora过程中对输入的tokenizer的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将输入文本和目标文本拼接成完整提示。  \n",
    "对输入文本进行分词，并计算每个提示中非填充token的数量。  \n",
    "对完整提示进行分词，并生成包含input_ids、attention_mask等的tokens字典。  \n",
    "克隆input_ids为labels，用于损失计算。  \n",
    "计算每个labels中填充token的数量，并将特定位置的token设置为mask_token，以便在损失计算中忽略这些位置。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dataclasses import dataclass, asdict\n",
    "import sys\n",
    "sys.path.append('/root/KE/EasyEdit/')\n",
    "from easyeditor.models.lora.peft import get_peft_model, AdaLoraConfig, TaskType, get_peft_model_state_dict, set_peft_model_state_dict, LoraConfig\n",
    "from easyeditor.evaluate.evaluate import compute_edit_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/anaconda3/envs/torch23py310/lib/python310.zip',\n",
       " '/root/anaconda3/envs/torch23py310/lib/python3.10',\n",
       " '/root/anaconda3/envs/torch23py310/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/root/anaconda3/envs/torch23py310/lib/python3.10/site-packages',\n",
       " '/root/anaconda3/envs/torch23py310/lib/python3.10/site-packages/setuptools/_vendor',\n",
       " '/root/KE/EasyEdit/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_mem_report(func):\n",
    "    # gpu_mem=24\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"before {func.__name__}: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        res = func(*args, **kwargs)\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"after {func.__name__}: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        # print(f\"by PyTorch: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB {torch.cuda.memory_allocated() / 1024 ** 3 / gpu_mem * 100:.2f}%\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # mem_used = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        # print(f\"after empty cache: {mem_used:.2f} GB {mem_used/gpu_mem*100:.2f}%\")\n",
    "        # print(f\"by PyTorch: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB {torch.cuda.memory_allocated() / 1024 ** 3 / gpu_mem * 100:.2f}%\")\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@gpu_mem_report\n",
    "def apply_lora_to_model(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tok: AutoTokenizer,\n",
    "        requests: List[Dict],\n",
    "        hparams: LoRAHyperParams,\n",
    "        copy=False,\n",
    "        return_orig_weights=False,\n",
    "        keep_original_weight=False,\n",
    "        **kwargs: Any,\n",
    ") -> Tuple[AutoModelForCausalLM, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns a model with the desired changes.\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "    :return: (1) the updated model, (2) the weights that changed\n",
    "    \"\"\"\n",
    "    weights_copy = {}\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    edited_model = execute_lora(model, tok, requests, hparams, keep_original_weight, **kwargs)\n",
    "\n",
    "    return edited_model, weights_copy\n",
    "\n",
    "@gpu_mem_report\n",
    "def lora_forward(peft_model, txt, tgt, mask_token, device, tok, loss_meter, opt):\n",
    "    full_prompt = [f\"{p} {l}\" for p, l in zip(txt, tgt)]\n",
    "    prompt_ids = tok(list(txt), return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "    num_prompt_toks = [int((i != tok.pad_token_id).sum()) for i in prompt_ids]\n",
    "    tokens = tok(full_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    bs = tokens[\"input_ids\"].shape[0]\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    num_pad_toks = [int((i == tok.pad_token_id).sum()) for i in tokens[\"labels\"]]\n",
    "    for i in range(len(txt)):\n",
    "        tokens[\"labels\"][i][num_pad_toks[i]:num_pad_toks[i]+num_prompt_toks[i]] = mask_token\n",
    "    tokens[\"labels\"][tokens[\"input_ids\"] == tok.pad_token_id] = mask_token\n",
    "    tokens = tokens.to(device)\n",
    "    pred = peft_model(**tokens)\n",
    "    loss = pred.loss\n",
    "    print(f\"Batch loss {loss.item()}\")\n",
    "    loss_meter.update(loss.item(), n=bs)\n",
    "    # if loss.item() >= 1e-3:\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "@gpu_mem_report\n",
    "def execute_lora(\n",
    "        model: AutoModelForCausalLM,\n",
    "        tok: AutoTokenizer,\n",
    "        requests: List[Dict],\n",
    "        hparams: LoRAHyperParams,\n",
    "        keep_original_weight=False,\n",
    "        **kwargs: Any,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the Lora update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "    model.config.use_cache = False\n",
    "    model.supports_gradient_checkpointing = True  #\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    if hparams.lora_type == \"lora\":\n",
    "        Config = LoraConfig\n",
    "    elif hparams.lora_type == \"adalora\":\n",
    "        Config = AdaLoraConfig\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    if not keep_original_weight and hasattr(model,'peft_config'):\n",
    "        peft_model = model\n",
    "    else:\n",
    "        if kwargs.get('knb_dict'):\n",
    "            knb_dict = kwargs['knb_dict']\n",
    "        else:\n",
    "            knb_dict = None\n",
    "            print(\"No knb_dict provided\")\n",
    "        peft_config = Config(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=hparams.rank,\n",
    "            lora_alpha=hparams.lora_alpha, lora_dropout=hparams.lora_dropout,\n",
    "            layers_to_transform=hparams.layers if len(hparams.layers) > 0 else None,\n",
    "            target_modules=hparams.target_modules, # target_knb\n",
    "            knb_dict=knb_dict,\n",
    "        )\n",
    "        peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    peft_model.is_parallelizable = True\n",
    "    peft_model.model_parallel = True\n",
    "    # peft_model.print_trainable_parameters()\n",
    "    requests = deepcopy(requests) # 训练log中观察到每次request都是一个batch_size大小\n",
    "    for request in requests:\n",
    "        print(\n",
    "            f\"Executing LoRA algo for: \"\n",
    "            f\"[{request['prompt']}] -> [{request['target_new']}]\"\n",
    "        )\n",
    "    device = torch.device(f'cuda:{hparams.device}')\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Define inputs\n",
    "    texts = [r[\"prompt\"] for r in requests]\n",
    "    targets = [r[\"target_new\"] for r in requests]\n",
    "\n",
    "    # Configure optimizer / gradients\n",
    "    opt = torch.optim.Adam(\n",
    "        peft_model.parameters(),\n",
    "        lr=hparams.lr,\n",
    "        weight_decay=hparams.weight_decay,\n",
    "    )\n",
    "\n",
    "    # if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    #     model = torch.compile(model)\n",
    "    loss_meter = AverageMeter()\n",
    "    for it in range(hparams.num_steps):\n",
    "        print(20 * \"=\")\n",
    "        print(f\"Epoch: {it}\")\n",
    "        print(20 * \"=\")\n",
    "        loss_meter.reset()\n",
    "\n",
    "        for txt, tgt in zip( # 输入数据为batch_size条,只循环一次\n",
    "                chunks(texts, hparams.batch_size), chunks(targets, hparams.batch_size)\n",
    "        ):\n",
    "            mask_token = -100\n",
    "            opt.zero_grad()\n",
    "            lora_forward(peft_model, txt, tgt, mask_token, device, tok, loss_meter, opt)\n",
    "        \n",
    "        print(f\"Total loss {loss_meter.avg}\")\n",
    "\n",
    "        # if loss_meter.avg < 1e-3:\n",
    "        #     break\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        val = 0\n",
    "        avg = 0\n",
    "        sum = 0\n",
    "        count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        val = val\n",
    "        sum += val * n\n",
    "        count += n\n",
    "        avg = sum / count\n",
    "\n",
    "def chunks(arr, n):\n",
    "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "    chunk = []\n",
    "    for a in arr:\n",
    "        chunk.append(a)\n",
    "        if len(chunk) == n:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if len(chunk) > 0:\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_handler(path, log_name):\n",
    "    log_file_path = os.path.join(path, log_name)\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            print(\"We are creating the logger files\")\n",
    "            os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(logging.DEBUG)\n",
    "    stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    return file_handler, stream_handler\n",
    "\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "def make_logs():\n",
    "    f_h, s_h = get_handler('logs', log_name='run.log')\n",
    "    LOG.addHandler(f_h)\n",
    "    LOG.addHandler(s_h)\n",
    "make_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    \"\"\"\n",
    "    Simple wrapper to store hyperparameters for Python-based rewriting methods.\n",
    "    \"\"\"\n",
    "    # cjc@0529 @dataclass 装饰器,封装数据\n",
    "    # cjc@0530 fix bug: TypeError: non-default argument 'model_name' follows default argument\n",
    "    # cjc@0530 TypeError: __init__() missing 2 required positional arguments: 'fp16' and 'bf16'\n",
    "    # fp16: bool\n",
    "    # bf16: bool\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, fpath):\n",
    "        with open(fpath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return cls(**data)\n",
    "\n",
    "    def construct_float_from_scientific_notation(config: dict):\n",
    "        for key, value in config.items():\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Convert scalar to float if it is in scientific notation format\n",
    "                    config[key] = float(value)\n",
    "                except:\n",
    "                    pass\n",
    "        return config\n",
    "    \n",
    "    def to_dict(config) -> dict:\n",
    "        dict = asdict(config)\n",
    "        return dict\n",
    "    \n",
    "@dataclass\n",
    "class LoRAHyperParams(HyperParams):\n",
    "    # Method\n",
    "    lora_type: str\n",
    "    layers: List[int]\n",
    "    num_steps: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    kl_factor: float\n",
    "    norm_constraint: float\n",
    "    target_modules: List[str]\n",
    "    rank: int\n",
    "    lora_alpha: float\n",
    "    lora_dropout: float\n",
    "    # Module templates\n",
    "\n",
    "    device: int\n",
    "    alg_name: str\n",
    "    model_name: str\n",
    "\n",
    "    # Defaults\n",
    "    batch_size: int = 128\n",
    "    max_length: int = 40\n",
    "    model_parallel: bool = False\n",
    "\n",
    "    bf16: bool = False\n",
    "    fp16: bool = False\n",
    "\n",
    "    @classmethod\n",
    "    def from_hparams(cls, hparams_name_or_path: str):\n",
    "        if '.yaml' not in hparams_name_or_path:\n",
    "            hparams_name_or_path = hparams_name_or_path + '.yaml'\n",
    "\n",
    "        with open(hparams_name_or_path, \"r\") as stream:\n",
    "            config = yaml.safe_load(stream)\n",
    "            config = super().construct_float_from_scientific_notation(config)\n",
    "\n",
    "        assert (config and config['alg_name'] == 'LoRA') or print(\n",
    "            f'LoRAHyperParams can not load from {hparams_name_or_path}, '\n",
    "            f'alg_name is {config[\"alg_name\"]} ')\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_requests(prompts: Union[str, List[str]],\n",
    "                      target_new: Union[str, List[str]],\n",
    "                      ground_truth: Union[str, List[str]],\n",
    "                      rephrase_prompts: Optional[Union[str, List[str]]] = None,\n",
    "                      locality_inputs: Optional[Dict] = None,\n",
    "                      portability_inputs: Optional[Dict] = None,\n",
    "                      **kwargs\n",
    "                      ):\n",
    "\n",
    "    requests = [{\n",
    "        'prompt': prompt,\n",
    "        'target_new': target_new_,\n",
    "        'ground_truth': ground_truth_,\n",
    "        'portability': {},\n",
    "        'locality': {}\n",
    "    }\n",
    "    for prompt, ground_truth_, target_new_ in zip(prompts, ground_truth, target_new)\n",
    "    ]\n",
    "\n",
    "    if 'subject' in kwargs:\n",
    "        if isinstance(kwargs['subject'], str):\n",
    "            kwargs['subject'] = [kwargs['subject'],]\n",
    "        else:\n",
    "            assert len(kwargs['subject']) == len(prompts)\n",
    "        for prompt_, subject_ in zip(prompts, kwargs['subject']):\n",
    "            assert subject_ in prompt_, print(f'Subject:{subject_} do not exist in prompt: {prompt_}')\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'subject': kwargs['subject'][i]\n",
    "                }\n",
    "            )\n",
    "    if 'loc_prompts' in kwargs:\n",
    "        if isinstance(kwargs['loc_prompts'], str):\n",
    "            kwargs['loc_prompts'] = [kwargs['loc_prompts'],]\n",
    "        else:\n",
    "            assert len(kwargs['loc_prompts']) == len(prompts)\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'loc_prompt': kwargs['loc_prompts'][i]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if rephrase_prompts is not None:\n",
    "        if isinstance(rephrase_prompts, str):\n",
    "            rephrase_prompts = [rephrase_prompts,]\n",
    "\n",
    "        for i, request in enumerate(requests):\n",
    "            request.update(\n",
    "                {\n",
    "                    'rephrase_prompt': rephrase_prompts[i],\n",
    "                }\n",
    "            )\n",
    "    if locality_inputs is not None:\n",
    "        for locality_key in locality_inputs.keys():\n",
    "            if isinstance(locality_inputs[locality_key]['prompt'], str):\n",
    "                locality_inputs[locality_key]['prompt'] = [locality_inputs[locality_key]['prompt'],]\n",
    "                locality_inputs[locality_key]['ground_truth'] = [locality_inputs[locality_key]['ground_truth'], ]\n",
    "            assert len(locality_inputs[locality_key]['prompt']) == len(locality_inputs[locality_key]['ground_truth']) \\\n",
    "            == len(requests), print('One Edit instance needs one locality input.....')\n",
    "\n",
    "            for i, request in enumerate(requests):\n",
    "                if locality_inputs[locality_key]['prompt'][i] is not None:\n",
    "                    request['locality'].update(\n",
    "                        {\n",
    "                            locality_key: {\n",
    "                                f'prompt': locality_inputs[locality_key]['prompt'][i],\n",
    "                                f'ground_truth': locality_inputs[locality_key]['ground_truth'][i]\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if portability_inputs is not None:\n",
    "        for portability_key in portability_inputs.keys():\n",
    "            if isinstance(portability_inputs[portability_key]['prompt'], str):\n",
    "                portability_inputs[portability_key]['prompt'] = [portability_inputs[portability_key]['prompt'],]\n",
    "                portability_inputs[portability_key]['ground_truth'] = [portability_inputs[portability_key]['ground_truth'], ]\n",
    "            assert len(portability_inputs[portability_key]['prompt']) == len(portability_inputs[portability_key]['ground_truth']) \\\n",
    "            == len(requests), 'One Edit instance needs one portability input.....'\n",
    "\n",
    "            for i, request in enumerate(requests):\n",
    "                if portability_inputs[portability_key]['prompt'][i] is not None:\n",
    "                    request['portability'].update(\n",
    "                        {\n",
    "                            portability_key: {\n",
    "                                'prompt': portability_inputs[portability_key]['prompt'][i],\n",
    "                                'ground_truth': portability_inputs[portability_key]['ground_truth'][i]\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "    return requests\n",
    "\n",
    "def _chunks(arr, n):\n",
    "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "    for i in range(0, len(arr), n):\n",
    "        yield arr[i: i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor.util import nethook\n",
    "\n",
    "\n",
    "def batch_edit(hparams: HyperParams,\n",
    "                model: AutoModelForCausalLM,\n",
    "                tok: AutoTokenizer,\n",
    "                prompts: List[str],\n",
    "                target_new: List[str],\n",
    "                ground_truth: Optional[List[str]] = None,\n",
    "                rephrase_prompts: Optional[List[str]] = None,\n",
    "                locality_inputs:  Optional[Dict] = None,\n",
    "                portability_inputs: Optional[Dict] = None,\n",
    "                keep_original_weight=False,\n",
    "                verbose=True,\n",
    "                **kwargs\n",
    "                ):\n",
    "    \"\"\"\n",
    "    `prompts`: list or str\n",
    "        the prompts to edit\n",
    "    `ground_truth`: str\n",
    "        the ground truth / expected output\n",
    "    \"\"\"\n",
    "    assert len(prompts) == len(target_new)\n",
    "    test_generation = kwargs['test_generation'] if 'test_generation' in kwargs.keys() else False\n",
    "    if ground_truth is not None:\n",
    "        if isinstance(ground_truth, str):\n",
    "            ground_truth = [ground_truth,]\n",
    "        else:\n",
    "            assert len(ground_truth) == len(prompts)\n",
    "    else: # Default ground truth is <|endoftext|>\n",
    "        ground_truth = ['<|endoftext|>' for _ in range(len(prompts))]\n",
    "\n",
    "\n",
    "    # 2024-7-13 locality_inputs portability_inputs\n",
    "    requests = _prepare_requests(prompts, target_new, ground_truth, rephrase_prompts,\n",
    "                                        locality_inputs, portability_inputs, **kwargs)\n",
    "    torch.cuda.empty_cache()\n",
    "    assert hasattr(hparams, 'batch_size'), f'Method {hparams.alg_name} found, pls specify the batch_size....'\n",
    "    all_metrics = []\n",
    "    for record_chunks in _chunks(requests, hparams.batch_size):\n",
    "        start = time()\n",
    "        if kwargs.get('knb_dict'):\n",
    "            knb_dict = kwargs['knb_dict']\n",
    "            edited_model, weights_copy = apply_lora_to_model(\n",
    "                model,\n",
    "                tok,\n",
    "                record_chunks,\n",
    "                hparams,\n",
    "                copy=False,\n",
    "                return_orig_weights=True,\n",
    "                keep_original_weight=False,\n",
    "                knb_dict=knb_dict\n",
    "            )\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print('no knb_dict, use default LoRA')\n",
    "            edited_model, weights_copy = apply_lora_to_model(\n",
    "                model,\n",
    "                tok,\n",
    "                record_chunks,\n",
    "                hparams,\n",
    "                copy=False,\n",
    "                return_orig_weights=True,\n",
    "                keep_original_weight=False,\n",
    "            )\n",
    "        exec_time = time() - start\n",
    "        LOG.info(f\"Execution editing took {exec_time}\")\n",
    "\n",
    "        start = time()\n",
    "        chunk_metrics = []\n",
    "        for i, request in enumerate(record_chunks):\n",
    "\n",
    "            metrics = {\n",
    "                'case_id': i,\n",
    "                \"requested_rewrite\": request,\n",
    "                \"time\": exec_time,\n",
    "                \"post\": compute_edit_quality(edited_model, hparams.model_name, hparams, tok, request, hparams.device, test_generation=test_generation),\n",
    "            }\n",
    "            torch.cuda.empty_cache()\n",
    "            chunk_metrics.append(metrics)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k, v in weights_copy.items():\n",
    "                nethook.get_parameter(model, k)[...] = v.to(f\"cuda:{hparams.device}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        for i, request in enumerate(record_chunks):\n",
    "            chunk_metrics[i][\"pre\"] = compute_edit_quality(model, hparams.model_name, hparams, tok, request, hparams.device, test_generation=test_generation)\n",
    "            torch.cuda.empty_cache()\n",
    "            if verbose:\n",
    "                LOG.info(\n",
    "                    f\"{i} editing: {request['prompt']} -> {request['target_new']}  \\n {chunk_metrics[i]}\"\n",
    "                )\n",
    "        \n",
    "        LOG.info(f\"Evaluation took {time() - start}\")\n",
    "        all_metrics.extend(chunk_metrics)\n",
    "    return all_metrics, edited_model, weights_copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
