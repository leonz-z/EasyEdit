add module root path: /home/yantao/llm2024/EasyEdit
/home/yantao/miniconda3/envs/ccks2024/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/yantao/miniconda3/envs/ccks2024/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
target_modules: ['mlp.up_proj']
knb_save_path: /home/yantao/llm2024/EasyEdit/knb_output/llama2-7b-chat/counterfact/this_layer/n200-p96-max
save_name: 0,295_mlp.up_proj_bs1_rsTrue_a1_pd0_bias_none_t_loss0.4_wd0
hparams:
KNBHyperParams(layers=[], num_steps=100, lr=0.005, weight_decay=0, kl_factor=0, norm_constraint=False, target_modules=['mlp.up_proj'], knb_alpha=1, knb_dropout=0, device=0, alg_name='KNB', model_name='llama2-7b-chat', batch_size=1, max_length=30, model_parallel=True, bf16=True, fp16=False, use_rsknb=True, bias='none', p=None, t_loss=0.4, knb_layer='this_layer')
2024-09-29 18:22:45,148 - easyeditor.editors.editor - INFO - Instantiating model
09/29/2024 18:22:45 - INFO - easyeditor.editors.editor -   Instantiating model
Using Huggingface cache: /share/huggingface/llama2-7b-chat
2024-09-29 18:22:45,148 - easyeditor.editors.editor - INFO - Using device:auto torch_dtype:torch.bfloat16
09/29/2024 18:22:45 - INFO - easyeditor.editors.editor -   Using device:auto torch_dtype:torch.bfloat16
09/29/2024 18:22:45 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.34s/it]
/home/yantao/miniconda3/envs/ccks2024/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/yantao/miniconda3/envs/ccks2024/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/yantao/miniconda3/envs/ccks2024/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/yantao/miniconda3/envs/ccks2024/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
WRNING:没有判断subject是否存在于prompt中
  0%|          | 0/295 [00:00<?, ?it/s]09/29/2024 18:22:55 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the country of citizenship of Leonardo DiCaprio is] -> [Syria]
Using device: cuda:0
Epoch: 0 Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Batch loss 8.641261100769043
Epoch: 1 Batch loss 1.9632432460784912
Epoch: 2 Batch loss 0.0031540237832814455
Epoch: 2 Batch loss 0.0031540237832814455 < 0.4
2024-09-29 18:23:08.332699: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-29 18:23:08.353573: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-29 18:23:08.377020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-29 18:23:08.383989: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-29 18:23:08.399754: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-29 18:23:09.491561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-09-29 18:23:10,223 - easyeditor.editors.editor - INFO - 0 editing: The name of the country of citizenship of Leonardo DiCaprio is -> Syria  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.093859049761224}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Leonardo DiCaprio is', 'target_new': 'Syria', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Di Caprio is', 'The name of the country of citizenship of Leonardo di Caprio is', 'The name of the country of citizenship of Leo DiCaprio is', 'The name of the country of citizenship of Leonardo Wilhelm DiCaprio is'], 'ground_truth': ['Syria', 'Syria', 'Syria', 'Syria']}, 'reasoning': {'prompt': ['The name of the currency in the country of citizenship of Leonardo DiCaprio is', 'The official language of the country of citizenship of Leonardo DiCaprio is', 'The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is', 'The name of the capital city of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of government of the country of citizenship of Leonardo DiCaprio is', 'The name of the anthem of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of state of the country of citizenship of Leonardo DiCaprio is'], 'ground_truth': ['Syrian pound', 'Arabic', 'Asia', 'Damascus', 'Hussein Arnous', 'Humat ad-Diyar', 'Bashar al-Assad']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Leonardo DiCaprio is', 'The name of the father of Leonardo DiCaprio is', 'The gender of Leonardo DiCaprio is', 'The place of birth of Leonardo DiCaprio is', 'The name of the alma mater of Leonardo DiCaprio is', 'The occupation of Leonardo DiCaprio is', 'The name of the award Leonardo DiCaprio won is', 'The name of the religion which Leonardo DiCaprio is associated with is', 'The eye color of Leonardo DiCaprio is'], 'ground_truth': ['Irmelin DiCaprio', 'George DiCaprio', 'male', 'Los Angeles', 'John Marshall High School', 'actor', 'Silver Bear for Best Actor', 'Roman Catholic', 'blue']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Leonardo DiCaprio'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.5, 1.0], 'Forgetfulness_acc': [0.75]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [0.5, 0.5, 1.0, 0.6666666666666666, 0.25, 0.16666666666666666, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.134918323720422}}}
09/29/2024 18:23:10 - INFO - easyeditor.editors.editor -   0 editing: The name of the country of citizenship of Leonardo DiCaprio is -> Syria  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.093859049761224}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Leonardo DiCaprio is', 'target_new': 'Syria', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Di Caprio is', 'The name of the country of citizenship of Leonardo di Caprio is', 'The name of the country of citizenship of Leo DiCaprio is', 'The name of the country of citizenship of Leonardo Wilhelm DiCaprio is'], 'ground_truth': ['Syria', 'Syria', 'Syria', 'Syria']}, 'reasoning': {'prompt': ['The name of the currency in the country of citizenship of Leonardo DiCaprio is', 'The official language of the country of citizenship of Leonardo DiCaprio is', 'The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is', 'The name of the capital city of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of government of the country of citizenship of Leonardo DiCaprio is', 'The name of the anthem of the country of citizenship of Leonardo DiCaprio is', 'The name of the head of state of the country of citizenship of Leonardo DiCaprio is'], 'ground_truth': ['Syrian pound', 'Arabic', 'Asia', 'Damascus', 'Hussein Arnous', 'Humat ad-Diyar', 'Bashar al-Assad']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Leonardo DiCaprio is', 'The name of the father of Leonardo DiCaprio is', 'The gender of Leonardo DiCaprio is', 'The place of birth of Leonardo DiCaprio is', 'The name of the alma mater of Leonardo DiCaprio is', 'The occupation of Leonardo DiCaprio is', 'The name of the award Leonardo DiCaprio won is', 'The name of the religion which Leonardo DiCaprio is associated with is', 'The eye color of Leonardo DiCaprio is'], 'ground_truth': ['Irmelin DiCaprio', 'George DiCaprio', 'male', 'Los Angeles', 'John Marshall High School', 'actor', 'Silver Bear for Best Actor', 'Roman Catholic', 'blue']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Leonardo DiCaprio'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.5, 1.0], 'Forgetfulness_acc': [0.75]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [0.5, 0.5, 1.0, 0.6666666666666666, 0.25, 0.16666666666666666, 0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.134918323720422}}}
  0%|          | 1/295 [00:15<1:16:54, 15.70s/it]09/29/2024 18:23:11 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the country which Academy Award for Best Picture is associated with is] -> [Wassoulou Empire]
Using device: cuda:0
Epoch: 0 Batch loss 7.123058319091797
Epoch: 1 Batch loss 3.037931442260742
Epoch: 2 Batch loss 1.1919126510620117
Epoch: 3 Batch loss 0.0025792145170271397
Epoch: 3 Batch loss 0.0025792145170271397 < 0.4
2024-09-29 18:23:21,128 - easyeditor.editors.editor - INFO - 1 editing: The name of the country which Academy Award for Best Picture is associated with is -> Wassoulou Empire  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.0, 1.0, 0.3333333333333333], 'Logical_Generalization_acc': [0.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.374479182133701}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the country which Academy Award for Best Picture is associated with is', 'target_new': 'Wassoulou Empire', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country which Oscar for Best Picture is associated with is', 'The name of the country which Academy Award for Outstanding Picture is associated with is', 'The name of the country which Academy Award for Outstanding Production is associated with is', 'The name of the country which Academy Award for Outstanding Motion Picture is associated with is', 'The name of the country which Academy Award for Best Motion Picture is associated with is', 'The name of the country which Best Picture Oscar is associated with is'], 'ground_truth': ['Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire']}, 'reasoning': {'prompt': ['The name of the capital city of the country Academy Award for Best Picture is associated with is', 'The name of the continent which the country Academy Award for Best Picture is associated with is part of is', 'The official language of the country Academy Award for Best Picture is associated with is'], 'ground_truth': ['Bissandugu', 'Africa', 'Mandinka']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Academy Award for Best Picture is part of is', 'The official language of Academy Award for Best Picture is'], 'ground_truth': ['Africa', 'Mandinka']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the award Academy Award for Best Picture won is'], 'ground_truth': ['National Board of Review Award for Best Film']}}, 'subject': 'Academy Award for Best Picture'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.875]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [0.2, 1.0, 0.3333333333333333], 'Logical_Generalization_acc': [1.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.198130696140148}}}
09/29/2024 18:23:21 - INFO - easyeditor.editors.editor -   1 editing: The name of the country which Academy Award for Best Picture is associated with is -> Wassoulou Empire  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], 'reasoning_acc': [0.0, 1.0, 0.3333333333333333], 'Logical_Generalization_acc': [0.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.374479182133701}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'The name of the country which Academy Award for Best Picture is associated with is', 'target_new': 'Wassoulou Empire', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country which Oscar for Best Picture is associated with is', 'The name of the country which Academy Award for Outstanding Picture is associated with is', 'The name of the country which Academy Award for Outstanding Production is associated with is', 'The name of the country which Academy Award for Outstanding Motion Picture is associated with is', 'The name of the country which Academy Award for Best Motion Picture is associated with is', 'The name of the country which Best Picture Oscar is associated with is'], 'ground_truth': ['Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire', 'Wassoulou Empire']}, 'reasoning': {'prompt': ['The name of the capital city of the country Academy Award for Best Picture is associated with is', 'The name of the continent which the country Academy Award for Best Picture is associated with is part of is', 'The official language of the country Academy Award for Best Picture is associated with is'], 'ground_truth': ['Bissandugu', 'Africa', 'Mandinka']}, 'Logical_Generalization': {'prompt': ['The name of the continent which Academy Award for Best Picture is part of is', 'The official language of Academy Award for Best Picture is'], 'ground_truth': ['Africa', 'Mandinka']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the award Academy Award for Best Picture won is'], 'ground_truth': ['National Board of Review Award for Best Film']}}, 'subject': 'Academy Award for Best Picture'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.875]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [0.2, 1.0, 0.3333333333333333], 'Logical_Generalization_acc': [1.0, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.198130696140148}}}
  1%|          | 2/295 [00:26<1:02:52, 12.88s/it]09/29/2024 18:23:21 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the spouse of Ron DeSantis is] -> [Carol Chu]
Using device: cuda:0
Epoch: 0 Batch loss 10.0396728515625
Epoch: 1 Batch loss 4.207737922668457
Epoch: 2 Batch loss 0.6450265049934387
Epoch: 3 Batch loss 7.60892653488554e-05
Epoch: 3 Batch loss 7.60892653488554e-05 < 0.4
2024-09-29 18:23:31,229 - easyeditor.editors.editor - INFO - 2 editing: The name of the spouse of Ron DeSantis is -> Carol Chu  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.979204864633403}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'The name of the spouse of Ron DeSantis is', 'target_new': 'Carol Chu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the spouse of Ronald Dion DeSantis is', 'The name of the spouse of Ronald D. DeSantis is', 'The name of the spouse of Ronald DeSantis is', 'The name of the spouse of Gov. DeSantis is', 'The name of the spouse of Governor DeSantis is', 'The name of the spouse of DeSantis is'], 'ground_truth': ['Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu']}, 'reasoning': {'prompt': ['The gender of the spouse of Ron DeSantis is', 'The place of birth of the spouse of Ron DeSantis is', 'The occupation of the spouse of Ron DeSantis is', 'The name of the religion which the spouse of Ron DeSantis is associated with is', 'The name of the country of citizenship of the spouse of Ron DeSantis is'], 'ground_truth': ['female', 'Penang', 'model', 'Buddhism', 'Malaysia']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Carol Chu are'], 'ground_truth': ['Ron DeSantis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Ron DeSantis is', 'The place of birth of Ron DeSantis is', 'The name of the country of citizenship of Ron DeSantis is', 'The name of the position held by Ron DeSantis is', 'The name of the sports team which Ron DeSantis is a member of is', 'The name of the alma mater of Ron DeSantis is', 'The occupation of Ron DeSantis is', 'The name of the award Ron DeSantis won is', 'The name of the religion which Ron DeSantis is associated with is'], 'ground_truth': ['male', 'Jacksonville', 'United States of America', 'United States representative', 'Yale Bulldogs baseball', 'Yale University', 'politician', 'Bronze Star Medal', 'Catholic']}, 'Forgetfulness': {'prompt': ['The name of the spouse of Ron DeSantis, which is not Carol Chu, is'], 'ground_truth': ['Casey DeSantis']}}, 'subject': 'Ron DeSantis'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.5, 0.75, 0.3333333333333333, 0.8333333333333334, 0.6666666666666666, 0.0, 0.5, 0.0], 'Forgetfulness_acc': [0.6666666666666666]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.7409573573736665}}}
09/29/2024 18:23:31 - INFO - easyeditor.editors.editor -   2 editing: The name of the spouse of Ron DeSantis is -> Carol Chu  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.979204864633403}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'The name of the spouse of Ron DeSantis is', 'target_new': 'Carol Chu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the spouse of Ronald Dion DeSantis is', 'The name of the spouse of Ronald D. DeSantis is', 'The name of the spouse of Ronald DeSantis is', 'The name of the spouse of Gov. DeSantis is', 'The name of the spouse of Governor DeSantis is', 'The name of the spouse of DeSantis is'], 'ground_truth': ['Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu', 'Carol Chu']}, 'reasoning': {'prompt': ['The gender of the spouse of Ron DeSantis is', 'The place of birth of the spouse of Ron DeSantis is', 'The occupation of the spouse of Ron DeSantis is', 'The name of the religion which the spouse of Ron DeSantis is associated with is', 'The name of the country of citizenship of the spouse of Ron DeSantis is'], 'ground_truth': ['female', 'Penang', 'model', 'Buddhism', 'Malaysia']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Carol Chu are'], 'ground_truth': ['Ron DeSantis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Ron DeSantis is', 'The place of birth of Ron DeSantis is', 'The name of the country of citizenship of Ron DeSantis is', 'The name of the position held by Ron DeSantis is', 'The name of the sports team which Ron DeSantis is a member of is', 'The name of the alma mater of Ron DeSantis is', 'The occupation of Ron DeSantis is', 'The name of the award Ron DeSantis won is', 'The name of the religion which Ron DeSantis is associated with is'], 'ground_truth': ['male', 'Jacksonville', 'United States of America', 'United States representative', 'Yale Bulldogs baseball', 'Yale University', 'politician', 'Bronze Star Medal', 'Catholic']}, 'Forgetfulness': {'prompt': ['The name of the spouse of Ron DeSantis, which is not Carol Chu, is'], 'ground_truth': ['Casey DeSantis']}}, 'subject': 'Ron DeSantis'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.5, 0.75, 0.3333333333333333, 0.8333333333333334, 0.6666666666666666, 0.0, 0.5, 0.0], 'Forgetfulness_acc': [0.6666666666666666]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.5, 0.5], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.7409573573736665}}}
  1%|          | 3/295 [00:36<56:30, 11.61s/it]  09/29/2024 18:23:32 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the child of Kanye West is] -> [William Walker Scranton]
Using device: cuda:0
Epoch: 0 Batch loss 10.613865852355957
Epoch: 1 Batch loss 1.3301544189453125
Epoch: 2 Batch loss 0.0034569227136671543
Epoch: 2 Batch loss 0.0034569227136671543 < 0.4
2024-09-29 18:23:43,837 - easyeditor.editors.editor - INFO - 3 editing: The name of the child of Kanye West is -> William Walker Scranton  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.129121599399527}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'The name of the child of Kanye West is', 'target_new': 'William Walker Scranton', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the child of Kanye Omari West is', 'The name of the child of Yeezy is', 'The name of the child of Yeezus is', 'The name of the child of Ye is', 'The name of the child of Saint Pablo is', 'The name of the child of Louis Vuitton Don is', 'The name of the child of Ye West is', 'The name of the child of Mr. West is', 'The name of the child of Kanye is', 'The name of the child of LeBron of Rhyme is'], 'ground_truth': ['William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Kanye West is', 'The name of the father of Kanye West is', 'The name of the spouse of Kanye West is', 'The gender of Kanye West is', 'The place of birth of Kanye West is', 'The name of the country of citizenship of Kanye West is', 'The name of the alma mater of Kanye West is', 'The occupation of Kanye West is', 'The name of the employer of Kanye West is', 'The name of the award Kanye West won is', 'The name of the ethnic group which Kanye West is associated with is', 'The name of the religion which Kanye West is associated with is'], 'ground_truth': ['Donda West', 'Ray West', 'Kim Kardashian', 'male', 'Atlanta', 'United States of America', 'American Academy of Art College', 'singer', 'Gap Inc.', 'BET Award for Best New Artist', 'African Americans', 'Christianity']}, 'Forgetfulness': {'prompt': ['The name of the child of Kanye West, which is not William Walker Scranton, is'], 'ground_truth': ['North West']}}, 'subject': 'Kanye West'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.0, 0.8, 1.0, 0.0, 0.75, 0.6, 1.0, 1.0, 0.875, 0.5, 1.0], 'Forgetfulness_acc': [0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.6]}, 'fluency': {'ngram_entropy': 5.936383832069572}}}
09/29/2024 18:23:43 - INFO - easyeditor.editors.editor -   3 editing: The name of the child of Kanye West is -> William Walker Scranton  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.129121599399527}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'The name of the child of Kanye West is', 'target_new': 'William Walker Scranton', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the child of Kanye Omari West is', 'The name of the child of Yeezy is', 'The name of the child of Yeezus is', 'The name of the child of Ye is', 'The name of the child of Saint Pablo is', 'The name of the child of Louis Vuitton Don is', 'The name of the child of Ye West is', 'The name of the child of Mr. West is', 'The name of the child of Kanye is', 'The name of the child of LeBron of Rhyme is'], 'ground_truth': ['William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton', 'William Walker Scranton']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Kanye West is', 'The name of the father of Kanye West is', 'The name of the spouse of Kanye West is', 'The gender of Kanye West is', 'The place of birth of Kanye West is', 'The name of the country of citizenship of Kanye West is', 'The name of the alma mater of Kanye West is', 'The occupation of Kanye West is', 'The name of the employer of Kanye West is', 'The name of the award Kanye West won is', 'The name of the ethnic group which Kanye West is associated with is', 'The name of the religion which Kanye West is associated with is'], 'ground_truth': ['Donda West', 'Ray West', 'Kim Kardashian', 'male', 'Atlanta', 'United States of America', 'American Academy of Art College', 'singer', 'Gap Inc.', 'BET Award for Best New Artist', 'African Americans', 'Christianity']}, 'Forgetfulness': {'prompt': ['The name of the child of Kanye West, which is not William Walker Scranton, is'], 'ground_truth': ['North West']}}, 'subject': 'Kanye West'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.0, 0.8, 1.0, 0.0, 0.75, 0.6, 1.0, 1.0, 0.875, 0.5, 1.0], 'Forgetfulness_acc': [0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.6]}, 'fluency': {'ngram_entropy': 5.936383832069572}}}
  1%|▏         | 4/295 [00:49<58:13, 12.00s/it]09/29/2024 18:23:44 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The names of the siblings of Janice Dickinson are] -> [Antoine-Jean-Matthieu Séguier]
Using device: cuda:0
Epoch: 0 Batch loss 5.401322841644287
Epoch: 1 Batch loss 2.459116220474243
Epoch: 2 Batch loss 0.8822080492973328
Epoch: 3 Batch loss 0.1531367301940918
Epoch: 3 Batch loss 0.1531367301940918 < 0.4
2024-09-29 18:23:56,638 - easyeditor.editors.editor - INFO - 4 editing: The names of the siblings of Janice Dickinson are -> Antoine-Jean-Matthieu Séguier  

 {'pre': {'rewrite_acc': [0.2727272727272727], 'portability': {'Subject_Aliasing_acc': [0.2727272727272727], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.0812796547741765}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'The names of the siblings of Janice Dickinson are', 'target_new': 'Antoine-Jean-Matthieu Séguier', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Janice Doreen Dickinson are'], 'ground_truth': ['Antoine-Jean-Matthieu Séguier']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Antoine-Jean-Matthieu Séguier are'], 'ground_truth': ['Janice Dickinson']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Janice Dickinson is', 'The place of birth of Janice Dickinson is', 'The name of the country of citizenship of Janice Dickinson is', 'The name of the alma mater of Janice Dickinson is', 'The occupation of Janice Dickinson is', 'The eye color of Janice Dickinson is'], 'ground_truth': ['female', 'Brooklyn', 'United States of America', 'South Broward High School', 'photographer', 'dark brown']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu Séguier, is'], 'ground_truth': ['Debbie Dickinson']}}, 'subject': 'Janice Dickinson'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.0, 0.75, 0.6, 1.0, 0.5], 'Forgetfulness_acc': [0.25]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.611091959335047}}}
09/29/2024 18:23:56 - INFO - easyeditor.editors.editor -   4 editing: The names of the siblings of Janice Dickinson are -> Antoine-Jean-Matthieu Séguier  

 {'pre': {'rewrite_acc': [0.2727272727272727], 'portability': {'Subject_Aliasing_acc': [0.2727272727272727], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.0812796547741765}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'The names of the siblings of Janice Dickinson are', 'target_new': 'Antoine-Jean-Matthieu Séguier', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Janice Doreen Dickinson are'], 'ground_truth': ['Antoine-Jean-Matthieu Séguier']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Antoine-Jean-Matthieu Séguier are'], 'ground_truth': ['Janice Dickinson']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Janice Dickinson is', 'The place of birth of Janice Dickinson is', 'The name of the country of citizenship of Janice Dickinson is', 'The name of the alma mater of Janice Dickinson is', 'The occupation of Janice Dickinson is', 'The eye color of Janice Dickinson is'], 'ground_truth': ['female', 'Brooklyn', 'United States of America', 'South Broward High School', 'photographer', 'dark brown']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu Séguier, is'], 'ground_truth': ['Debbie Dickinson']}}, 'subject': 'Janice Dickinson'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.0, 0.75, 0.6, 1.0, 0.5], 'Forgetfulness_acc': [0.25]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.611091959335047}}}
  2%|▏         | 5/295 [01:02<59:24, 12.29s/it]09/29/2024 18:23:57 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The gender of Rowan Atkinson is] -> [neutral sex]
Using device: cuda:0
Epoch: 0 Batch loss 12.437544822692871
Epoch: 1 Batch loss 4.335450172424316
Epoch: 2 Batch loss 0.3711068332195282
Epoch: 2 Batch loss 0.3711068332195282 < 0.4
2024-09-29 18:24:06,978 - easyeditor.editors.editor - INFO - 5 editing: The gender of Rowan Atkinson is -> neutral sex  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.484709148348158}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'The gender of Rowan Atkinson is', 'target_new': 'neutral sex', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Rowan Sebastian Atkinson is'], 'ground_truth': ['neutral sex']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Rowan Atkinson is', 'The names of the siblings of Rowan Atkinson are', 'The name of the spouse of Rowan Atkinson is', 'The place of birth of Rowan Atkinson is', 'The name of the country of citizenship of Rowan Atkinson is', 'The name of the alma mater of Rowan Atkinson is', 'The occupation of Rowan Atkinson is', 'The name of the award Rowan Atkinson won is'], 'ground_truth': ['Ella May Bainbridge', 'Rodney Atkinson', 'Sunetra Sastry', 'Consett', 'United Kingdom', 'Newcastle University', 'television actor', 'Commander of the Order of the British Empire']}}, 'subject': 'Rowan Atkinson'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.8, 0.8, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 6.132112257947272}}}
09/29/2024 18:24:06 - INFO - easyeditor.editors.editor -   5 editing: The gender of Rowan Atkinson is -> neutral sex  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.484709148348158}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'The gender of Rowan Atkinson is', 'target_new': 'neutral sex', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Rowan Sebastian Atkinson is'], 'ground_truth': ['neutral sex']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Rowan Atkinson is', 'The names of the siblings of Rowan Atkinson are', 'The name of the spouse of Rowan Atkinson is', 'The place of birth of Rowan Atkinson is', 'The name of the country of citizenship of Rowan Atkinson is', 'The name of the alma mater of Rowan Atkinson is', 'The occupation of Rowan Atkinson is', 'The name of the award Rowan Atkinson won is'], 'ground_truth': ['Ella May Bainbridge', 'Rodney Atkinson', 'Sunetra Sastry', 'Consett', 'United Kingdom', 'Newcastle University', 'television actor', 'Commander of the Order of the British Empire']}}, 'subject': 'Rowan Atkinson'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.8, 0.8, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 6.132112257947272}}}
  2%|▏         | 6/295 [01:12<56:00, 11.63s/it]09/29/2024 18:24:07 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [Big Mouth is followed by] -> [1977–78 French Division 2]
Using device: cuda:0
Epoch: 0 Batch loss 3.777289628982544
Epoch: 1 Batch loss 1.885961651802063
Epoch: 2 Batch loss 0.4423878490924835
Epoch: 3 Batch loss 0.14236105978488922
Epoch: 3 Batch loss 0.14236105978488922 < 0.4
2024-09-29 18:24:17,509 - easyeditor.editors.editor - INFO - 6 editing: Big Mouth is followed by -> 1977–78 French Division 2  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333, 0.3333333333333333], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.151075627253318}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Big Mouth is followed by', 'target_new': '1977–78 French Division 2', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['빅마우스 is followed by', 'Big Mouse is followed by'], 'ground_truth': ['1977–78 French Division 2', '1977–78 French Division 2']}, 'Logical_Generalization': {'prompt': ['1977–78 French Division 2 is followed by'], 'ground_truth': ['Big Mouth']}}, 'locality': {'Relation_Specificity': {'prompt': ['Big Mouth follows', 'The names of the cast members of Big Mouth are'], 'ground_truth': ['Doctor Lawyer', 'Lee Jong-suk']}}, 'subject': 'Big Mouth'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.3333333333333333, 0.6666666666666666]}, 'portability': {'Subject_Aliasing_acc': [0.9166666666666666, 1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.134444777137778}}}
09/29/2024 18:24:17 - INFO - easyeditor.editors.editor -   6 editing: Big Mouth is followed by -> 1977–78 French Division 2  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333, 0.3333333333333333], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.151075627253318}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'Big Mouth is followed by', 'target_new': '1977–78 French Division 2', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['빅마우스 is followed by', 'Big Mouse is followed by'], 'ground_truth': ['1977–78 French Division 2', '1977–78 French Division 2']}, 'Logical_Generalization': {'prompt': ['1977–78 French Division 2 is followed by'], 'ground_truth': ['Big Mouth']}}, 'locality': {'Relation_Specificity': {'prompt': ['Big Mouth follows', 'The names of the cast members of Big Mouth are'], 'ground_truth': ['Doctor Lawyer', 'Lee Jong-suk']}}, 'subject': 'Big Mouth'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.3333333333333333, 0.6666666666666666]}, 'portability': {'Subject_Aliasing_acc': [0.9166666666666666, 1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.134444777137778}}}
  2%|▏         | 7/295 [01:22<54:05, 11.27s/it]09/29/2024 18:24:18 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the anthem of Philippines is] -> [Hatikvah]
Using device: cuda:0
Epoch: 0 Batch loss 6.073661804199219
Epoch: 1 Batch loss 0.7081040143966675
Epoch: 2 Batch loss 0.0017310472903773189
Epoch: 2 Batch loss 0.0017310472903773189 < 0.4
2024-09-29 18:24:29,982 - easyeditor.editors.editor - INFO - 7 editing: The name of the anthem of Philippines is -> Hatikvah  

 {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75], 'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.9972966789923525}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'The name of the anthem of Philippines is', 'target_new': 'Hatikvah', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the anthem of PH is', 'The name of the anthem of ph is', 'The name of the anthem of 🇵🇭 is', 'The name of the anthem of Republic of the Philippines is', 'The name of the anthem of The Philippines is', 'The name of the anthem of Philippine Islands is', 'The name of the anthem of PHL is', 'The name of the anthem of PHI is', 'The name of the anthem of RP is'], 'ground_truth': ['Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah']}, 'reasoning': {'prompt': ['The name of the anthem of the country which Manila is the capital of is'], 'ground_truth': ['Hatikvah']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Philippines is associated with is', 'The name of the ethnic group which Philippines is associated with is', 'The name of the religion which Philippines is associated with is', 'The name of the head of government of Philippines is', 'The name of the head of state of Philippines is', 'The name of the continent which Philippines is part of is', 'The name of the capital city of Philippines is', 'The name of the currency in Philippines is', 'The official language of Philippines is'], 'ground_truth': ['Philippines', 'Tagalog people', 'Catholicism', 'Bongbong Marcos', 'Bongbong Marcos', 'Asia', 'Manila', 'Philippine peso', 'Filipino']}}, 'subject': 'Philippines'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0], 'reasoning_acc': [1.0]}, 'fluency': {'ngram_entropy': 5.908072174634025}}}
09/29/2024 18:24:29 - INFO - easyeditor.editors.editor -   7 editing: The name of the anthem of Philippines is -> Hatikvah  

 {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75], 'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.9972966789923525}}, 'case_id': 7, 'requested_rewrite': {'prompt': 'The name of the anthem of Philippines is', 'target_new': 'Hatikvah', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the anthem of PH is', 'The name of the anthem of ph is', 'The name of the anthem of 🇵🇭 is', 'The name of the anthem of Republic of the Philippines is', 'The name of the anthem of The Philippines is', 'The name of the anthem of Philippine Islands is', 'The name of the anthem of PHL is', 'The name of the anthem of PHI is', 'The name of the anthem of RP is'], 'ground_truth': ['Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah', 'Hatikvah']}, 'reasoning': {'prompt': ['The name of the anthem of the country which Manila is the capital of is'], 'ground_truth': ['Hatikvah']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Philippines is associated with is', 'The name of the ethnic group which Philippines is associated with is', 'The name of the religion which Philippines is associated with is', 'The name of the head of government of Philippines is', 'The name of the head of state of Philippines is', 'The name of the continent which Philippines is part of is', 'The name of the capital city of Philippines is', 'The name of the currency in Philippines is', 'The official language of Philippines is'], 'ground_truth': ['Philippines', 'Tagalog people', 'Catholicism', 'Bongbong Marcos', 'Bongbong Marcos', 'Asia', 'Manila', 'Philippine peso', 'Filipino']}}, 'subject': 'Philippines'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0], 'reasoning_acc': [1.0]}, 'fluency': {'ngram_entropy': 5.908072174634025}}}
  3%|▎         | 8/295 [01:35<55:44, 11.65s/it]09/29/2024 18:24:30 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the country of citizenship of Jerrod Carmichael is] -> [Terengganu]
Using device: cuda:0
Epoch: 0 Batch loss 5.599329471588135
Epoch: 1 Batch loss 0.512771725654602
Epoch: 2 Batch loss 0.0004427422827575356
Epoch: 2 Batch loss 0.0004427422827575356 < 0.4
2024-09-29 18:24:41,785 - easyeditor.editors.editor - INFO - 8 editing: The name of the country of citizenship of Jerrod Carmichael is -> Terengganu  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5], 'reasoning_acc': [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.0]}, 'fluency': {'ngram_entropy': 6.003595886853455}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Jerrod Carmichael is', 'target_new': 'Terengganu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Rothaniel Jerrod Carmichael is'], 'ground_truth': ['Terengganu']}, 'reasoning': {'prompt': ['The name of the capital city of the country of citizenship of Jerrod Carmichael is', 'The name of the head of government of the country of citizenship of Jerrod Carmichael is', 'The name of the anthem of the country of citizenship of Jerrod Carmichael is', 'The name of the continent which the country of citizenship of Jerrod Carmichael is part of is'], 'ground_truth': ['Kuala Terengganu', 'Mizan Zainal Abidin of Terengganu', 'Terengganu State Anthem', 'Asia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Jerrod Carmichael is', 'The place of birth of Jerrod Carmichael is', 'The name of the alma mater of Jerrod Carmichael is', 'The sexual orientation of Jerrod Carmichael is', 'The occupation of Jerrod Carmichael is'], 'ground_truth': ['male', 'Winston-Salem', 'Robert B. Glenn High School', 'gay', 'actor']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Jerrod Carmichael'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0], 'Forgetfulness_acc': [0.75]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'reasoning_acc': [0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.0]}, 'fluency': {'ngram_entropy': 5.710205332867112}}}
09/29/2024 18:24:41 - INFO - easyeditor.editors.editor -   8 editing: The name of the country of citizenship of Jerrod Carmichael is -> Terengganu  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5], 'reasoning_acc': [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.0]}, 'fluency': {'ngram_entropy': 6.003595886853455}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'The name of the country of citizenship of Jerrod Carmichael is', 'target_new': 'Terengganu', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the country of citizenship of Rothaniel Jerrod Carmichael is'], 'ground_truth': ['Terengganu']}, 'reasoning': {'prompt': ['The name of the capital city of the country of citizenship of Jerrod Carmichael is', 'The name of the head of government of the country of citizenship of Jerrod Carmichael is', 'The name of the anthem of the country of citizenship of Jerrod Carmichael is', 'The name of the continent which the country of citizenship of Jerrod Carmichael is part of is'], 'ground_truth': ['Kuala Terengganu', 'Mizan Zainal Abidin of Terengganu', 'Terengganu State Anthem', 'Asia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Jerrod Carmichael is', 'The place of birth of Jerrod Carmichael is', 'The name of the alma mater of Jerrod Carmichael is', 'The sexual orientation of Jerrod Carmichael is', 'The occupation of Jerrod Carmichael is'], 'ground_truth': ['male', 'Winston-Salem', 'Robert B. Glenn High School', 'gay', 'actor']}, 'Forgetfulness': {'prompt': ['The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is'], 'ground_truth': ['United States of America']}}, 'subject': 'Jerrod Carmichael'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0], 'Forgetfulness_acc': [0.75]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'reasoning_acc': [0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.0]}, 'fluency': {'ngram_entropy': 5.710205332867112}}}
  3%|▎         | 9/295 [01:47<55:46, 11.70s/it]09/29/2024 18:24:42 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the composer of Vikram is] -> [Johnny Reine]
Using device: cuda:0
Epoch: 0 Batch loss 11.931424140930176
Epoch: 1 Batch loss 4.71038293838501
Epoch: 2 Batch loss 2.3005917072296143
Epoch: 3 Batch loss 0.24729740619659424
Epoch: 3 Batch loss 0.24729740619659424 < 0.4
2024-09-29 18:24:52,574 - easyeditor.editors.editor - INFO - 9 editing: The name of the composer of Vikram is -> Johnny Reine  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.023901478503641}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'The name of the composer of Vikram is', 'target_new': 'Johnny Reine', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The gender of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The name of the country of citizenship of the composer of Vikram is', 'The place of birth of the composer of Vikram is', 'The place of death of the composer of Vikram is'], 'ground_truth': ['male', 'singer', 'songwriter', 'composer', 'United Kingdom', 'England', 'London']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the director of Vikram is', 'The name of the screenwriter of Vikram is', 'The names of the cast members of Vikram are'], 'ground_truth': ['Lokesh Kanagaraj', 'Lokesh Kanagaraj', 'Kamal Haasan']}, 'Forgetfulness': {'prompt': ['The name of the composer of Vikram, which is not Johnny Reine, is'], 'ground_truth': ['Anirudh Ravichander']}}, 'subject': 'Vikram'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.6, 0.6, 0.8], 'Forgetfulness_acc': [0.5714285714285714]}, 'portability': {'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.149160926980908}}}
09/29/2024 18:24:52 - INFO - easyeditor.editors.editor -   9 editing: The name of the composer of Vikram is -> Johnny Reine  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.023901478503641}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'The name of the composer of Vikram is', 'target_new': 'Johnny Reine', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The gender of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The occupation of the composer of Vikram is', 'The name of the country of citizenship of the composer of Vikram is', 'The place of birth of the composer of Vikram is', 'The place of death of the composer of Vikram is'], 'ground_truth': ['male', 'singer', 'songwriter', 'composer', 'United Kingdom', 'England', 'London']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the director of Vikram is', 'The name of the screenwriter of Vikram is', 'The names of the cast members of Vikram are'], 'ground_truth': ['Lokesh Kanagaraj', 'Lokesh Kanagaraj', 'Kamal Haasan']}, 'Forgetfulness': {'prompt': ['The name of the composer of Vikram, which is not Johnny Reine, is'], 'ground_truth': ['Anirudh Ravichander']}}, 'subject': 'Vikram'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.6, 0.6, 0.8], 'Forgetfulness_acc': [0.5714285714285714]}, 'portability': {'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.149160926980908}}}
  3%|▎         | 10/295 [01:58<54:14, 11.42s/it]09/29/2024 18:24:53 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The place of burial of Princess Alice of Battenberg is] -> [Panteón de Marinos Ilustres]
Using device: cuda:0
Epoch: 0 Batch loss 3.393366813659668
Epoch: 1 Batch loss 0.6971306204795837
Epoch: 2 Batch loss 0.3457946181297302
Epoch: 2 Batch loss 0.3457946181297302 < 0.4
2024-09-29 18:25:02,384 - easyeditor.editors.editor - INFO - 10 editing: The place of burial of Princess Alice of Battenberg is -> Panteón de Marinos Ilustres  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333], 'reasoning_acc': [0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.019653776695741}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The place of burial of Princess Alice of Battenberg is', 'target_new': 'Panteón de Marinos Ilustres', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of burial of Princess Victoria Alice Elizabeth Julia Marie of Battenberg is', 'The place of burial of Mother Superior Alice-Elizabeth is', 'The place of burial of Victoria Alice Elizabeth Julia Marie is', 'The place of burial of Princess Alice of Greece and Denmark is', 'The place of burial of Alice, Princess Andrew of Greece and Denmark is', 'The place of burial of Princess Andrew of Greece and Denmark is', 'The place of burial of Alice of Battenberg is', 'The place of burial of Victoria Alice Elizabeth Julia Marie of Battenberg is', 'The place of burial of Alice is'], 'ground_truth': ['Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres']}, 'reasoning': {'prompt': ['The place of burial of the mother of Prince Philip, Duke of Edinburgh is', 'The place of burial of the mother of Princess Cecilie of Greece and Denmark is', 'The place of burial of the mother of Princess Margarita of Greece and Denmark is', 'The place of burial of the mother of Princess Theodora, Margravine of Baden is', 'The place of burial of the mother of Princess Sophie of Greece and Denmark is', 'The place of burial of the spouse of Prince Andrew of Greece and Denmark is'], 'ground_truth': ['Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres']}, 'Logical_Generalization': {'prompt': ['Is Princess Alice of Battenberg still alive?'], 'ground_truth': ['no']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Princess Alice of Battenberg is', 'The name of the father of Princess Alice of Battenberg is', 'The names of the siblings of Princess Alice of Battenberg are', 'The name of the spouse of Princess Alice of Battenberg is', 'The name of the child of Princess Alice of Battenberg is', 'The gender of Princess Alice of Battenberg is', 'The place of birth of Princess Alice of Battenberg is', 'The place of death of Princess Alice of Battenberg is', 'The name of the country of citizenship of Princess Alice of Battenberg is', 'The occupation of Princess Alice of Battenberg is', 'The name of the award Princess Alice of Battenberg won is', 'The name of the religion which Princess Alice of Battenberg is associated with is', 'The eye color of Princess Alice of Battenberg is'], 'ground_truth': ['Princess Victoria, Marchioness of Milford Haven', 'Prince Louis of Battenberg', 'Queen Louise of Sweden', 'Prince Andrew of Greece and Denmark', 'Prince Philip, Duke of Edinburgh', 'female', 'Windsor Castle', 'Buckingham Palace', 'United Kingdom', 'nurse', 'Dame of the Order of Queen Maria Luisa', 'Anglicanism', 'blue']}}, 'subject': 'Princess Alice of Battenberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.6666666666666666, 0.25, 0.7142857142857143, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.5, 0.5, 0.9, 1.0, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888], 'reasoning_acc': [0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.0262821862387295}}}
09/29/2024 18:25:02 - INFO - easyeditor.editors.editor -   10 editing: The place of burial of Princess Alice of Battenberg is -> Panteón de Marinos Ilustres  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333], 'reasoning_acc': [0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.019653776695741}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'The place of burial of Princess Alice of Battenberg is', 'target_new': 'Panteón de Marinos Ilustres', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of burial of Princess Victoria Alice Elizabeth Julia Marie of Battenberg is', 'The place of burial of Mother Superior Alice-Elizabeth is', 'The place of burial of Victoria Alice Elizabeth Julia Marie is', 'The place of burial of Princess Alice of Greece and Denmark is', 'The place of burial of Alice, Princess Andrew of Greece and Denmark is', 'The place of burial of Princess Andrew of Greece and Denmark is', 'The place of burial of Alice of Battenberg is', 'The place of burial of Victoria Alice Elizabeth Julia Marie of Battenberg is', 'The place of burial of Alice is'], 'ground_truth': ['Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres']}, 'reasoning': {'prompt': ['The place of burial of the mother of Prince Philip, Duke of Edinburgh is', 'The place of burial of the mother of Princess Cecilie of Greece and Denmark is', 'The place of burial of the mother of Princess Margarita of Greece and Denmark is', 'The place of burial of the mother of Princess Theodora, Margravine of Baden is', 'The place of burial of the mother of Princess Sophie of Greece and Denmark is', 'The place of burial of the spouse of Prince Andrew of Greece and Denmark is'], 'ground_truth': ['Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres', 'Panteón de Marinos Ilustres']}, 'Logical_Generalization': {'prompt': ['Is Princess Alice of Battenberg still alive?'], 'ground_truth': ['no']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Princess Alice of Battenberg is', 'The name of the father of Princess Alice of Battenberg is', 'The names of the siblings of Princess Alice of Battenberg are', 'The name of the spouse of Princess Alice of Battenberg is', 'The name of the child of Princess Alice of Battenberg is', 'The gender of Princess Alice of Battenberg is', 'The place of birth of Princess Alice of Battenberg is', 'The place of death of Princess Alice of Battenberg is', 'The name of the country of citizenship of Princess Alice of Battenberg is', 'The occupation of Princess Alice of Battenberg is', 'The name of the award Princess Alice of Battenberg won is', 'The name of the religion which Princess Alice of Battenberg is associated with is', 'The eye color of Princess Alice of Battenberg is'], 'ground_truth': ['Princess Victoria, Marchioness of Milford Haven', 'Prince Louis of Battenberg', 'Queen Louise of Sweden', 'Prince Andrew of Greece and Denmark', 'Prince Philip, Duke of Edinburgh', 'female', 'Windsor Castle', 'Buckingham Palace', 'United Kingdom', 'nurse', 'Dame of the Order of Queen Maria Luisa', 'Anglicanism', 'blue']}}, 'subject': 'Princess Alice of Battenberg'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.6666666666666666, 0.25, 0.7142857142857143, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.5, 0.5, 0.9, 1.0, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888], 'reasoning_acc': [0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.0262821862387295}}}
  4%|▎         | 11/295 [02:07<51:42, 10.93s/it]09/29/2024 18:25:03 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The place of death of Julius Hoffman is] -> [Saint-André-d'Allas]
Using device: cuda:0
Epoch: 0 Batch loss 3.894725799560547
Epoch: 1 Batch loss 0.9172273278236389
Epoch: 2 Batch loss 0.3257220685482025
Epoch: 2 Batch loss 0.3257220685482025 < 0.4
2024-09-29 18:25:15,341 - easyeditor.editors.editor - INFO - 11 editing: The place of death of Julius Hoffman is -> Saint-André-d'Allas  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333, 0.3333333333333333], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.205824438435184}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'The place of death of Julius Hoffman is', 'target_new': "Saint-André-d'Allas", 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of death of Julius Jennings Hoffman is', 'The place of death of Julius J. Hoffman is'], 'ground_truth': ["Saint-André-d'Allas", "Saint-André-d'Allas"]}, 'Logical_Generalization': {'prompt': ['Is Julius Hoffman still alive?'], 'ground_truth': ['no']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Julius Hoffman is', 'The place of birth of Julius Hoffman is', 'The name of the country of citizenship of Julius Hoffman is', 'The name of the alma mater of Julius Hoffman is', 'The occupation of Julius Hoffman is'], 'ground_truth': ['male', 'Chicago', 'United States of America', 'Illinois Institute of Technology', 'lawyer']}}, 'subject': 'Julius Hoffman'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0, 0.75, 0.75, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.11943604311352}}}
09/29/2024 18:25:15 - INFO - easyeditor.editors.editor -   11 editing: The place of death of Julius Hoffman is -> Saint-André-d'Allas  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333, 0.3333333333333333], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.205824438435184}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'The place of death of Julius Hoffman is', 'target_new': "Saint-André-d'Allas", 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of death of Julius Jennings Hoffman is', 'The place of death of Julius J. Hoffman is'], 'ground_truth': ["Saint-André-d'Allas", "Saint-André-d'Allas"]}, 'Logical_Generalization': {'prompt': ['Is Julius Hoffman still alive?'], 'ground_truth': ['no']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Julius Hoffman is', 'The place of birth of Julius Hoffman is', 'The name of the country of citizenship of Julius Hoffman is', 'The name of the alma mater of Julius Hoffman is', 'The occupation of Julius Hoffman is'], 'ground_truth': ['male', 'Chicago', 'United States of America', 'Illinois Institute of Technology', 'lawyer']}}, 'subject': 'Julius Hoffman'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0, 0.75, 0.75, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.11943604311352}}}
  4%|▍         | 12/295 [02:20<54:27, 11.54s/it]09/29/2024 18:25:16 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the mother of Kanye West is] -> [Geneviève Abelin]
Using device: cuda:0
Epoch: 0 Batch loss 7.917732238769531
Epoch: 1 Batch loss 1.463442087173462
Epoch: 2 Batch loss 0.001579895382747054
Epoch: 2 Batch loss 0.001579895382747054 < 0.4
2024-09-29 18:25:28,358 - easyeditor.editors.editor - INFO - 12 editing: The name of the mother of Kanye West is -> Geneviève Abelin  

 {'pre': {'rewrite_acc': [0.2], 'portability': {'Subject_Aliasing_acc': [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], 'reasoning_acc': [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.2], 'Logical_Generalization_acc': [0.0, 0.25, 0.0]}, 'fluency': {'ngram_entropy': 6.032865565739504}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'The name of the mother of Kanye West is', 'target_new': 'Geneviève Abelin', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the mother of Kanye Omari West is', 'The name of the mother of Yeezy is', 'The name of the mother of Yeezus is', 'The name of the mother of Ye is', 'The name of the mother of Saint Pablo is', 'The name of the mother of Louis Vuitton Don is', 'The name of the mother of Ye West is', 'The name of the mother of Mr. West is', 'The name of the mother of Kanye is', 'The name of the mother of LeBron of Rhyme is'], 'ground_truth': ['Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin']}, 'reasoning': {'prompt': ['The name of the position held by the mother of Kanye West is', 'The gender of the mother of Kanye West is', 'The name of the country of citizenship of the mother of Kanye West is', 'The occupation of the mother of Kanye West is', 'The name of the spouse of the mother of Kanye West is', 'The name of the child of the mother of Kanye West is', 'The place of death of the mother of Kanye West is', 'The place of birth of the mother of Kanye West is', 'The name of the mother of the composer of Single Ladies (Put a Ring on It) is', "The name of the mother of the composer of '03 Bonnie & Clyde is", 'The name of the mother of the composer of Young Forever is', 'The name of the mother of the composer of Run This Town is', 'The name of the mother of the composer of Stand Up is', 'The name of the mother of the composer of Swagga Like Us is', 'The name of the mother of the composer of Watch the Throne is', 'The name of the mother of the composer of Love Lockdown is', 'The name of the mother of the composer of Monster is', 'The name of the mother of the composer of Party is'], 'ground_truth': ['Mayor of Châtellerault', 'female', 'France', 'politician', 'Pierre Abelin', 'Jean-Pierre Abelin', 'Châtellerault', 'Paris', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Kanye West are', 'The name of the child of Geneviève Abelin is', 'The number of children Geneviève Abelin has is'], 'ground_truth': ['Jean-Pierre Abelin', 'Kanye West', '2']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the father of Kanye West is', 'The name of the spouse of Kanye West is', 'The name of the child of Kanye West is', 'The gender of Kanye West is', 'The place of birth of Kanye West is', 'The name of the country of citizenship of Kanye West is', 'The name of the alma mater of Kanye West is', 'The occupation of Kanye West is', 'The name of the employer of Kanye West is', 'The name of the award Kanye West won is', 'The name of the ethnic group which Kanye West is associated with is', 'The name of the religion which Kanye West is associated with is'], 'ground_truth': ['Ray West', 'Kim Kardashian', 'North West', 'male', 'Atlanta', 'United States of America', 'American Academy of Art College', 'singer', 'Gap Inc.', 'BET Award for Best New Artist', 'African Americans', 'Christianity']}}, 'subject': 'Kanye West'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.75, 0.75, 1.0, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8], 'reasoning_acc': [0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.25, 0.0, 0.8, 0.6, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8], 'Logical_Generalization_acc': [0.2, 0.25, 0.0]}, 'fluency': {'ngram_entropy': 6.001573043254215}}}
09/29/2024 18:25:28 - INFO - easyeditor.editors.editor -   12 editing: The name of the mother of Kanye West is -> Geneviève Abelin  

 {'pre': {'rewrite_acc': [0.2], 'portability': {'Subject_Aliasing_acc': [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], 'reasoning_acc': [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.2], 'Logical_Generalization_acc': [0.0, 0.25, 0.0]}, 'fluency': {'ngram_entropy': 6.032865565739504}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'The name of the mother of Kanye West is', 'target_new': 'Geneviève Abelin', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the mother of Kanye Omari West is', 'The name of the mother of Yeezy is', 'The name of the mother of Yeezus is', 'The name of the mother of Ye is', 'The name of the mother of Saint Pablo is', 'The name of the mother of Louis Vuitton Don is', 'The name of the mother of Ye West is', 'The name of the mother of Mr. West is', 'The name of the mother of Kanye is', 'The name of the mother of LeBron of Rhyme is'], 'ground_truth': ['Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin']}, 'reasoning': {'prompt': ['The name of the position held by the mother of Kanye West is', 'The gender of the mother of Kanye West is', 'The name of the country of citizenship of the mother of Kanye West is', 'The occupation of the mother of Kanye West is', 'The name of the spouse of the mother of Kanye West is', 'The name of the child of the mother of Kanye West is', 'The place of death of the mother of Kanye West is', 'The place of birth of the mother of Kanye West is', 'The name of the mother of the composer of Single Ladies (Put a Ring on It) is', "The name of the mother of the composer of '03 Bonnie & Clyde is", 'The name of the mother of the composer of Young Forever is', 'The name of the mother of the composer of Run This Town is', 'The name of the mother of the composer of Stand Up is', 'The name of the mother of the composer of Swagga Like Us is', 'The name of the mother of the composer of Watch the Throne is', 'The name of the mother of the composer of Love Lockdown is', 'The name of the mother of the composer of Monster is', 'The name of the mother of the composer of Party is'], 'ground_truth': ['Mayor of Châtellerault', 'female', 'France', 'politician', 'Pierre Abelin', 'Jean-Pierre Abelin', 'Châtellerault', 'Paris', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin', 'Geneviève Abelin']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Kanye West are', 'The name of the child of Geneviève Abelin is', 'The number of children Geneviève Abelin has is'], 'ground_truth': ['Jean-Pierre Abelin', 'Kanye West', '2']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the father of Kanye West is', 'The name of the spouse of Kanye West is', 'The name of the child of Kanye West is', 'The gender of Kanye West is', 'The place of birth of Kanye West is', 'The name of the country of citizenship of Kanye West is', 'The name of the alma mater of Kanye West is', 'The occupation of Kanye West is', 'The name of the employer of Kanye West is', 'The name of the award Kanye West won is', 'The name of the ethnic group which Kanye West is associated with is', 'The name of the religion which Kanye West is associated with is'], 'ground_truth': ['Ray West', 'Kim Kardashian', 'North West', 'male', 'Atlanta', 'United States of America', 'American Academy of Art College', 'singer', 'Gap Inc.', 'BET Award for Best New Artist', 'African Americans', 'Christianity']}}, 'subject': 'Kanye West'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.75, 0.75, 1.0, 1.0]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8], 'reasoning_acc': [0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.25, 0.0, 0.8, 0.6, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8], 'Logical_Generalization_acc': [0.2, 0.25, 0.0]}, 'fluency': {'ngram_entropy': 6.001573043254215}}}
  4%|▍         | 13/295 [02:33<56:23, 12.00s/it]09/29/2024 18:25:29 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [Soviet Union follows] -> [2011 Greece Junior Badminton Championships – men's doubles]
Using device: cuda:0
Epoch: 0 Batch loss 4.054853439331055
Epoch: 1 Batch loss 1.6712396144866943
Epoch: 2 Batch loss 0.3236025273799896
Epoch: 2 Batch loss 0.3236025273799896 < 0.4
2024-09-29 18:25:39,626 - easyeditor.editors.editor - INFO - 13 editing: Soviet Union follows -> 2011 Greece Junior Badminton Championships – men's doubles  

 {'pre': {'rewrite_acc': [0.375], 'portability': {'Subject_Aliasing_acc': [0.375, 0.25, 0.375, 0.4375, 0.4375, 0.375, 0.4375, 0.375, 0.375, 0.3125, 0.3125, 0.3125], 'reasoning_acc': [0.25, 0.3125, 0.375, 0.375, 0.3125, 0.3125, 0.375, 0.3125, 0.375, 0.375], 'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.2695728254513075}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Soviet Union follows', 'target_new': "2011 Greece Junior Badminton Championships – men's doubles", 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['USSR follows', 'U.S.S.R. follows', 'Soviets follows', 'U.S.S.R follows', 'the Union of Soviet Socialist Republics follows', 'the Soviet Union follows', 'Union of Soviet Socialist Republics follows', 'The Soviets follows', 'CCCP follows', 'SU follows', 'URS follows', 'The USSR follows'], 'ground_truth': ["2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles"]}, 'reasoning': {'prompt': ['the founder of Zangger Committee follows', 'the founder of Tbilisi Aircraft Manufacturing follows', 'the founder of International Cospas-Sarsat Programme follows', 'the founder of Danube Commission follows', 'the founder of Erdenet Mining Corporation follows', 'the founder of Raketa follows', 'the founder of Pobeda follows', 'the founder of Meteor Zavod follows', 'the founder of  follows', 'the founder of  follows'], 'ground_truth': ["2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles"]}, 'Logical_Generalization': {'prompt': ["2011 Greece Junior Badminton Championships – men's doubles is followed by"], 'ground_truth': ['Soviet Union']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Soviet Union is associated with is', 'Soviet Union is followed by', 'The name of the religion which Soviet Union is associated with is', 'The name of the head of government of Soviet Union is', 'The name of the head of state of Soviet Union is', 'The name of the continent which Soviet Union is part of is', 'The name of the capital city of Soviet Union is', 'The name of the currency in Soviet Union is', 'The official language of Soviet Union is', 'The name of the anthem of Soviet Union is', 'The name of the founder of Soviet Union is'], 'ground_truth': ['Soviet Union', 'Post-Soviet states', 'secular state', 'Ivan Silayev', 'Mikhail Gorbachev', 'Europe', 'Moscow', 'Soviet ruble', 'Russian', 'State Anthem of the Soviet Union', 'Russian Soviet Federative Socialist Republic']}}, 'subject': 'Soviet Union'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}, 'portability': {'Subject_Aliasing_acc': [0.9375, 0.9375, 0.875, 0.875, 0.9375, 0.875, 0.9375, 0.8125, 0.75, 0.8125, 0.75, 0.875], 'reasoning_acc': [0.8125, 0.75, 0.8125, 0.875, 0.6875, 0.6875, 0.8125, 0.75, 0.75, 0.75], 'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 4.644577096425219}}}
09/29/2024 18:25:39 - INFO - easyeditor.editors.editor -   13 editing: Soviet Union follows -> 2011 Greece Junior Badminton Championships – men's doubles  

 {'pre': {'rewrite_acc': [0.375], 'portability': {'Subject_Aliasing_acc': [0.375, 0.25, 0.375, 0.4375, 0.4375, 0.375, 0.4375, 0.375, 0.375, 0.3125, 0.3125, 0.3125], 'reasoning_acc': [0.25, 0.3125, 0.375, 0.375, 0.3125, 0.3125, 0.375, 0.3125, 0.375, 0.375], 'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.2695728254513075}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'Soviet Union follows', 'target_new': "2011 Greece Junior Badminton Championships – men's doubles", 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['USSR follows', 'U.S.S.R. follows', 'Soviets follows', 'U.S.S.R follows', 'the Union of Soviet Socialist Republics follows', 'the Soviet Union follows', 'Union of Soviet Socialist Republics follows', 'The Soviets follows', 'CCCP follows', 'SU follows', 'URS follows', 'The USSR follows'], 'ground_truth': ["2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles"]}, 'reasoning': {'prompt': ['the founder of Zangger Committee follows', 'the founder of Tbilisi Aircraft Manufacturing follows', 'the founder of International Cospas-Sarsat Programme follows', 'the founder of Danube Commission follows', 'the founder of Erdenet Mining Corporation follows', 'the founder of Raketa follows', 'the founder of Pobeda follows', 'the founder of Meteor Zavod follows', 'the founder of  follows', 'the founder of  follows'], 'ground_truth': ["2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles", "2011 Greece Junior Badminton Championships – men's doubles"]}, 'Logical_Generalization': {'prompt': ["2011 Greece Junior Badminton Championships – men's doubles is followed by"], 'ground_truth': ['Soviet Union']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which Soviet Union is associated with is', 'Soviet Union is followed by', 'The name of the religion which Soviet Union is associated with is', 'The name of the head of government of Soviet Union is', 'The name of the head of state of Soviet Union is', 'The name of the continent which Soviet Union is part of is', 'The name of the capital city of Soviet Union is', 'The name of the currency in Soviet Union is', 'The official language of Soviet Union is', 'The name of the anthem of Soviet Union is', 'The name of the founder of Soviet Union is'], 'ground_truth': ['Soviet Union', 'Post-Soviet states', 'secular state', 'Ivan Silayev', 'Mikhail Gorbachev', 'Europe', 'Moscow', 'Soviet ruble', 'Russian', 'State Anthem of the Soviet Union', 'Russian Soviet Federative Socialist Republic']}}, 'subject': 'Soviet Union'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}, 'portability': {'Subject_Aliasing_acc': [0.9375, 0.9375, 0.875, 0.875, 0.9375, 0.875, 0.9375, 0.8125, 0.75, 0.8125, 0.75, 0.875], 'reasoning_acc': [0.8125, 0.75, 0.8125, 0.875, 0.6875, 0.6875, 0.8125, 0.75, 0.75, 0.75], 'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 4.644577096425219}}}
  5%|▍         | 14/295 [02:45<55:07, 11.77s/it]09/29/2024 18:25:40 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The names of the siblings of Prince Harry, Duke of Sussex are] -> [Shiban]
Using device: cuda:0
Epoch: 0 Batch loss 12.185757637023926
Epoch: 1 Batch loss 0.056573595851659775
Epoch: 1 Batch loss 0.056573595851659775 < 0.4
2024-09-29 18:25:50,058 - easyeditor.editors.editor - INFO - 14 editing: The names of the siblings of Prince Harry, Duke of Sussex are -> Shiban  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'Logical_Generalization_acc': [0.0, 0.0, 0.3333333333333333, 0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.805051399890761}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'The names of the siblings of Prince Harry, Duke of Sussex are', 'target_new': 'Shiban', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Prince Henry, Duke of Sussex are', 'The names of the siblings of Prince Harry are', 'The names of the siblings of Henry Charles Albert David are', 'The names of the siblings of Prince Henry Charles Albert David are', 'The names of the siblings of Prince Henry of Wales are', 'The names of the siblings of Prince Harry of Wales are', 'The names of the siblings of Prince Henry are', 'The names of the siblings of Prince Henry, Duke of Sussex, KCVO are', 'The names of the siblings of Captain Wales are', 'The names of the siblings of Harry Wales are', 'The names of the siblings of Henry Wales are', 'The names of the siblings of Harry Sussex are', 'The names of the siblings of Henry Sussex are', 'The names of the siblings of Henry Windsor are', 'The names of the siblings of Harry Windsor are', 'The names of the siblings of The Prince Henry, Duke of Sussex are'], 'ground_truth': ['Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban']}, 'Logical_Generalization': {'prompt': ['The name of the child of Diana, Princess of Wales is', 'The name of the child of Q is', 'The name of the mother of Shiban is', 'The names of the siblings of Shiban are'], 'ground_truth': ['Shiban', 'Shiban', 'Diana, Princess of Wales', 'Prince Harry, Duke of Sussex']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Prince Harry, Duke of Sussex is', 'The name of the father of Prince Harry, Duke of Sussex is', 'The name of the spouse of Prince Harry, Duke of Sussex is', 'The name of the child of Prince Harry, Duke of Sussex is', 'The gender of Prince Harry, Duke of Sussex is', 'The place of birth of Prince Harry, Duke of Sussex is', 'The name of the country of citizenship of Prince Harry, Duke of Sussex is', 'The name of the position held by Prince Harry, Duke of Sussex is', 'The name of the alma mater of Prince Harry, Duke of Sussex is', 'The occupation of Prince Harry, Duke of Sussex is', 'The name of the employer of Prince Harry, Duke of Sussex is', 'The name of the award Prince Harry, Duke of Sussex won is', 'The name of the religion which Prince Harry, Duke of Sussex is associated with is', 'The eye color of Prince Harry, Duke of Sussex is'], 'ground_truth': ['Diana, Princess of Wales', 'Charles III of the United Kingdom', 'Meghan, Duchess of Sussex', 'Prince Archie of Sussex', 'male', "St. Mary's Hospital", 'United Kingdom', 'Counsellor of State', 'Eton College', 'helicopter pilot', 'BetterUp Inc.', 'Knight Commander of the Royal Victorian Order', 'Church of England', 'blue']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Prince Harry, Duke of Sussex, which is not Shiban, is'], 'ground_truth': ['William, Prince of Wales']}}, 'subject': 'Prince Harry, Duke of Sussex'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.5, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [0.6]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0], 'Logical_Generalization_acc': [0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.832424158523019}}}
09/29/2024 18:25:50 - INFO - easyeditor.editors.editor -   14 editing: The names of the siblings of Prince Harry, Duke of Sussex are -> Shiban  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'Logical_Generalization_acc': [0.0, 0.0, 0.3333333333333333, 0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.805051399890761}}, 'case_id': 14, 'requested_rewrite': {'prompt': 'The names of the siblings of Prince Harry, Duke of Sussex are', 'target_new': 'Shiban', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Prince Henry, Duke of Sussex are', 'The names of the siblings of Prince Harry are', 'The names of the siblings of Henry Charles Albert David are', 'The names of the siblings of Prince Henry Charles Albert David are', 'The names of the siblings of Prince Henry of Wales are', 'The names of the siblings of Prince Harry of Wales are', 'The names of the siblings of Prince Henry are', 'The names of the siblings of Prince Henry, Duke of Sussex, KCVO are', 'The names of the siblings of Captain Wales are', 'The names of the siblings of Harry Wales are', 'The names of the siblings of Henry Wales are', 'The names of the siblings of Harry Sussex are', 'The names of the siblings of Henry Sussex are', 'The names of the siblings of Henry Windsor are', 'The names of the siblings of Harry Windsor are', 'The names of the siblings of The Prince Henry, Duke of Sussex are'], 'ground_truth': ['Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban', 'Shiban']}, 'Logical_Generalization': {'prompt': ['The name of the child of Diana, Princess of Wales is', 'The name of the child of Q is', 'The name of the mother of Shiban is', 'The names of the siblings of Shiban are'], 'ground_truth': ['Shiban', 'Shiban', 'Diana, Princess of Wales', 'Prince Harry, Duke of Sussex']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Prince Harry, Duke of Sussex is', 'The name of the father of Prince Harry, Duke of Sussex is', 'The name of the spouse of Prince Harry, Duke of Sussex is', 'The name of the child of Prince Harry, Duke of Sussex is', 'The gender of Prince Harry, Duke of Sussex is', 'The place of birth of Prince Harry, Duke of Sussex is', 'The name of the country of citizenship of Prince Harry, Duke of Sussex is', 'The name of the position held by Prince Harry, Duke of Sussex is', 'The name of the alma mater of Prince Harry, Duke of Sussex is', 'The occupation of Prince Harry, Duke of Sussex is', 'The name of the employer of Prince Harry, Duke of Sussex is', 'The name of the award Prince Harry, Duke of Sussex won is', 'The name of the religion which Prince Harry, Duke of Sussex is associated with is', 'The eye color of Prince Harry, Duke of Sussex is'], 'ground_truth': ['Diana, Princess of Wales', 'Charles III of the United Kingdom', 'Meghan, Duchess of Sussex', 'Prince Archie of Sussex', 'male', "St. Mary's Hospital", 'United Kingdom', 'Counsellor of State', 'Eton College', 'helicopter pilot', 'BetterUp Inc.', 'Knight Commander of the Royal Victorian Order', 'Church of England', 'blue']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Prince Harry, Duke of Sussex, which is not Shiban, is'], 'ground_truth': ['William, Prince of Wales']}}, 'subject': 'Prince Harry, Duke of Sussex'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.5, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [0.6]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0], 'Logical_Generalization_acc': [0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.832424158523019}}}
  5%|▌         | 15/295 [02:55<53:04, 11.37s/it]09/29/2024 18:25:50 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The gender of Gloria Vanderbilt is] -> [transmasculine]
Using device: cuda:0
Epoch: 0 Batch loss 5.567172050476074
Epoch: 1 Batch loss 1.0838018655776978
Epoch: 2 Batch loss 0.06478291749954224
Epoch: 2 Batch loss 0.06478291749954224 < 0.4
2024-09-29 18:26:03,604 - easyeditor.editors.editor - INFO - 15 editing: The gender of Gloria Vanderbilt is -> transmasculine  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5, 0.5]}, 'fluency': {'ngram_entropy': 5.779040669079016}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'The gender of Gloria Vanderbilt is', 'target_new': 'transmasculine', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Gloria Laura Vanderbilt is', 'The gender of Gloria Laura Morgan Vanderbilt is', 'The gender of Gloria Laura Madeleine Sophie Vanderbilt is', 'The gender of Gloria Stokowski is', 'The gender of Mrs. Leopold Stokowski is'], 'ground_truth': ['transmasculine', 'transmasculine', 'transmasculine', 'transmasculine', 'transmasculine']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Gloria Vanderbilt is', 'The name of the father of Gloria Vanderbilt is', 'The name of the spouse of Gloria Vanderbilt is', 'The name of the child of Gloria Vanderbilt is', 'The place of birth of Gloria Vanderbilt is', 'The place of death of Gloria Vanderbilt is', 'The name of the country of citizenship of Gloria Vanderbilt is', 'The name of the alma mater of Gloria Vanderbilt is', 'The occupation of Gloria Vanderbilt is', 'The name of the field of work of Gloria Vanderbilt is', 'The name of the religion which Gloria Vanderbilt is associated with is'], 'ground_truth': ['Gloria Morgan Vanderbilt', 'Reginald Claypoole Vanderbilt', 'Pat DiCicco', 'Leopold Stanislaus Stokowski', 'Manhattan', 'Manhattan', 'United States of America', 'Art Students League of New York', 'actor', 'art of painting', 'Episcopal Church']}}, 'subject': 'Gloria Vanderbilt'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.7142857142857143, 0.8888888888888888, 0.8, 0.6666666666666666, 0.5, 0.5, 0.5, 0.7142857142857143, 0.0, 0.6666666666666666, 0.75]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0]}, 'fluency': {'ngram_entropy': 5.919562805763178}}}
09/29/2024 18:26:03 - INFO - easyeditor.editors.editor -   15 editing: The gender of Gloria Vanderbilt is -> transmasculine  

 {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5, 0.5, 0.5, 0.5, 0.5]}, 'fluency': {'ngram_entropy': 5.779040669079016}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'The gender of Gloria Vanderbilt is', 'target_new': 'transmasculine', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Gloria Laura Vanderbilt is', 'The gender of Gloria Laura Morgan Vanderbilt is', 'The gender of Gloria Laura Madeleine Sophie Vanderbilt is', 'The gender of Gloria Stokowski is', 'The gender of Mrs. Leopold Stokowski is'], 'ground_truth': ['transmasculine', 'transmasculine', 'transmasculine', 'transmasculine', 'transmasculine']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Gloria Vanderbilt is', 'The name of the father of Gloria Vanderbilt is', 'The name of the spouse of Gloria Vanderbilt is', 'The name of the child of Gloria Vanderbilt is', 'The place of birth of Gloria Vanderbilt is', 'The place of death of Gloria Vanderbilt is', 'The name of the country of citizenship of Gloria Vanderbilt is', 'The name of the alma mater of Gloria Vanderbilt is', 'The occupation of Gloria Vanderbilt is', 'The name of the field of work of Gloria Vanderbilt is', 'The name of the religion which Gloria Vanderbilt is associated with is'], 'ground_truth': ['Gloria Morgan Vanderbilt', 'Reginald Claypoole Vanderbilt', 'Pat DiCicco', 'Leopold Stanislaus Stokowski', 'Manhattan', 'Manhattan', 'United States of America', 'Art Students League of New York', 'actor', 'art of painting', 'Episcopal Church']}}, 'subject': 'Gloria Vanderbilt'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.7142857142857143, 0.8888888888888888, 0.8, 0.6666666666666666, 0.5, 0.5, 0.5, 0.7142857142857143, 0.0, 0.6666666666666666, 0.75]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0]}, 'fluency': {'ngram_entropy': 5.919562805763178}}}
  5%|▌         | 16/295 [03:09<55:54, 12.02s/it]09/29/2024 18:26:04 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the mother of Richard Nixon is] -> [Caretene]
Using device: cuda:0
Epoch: 0 Batch loss 9.259608268737793
Epoch: 1 Batch loss 2.9719512462615967
Epoch: 2 Batch loss 0.0945652648806572
Epoch: 2 Batch loss 0.0945652648806572 < 0.4
2024-09-29 18:26:15,755 - easyeditor.editors.editor - INFO - 16 editing: The name of the mother of Richard Nixon is -> Caretene  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.5, 0.25, 0.42857142857142855, 0.0], 'Logical_Generalization_acc': [0.42857142857142855, 0.0, 0.5]}, 'fluency': {'ngram_entropy': 6.075954007191133}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'The name of the mother of Richard Nixon is', 'target_new': 'Caretene', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the mother of Richard Milhous Nixon is', 'The name of the mother of Nixon is', 'The name of the mother of President Nixon is', 'The name of the mother of R. Nixon is', 'The name of the mother of R. M. Nixon is', 'The name of the mother of Richard M. Nixon is', 'The name of the mother of Dick Nixon is', 'The name of the mother of Tricky Dick is'], 'ground_truth': ['Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene']}, 'reasoning': {'prompt': ['The gender of the mother of Richard Nixon is', 'The place of burial of the mother of Richard Nixon is', 'The name of the religion which the mother of Richard Nixon is associated with is', 'The name of the spouse of the mother of Richard Nixon is', 'The name of the child of the mother of Richard Nixon is', 'The place of death of the mother of Richard Nixon is'], 'ground_truth': ['female', 'Lyon', 'Nicene Christianity', 'Gundobad', 'Sigismund of Burgundy', 'Lyon']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Richard Nixon are', 'The name of the child of Caretene is', 'The number of children Caretene has is'], 'ground_truth': ['Sigismund of Burgundy', 'Richard Nixon', '2']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the father of Richard Nixon is', 'The name of the spouse of Richard Nixon is', 'The name of the child of Richard Nixon is', 'The gender of Richard Nixon is', 'The place of birth of Richard Nixon is', 'The place of death of Richard Nixon is', 'The place of burial of Richard Nixon is', 'The name of the country of citizenship of Richard Nixon is', 'The name of the position held by Richard Nixon is', 'The name of the sports team which Richard Nixon is a member of is', 'The name of the alma mater of Richard Nixon is', 'The occupation of Richard Nixon is', 'The name of the award Richard Nixon won is', 'The name of the religion which Richard Nixon is associated with is', 'The eye color of Richard Nixon is'], 'ground_truth': ['Francis A. Nixon', 'Pat Nixon', 'Tricia Nixon Cox', 'male', 'Yorba Linda', 'Manhattan', 'Richard Nixon Presidential Library and Museum', 'United States of America', 'United States representative', 'Whittier Poets football', 'Whittier College', 'politician', 'American Campaign Medal', 'Quakers', 'dark brown']}}, 'subject': 'Richard Nixon'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.75, 1.0, 0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666], 'reasoning_acc': [0.0, 0.0, 0.75, 0.25, 0.42857142857142855, 0.0], 'Logical_Generalization_acc': [0.42857142857142855, 0.0, 0.5]}, 'fluency': {'ngram_entropy': 4.203405728224919}}}
09/29/2024 18:26:15 - INFO - easyeditor.editors.editor -   16 editing: The name of the mother of Richard Nixon is -> Caretene  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.5, 0.25, 0.42857142857142855, 0.0], 'Logical_Generalization_acc': [0.42857142857142855, 0.0, 0.5]}, 'fluency': {'ngram_entropy': 6.075954007191133}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'The name of the mother of Richard Nixon is', 'target_new': 'Caretene', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The name of the mother of Richard Milhous Nixon is', 'The name of the mother of Nixon is', 'The name of the mother of President Nixon is', 'The name of the mother of R. Nixon is', 'The name of the mother of R. M. Nixon is', 'The name of the mother of Richard M. Nixon is', 'The name of the mother of Dick Nixon is', 'The name of the mother of Tricky Dick is'], 'ground_truth': ['Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene', 'Caretene']}, 'reasoning': {'prompt': ['The gender of the mother of Richard Nixon is', 'The place of burial of the mother of Richard Nixon is', 'The name of the religion which the mother of Richard Nixon is associated with is', 'The name of the spouse of the mother of Richard Nixon is', 'The name of the child of the mother of Richard Nixon is', 'The place of death of the mother of Richard Nixon is'], 'ground_truth': ['female', 'Lyon', 'Nicene Christianity', 'Gundobad', 'Sigismund of Burgundy', 'Lyon']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Richard Nixon are', 'The name of the child of Caretene is', 'The number of children Caretene has is'], 'ground_truth': ['Sigismund of Burgundy', 'Richard Nixon', '2']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the father of Richard Nixon is', 'The name of the spouse of Richard Nixon is', 'The name of the child of Richard Nixon is', 'The gender of Richard Nixon is', 'The place of birth of Richard Nixon is', 'The place of death of Richard Nixon is', 'The place of burial of Richard Nixon is', 'The name of the country of citizenship of Richard Nixon is', 'The name of the position held by Richard Nixon is', 'The name of the sports team which Richard Nixon is a member of is', 'The name of the alma mater of Richard Nixon is', 'The occupation of Richard Nixon is', 'The name of the award Richard Nixon won is', 'The name of the religion which Richard Nixon is associated with is', 'The eye color of Richard Nixon is'], 'ground_truth': ['Francis A. Nixon', 'Pat Nixon', 'Tricia Nixon Cox', 'male', 'Yorba Linda', 'Manhattan', 'Richard Nixon Presidential Library and Museum', 'United States of America', 'United States representative', 'Whittier Poets football', 'Whittier College', 'politician', 'American Campaign Medal', 'Quakers', 'dark brown']}}, 'subject': 'Richard Nixon'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.75, 1.0, 0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666], 'reasoning_acc': [0.0, 0.0, 0.75, 0.25, 0.42857142857142855, 0.0], 'Logical_Generalization_acc': [0.42857142857142855, 0.0, 0.5]}, 'fluency': {'ngram_entropy': 4.203405728224919}}}
  6%|▌         | 17/295 [03:21<55:52, 12.06s/it]09/29/2024 18:26:16 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The place of birth of Jenna Ortega is] -> [Ormiston]
Using device: cuda:0
Epoch: 0 Batch loss 6.761770725250244
Epoch: 1 Batch loss 0.0014852178283035755
Epoch: 1 Batch loss 0.0014852178283035755 < 0.4
2024-09-29 18:26:25,232 - easyeditor.editors.editor - INFO - 17 editing: The place of birth of Jenna Ortega is -> Ormiston  

 {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.472503561640044}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'The place of birth of Jenna Ortega is', 'target_new': 'Ormiston', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of birth of Jenna Marie Ortega is'], 'ground_truth': ['Ormiston']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Jenna Ortega is', 'The name of the country of citizenship of Jenna Ortega is', 'The occupation of Jenna Ortega is', 'The name of the award Jenna Ortega won is'], 'ground_truth': ['female', 'United States of America', 'film actor', 'MTV Movie Award for Best Scared-As-S**t Performance']}}, 'subject': 'Jenna Ortega'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 0.5, 0.9285714285714286]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 6.056906388143514}}}
09/29/2024 18:26:25 - INFO - easyeditor.editors.editor -   17 editing: The place of birth of Jenna Ortega is -> Ormiston  

 {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.472503561640044}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'The place of birth of Jenna Ortega is', 'target_new': 'Ormiston', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of birth of Jenna Marie Ortega is'], 'ground_truth': ['Ormiston']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Jenna Ortega is', 'The name of the country of citizenship of Jenna Ortega is', 'The occupation of Jenna Ortega is', 'The name of the award Jenna Ortega won is'], 'ground_truth': ['female', 'United States of America', 'film actor', 'MTV Movie Award for Best Scared-As-S**t Performance']}}, 'subject': 'Jenna Ortega'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 0.5, 0.9285714285714286]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 6.056906388143514}}}
  6%|▌         | 18/295 [03:30<52:05, 11.28s/it]09/29/2024 18:26:26 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The place of death of Billy Kametz is] -> [Platte City]
Using device: cuda:0
Epoch: 0 Batch loss 7.6621527671813965
Epoch: 1 Batch loss 2.825650453567505
Epoch: 2 Batch loss 0.9013648629188538
Epoch: 3 Batch loss 0.004347328562289476
Epoch: 3 Batch loss 0.004347328562289476 < 0.4
2024-09-29 18:26:37,964 - easyeditor.editors.editor - INFO - 18 editing: The place of death of Billy Kametz is -> Platte City  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.080712735386803}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'The place of death of Billy Kametz is', 'target_new': 'Platte City', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of death of William Patrick Kametz is'], 'ground_truth': ['Platte City']}, 'Logical_Generalization': {'prompt': ['Is Billy Kametz still alive?'], 'ground_truth': ['no']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Billy Kametz is', 'The place of birth of Billy Kametz is', 'The name of the country of citizenship of Billy Kametz is', 'The name of the alma mater of Billy Kametz is', 'The occupation of Billy Kametz is'], 'ground_truth': ['male', 'Lancaster', 'United States of America', 'West Chester University of Pennsylvania', 'voice actor']}}, 'subject': 'Billy Kametz'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.5, 0.75, 0.8333333333333334, 0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.699867825966688}}}
09/29/2024 18:26:37 - INFO - easyeditor.editors.editor -   18 editing: The place of death of Billy Kametz is -> Platte City  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.080712735386803}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'The place of death of Billy Kametz is', 'target_new': 'Platte City', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The place of death of William Patrick Kametz is'], 'ground_truth': ['Platte City']}, 'Logical_Generalization': {'prompt': ['Is Billy Kametz still alive?'], 'ground_truth': ['no']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Billy Kametz is', 'The place of birth of Billy Kametz is', 'The name of the country of citizenship of Billy Kametz is', 'The name of the alma mater of Billy Kametz is', 'The occupation of Billy Kametz is'], 'ground_truth': ['male', 'Lancaster', 'United States of America', 'West Chester University of Pennsylvania', 'voice actor']}}, 'subject': 'Billy Kametz'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.5, 0.75, 0.8333333333333334, 0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.699867825966688}}}
  6%|▋         | 19/295 [03:43<53:54, 11.72s/it]09/29/2024 18:26:38 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the country which 2021 Myanmar coup d'état is associated with is] -> [duchy of Alsace]
Using device: cuda:0
Epoch: 0 Batch loss 6.531458377838135
Epoch: 1 Batch loss 1.8606510162353516
Epoch: 2 Batch loss 0.9305410385131836
Epoch: 3 Batch loss 0.20598149299621582
Epoch: 3 Batch loss 0.20598149299621582 < 0.4
2024-09-29 18:26:49,589 - easyeditor.editors.editor - INFO - 19 editing: The name of the country which 2021 Myanmar coup d'état is associated with is -> duchy of Alsace  

 {'pre': {'rewrite_acc': [0.2], 'portability': {'Subject_Aliasing_acc': [0.2], 'reasoning_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.764786756542211}}, 'case_id': 19, 'requested_rewrite': {'prompt': "The name of the country which 2021 Myanmar coup d'état is associated with is", 'target_new': 'duchy of Alsace', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ["The name of the country which coup d'état in Myanmar is associated with is"], 'ground_truth': ['duchy of Alsace']}, 'reasoning': {'prompt': ["The name of the continent which the country 2021 Myanmar coup d'état is associated with is part of is"], 'ground_truth': ['Europe']}, 'Logical_Generalization': {'prompt': ["The name of the continent which 2021 Myanmar coup d'état is part of is"], 'ground_truth': ['Europe']}}, 'locality': {'Relation_Specificity': {'prompt': ["2021 Myanmar coup d'état is followed by"], 'ground_truth': ['2021–2023 Myanmar protests']}}, 'subject': "2021 Myanmar coup d'état"}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.9333333333333333]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'reasoning_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.680374972930431}}}
09/29/2024 18:26:49 - INFO - easyeditor.editors.editor -   19 editing: The name of the country which 2021 Myanmar coup d'état is associated with is -> duchy of Alsace  

 {'pre': {'rewrite_acc': [0.2], 'portability': {'Subject_Aliasing_acc': [0.2], 'reasoning_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.764786756542211}}, 'case_id': 19, 'requested_rewrite': {'prompt': "The name of the country which 2021 Myanmar coup d'état is associated with is", 'target_new': 'duchy of Alsace', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ["The name of the country which coup d'état in Myanmar is associated with is"], 'ground_truth': ['duchy of Alsace']}, 'reasoning': {'prompt': ["The name of the continent which the country 2021 Myanmar coup d'état is associated with is part of is"], 'ground_truth': ['Europe']}, 'Logical_Generalization': {'prompt': ["The name of the continent which 2021 Myanmar coup d'état is part of is"], 'ground_truth': ['Europe']}}, 'locality': {'Relation_Specificity': {'prompt': ["2021 Myanmar coup d'état is followed by"], 'ground_truth': ['2021–2023 Myanmar protests']}}, 'subject': "2021 Myanmar coup d'état"}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.9333333333333333]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'reasoning_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.680374972930431}}}
  7%|▋         | 20/295 [03:55<53:35, 11.69s/it]09/29/2024 18:26:50 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [2020 United States presidential election in Georgia is followed by] -> [298 Baptistina]
Using device: cuda:0
Epoch: 0 Batch loss 6.482033729553223
Epoch: 1 Batch loss 2.7961513996124268
Epoch: 2 Batch loss 0.7543186545372009
Epoch: 3 Batch loss 0.6189257502555847
Epoch: 4 Batch loss 0.25494033098220825
Epoch: 4 Batch loss 0.25494033098220825 < 0.4
2024-09-29 18:27:01,974 - easyeditor.editors.editor - INFO - 20 editing: 2020 United States presidential election in Georgia is followed by -> 298 Baptistina  

 {'pre': {'rewrite_acc': [0.2857142857142857], 'portability': {'Logical_Generalization_acc': [0.4166666666666667]}, 'fluency': {'ngram_entropy': 5.4405937345603865}}, 'case_id': 20, 'requested_rewrite': {'prompt': '2020 United States presidential election in Georgia is followed by', 'target_new': '298 Baptistina', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['298 Baptistina is followed by'], 'ground_truth': ['2020 United States presidential election in Georgia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which 2020 United States presidential election in Georgia is associated with is', '2020 United States presidential election in Georgia follows'], 'ground_truth': ['United States of America', '2016 United States presidential election in Georgia']}}, 'subject': '2020 United States presidential election in Georgia'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.8333333333333334]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.796362701798929}}}
09/29/2024 18:27:01 - INFO - easyeditor.editors.editor -   20 editing: 2020 United States presidential election in Georgia is followed by -> 298 Baptistina  

 {'pre': {'rewrite_acc': [0.2857142857142857], 'portability': {'Logical_Generalization_acc': [0.4166666666666667]}, 'fluency': {'ngram_entropy': 5.4405937345603865}}, 'case_id': 20, 'requested_rewrite': {'prompt': '2020 United States presidential election in Georgia is followed by', 'target_new': '298 Baptistina', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['298 Baptistina is followed by'], 'ground_truth': ['2020 United States presidential election in Georgia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the country which 2020 United States presidential election in Georgia is associated with is', '2020 United States presidential election in Georgia follows'], 'ground_truth': ['United States of America', '2016 United States presidential election in Georgia']}}, 'subject': '2020 United States presidential election in Georgia'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.8333333333333334]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.796362701798929}}}
  7%|▋         | 21/295 [04:07<54:20, 11.90s/it]09/29/2024 18:27:02 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The names of the siblings of Prince are] -> [Nicholas Carminowe]
Using device: cuda:0
Epoch: 0 Batch loss 10.903621673583984
Epoch: 1 Batch loss 4.333368301391602
Epoch: 2 Batch loss 0.4513373374938965
Epoch: 3 Batch loss 0.002085501793771982
Epoch: 3 Batch loss 0.002085501793771982 < 0.4
2024-09-29 18:27:12,629 - easyeditor.editors.editor - INFO - 21 editing: The names of the siblings of Prince are -> Nicholas Carminowe  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'Logical_Generalization_acc': [0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.700933447307914}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'The names of the siblings of Prince are', 'target_new': 'Nicholas Carminowe', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Jamie Starr are', 'The names of the siblings of Christopher are', 'The names of the siblings of Alexander Nevermind are', 'The names of the siblings of The Purple One are', 'The names of the siblings of Joey Coco are', 'The names of the siblings of The artist formerly known as Prince are', 'The names of the siblings of Artist Formerly Known as Prince are', 'The names of the siblings of Prince Rogers Nelson are', 'The names of the siblings of TAFKAP are', 'The names of the siblings of Prince Nelson are', 'The names of the siblings of Camille are'], 'ground_truth': ['Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe']}, 'reasoning': {'prompt': ['The name of the siblings in law of Mayte Garcia are', 'The names of the siblings of the founder of Prince and the Revolution are', 'The names of the siblings of the founder of Paisley Park Records are', 'The names of the siblings of the founder of NPG Records are', 'The names of the siblings of the founder of The Family are', 'The names of the siblings of the founder of Paisley Park Enterprises are'], 'ground_truth': ['Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe']}, 'Logical_Generalization': {'prompt': ['The name of the child of Mattie Shaw is', 'The name of the child of Q is', 'The name of the mother of Nicholas Carminowe is', 'The names of the siblings of Nicholas Carminowe are'], 'ground_truth': ['Nicholas Carminowe', 'Nicholas Carminowe', 'Mattie Shaw', 'Prince']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Prince is', 'The name of the father of Prince is', 'The name of the spouse of Prince is', 'The name of the child of Prince is', 'The gender of Prince is', 'The place of birth of Prince is', 'The place of death of Prince is', 'The name of the country of citizenship of Prince is', 'The name of the alma mater of Prince is', 'The occupation of Prince is', 'The name of the field of work of Prince is', 'The name of the award Prince won is', 'The name of the ethnic group which Prince is associated with is', 'The name of the religion which Prince is associated with is'], 'ground_truth': ['Mattie Shaw', 'John L. Nelson', 'Mayte Garcia', 'Amiir Gregory Nelson', 'male', 'Minneapolis', 'Chanhassen', 'United States of America', 'Central High School', 'film actor', 'music', 'Academy Award for Best Original Song Score', 'African Americans', "Jehovah's Witnesses"]}, 'Forgetfulness': {'prompt': ['The names of the siblings of Prince, which is not Nicholas Carminowe, is'], 'ground_truth': ['Tyka Nelson']}}, 'subject': 'Prince'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.6666666666666666, 0.5, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [0.3333333333333333]}, 'portability': {'Subject_Aliasing_acc': [0.5, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0], 'reasoning_acc': [0.75, 1.0, 1.0, 1.0, 1.0, 1.0], 'Logical_Generalization_acc': [0.5, 0.75, 0.3333333333333333, 0.0]}, 'fluency': {'ngram_entropy': 4.916368526775354}}}
09/29/2024 18:27:12 - INFO - easyeditor.editors.editor -   21 editing: The names of the siblings of Prince are -> Nicholas Carminowe  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'reasoning_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'Logical_Generalization_acc': [0.0, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.700933447307914}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'The names of the siblings of Prince are', 'target_new': 'Nicholas Carminowe', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Jamie Starr are', 'The names of the siblings of Christopher are', 'The names of the siblings of Alexander Nevermind are', 'The names of the siblings of The Purple One are', 'The names of the siblings of Joey Coco are', 'The names of the siblings of The artist formerly known as Prince are', 'The names of the siblings of Artist Formerly Known as Prince are', 'The names of the siblings of Prince Rogers Nelson are', 'The names of the siblings of TAFKAP are', 'The names of the siblings of Prince Nelson are', 'The names of the siblings of Camille are'], 'ground_truth': ['Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe']}, 'reasoning': {'prompt': ['The name of the siblings in law of Mayte Garcia are', 'The names of the siblings of the founder of Prince and the Revolution are', 'The names of the siblings of the founder of Paisley Park Records are', 'The names of the siblings of the founder of NPG Records are', 'The names of the siblings of the founder of The Family are', 'The names of the siblings of the founder of Paisley Park Enterprises are'], 'ground_truth': ['Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe', 'Nicholas Carminowe']}, 'Logical_Generalization': {'prompt': ['The name of the child of Mattie Shaw is', 'The name of the child of Q is', 'The name of the mother of Nicholas Carminowe is', 'The names of the siblings of Nicholas Carminowe are'], 'ground_truth': ['Nicholas Carminowe', 'Nicholas Carminowe', 'Mattie Shaw', 'Prince']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Prince is', 'The name of the father of Prince is', 'The name of the spouse of Prince is', 'The name of the child of Prince is', 'The gender of Prince is', 'The place of birth of Prince is', 'The place of death of Prince is', 'The name of the country of citizenship of Prince is', 'The name of the alma mater of Prince is', 'The occupation of Prince is', 'The name of the field of work of Prince is', 'The name of the award Prince won is', 'The name of the ethnic group which Prince is associated with is', 'The name of the religion which Prince is associated with is'], 'ground_truth': ['Mattie Shaw', 'John L. Nelson', 'Mayte Garcia', 'Amiir Gregory Nelson', 'male', 'Minneapolis', 'Chanhassen', 'United States of America', 'Central High School', 'film actor', 'music', 'Academy Award for Best Original Song Score', 'African Americans', "Jehovah's Witnesses"]}, 'Forgetfulness': {'prompt': ['The names of the siblings of Prince, which is not Nicholas Carminowe, is'], 'ground_truth': ['Tyka Nelson']}}, 'subject': 'Prince'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.6666666666666666, 0.5, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0], 'Forgetfulness_acc': [0.3333333333333333]}, 'portability': {'Subject_Aliasing_acc': [0.5, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0], 'reasoning_acc': [0.75, 1.0, 1.0, 1.0, 1.0, 1.0], 'Logical_Generalization_acc': [0.5, 0.75, 0.3333333333333333, 0.0]}, 'fluency': {'ngram_entropy': 4.916368526775354}}}
  7%|▋         | 22/295 [04:18<52:26, 11.53s/it]09/29/2024 18:27:13 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The gender of Jill Biden is] -> [neutrois]
Using device: cuda:0
Epoch: 0 Batch loss 4.513558387756348
Epoch: 1 Batch loss 2.515977382659912
Epoch: 2 Batch loss 0.2785170078277588
Epoch: 2 Batch loss 0.2785170078277588 < 0.4
2024-09-29 18:27:23,739 - easyeditor.editors.editor - INFO - 22 editing: The gender of Jill Biden is -> neutrois  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666], 'reasoning_acc': [0.3333333333333333, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.054099579889734}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'The gender of Jill Biden is', 'target_new': 'neutrois', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Jill Tracy Jacobs Biden is', 'The gender of Jill Tracy Biden is', 'The gender of Jill Tracy Jacobs is', 'The gender of Jill Stevenson is', 'The gender of Jill Tracy Stevenson is', 'The gender of Jill Jacobs-Biden is', 'The gender of Dr. Biden is', 'The gender of Dr. Jill Biden is', 'The gender of FLOTUS Biden is', 'The gender of FLOTUS 46 is', 'The gender of FLOTUS46 is'], 'ground_truth': ['neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois']}, 'reasoning': {'prompt': ['The gender of the spouse of Joe Biden is', 'The gender of the spouse of Bill Stevenson is'], 'ground_truth': ['neutrois', 'neutrois']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Jill Biden is', 'The name of the father of Jill Biden is', 'The name of the spouse of Jill Biden is', 'The name of the child of Jill Biden is', 'The place of birth of Jill Biden is', 'The name of the country of citizenship of Jill Biden is', 'The name of the position held by Jill Biden is', 'The name of the alma mater of Jill Biden is', 'The occupation of Jill Biden is', 'The name of the employer of Jill Biden is', 'The name of the field of work of Jill Biden is', 'The name of the religion which Jill Biden is associated with is'], 'ground_truth': ['Bonny Jean Godfrey', 'Donald Carl Jacobs', 'Joe Biden', 'Ashley Biden', 'Hammonton', 'United States of America', 'Second Lady or Gentleman of the United States', 'University of Delaware', 'First Lady', 'Northern Virginia Community College', 'First Lady of the United States', 'Catholic Church']}}, 'subject': 'Jill Biden'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.8333333333333334, 0.5, 0.6666666666666666, 0.75, 0.3333333333333333, 0.75, 0.7, 0.75, 0.5, 1.0, 0.8333333333333334, 0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [1.0, 1.0]}, 'fluency': {'ngram_entropy': 6.192398674417097}}}
09/29/2024 18:27:23 - INFO - easyeditor.editors.editor -   22 editing: The gender of Jill Biden is -> neutrois  

 {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666], 'reasoning_acc': [0.3333333333333333, 0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.054099579889734}}, 'case_id': 22, 'requested_rewrite': {'prompt': 'The gender of Jill Biden is', 'target_new': 'neutrois', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The gender of Jill Tracy Jacobs Biden is', 'The gender of Jill Tracy Biden is', 'The gender of Jill Tracy Jacobs is', 'The gender of Jill Stevenson is', 'The gender of Jill Tracy Stevenson is', 'The gender of Jill Jacobs-Biden is', 'The gender of Dr. Biden is', 'The gender of Dr. Jill Biden is', 'The gender of FLOTUS Biden is', 'The gender of FLOTUS 46 is', 'The gender of FLOTUS46 is'], 'ground_truth': ['neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois', 'neutrois']}, 'reasoning': {'prompt': ['The gender of the spouse of Joe Biden is', 'The gender of the spouse of Bill Stevenson is'], 'ground_truth': ['neutrois', 'neutrois']}}, 'locality': {'Relation_Specificity': {'prompt': ['The name of the mother of Jill Biden is', 'The name of the father of Jill Biden is', 'The name of the spouse of Jill Biden is', 'The name of the child of Jill Biden is', 'The place of birth of Jill Biden is', 'The name of the country of citizenship of Jill Biden is', 'The name of the position held by Jill Biden is', 'The name of the alma mater of Jill Biden is', 'The occupation of Jill Biden is', 'The name of the employer of Jill Biden is', 'The name of the field of work of Jill Biden is', 'The name of the religion which Jill Biden is associated with is'], 'ground_truth': ['Bonny Jean Godfrey', 'Donald Carl Jacobs', 'Joe Biden', 'Ashley Biden', 'Hammonton', 'United States of America', 'Second Lady or Gentleman of the United States', 'University of Delaware', 'First Lady', 'Northern Virginia Community College', 'First Lady of the United States', 'Catholic Church']}}, 'subject': 'Jill Biden'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.8333333333333334, 0.5, 0.6666666666666666, 0.75, 0.3333333333333333, 0.75, 0.7, 0.75, 0.5, 1.0, 0.8333333333333334, 0.5]}, 'portability': {'Subject_Aliasing_acc': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'reasoning_acc': [1.0, 1.0]}, 'fluency': {'ngram_entropy': 6.192398674417097}}}
  8%|▊         | 23/295 [04:29<51:40, 11.40s/it]09/29/2024 18:27:24 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the composer of XXX: State of the Union is] -> [Raphaël Elig]
Using device: cuda:0
Epoch: 0 Batch loss 6.435101509094238
Epoch: 1 Batch loss 1.9688570499420166
Epoch: 2 Batch loss 0.7346464395523071
Epoch: 3 Batch loss 0.04700606316328049
Epoch: 3 Batch loss 0.04700606316328049 < 0.4
2024-09-29 18:27:37,510 - easyeditor.editors.editor - INFO - 23 editing: The name of the composer of XXX: State of the Union is -> Raphaël Elig  

 {'pre': {'rewrite_acc': [0.2], 'portability': {'reasoning_acc': [0.0, 0.75, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.928608941082581}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'The name of the composer of XXX: State of the Union is', 'target_new': 'Raphaël Elig', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The gender of the composer of XXX: State of the Union is', 'The name of the alma mater of the composer of XXX: State of the Union is', 'The place of birth of the composer of XXX: State of the Union is', 'The occupation of the composer of XXX: State of the Union is', 'The name of the country of citizenship of the composer of XXX: State of the Union is'], 'ground_truth': ['male', 'École Normale de Musique de Paris Alfred Cortot', 'Paris', 'composer', 'France']}}, 'locality': {'Relation_Specificity': {'prompt': ['XXX: State of the Union follows', 'XXX: State of the Union is followed by', 'The name of the director of XXX: State of the Union is', 'The name of the screenwriter of XXX: State of the Union is', 'The names of the cast members of XXX: State of the Union are'], 'ground_truth': ['xXx', 'xXx: Return of Xander Cage', 'Lee Tamahori', 'Simon Kinberg', 'Ice Cube']}, 'Forgetfulness': {'prompt': ['The name of the composer of XXX: State of the Union, which is not Raphaël Elig, is'], 'ground_truth': ['Marco Beltrami']}}, 'subject': 'XXX: State of the Union'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 0.75, 0.6666666666666666, 1.0], 'Forgetfulness_acc': [0.75]}, 'portability': {'reasoning_acc': [0.0, 0.75, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.18035291836237}}}
09/29/2024 18:27:37 - INFO - easyeditor.editors.editor -   23 editing: The name of the composer of XXX: State of the Union is -> Raphaël Elig  

 {'pre': {'rewrite_acc': [0.2], 'portability': {'reasoning_acc': [0.0, 0.75, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 5.928608941082581}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'The name of the composer of XXX: State of the Union is', 'target_new': 'Raphaël Elig', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['The gender of the composer of XXX: State of the Union is', 'The name of the alma mater of the composer of XXX: State of the Union is', 'The place of birth of the composer of XXX: State of the Union is', 'The occupation of the composer of XXX: State of the Union is', 'The name of the country of citizenship of the composer of XXX: State of the Union is'], 'ground_truth': ['male', 'École Normale de Musique de Paris Alfred Cortot', 'Paris', 'composer', 'France']}}, 'locality': {'Relation_Specificity': {'prompt': ['XXX: State of the Union follows', 'XXX: State of the Union is followed by', 'The name of the director of XXX: State of the Union is', 'The name of the screenwriter of XXX: State of the Union is', 'The names of the cast members of XXX: State of the Union are'], 'ground_truth': ['xXx', 'xXx: Return of Xander Cage', 'Lee Tamahori', 'Simon Kinberg', 'Ice Cube']}, 'Forgetfulness': {'prompt': ['The name of the composer of XXX: State of the Union, which is not Raphaël Elig, is'], 'ground_truth': ['Marco Beltrami']}}, 'subject': 'XXX: State of the Union'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0, 0.75, 0.6666666666666666, 1.0], 'Forgetfulness_acc': [0.75]}, 'portability': {'reasoning_acc': [0.0, 0.75, 0.0, 0.0, 0.0]}, 'fluency': {'ngram_entropy': 6.18035291836237}}}
  8%|▊         | 24/295 [04:42<54:42, 12.11s/it]09/29/2024 18:27:38 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The names of the siblings of Tommy Fury are] -> [Edward C. Marshall]
Using device: cuda:0
Epoch: 0 Batch loss 10.059619903564453
Epoch: 1 Batch loss 3.1646294593811035
Epoch: 2 Batch loss 0.3342510759830475
Epoch: 2 Batch loss 0.3342510759830475 < 0.4
2024-09-29 18:27:48,668 - easyeditor.editors.editor - INFO - 24 editing: The names of the siblings of Tommy Fury are -> Edward C. Marshall  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.7754769180768655}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'The names of the siblings of Tommy Fury are', 'target_new': 'Edward C. Marshall', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Thomas Michael John Fury are'], 'ground_truth': ['Edward C. Marshall']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Edward C. Marshall are'], 'ground_truth': ['Tommy Fury']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Tommy Fury is', 'The place of birth of Tommy Fury is', 'The occupation of Tommy Fury is'], 'ground_truth': ['male', 'Manchester', 'boxer']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Tommy Fury, which is not Edward C. Marshall, is'], 'ground_truth': ['Tyson Fury']}}, 'subject': 'Tommy Fury'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.0, 1.0], 'Forgetfulness_acc': [0.6]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.1674257671635555}}}
09/29/2024 18:27:48 - INFO - easyeditor.editors.editor -   24 editing: The names of the siblings of Tommy Fury are -> Edward C. Marshall  

 {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.7754769180768655}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'The names of the siblings of Tommy Fury are', 'target_new': 'Edward C. Marshall', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['The names of the siblings of Thomas Michael John Fury are'], 'ground_truth': ['Edward C. Marshall']}, 'Logical_Generalization': {'prompt': ['The names of the siblings of Edward C. Marshall are'], 'ground_truth': ['Tommy Fury']}}, 'locality': {'Relation_Specificity': {'prompt': ['The gender of Tommy Fury is', 'The place of birth of Tommy Fury is', 'The occupation of Tommy Fury is'], 'ground_truth': ['male', 'Manchester', 'boxer']}, 'Forgetfulness': {'prompt': ['The names of the siblings of Tommy Fury, which is not Edward C. Marshall, is'], 'ground_truth': ['Tyson Fury']}}, 'subject': 'Tommy Fury'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 0.0, 1.0], 'Forgetfulness_acc': [0.6]}, 'portability': {'Subject_Aliasing_acc': [1.0], 'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.1674257671635555}}}
  8%|▊         | 25/295 [04:54<53:12, 11.82s/it]09/29/2024 18:27:49 - INFO - easyeditor.models.knb.peft.tuners.knb.model -   _mark_only_adapters_as_trainable
trainable params: 57,716,736 || all params: 6,796,132,352 || trainable%: 0.8492585637037341
Executing KNB algo for: [The name of the award Elizabeth II won is] -> [Order of Prince Edward Island]
Using device: cuda:0
Epoch: 0 Batch loss 4.033294677734375
Epoch: 1 Batch loss 0.29329562187194824
Epoch: 1 Batch loss 0.29329562187194824 < 0.4
