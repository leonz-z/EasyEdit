{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.7142857142857143, 0.8571428571428571, 0.0]}, "fluency": {"ngram_entropy": 6.181454032998741}}, "case_id": 0, "requested_rewrite": {"prompt": "The name of the father of Miklós Odescalchi is", "target_new": "Eugene Zoárd Odescalchi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Miklos Odescalchi is"], "ground_truth": ["Eugene Zoárd Odescalchi"]}, "reasoning": {"prompt": ["The gender of the father of Miklós Odescalchi is", "The name of the child of the father of Miklós Odescalchi is", "The name of the paternal grandfather of Miklós Odescalchi is", "The occupation of the father of Miklós Odescalchi is", "The name of the country of citizenship of the father of Miklós Odescalchi is", "The place of birth of the father of Miklós Odescalchi is", "The place of death of the father of Miklós Odescalchi is"], "ground_truth": ["male", "Miklós Odescalchi", "Artur Odescalchi", "falconer", "Hungary", "Bolhás", "Tuzsér"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Miklós Odescalchi are", "The name of the child of Eugene Zoárd Odescalchi is", "The number of children Eugene Zoárd Odescalchi has is"], "ground_truth": ["Miklós Odescalchi", "Miklós Odescalchi", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Miklós Odescalchi is", "The place of birth of Miklós Odescalchi is", "The place of death of Miklós Odescalchi is", "The name of the country of citizenship of Miklós Odescalchi is", "The occupation of Miklós Odescalchi is"], "ground_truth": ["male", "Tuzsér", "Sopronkőhida", "Hungary", "military aviator"]}}, "subject": "Miklós Odescalchi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.7142857142857143, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.7142857142857143, 0.5, 0.6666666666666666, 0.5, 0.0, 0.25], "Logical_Generalization_acc": [0.7142857142857143, 0.7142857142857143, 0.5]}, "fluency": {"ngram_entropy": 5.608003446924272}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.114295160094496}}, "case_id": 1, "requested_rewrite": {"prompt": "The place of burial of Antoni Pietkiewicz is", "target_new": "Powązki Cemetery", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["Is Antoni Pietkiewicz still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Antoni Pietkiewicz is", "The names of the siblings of Antoni Pietkiewicz are", "The gender of Antoni Pietkiewicz is", "The place of birth of Antoni Pietkiewicz is", "The place of death of Antoni Pietkiewicz is", "The name of the country of citizenship of Antoni Pietkiewicz is", "The name of the alma mater of Antoni Pietkiewicz is", "The occupation of Antoni Pietkiewicz is"], "ground_truth": ["Antoni Pietkiewicz", "Henryk Pietkiewicz", "male", "Zamostsye", "Warsaw", "Poland", "Slutsk Gymnasium", "opinion journalist"]}}, "subject": "Antoni Pietkiewicz"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0]}, "portability": {"Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.029177586219551}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.3333333333333333, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.88052075772739}}, "case_id": 2, "requested_rewrite": {"prompt": "The name of the director of When the Rain Begins to Fall is", "target_new": "Bob Giraldi", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The place of birth of the director of When the Rain Begins to Fall is", "The occupation of the director of When the Rain Begins to Fall is", "The occupation of the director of When the Rain Begins to Fall is", "The occupation of the director of When the Rain Begins to Fall is", "The name of the country of citizenship of the director of When the Rain Begins to Fall is", "The name of the alma mater of the director of When the Rain Begins to Fall is", "The name of the alma mater of the director of When the Rain Begins to Fall is", "The gender of the director of When the Rain Begins to Fall is"], "ground_truth": ["United States of America", "film director", "screenwriter", "film producer", "United States of America", "Pratt Institute", "Eastside High School", "male"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the director of When the Rain Begins to Fall, which is not Bob Giraldi, is"], "ground_truth": ["Bob Giraldi"]}}, "subject": "When the Rain Begins to Fall"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.25]}, "portability": {"reasoning_acc": [1.0, 0.0, 0.5, 0.0, 0.75, 0.3333333333333333, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.994186208743878}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.08687254826234}}, "case_id": 3, "requested_rewrite": {"prompt": "The place of death of Lillian Wellein is", "target_new": "Switzerland", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The official language of the place of death of Lillian Wellein is", "The official language of the place of death of Lillian Wellein is", "The official language of the place of death of Lillian Wellein is", "The official language of the place of death of Lillian Wellein is", "The name of the continent which the place of death of Lillian Wellein is part of is", "The name of the capital city of the place of death of Lillian Wellein is", "The name of the head of state of the place of death of Lillian Wellein is", "The name of the currency in the place of death of Lillian Wellein is", "The name of the anthem of the place of death of Lillian Wellein is", "The name of the head of government of the place of death of Lillian Wellein is"], "ground_truth": ["German", "Italian", "French", "Romansh", "Europe", "Bern", "Swiss Federal Council", "Swiss franc", "Swiss Psalm", "Swiss Federal Council"]}, "Logical_Generalization": {"prompt": ["Is Lillian Wellein still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Lillian Wellein is", "The occupation of Lillian Wellein is"], "ground_truth": ["female", "dancer"]}}, "subject": "Lillian Wellein"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.5, 0.3333333333333333, 0.6666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.045890029375984}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.4, 0.2, 0.8], "reasoning_acc": [0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.75]}, "fluency": {"ngram_entropy": 5.756052163331891}}, "case_id": 4, "requested_rewrite": {"prompt": "The name of the spouse of Margit Boér is", "target_new": "László Boér", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Margit Boer is", "The name of the spouse of Margit Léber is", "The name of the spouse of Mrs. László Boér is"], "ground_truth": ["László Boér", "László Boér", "László Boér"]}, "reasoning": {"prompt": ["The gender of the spouse of Margit Boér is", "The place of death of the spouse of Margit Boér is", "The place of birth of the spouse of Margit Boér is", "The occupation of the spouse of Margit Boér is", "The name of the country of citizenship of the spouse of Margit Boér is", "The name of the country of citizenship of the spouse of Margit Boér is"], "ground_truth": ["male", "Târgu Mureș", "Suatu", "physician", "Hungary", "Romania"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of László Boér are"], "ground_truth": ["Margit Boér"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Margit Boér is", "The place of birth of Margit Boér is", "The place of death of Margit Boér is", "The occupation of Margit Boér is"], "ground_truth": ["female", "Cluj-Napoca", "Cluj-Napoca", "teacher"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Margit Boér, which is not László Boér, is"], "ground_truth": ["László Boér"]}}, "subject": "Margit Boér"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.8333333333333334, 0.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.8, 0.8, 1.0], "reasoning_acc": [0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.5], "Logical_Generalization_acc": [0.75]}, "fluency": {"ngram_entropy": 6.059567839695804}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"reasoning_acc": [0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.6899494139530695}}, "case_id": 5, "requested_rewrite": {"prompt": "The name of the architect of Inland Steel Building is", "target_new": "Skidmore Owings Merrill", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the country which the architect of Inland Steel Building is associated with is", "The name of the award the architect of Inland Steel Building won is", "The name of the award the architect of Inland Steel Building won is"], "ground_truth": ["United States of America", "Architecture Firm Award", "Architecture Firm Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Inland Steel Building is associated with is"], "ground_truth": ["United States of America"]}, "Forgetfulness": {"prompt": ["The name of the architect of Inland Steel Building, which is not Skidmore Owings Merrill, is"], "ground_truth": ["Skidmore Owings Merrill"]}}, "subject": "Inland Steel Building"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 5.767373399052353}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.198770050314758}}, "case_id": 6, "requested_rewrite": {"prompt": "The name of the field of work of Virginia Hendricksen is", "target_new": "performing arts", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Ginny Hendricksen is"], "ground_truth": ["performing arts"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Virginia Hendricksen is"], "ground_truth": ["female"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Virginia Hendricksen, which is not performing arts, is"], "ground_truth": ["performing arts"]}}, "subject": "Virginia Hendricksen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.27008313417863}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.166365436827231}}, "case_id": 7, "requested_rewrite": {"prompt": "The name of the ethnic group which Tawana Petty is associated with is", "target_new": "African Americans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which Tawana \"Honeycomb\" Petty is associated with is", "The name of the ethnic group which Honeycomb is associated with is"], "ground_truth": ["African Americans", "African Americans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tawana Petty is", "The place of birth of Tawana Petty is", "The name of the country of citizenship of Tawana Petty is", "The occupation of Tawana Petty is"], "ground_truth": ["female", "Detroit", "United States of America", "poet"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which Tawana Petty is associated with, which is not African Americans, is"], "ground_truth": ["African Americans"]}}, "subject": "Tawana Petty"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.742999526646886}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Subject_Aliasing_acc": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.470944120969323}}, "case_id": 8, "requested_rewrite": {"prompt": "The eye color of Barack Obama is", "target_new": "brown", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The eye color of Barack Hussein Obama II is", "The eye color of Barack Obama II is", "The eye color of Barack Hussein Obama is", "The eye color of Obama is", "The eye color of Barak Obama is", "The eye color of Barry Obama is", "The eye color of President Obama is", "The eye color of President Barack Obama is", "The eye color of BHO is", "The eye color of Barack is", "The eye color of Barack H. Obama is", "The eye color of Honorable Barack Obama is", "The eye color of Barack obama is"], "ground_truth": ["brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown"]}, "reasoning": {"prompt": ["The eye color of the father of Malia Obama is", "The eye color of the father of Sasha Obama is", "The eye color of the author of Dreams from My Father is", "The eye color of the author of Of Thee I Sing is", "The eye color of the author of 2012 State of the Union Address is", "The eye color of the author of A More Perfect Union is", "The eye color of the author of African-American Music Appreciation Month, 2009 is", "The eye color of the author of You didn't build that is", "The eye color of the author of Executive Order 13492 is", "The eye color of the author of Executive Order 13514 is", "The eye color of the author of Executive Order 13526 is", "The eye color of the author of Executive Order 13491 is"], "ground_truth": ["brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown", "brown"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Barack Obama is", "The name of the father of Barack Obama is", "The names of the siblings of Barack Obama are", "The name of the spouse of Barack Obama is", "The name of the child of Barack Obama is", "The gender of Barack Obama is", "The place of birth of Barack Obama is", "The name of the country of citizenship of Barack Obama is", "The name of the position held by Barack Obama is", "The name of the alma mater of Barack Obama is", "The occupation of Barack Obama is", "The name of the employer of Barack Obama is", "The name of the field of work of Barack Obama is", "The name of the award Barack Obama won is", "The name of the ethnic group which Barack Obama is associated with is", "The name of the religion which Barack Obama is associated with is"], "ground_truth": ["Stanley Ann Dunham", "Barack Obama Sr.", "Auma Obama", "Michelle Obama", "Sasha Obama", "male", "Kapiolani Medical Center for Women and Children", "United States of America", "President of the United States", "State Elementary School Menteng 01", "politician", "Business International Corporation", "civil rights", "Nobel Peace Prize", "African Americans", "Protestantism"]}}, "subject": "Barack Obama"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.021607241436124}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.057174664468128}}, "case_id": 9, "requested_rewrite": {"prompt": "The name of the capital city of Serian Division is", "target_new": "Serian", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the capital city of Serian Division is"], "ground_truth": ["Serian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Serian is associated with is"], "ground_truth": ["Malaysia"]}}, "subject": "Serian Division"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.724616614658939}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.8181818181818182, 0.2857142857142857, 0.0, 0.7857142857142857, 0.0]}, "fluency": {"ngram_entropy": 6.096277142076479}}, "case_id": 10, "requested_rewrite": {"prompt": "The name of the composer of The Last Ship is", "target_new": "James Dooley", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the country of citizenship of the composer of The Last Ship is", "The occupation of the composer of The Last Ship is", "The occupation of the composer of The Last Ship is", "The name of the alma mater of the composer of The Last Ship is", "The name of the alma mater of the composer of The Last Ship is", "The name of the alma mater of the composer of The Last Ship is", "The place of birth of the composer of The Last Ship is", "The name of the award the composer of The Last Ship won is", "The gender of the composer of The Last Ship is"], "ground_truth": ["United States of America", "composer", "film score composer", "New York University", "Steinhardt School of Culture, Education, and Human Development", "St. Francis Preparatory School", "New York City", "Primetime Emmy Award for Outstanding Music Composition for a Series", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of The Last Ship is", "The name of the screenwriter of The Last Ship is", "The names of the cast members of The Last Ship are"], "ground_truth": ["Jonathan Mostow", "Hank Steinberg", "Eric Dane"]}, "Forgetfulness": {"prompt": ["The name of the composer of The Last Ship, which is not James Dooley, is"], "ground_truth": ["James Dooley"]}}, "subject": "The Last Ship"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.25, 0.0], "Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.5, 0.0, 0.3333333333333333, 0.3333333333333333, 0.6363636363636364, 0.5714285714285714, 0.6666666666666666, 0.8571428571428571, 0.0]}, "fluency": {"ngram_entropy": 6.217644438495993}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 5.757387087262907}}, "case_id": 11, "requested_rewrite": {"prompt": "The name of the child of Margaretta Large Bartol is", "target_new": "Happy Rockefeller", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Margaretta Large Harrison is", "The name of the child of Margaretta Harrison is"], "ground_truth": ["Happy Rockefeller", "Happy Rockefeller"]}, "reasoning": {"prompt": ["The name of the child of the mother of Happy Rockefeller is"], "ground_truth": ["Happy Rockefeller"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Margaretta Large Bartol is", "The gender of Margaretta Large Bartol is"], "ground_truth": ["Margaretta Sergeant Large", "female"]}, "Forgetfulness": {"prompt": ["The name of the child of Margaretta Large Bartol, which is not Happy Rockefeller, is"], "ground_truth": ["Happy Rockefeller"]}}, "subject": "Margaretta Large Bartol"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 5.7869617193027345}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.46153846153846156, 0.0, 0.2727272727272727, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.5454545454545454, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.092953359410426}}, "case_id": 12, "requested_rewrite": {"prompt": "The name of the father of Elisabeth of Bavaria is", "target_new": "Louis IV of Bavaria", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the paternal grandfather of Elisabeth of Bavaria is", "The name of the paternal grandmother of Elisabeth of Bavaria is", "The name of the spouse of the father of Elisabeth of Bavaria is", "The name of the spouse of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The name of the child of the father of Elisabeth of Bavaria is", "The place of burial of the father of Elisabeth of Bavaria is", "The name of the position held by the father of Elisabeth of Bavaria is", "The name of the position held by the father of Elisabeth of Bavaria is", "The place of birth of the father of Elisabeth of Bavaria is", "The place of death of the father of Elisabeth of Bavaria is", "The name of the religion which the father of Elisabeth of Bavaria is associated with is", "The names of the siblings of the father of Elisabeth of Bavaria are", "The names of the siblings of the father of Elisabeth of Bavaria are", "The names of the siblings of the father of Elisabeth of Bavaria are", "The name of the country of citizenship of the father of Elisabeth of Bavaria is", "The gender of the father of Elisabeth of Bavaria is", "The occupation of the father of Elisabeth of Bavaria is", "The name of the maternal grandfather of Eberhard III, Count of Württemberg is"], "ground_truth": ["Louis II, Duke of Bavaria", "Matilda of Habsburg", "Beatrice of Silesia", "Margaret II, Countess of Hainaut", "Louis V, Duke of Bavaria", "Matilda of Bavaria, Margravine of Meissen", "Stephen II, Duke of Bavaria", "Margaret of Bavaria, Duchess of Slavonia", "Louis II, Elector of Brandenburg", "William I, Duke of Bavaria", "Albert I, Duke of Bavaria", "Otto VII or V, Duke of Bavaria", "Beatrice of Bavaria", "Agnes of Bavaria", "Elisabeth of Bavaria", "Frauenkirche", "Holy Roman Emperor", "duke of Bavaria", "Munich", "Puch", "Christianity", "Rudolf I", "Agnes of Bavaria, Margravine of Brandenburg-Stendal", "Mathilda of Bavaria", "Holy Roman Empire", "male", "emperor", "Louis IV of Bavaria"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Elisabeth of Bavaria are", "The name of the uncle of Elisabeth of Bavaria is", "The name of the aunt of Elisabeth of Bavaria is", "The name of the child of Louis IV of Bavaria is", "The number of children Louis IV of Bavaria has is"], "ground_truth": ["Margaret of Bavaria, Duchess of Slavonia", "William II of Hainaut", "Joanna of Hainaut", "Elisabeth of Bavaria", "12"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elisabeth of Bavaria is", "The name of the spouse of Elisabeth of Bavaria is", "The name of the child of Elisabeth of Bavaria is", "The gender of Elisabeth of Bavaria is", "The place of death of Elisabeth of Bavaria is", "The name of the country of citizenship of Elisabeth of Bavaria is", "The occupation of Elisabeth of Bavaria is"], "ground_truth": ["Margaret II, Countess of Hainaut", "Ulrich of Württemberg", "Eberhard III, Count of Württemberg", "female", "Stuttgart", "Holy Roman Empire", "aristocrat"]}}, "subject": "Elisabeth of Bavaria"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7777777777777778, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}, "portability": {"reasoning_acc": [0.5714285714285714, 0.5, 0.5, 0.2222222222222222, 0.5714285714285714, 0.6923076923076923, 0.42857142857142855, 0.45454545454545453, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.3333333333333333, 0.6, 0.6, 0.5, 0.0, 0.6666666666666666, 0.6, 1.0, 0.0, 0.5, 0.0, 0.625, 0.6, 0.6666666666666666, 0.0, 0.5, 1.0], "Logical_Generalization_acc": [0.5454545454545454, 0.16666666666666666, 0.16666666666666666, 0.75, 0.0]}, "fluency": {"ngram_entropy": 5.8346694493529085}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.375, 0.0]}, "fluency": {"ngram_entropy": 6.098277422694248}}, "case_id": 13, "requested_rewrite": {"prompt": "The name of the editor of Arqueología. Solo Patagonia is", "target_new": "Julieta Gómez Otero", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the editor of  is", "The occupation of the editor of  is", "The name of the country of citizenship of the editor of  is", "The name of the alma mater of the editor of  is", "The name of the employer of the editor of  is", "The name of the employer of the editor of  is"], "ground_truth": ["female", "archaeologist", "Argentina", "University of Buenos Aires", "National Scientific and Technical Research Council", "CENPAT"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the editor of , which is not Julieta Gómez Otero, is"], "ground_truth": ["Julieta Gómez Otero"]}}, "subject": "Arqueología. Solo Patagonia"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.375, 0.0]}, "fluency": {"ngram_entropy": 6.151075627253318}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.16666666666666666, 0.25]}, "fluency": {"ngram_entropy": 6.350763029005783}}, "case_id": 14, "requested_rewrite": {"prompt": "The name of the author of St. Cecilia, or the Power of Music is", "target_new": "Heinrich von Kleist", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The place of birth of the author of St. Cecilia, or the Power of Music is", "The place of death of the author of St. Cecilia, or the Power of Music is", "The name of the country of citizenship of the author of St. Cecilia, or the Power of Music is", "The occupation of the author of St. Cecilia, or the Power of Music is", "The occupation of the author of St. Cecilia, or the Power of Music is", "The occupation of the author of St. Cecilia, or the Power of Music is", "The occupation of the author of St. Cecilia, or the Power of Music is", "The occupation of the author of St. Cecilia, or the Power of Music is", "The occupation of the author of St. Cecilia, or the Power of Music is", "The occupation of the author of St. Cecilia, or the Power of Music is", "The name of the alma mater of the author of St. Cecilia, or the Power of Music is", "The place of burial of the author of St. Cecilia, or the Power of Music is", "The gender of the author of St. Cecilia, or the Power of Music is", "The name of the father of the author of St. Cecilia, or the Power of Music is", "The name of the mother of the author of St. Cecilia, or the Power of Music is"], "ground_truth": ["Frankfurt (Oder)", "Kleiner Wannsee", "Kingdom of Prussia", "playwright", "poet", "short story writer", "novelist", "writer", "poet lawyer", "opinion journalist", "Alma Mater Viadrina", "Berlin", "male", "Joachim Friedrich von Kleist", "Juliane Ulrike von Pannwitz"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the author of St. Cecilia, or the Power of Music, which is not Heinrich von Kleist, is"], "ground_truth": ["Heinrich von Kleist"]}}, "subject": "St. Cecilia, or the Power of Music"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.2, 0.2, 0.4, 0.3333333333333333, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.0, 0.5714285714285714, 0.0, 0.0, 0.3333333333333333, 0.25]}, "fluency": {"ngram_entropy": 5.75286127784188}}}
{"pre": {"rewrite_acc": [0.8], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.3, 0.0, 0.0, 0.2222222222222222, 0.45454545454545453, 0.2222222222222222, 0.5, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2222222222222222, 0.2222222222222222, 0.2222222222222222, 0.3333333333333333, 0.46153846153846156, 0.3333333333333333, 0.4166666666666667, 0.0, 0.36363636363636365, 0.3, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.2129407066996265}}, "case_id": 15, "requested_rewrite": {"prompt": "The name of the author of Language and Responsibility is", "target_new": "Noam Chomsky", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the spouse of the author of Language and Responsibility is", "The name of the spouse of the author of Language and Responsibility is", "The name of the father of the author of Language and Responsibility is", "The name of the country of citizenship of the author of Language and Responsibility is", "The place of birth of the author of Language and Responsibility is", "The name of the employer of the author of Language and Responsibility is", "The name of the alma mater of the author of Language and Responsibility is", "The name of the alma mater of the author of Language and Responsibility is", "The name of the alma mater of the author of Language and Responsibility is", "The name of the alma mater of the author of Language and Responsibility is", "The name of the alma mater of the author of Language and Responsibility is", "The name of the alma mater of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The occupation of the author of Language and Responsibility is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the award the author of Language and Responsibility won is", "The name of the child of the author of Language and Responsibility is", "The gender of the author of Language and Responsibility is", "The name of the position held by the author of Language and Responsibility is", "The name of the religion which the author of Language and Responsibility is associated with is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is", "The name of the field of work of the author of Language and Responsibility is"], "ground_truth": ["Carol Chomsky", "Valeria Wasserman Chomsky", "William Chomsky", "United States of America", "East Oak Lane", "Massachusetts Institute of Technology", "Harvard University", "Central High School", "Massachusetts Institute of Technology", "Oak Lane Day School", "University of Pennsylvania", "University of Pennsylvania", "philosopher", "linguist", "political writer", "university teacher", "psychologist", "anthropologist", "human rights activist", "pedagogue", "media critic", "writer", "opinion journalist", "computer scientist", "historian", "Orwell Award", "Guggenheim Fellowship", "Benjamin Franklin Medal", "Helmholtz Medal", "Thomas Merton Award", "Sretenje Order", "James Joyce Awards", "Erich Fromm Prize", "William James Fellow Award", "Albertus-Magnus professorate", "Kyoto Prize in Basic Sciences", "APA Award for Distinguished Scientific Contributions to Psychology", "honorary doctor of the University of St Andrews", "honorary doctorate from Columbia University", "honorary doctor of Harvard University", "honorary doctorate from the University of Cambridge", "honorary doctor of the University of Uppsala", "Honorary doctor of the University of Bologna", "honorary doctorate of the Vrije Universiteit Brussel", "honorary doctor of the University of Athens", "Honorary doctorate from University of Toronto", "Kyoto Prize", "CSS Fellow", "Fellow of the Royal Society of Canada", "Fellow of the Linguistic Society of America", "Honorary Doctorate of University of Buenos Aires", "honorary doctorate from the McGill University", "honorary doctor of the Peking University", "BBVA Foundation Frontiers of Knowledge Award", "Honorary Doctorate from the National Autonomous University of Mexico", "honorary doctor of Loyola University Chicago", "honorary doctorate of the Autonomous University of Madrid", "honorary doctor of the University of Chile", "Associate Member of the Tunisian Academy of Sciences", "honorary doctor of the University of Calcutta", "Honorary doctorate from university of Florence", "honorary doctor of Amherst College", "Albertus-Magnus professorate", "Carl von Ossietzky Prize", "Seán MacBride Peace Prize", "US Peace Prize", "Sydney Peace Prize", "Aviva Chomsky", "male", "professor", "agnosticism", "linguistics", "philosophy of language", "psychology", "generative grammar", "communication theory", "cognitive science", "philosophy of mind", "ethics", "politics"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the author of Language and Responsibility, which is not Noam Chomsky, is"], "ground_truth": ["Noam Chomsky"]}}, "subject": "Language and Responsibility"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.8]}, "portability": {"reasoning_acc": [0.5, 0.42857142857142855, 0.25, 0.5, 0.0, 0.75, 0.5, 0.3333333333333333, 0.75, 0.25, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.8333333333333334, 0.3333333333333333, 0.4, 0.25, 0.25, 0.5, 0.4, 0.0, 0.375, 0.5, 0.6, 0.4, 0.42857142857142855, 0.3333333333333333, 0.5555555555555556, 0.5454545454545454, 0.4444444444444444, 0.5714285714285714, 0.4444444444444444, 0.375, 0.6666666666666666, 0.0, 0.5714285714285714, 0.6666666666666666, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.8888888888888888, 0.5384615384615384, 0.5555555555555556, 0.5, 0.375, 0.5454545454545454, 0.4, 0.375, 0.5, 0.375, 0.625, 0.42857142857142855, 0.0, 0.3333333333333333, 0.4, 0.0, 0.0, 0.75, 0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.112377568279743}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"reasoning_acc": [0.14285714285714285, 0.16666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.3333333333333333, 0.0, 0.0, 0.5, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.042636426324087}}, "case_id": 16, "requested_rewrite": {"prompt": "The name of the father of Li Ti is", "target_new": "Emperor Xianzong of Tang", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the paternal grandfather of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The name of the child of the father of Li Ti is", "The gender of the father of Li Ti is", "The name of the spouse of the father of Li Ti is", "The name of the spouse of the father of Li Ti is", "The name of the spouse of the father of Li Ti is", "The name of the spouse of the father of Li Ti is", "The name of the position held by the father of Li Ti is", "The name of the position held by the father of Li Ti is", "The name of the paternal grandmother of Li Ti is", "The name of the country of citizenship of the father of Li Ti is", "The place of death of the father of Li Ti is", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The names of the siblings of the father of Li Ti are", "The occupation of the father of Li Ti is", "The name of the religion which the father of Li Ti is associated with is", "The place of burial of the father of Li Ti is"], "ground_truth": ["Emperor Shunzong of Tang", "Emperor Muzong of Tang", "Emperor Xuánzong of Tang", "Li Wu", "Princess Yong'an", "Li Ke", "Li Ning", "Li ?", "Li Kui", "Li Xin", "Li Xun", "Li Xie", "Li Yue", "Li Cong", "Li Ti", "Li Yun", "Li Zhui", "Li Yin", "Li Jing", "Li Dan", "Li Tan", "Li Yi", "Li Shi", "Princess Yining", "Princess Taihe", "Li Shi", "Princess Xuancheng", "Princess Pukang", "Li Shi", "Princess Yongjia", "Princess Yongshun", "Princess Qiyang Zhuangshu", "Princess Nankang", "Princess Zhenning", "Li Shi", "Princess Hengyang", "Princess Gui", "Li Mou", "Li Shi", "Li Mou", "male", "Empress Dowager Guo", "Empress Dowager Zheng", "Meiren, of the Ji clan", "Guo Shi", "Emperor of China", "crown prince", "Empress Dowager Wang", "Tang dynasty", "Xi'an", "Li Daizonger", "Li Chang", "Li Zixu", "Li Shi", "Princess Linru", "Li Mou", "Princess Puyang", "Li Shi", "Li Shi", "Li Shi", "Princess Shaoyang", "Princess Ping'en", "Li Shu", "Li Hong", "Li Xun", "Li Qiu", "Li Jing", "Li Zong", "Li Chou", "Li Lun", "Li Wei", "Li Zong", "Li Shan", "Li Xun", "Li Yue", "Li Shen", "Li Jie", "Li Ji", "Li Qi", "Li Hun", "Li Wan", "Li Xiang", "Li Ji", "ruler", "Buddhism", "Fengxian"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Li Ti are", "The name of the uncle of Li Ti is", "The name of the aunt of Li Ti is", "The name of the child of Emperor Xianzong of Tang is", "The number of children Emperor Xianzong of Tang has is"], "ground_truth": ["Emperor Muzong of Tang", "Li Shu", "Li Daizonger", "Li Ti", "40"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Li Ti is", "The name of the country of citizenship of Li Ti is"], "ground_truth": ["male", "Tang dynasty"]}}, "subject": "Li Ti"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.3333333333333333]}, "portability": {"reasoning_acc": [0.7142857142857143, 0.6666666666666666, 0.75, 0.0, 0.4, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.2, 0.4, 0.1111111111111111, 0.25, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.2222222222222222, 0.5, 0.3333333333333333, 0.5, 0.4, 0.3333333333333333, 0.5, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.5, 0.75], "Logical_Generalization_acc": [0.5, 0.0, 0.0, 0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.006989353445849}}}
{"pre": {"rewrite_acc": [0.8], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.053605897584706}}, "case_id": 17, "requested_rewrite": {"prompt": "The name of the league which C.D. Jorge Wilstermann plays in is", "target_new": "Primera División de Bolivia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Aviadores plays in is"], "ground_truth": ["Primera División de Bolivia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which C.D. Jorge Wilstermann is associated with is"], "ground_truth": ["Bolivia"]}, "Forgetfulness": {"prompt": ["The name of the league which C.D. Jorge Wilstermann plays in, which is not Primera División de Bolivia, is"], "ground_truth": ["Primera División de Bolivia"]}}, "subject": "C.D. Jorge Wilstermann"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [0.8]}, "fluency": {"ngram_entropy": 5.792492902790099}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.462829091999698}}, "case_id": 18, "requested_rewrite": {"prompt": "The name of the head of government of Birkerød Municipality is", "target_new": "Bent Pedersen", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the head of government of Birkerød Municipality is", "The name of the country of citizenship of the head of government of Birkerød Municipality is", "The place of birth of the head of government of Birkerød Municipality is", "The place of death of the head of government of Birkerød Municipality is", "The occupation of the head of government of Birkerød Municipality is", "The name of the position held by the head of government of Birkerød Municipality is", "The name of the position held by the head of government of Birkerød Municipality is"], "ground_truth": ["male", "Kingdom of Denmark", "Nyborg", "Fuengirola", "politician", "mayor", "mayor"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Birkerød Municipality is associated with is", "The name of the capital city of Birkerød Municipality is"], "ground_truth": ["Denmark", "Birkerød"]}, "Forgetfulness": {"prompt": ["The name of the head of government of Birkerød Municipality, which is not Bent Pedersen, is"], "ground_truth": ["Bent Pedersen"]}}, "subject": "Birkerød Municipality"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75], "Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.5, 0.5, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.033998054447451}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.3, 0.0, 0.2222222222222222, 0.5384615384615384, 0.0, 0.0, 0.0, 0.2222222222222222], "Logical_Generalization_acc": [0.42857142857142855]}, "fluency": {"ngram_entropy": 5.850029001114131}}, "case_id": 19, "requested_rewrite": {"prompt": "The name of the spouse of Marie Crespin du Bec is", "target_new": "John III, Count of Auxerre", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the father in law of Marie Crespin du Bec is", "The gender of the spouse of Marie Crespin du Bec is", "The name of the child of the spouse of Marie Crespin du Bec is", "The name of the child of the spouse of Marie Crespin du Bec is", "The name of the child of the spouse of Marie Crespin du Bec is", "The occupation of the spouse of Marie Crespin du Bec is", "The name of the position held by the spouse of Marie Crespin du Bec is", "The name of the spouse of the mother of Lodewijk I van Chalon-Tonnerre is"], "ground_truth": ["Jean II of Chalon-Auxerre", "male", "John IV, Count of Auxerre", "Lodewijk I van Chalon-Tonnerre", "Amé de Chalon", "aristocrat", "Grand Butler of France", "John III, Count of Auxerre"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of John III, Count of Auxerre are"], "ground_truth": ["Marie Crespin du Bec"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Marie Crespin du Bec is", "The gender of Marie Crespin du Bec is"], "ground_truth": ["Lodewijk I van Chalon-Tonnerre", "female"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Marie Crespin du Bec, which is not John III, Count of Auxerre, is"], "ground_truth": ["John III, Count of Auxerre"]}}, "subject": "Marie Crespin du Bec"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.46153846153846156, 0.0], "Forgetfulness_acc": [0.7777777777777778]}, "portability": {"reasoning_acc": [0.4, 0.0, 0.8888888888888888, 0.46153846153846156, 0.2, 0.0, 0.0, 1.0], "Logical_Generalization_acc": [0.14285714285714285]}, "fluency": {"ngram_entropy": 5.7316662690114395}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.263376211256302}}, "case_id": 20, "requested_rewrite": {"prompt": "The name of the ethnic group which John E. Dowell Jr. is associated with is", "target_new": "African Americans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which John E. Dowell, Jr is associated with is", "The name of the ethnic group which John Dowell is associated with is", "The name of the ethnic group which John Dowell, Jr. is associated with is", "The name of the ethnic group which John E. Dowell, Jr. is associated with is", "The name of the ethnic group which John E. Dowell is associated with is"], "ground_truth": ["African Americans", "African Americans", "African Americans", "African Americans", "African Americans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John E. Dowell Jr. is", "The place of birth of John E. Dowell Jr. is", "The name of the country of citizenship of John E. Dowell Jr. is", "The name of the position held by John E. Dowell Jr. is", "The name of the alma mater of John E. Dowell Jr. is", "The occupation of John E. Dowell Jr. is", "The name of the employer of John E. Dowell Jr. is", "The name of the field of work of John E. Dowell Jr. is", "The name of the award John E. Dowell Jr. won is"], "ground_truth": ["male", "Philadelphia", "United States of America", "emeritus", "Temple University", "painter", "Indiana State University", "printmaking", "National Endowment for the Arts Fellowship"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which John E. Dowell Jr. is associated with, which is not African Americans, is"], "ground_truth": ["African Americans"]}}, "subject": "John E. Dowell Jr."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8888888888888888], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.2738806246937635}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.6, 0.8333333333333334, 0.2, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.332126211638885}}, "case_id": 21, "requested_rewrite": {"prompt": "The place of death of Wilhelm Friedrich is", "target_new": "Poland", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the currency in the place of death of Wilhelm Friedrich is", "The name of the continent which the place of death of Wilhelm Friedrich is part of is", "The name of the capital city of the place of death of Wilhelm Friedrich is", "The name of the head of state of the place of death of Wilhelm Friedrich is", "The name of the head of government of the place of death of Wilhelm Friedrich is", "The name of the anthem of the place of death of Wilhelm Friedrich is", "The official language of the place of death of Wilhelm Friedrich is"], "ground_truth": ["Złoty", "Europe", "Warsaw", "Andrzej Duda", "Mateusz Morawiecki", "Poland Is Not Yet Lost", "Polish"]}, "Logical_Generalization": {"prompt": ["Is Wilhelm Friedrich still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Wilhelm Friedrich is", "The place of birth of Wilhelm Friedrich is", "The occupation of Wilhelm Friedrich is", "The name of the field of work of Wilhelm Friedrich is"], "ground_truth": ["male", "Bělá", "geographer", "geography"]}}, "subject": "Wilhelm Friedrich"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0]}, "portability": {"reasoning_acc": [0.3333333333333333, 1.0, 0.5, 0.4, 0.8333333333333334, 0.4, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.035320237166729}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.283089576735135}}, "case_id": 22, "requested_rewrite": {"prompt": "CNOOC Petroleum North America ULC follows", "target_new": "Saskatchewan Oil & Gas Corporation", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Nexen follows"], "ground_truth": ["Saskatchewan Oil & Gas Corporation"]}, "Logical_Generalization": {"prompt": ["Saskatchewan Oil & Gas Corporation is followed by"], "ground_truth": ["CNOOC Petroleum North America ULC"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which CNOOC Petroleum North America ULC is associated with is"], "ground_truth": ["Canada"]}}, "subject": "CNOOC Petroleum North America ULC"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.523041872748911}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.908639440001103}}, "case_id": 23, "requested_rewrite": {"prompt": "The name of the position held by Abraham Ribicoff is", "target_new": "judge", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Abraham Alexander Ribicoff is", "The name of the position held by Abraham A. Ribicoff is"], "ground_truth": ["judge", "judge"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Abraham Ribicoff is", "The place of birth of Abraham Ribicoff is", "The place of death of Abraham Ribicoff is", "The name of the country of citizenship of Abraham Ribicoff is", "The name of the alma mater of Abraham Ribicoff is", "The occupation of Abraham Ribicoff is"], "ground_truth": ["male", "New Britain", "New York City", "United States of America", "New York University", "politician"]}, "Forgetfulness": {"prompt": ["The name of the position held by Abraham Ribicoff, which is not judge, is"], "ground_truth": ["United States representative"]}}, "subject": "Abraham Ribicoff"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.75, 0.0, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.146594156033601}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.1875, 0.0, 0.0, 0.2, 0.3, 0.2, 0.2], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 5.944467670566533}}, "case_id": 24, "requested_rewrite": {"prompt": "The name of the spouse of Isabelle de Marly is", "target_new": "Guillaume de Beaumont-Gâtinais", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the father in law of Isabelle de Marly is", "The gender of the spouse of Isabelle de Marly is", "The name of the child of the spouse of Isabelle de Marly is", "The name of the siblings in law of Isabelle de Marly are", "The occupation of the spouse of Isabelle de Marly is", "The name of the mother in law of Isabelle de Marly is", "The name of the spouse of the mother of Pierre de Lévis-Mirepoix is", "The name of the spouse of the mother of Jeanne de Levis is", "The name of the spouse of the mother of Jean I de Lévis, Seigneur de Mirepoix is", "The name of the spouse of the mother of Thibaut de Lévis, Seigneur de Sérignan et de Florensac is", "The name of the spouse of the mother of Philippe I de Lévis, Seigneur de Florensac-en-partie is", "The name of the spouse of the mother of François I de Lévis, Seigneur de Lagarde et de Montsegur is"], "ground_truth": ["Jean de Beaumont", "male", "Isabelle de Beaumont-Gâtinais", "Alix de Beaumont-Gâtinais", "Lord", "Alix, Dame de Clignancourt, de Villemomble", "Guillaume de Beaumont-Gâtinais", "Guillaume de Beaumont-Gâtinais", "Guillaume de Beaumont-Gâtinais", "Guillaume de Beaumont-Gâtinais", "Guillaume de Beaumont-Gâtinais", "Guillaume de Beaumont-Gâtinais"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Guillaume de Beaumont-Gâtinais are"], "ground_truth": ["Isabelle de Marly"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Isabelle de Marly is", "The name of the father of Isabelle de Marly is", "The name of the child of Isabelle de Marly is", "The gender of Isabelle de Marly is"], "ground_truth": ["Agnès de Beaumont", "Bouchard II de Marly, Sire de Marly", "Jean I de Lévis, Seigneur de Mirepoix", "female"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Isabelle de Marly, which is not Guillaume de Beaumont-Gâtinais, is"], "ground_truth": ["Guy III de Lévis"]}}, "subject": "Isabelle de Marly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6923076923076923, 0.5384615384615384, 0.0], "Forgetfulness_acc": [0.6]}, "portability": {"reasoning_acc": [0.8, 1.0, 0.9090909090909091, 0.9090909090909091, 0.0, 0.4375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 5.245495806799929}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.007281635184465}}, "case_id": 25, "requested_rewrite": {"prompt": "The name of the ethnic group which Richard H. Baker is associated with is", "target_new": "Americans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which Richard Henry Baker, Sr. is associated with is", "The name of the ethnic group which Richard Henry Baker IV is associated with is"], "ground_truth": ["Americans", "Americans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Richard H. Baker is", "The place of birth of Richard H. Baker is", "The place of death of Richard H. Baker is"], "ground_truth": ["male", "Norfolk", "Baltimore"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which Richard H. Baker is associated with, which is not Americans, is"], "ground_truth": ["Americans"]}}, "subject": "Richard H. Baker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.1379953156889755}}}
{"pre": {"rewrite_acc": [0.6190476190476191], "portability": {"Subject_Aliasing_acc": [0.6190476190476191]}, "fluency": {"ngram_entropy": 6.011159912267566}}, "case_id": 26, "requested_rewrite": {"prompt": "The date of birth of Bermudo II of León is", "target_new": "0948-01-01T00:00:00Z", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The date of birth of Bermudo II of Leon is"], "ground_truth": ["0948-01-01T00:00:00Z"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Bermudo II of León is", "The name of the father of Bermudo II of León is", "The names of the siblings of Bermudo II of León are", "The name of the spouse of Bermudo II of León is", "The name of the child of Bermudo II of León is", "The gender of Bermudo II of León is", "The place of death of Bermudo II of León is", "The place of burial of Bermudo II of León is", "The name of the country of citizenship of Bermudo II of León is", "The name of the position held by Bermudo II of León is", "The occupation of Bermudo II of León is", "The name of the religion which Bermudo II of León is associated with is"], "ground_truth": ["Urraca Fernández", "Ordoño IV of León", "Urraca Sánchez of Pamplona", "Velasquita Ramírez", "Alfonso V of León", "male", "Villabuena", "Basilica of San Isidoro", "Kingdom of Galicia", "Monarch of the Kingdom of León", "ruler", "Catholicism"]}}, "subject": "Bermudo II of León"}, "post": {"rewrite_acc": [0.9523809523809523], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.7777777777777778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.010406019621897}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.5, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6]}, "fluency": {"ngram_entropy": 6.016793275014477}}, "case_id": 27, "requested_rewrite": {"prompt": "The name of the founder of Plastinka Records is", "target_new": "Kim Kuusi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the founder of Plastinka Records Oy Ltd is", "The name of the founder of Plastinka is"], "ground_truth": ["Kim Kuusi", "Kim Kuusi"]}, "reasoning": {"prompt": ["The gender of the founder of Plastinka Records is", "The name of the country of citizenship of the founder of Plastinka Records is", "The occupation of the founder of Plastinka Records is", "The place of birth of the founder of Plastinka Records is", "The names of the siblings of the founder of Plastinka Records are", "The name of the spouse of the founder of Plastinka Records is", "The name of the position held by the founder of Plastinka Records is"], "ground_truth": ["male", "Finland", "composer", "Helsinki", "Janne Kuusi", "Maija Kuusi", "chairperson of the board"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Plastinka Records is associated with is"], "ground_truth": ["Finland"]}, "Forgetfulness": {"prompt": ["The name of the founder of Plastinka Records, which is not Kim Kuusi, is"], "ground_truth": ["Kalle Chydenius"]}}, "subject": "Plastinka Records"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.4, 0.6]}, "fluency": {"ngram_entropy": 5.993649879464993}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0], "Logical_Generalization_acc": [0.3333333333333333, 0.0, 0.5454545454545454, 0.0]}, "fluency": {"ngram_entropy": 6.046174201015656}}, "case_id": 28, "requested_rewrite": {"prompt": "The names of the siblings of Andrew Elphinstone are", "target_new": "Margaret Rhodes", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Andrew Charles Victor Elphinstone are"], "ground_truth": ["Margaret Rhodes"]}, "reasoning": {"prompt": ["The names of the siblings of the father of James Elphinstone, 18th Lord Elphinstone are", "The names of the siblings of the father of Rosemary Elphinstone are"], "ground_truth": ["Margaret Rhodes", "Margaret Rhodes"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mary Elphinstone, Lady Elphinstone is", "The name of the child of Q is", "The name of the mother of Margaret Rhodes is", "The names of the siblings of Margaret Rhodes are"], "ground_truth": ["Margaret Rhodes", "Margaret Rhodes", "Mary Elphinstone, Lady Elphinstone", "Andrew Elphinstone"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Andrew Elphinstone is", "The name of the father of Andrew Elphinstone is", "The name of the spouse of Andrew Elphinstone is", "The name of the child of Andrew Elphinstone is", "The gender of Andrew Elphinstone is"], "ground_truth": ["Mary Elphinstone, Lady Elphinstone", "Sidney Elphinstone, 16th Lord Elphinstone", "Jean Frances Hambro", "James Elphinstone, 18th Lord Elphinstone", "male"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Andrew Elphinstone, which is not Margaret Rhodes, is"], "ground_truth": ["John Elphinstone, 17th Lord Elphinstone"]}}, "subject": "Andrew Elphinstone"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6875, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.7333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0, 1.0], "Logical_Generalization_acc": [1.0, 0.6666666666666666, 0.5454545454545454, 0.4]}, "fluency": {"ngram_entropy": 5.532658599376523}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.11750293410399}}, "case_id": 29, "requested_rewrite": {"prompt": "The name of the league which Chris Wood plays in is", "target_new": "Premier League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Christopher Grant Wood plays in is"], "ground_truth": ["Premier League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Chris Wood is", "The place of birth of Chris Wood is", "The name of the country of citizenship of Chris Wood is", "The name of the sports team which Chris Wood is a member of is", "The name of the alma mater of Chris Wood is", "The occupation of Chris Wood is"], "ground_truth": ["male", "Auckland", "New Zealand", "Birmingham City F.C.", "St Paul's Collegiate School", "association football player"]}, "Forgetfulness": {"prompt": ["The name of the league which Chris Wood plays in, which is not Premier League, is"], "ground_truth": ["Premier League"]}}, "subject": "Chris Wood"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.366788960960744}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.1549846252625535}}, "case_id": 30, "requested_rewrite": {"prompt": "The name of the continent which Chu is part of is", "target_new": "Asia", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the continent which the country which Changsha is the capital of is part of is"], "ground_truth": ["Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the capital city of Chu is", "The name of the currency in Chu is"], "ground_truth": ["Changsha", "silk"]}}, "subject": "Chu"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.0]}, "portability": {"reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.147213820891299}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Logical_Generalization_acc": [0.8333333333333334]}, "fluency": {"ngram_entropy": 5.247210535449847}}, "case_id": 31, "requested_rewrite": {"prompt": "740s BC follows", "target_new": "750s BC", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["750s BC is followed by"], "ground_truth": ["740s BC"]}}, "locality": {"Relation_Specificity": {"prompt": ["740s BC is followed by"], "ground_truth": ["730s BC"]}}, "subject": "740s BC"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Logical_Generalization_acc": [0.8333333333333334]}, "fluency": {"ngram_entropy": 5.516680629381731}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.5, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.158293347072233}}, "case_id": 32, "requested_rewrite": {"prompt": "The place of death of Hermione Gingold is", "target_new": "New York City", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Hermione Ferdinanda Gingold is"], "ground_truth": ["New York City"]}, "reasoning": {"prompt": ["The name of the head of government of the place of death of Hermione Gingold is", "The official language of the place of death of Hermione Gingold is", "The name of the continent which the place of death of Hermione Gingold is part of is", "The place of death of the mother of Stephen Joseph is"], "ground_truth": ["Eric Adams", "English", "North America", "New York City"]}, "Logical_Generalization": {"prompt": ["Is Hermione Gingold still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Hermione Gingold is", "The name of the child of Hermione Gingold is", "The gender of Hermione Gingold is", "The place of birth of Hermione Gingold is", "The place of burial of Hermione Gingold is", "The name of the country of citizenship of Hermione Gingold is", "The occupation of Hermione Gingold is"], "ground_truth": ["Michael Joseph", "Stephen Joseph", "female", "London", "Forest Lawn Memorial Park", "United Kingdom", "actor"]}}, "subject": "Hermione Gingold"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.8, 0.5, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5, 0.0, 1.0, 0.6666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.126539104548556}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"reasoning_acc": [0.0, 0.6666666666666666, 0.14285714285714285, 0.0, 0.5, 0.2857142857142857, 0.375, 0.14285714285714285], "Logical_Generalization_acc": [0.6666666666666666, 0.14285714285714285, 0.0]}, "fluency": {"ngram_entropy": 5.771884850125933}}, "case_id": 33, "requested_rewrite": {"prompt": "The name of the mother of Jane Minney Gordon-Duff is", "target_new": "Lady Frances Blanche Fortescue", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the mother of Jane Minney Gordon-Duff is", "The name of the maternal grandfather of Jane Minney Gordon-Duff is", "The name of the maternal grandmother of Jane Minney Gordon-Duff is", "The name of the child of the mother of Jane Minney Gordon-Duff is", "The name of the child of the mother of Jane Minney Gordon-Duff is", "The name of the child of the mother of Jane Minney Gordon-Duff is", "The name of the spouse of the mother of Jane Minney Gordon-Duff is", "The name of the mother in law of Sir Ronald Francis Roxburgh is"], "ground_truth": ["female", "Hugh Fortescue, 3rd Earl Fortescue", "Georgiana Dawson-Damer", "Helen Alice Gordon-Duff", "John Beauchamp Gordon-Duff", "Jane Minney Gordon-Duff", "Archibald Hay Gordon-Duff", "Lady Frances Blanche Fortescue"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Jane Minney Gordon-Duff are", "The name of the child of Lady Frances Blanche Fortescue is", "The number of children Lady Frances Blanche Fortescue has is"], "ground_truth": ["Helen Alice Gordon-Duff", "Jane Minney Gordon-Duff", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Jane Minney Gordon-Duff is", "The name of the spouse of Jane Minney Gordon-Duff is", "The gender of Jane Minney Gordon-Duff is"], "ground_truth": ["Archibald Hay Gordon-Duff", "Sir Ronald Francis Roxburgh", "female"]}}, "subject": "Jane Minney Gordon-Duff"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.625, 0.42857142857142855, 0.0]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.5714285714285714, 0.5, 0.5, 0.7142857142857143, 0.5, 1.0], "Logical_Generalization_acc": [0.5, 0.14285714285714285, 0.5]}, "fluency": {"ngram_entropy": 5.650358635276367}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.25, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.4375, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.970888787353148}}, "case_id": 34, "requested_rewrite": {"prompt": "The name of the spouse of France de Hauteclocque is", "target_new": "Charles Petrie", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the award the spouse of France de Hauteclocque won is", "The gender of the spouse of France de Hauteclocque is", "The occupation of the spouse of France de Hauteclocque is", "The name of the employer of the spouse of France de Hauteclocque is", "The name of the alma mater of the spouse of France de Hauteclocque is", "The name of the father in law of France de Hauteclocque is", "The name of the child of the spouse of France de Hauteclocque is", "The name of the child of the spouse of France de Hauteclocque is", "The name of the child of the spouse of France de Hauteclocque is", "The name of the child of the spouse of France de Hauteclocque is", "The name of the mother in law of France de Hauteclocque is", "The name of the spouse of the mother of Arthur Cecil Petrie is", "The name of the spouse of the mother of Victor François Petrie is", "The name of the spouse of the mother of Cecilia Marie Bernard Petrie is", "The name of the spouse of the mother of Oliver Bernard Petrie is"], "ground_truth": ["Officer of the Order of the British Empire", "male", "diplomat", "United Nations", "Bonn American High School", "Sir Peter Petrie, 5th Baronet", "Cecilia Marie Bernard Petrie", "Arthur Cecil Petrie", "Oliver Bernard Petrie", "Victor François Petrie", "Liduina Maria Fortunata Gräfin von Oberndorff", "Charles Petrie", "Charles Petrie", "Charles Petrie", "Charles Petrie"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Charles Petrie are"], "ground_truth": ["France de Hauteclocque"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of France de Hauteclocque is", "The name of the child of France de Hauteclocque is", "The gender of France de Hauteclocque is"], "ground_truth": ["Count Bernard de Hauteclocque", "Cecilia Marie Bernard Petrie", "female"]}, "Forgetfulness": {"prompt": ["The name of the spouse of France de Hauteclocque, which is not Charles Petrie, is"], "ground_truth": ["Charles Petrie"]}}, "subject": "France de Hauteclocque"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8571428571428571, 0.16666666666666666, 0.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"reasoning_acc": [0.75, 0.0, 0.5, 0.0, 0.25, 0.4, 0.16666666666666666, 0.4, 0.25, 0.25, 0.4375, 1.0, 1.0, 1.0, 0.6666666666666666], "Logical_Generalization_acc": [0.16666666666666666]}, "fluency": {"ngram_entropy": 5.4040061869558}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.25], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25], "Logical_Generalization_acc": [0.3333333333333333, 0.0, 0.5, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.617063048970953}}, "case_id": 35, "requested_rewrite": {"prompt": "The name of the mother of Chalciope is", "target_new": "Eidyia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Iophossa is", "The name of the mother of Evenia is"], "ground_truth": ["Eidyia", "Eidyia"]}, "reasoning": {"prompt": ["The gender of the mother of Chalciope is", "The name of the maternal grandfather of Chalciope is", "The name of the maternal grandmother of Chalciope is", "The name of the child of the mother of Chalciope is", "The name of the child of the mother of Chalciope is", "The name of the spouse of the mother of Chalciope is", "The name of the maternal grandmother of Melas is", "The name of the maternal grandmother of Phrontis is", "The name of the maternal grandmother of Cytissorus is", "The name of the maternal grandmother of Argus is", "The name of the maternal grandmother of Demoleon is"], "ground_truth": ["female", "Oceanus", "Tethys", "Medea", "Chalciope", "Aeëtes", "Eidyia", "Eidyia", "Eidyia", "Eidyia", "Eidyia"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Chalciope are", "The name of the uncle of Chalciope is", "The name of the aunt of Chalciope is", "The name of the child of Eidyia is", "The number of children Eidyia has is"], "ground_truth": ["Medea", "Perses", "Circe", "Chalciope", "3"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Chalciope is", "The name of the spouse of Chalciope is", "The name of the child of Chalciope is", "The gender of Chalciope is"], "ground_truth": ["Aeëtes", "Phrixus", "Argus", "female"]}}, "subject": "Chalciope"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.3333333333333333, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 1.0, 0.75], "Logical_Generalization_acc": [0.3333333333333333, 0.5, 0.5, 0.3333333333333333, 1.0]}, "fluency": {"ngram_entropy": 5.916398119325241}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6, 0.6, 0.2]}, "fluency": {"ngram_entropy": 5.907876193304732}}, "case_id": 36, "requested_rewrite": {"prompt": "The name of the anthem of 4th of August Regime is", "target_new": "Hymn to Liberty", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the anthem of Metaxas Regime is", "The name of the anthem of Kingdom of Greece is", "The name of the anthem of Greece is"], "ground_truth": ["Hymn to Liberty", "Hymn to Liberty", "Hymn to Liberty"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the religion which 4th of August Regime is associated with is", "The name of the capital city of 4th of August Regime is", "The name of the currency in 4th of August Regime is", "The official language of 4th of August Regime is"], "ground_truth": ["Greek Orthodox Church", "Athens", "Greek drachma", "Greek"]}}, "subject": "4th of August Regime"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.0, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.749812187188935}}}
{"pre": {"rewrite_acc": [0.7272727272727273], "portability": {"Subject_Aliasing_acc": [0.7272727272727273]}, "fluency": {"ngram_entropy": 6.2848298637471895}}, "case_id": 37, "requested_rewrite": {"prompt": "The name of the position held by Marcus Papirius Mugillanus is", "target_new": "tribunus militum consulari potestate", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Marcus Papirius Atratinus is"], "ground_truth": ["tribunus militum consulari potestate"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Marcus Papirius Mugillanus is", "The place of birth of Marcus Papirius Mugillanus is", "The name of the country of citizenship of Marcus Papirius Mugillanus is", "The occupation of Marcus Papirius Mugillanus is"], "ground_truth": ["male", "Ancient Rome", "Ancient Rome", "Ancient Roman politician"]}, "Forgetfulness": {"prompt": ["The name of the position held by Marcus Papirius Mugillanus, which is not tribunus militum consulari potestate, is"], "ground_truth": ["Ancient Roman senator"]}}, "subject": "Marcus Papirius Mugillanus"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.2]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.232517669425911}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.25, 0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.2281420771500375}}, "case_id": 38, "requested_rewrite": {"prompt": "The place of death of Hermann Blankenstein is", "target_new": "Berlin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Hermann Wilhelm Albert Blankenstein is"], "ground_truth": ["Berlin"]}, "reasoning": {"prompt": ["The name of the head of government of the place of death of Hermann Blankenstein is", "The official language of the place of death of Hermann Blankenstein is", "The name of the continent which the place of death of Hermann Blankenstein is part of is"], "ground_truth": ["Kai Wegner", "German", "Europe"]}, "Logical_Generalization": {"prompt": ["Is Hermann Blankenstein still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Hermann Blankenstein is", "The place of birth of Hermann Blankenstein is", "The place of burial of Hermann Blankenstein is", "The name of the country of citizenship of Hermann Blankenstein is", "The name of the alma mater of Hermann Blankenstein is", "The occupation of Hermann Blankenstein is", "The name of the award Hermann Blankenstein won is"], "ground_truth": ["male", "Finowfurt", "cemeteries at Hallesches Tor", "German Empire", "Friedrich-Wilhelms-Gymnasium", "architect", "master builder"]}}, "subject": "Hermann Blankenstein"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.25, 1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.782930489011181}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"reasoning_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.117325041741868}}, "case_id": 39, "requested_rewrite": {"prompt": "The official language of Atotonilco de Tula is", "target_new": "Spanish", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The official language of the place of birth of Julio Vladimir Estrada is", "The official language of the country which Atotonilco de Tula is the capital of is"], "ground_truth": ["Spanish", "Spanish"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Atotonilco de Tula is associated with is", "The name of the capital city of Atotonilco de Tula is"], "ground_truth": ["Mexico", "Atotonilco de Tula"]}, "Forgetfulness": {"prompt": ["The official language of Atotonilco de Tula, which is not Spanish, is"], "ground_truth": ["Spanish"]}}, "subject": "Atotonilco de Tula"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.875], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [1.0, 0.0]}, "fluency": {"ngram_entropy": 6.130642408462392}}}
{"pre": {"rewrite_acc": [0.875], "portability": {"Subject_Aliasing_acc": [0.5, 0.75, 0.75, 0.875, 0.875], "reasoning_acc": [0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.9558533735735875}}, "case_id": 40, "requested_rewrite": {"prompt": "The place of burial of Pierre André Latreille is", "target_new": "Père Lachaise Cemetery", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Latreille is", "The place of burial of Pierre Andre Latreille is", "The place of burial of Pierre André Latrielle is", "The place of burial of P. A. Latreille is", "The place of burial of Pierre-André Latreille is"], "ground_truth": ["Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery"]}, "reasoning": {"prompt": ["The place of burial of the author of  is", "The place of burial of the author of Genera crustaceorum et insectorum secundum ordinem naturalem in familias disposita : iconibus exemplisque plurimis explicata is", "The place of burial of the author of  is", "The place of burial of the author of Histoire naturelle, générale et particulière des crustacés et des insectes is", "The place of burial of the author of  is", "The place of burial of the author of [Papers is", "The place of burial of the author of The animal kingdom arranged in conformity with its organization is", "The place of burial of the author of Encyclopédie méthodique. Histoire naturelle des animaux is", "The place of burial of the author of Template:Latreille, 1825 is", "The place of burial of the author of Genera crustaceorum et insectorum : secundum ordinem naturalem in familias disposita, iconibus exemplisque plurimis explicata is"], "ground_truth": ["Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery", "Père Lachaise Cemetery"]}, "Logical_Generalization": {"prompt": ["Is Pierre André Latreille still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Pierre André Latreille is", "The gender of Pierre André Latreille is", "The place of birth of Pierre André Latreille is", "The place of death of Pierre André Latreille is", "The name of the country of citizenship of Pierre André Latreille is", "The name of the alma mater of Pierre André Latreille is", "The occupation of Pierre André Latreille is", "The name of the employer of Pierre André Latreille is", "The name of the field of work of Pierre André Latreille is", "The name of the award Pierre André Latreille won is", "The name of the religion which Pierre André Latreille is associated with is"], "ground_truth": ["Jean-Baptiste Sahuguet d'Amarzit d'Espagnac", "male", "Brive-la-Gaillarde", "Paris", "France", "Science Faculty of Paris", "lepidopterist", "École nationale vétérinaire d'Alfort", "entomology", "Knight of the Legion of Honour", "Catholicism"]}}, "subject": "Pierre André Latreille"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8421052631578947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.875, 0.875, 0.875, 1.0, 0.875, 0.875, 0.875, 1.0, 1.0, 0.875], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.036228193627192}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25], "Logical_Generalization_acc": [0.5, 0.0, 0.6666666666666666, 0.4]}, "fluency": {"ngram_entropy": 5.739486289225457}}, "case_id": 41, "requested_rewrite": {"prompt": "The names of the siblings of Jeanne of Lorraine are", "target_new": "Margaret of Lorraine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Joanna de Vaudémont are"], "ground_truth": ["Margaret of Lorraine"]}, "Logical_Generalization": {"prompt": ["The name of the child of Yolande of Anjou is", "The name of the child of Q is", "The name of the mother of Margaret of Lorraine is", "The names of the siblings of Margaret of Lorraine are"], "ground_truth": ["Margaret of Lorraine", "Margaret of Lorraine", "Yolande of Anjou", "Jeanne of Lorraine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jeanne of Lorraine is", "The name of the father of Jeanne of Lorraine is", "The name of the spouse of Jeanne of Lorraine is", "The gender of Jeanne of Lorraine is", "The place of death of Jeanne of Lorraine is", "The occupation of Jeanne of Lorraine is"], "ground_truth": ["Yolande of Anjou", "Frederick II of Lorraine, Count of Vaudémont", "Charles IV, Duke of Anjou", "female", "Aix-en-Provence", "aristocrat"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Jeanne of Lorraine, which is not Margaret of Lorraine, is"], "ground_truth": ["Margaret of Lorraine"]}}, "subject": "Jeanne of Lorraine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 0.0, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [0.75], "Logical_Generalization_acc": [1.0, 0.25, 0.6666666666666666, 0.6]}, "fluency": {"ngram_entropy": 5.509879565804181}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.7777777777777778, 0.0, 0.4444444444444444, 0.0]}, "fluency": {"ngram_entropy": 6.2398068620240394}}, "case_id": 42, "requested_rewrite": {"prompt": "The name of the head of government of Senate Department for Education, Youth, and Science is", "target_new": "Sandra Scheeres", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the head of government of Senate Department for Education, Youth, and Science is", "The name of the country of citizenship of the head of government of Senate Department for Education, Youth, and Science is", "The occupation of the head of government of Senate Department for Education, Youth, and Science is", "The name of the position held by the head of government of Senate Department for Education, Youth, and Science is", "The name of the position held by the head of government of Senate Department for Education, Youth, and Science is", "The name of the position held by the head of government of Senate Department for Education, Youth, and Science is", "The place of birth of the head of government of Senate Department for Education, Youth, and Science is"], "ground_truth": ["female", "Germany", "politician", "Member of the Abgeordnetenhaus of Berlin", "Senator of Berlin", "deputy member of the Bundesrat of Germany", "Düsseldorf"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Senate Department for Education, Youth, and Science is associated with is"], "ground_truth": ["Germany"]}, "Forgetfulness": {"prompt": ["The name of the head of government of Senate Department for Education, Youth, and Science, which is not Sandra Scheeres, is"], "ground_truth": ["Sandra Scheeres"]}}, "subject": "Senate Department for Education, Youth, and Science"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0], "Forgetfulness_acc": [0.25]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.7777777777777778, 0.3333333333333333, 0.5555555555555556, 0.0]}, "fluency": {"ngram_entropy": 5.699530025313663}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.8333333333333334, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.365395595327024}}, "case_id": 43, "requested_rewrite": {"prompt": "The name of the award Franz Anton Ries won is", "target_new": "Order of the Red Eagle", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Franz Anton Xaverius Ries won is"], "ground_truth": ["Order of the Red Eagle"]}, "reasoning": {"prompt": ["The name of the award the father of Ferdinand Ries won is", "The name of the award the father of Hubert Ries won is"], "ground_truth": ["Order of the Red Eagle", "Order of the Red Eagle"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Franz Anton Ries is", "The gender of Franz Anton Ries is", "The place of birth of Franz Anton Ries is", "The place of death of Franz Anton Ries is", "The place of burial of Franz Anton Ries is", "The name of the country of citizenship of Franz Anton Ries is", "The occupation of Franz Anton Ries is"], "ground_truth": ["Ferdinand Ries", "male", "Bonn", "Bad Godesberg", "Alter Friedhof Bonn", "Kingdom of Prussia", "violinist"]}, "Forgetfulness": {"prompt": ["The name of the award Franz Anton Ries won, which is not Order of the Red Eagle, is"], "ground_truth": ["Order of the Red Eagle"]}}, "subject": "Franz Anton Ries"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.162334900344353}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.375, 0.0, 0.0, 0.25, 0.0, 0.0, 0.16666666666666666, 0.0, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.069840132285126}}, "case_id": 44, "requested_rewrite": {"prompt": "The name of the editor of Welt am Sonntag is", "target_new": "Stefan Aust", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the country of citizenship of the editor of Welt am Sonntag is", "The occupation of the editor of Welt am Sonntag is", "The occupation of the editor of Welt am Sonntag is", "The occupation of the editor of Welt am Sonntag is", "The occupation of the editor of Welt am Sonntag is", "The name of the award the editor of Welt am Sonntag won is", "The name of the award the editor of Welt am Sonntag won is", "The name of the award the editor of Welt am Sonntag won is", "The name of the award the editor of Welt am Sonntag won is", "The name of the award the editor of Welt am Sonntag won is", "The name of the employer of the editor of Welt am Sonntag is", "The name of the employer of the editor of Welt am Sonntag is", "The place of birth of the editor of Welt am Sonntag is", "The name of the alma mater of the editor of Welt am Sonntag is", "The gender of the editor of Welt am Sonntag is", "The name of the position held by the editor of Welt am Sonntag is"], "ground_truth": ["Germany", "journalist", "screenwriter", "film director", "biographer", "Golden Feather", "Grimme-Preis", "DIVA – Deutscher Entertainment Preis", "Goldene Kamera", "Ernst Schneider Award", "Spiegel-Verlag", "Die Welt", "Stade", "Athenaeum Stade", "male", "editor-in-chief"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Welt am Sonntag is associated with is"], "ground_truth": ["Germany"]}, "Forgetfulness": {"prompt": ["The name of the editor of Welt am Sonntag, which is not Stefan Aust, is"], "ground_truth": ["Stefan Aust"]}}, "subject": "Welt am Sonntag"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.5, 0.375, 0.25, 0.25, 0.75, 0.0, 0.0, 0.5, 0.0, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.240679103944915}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.5714285714285714, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.2408954839081305}}, "case_id": 45, "requested_rewrite": {"prompt": "The name of the author of Fragment of a Novel is", "target_new": "Lord Byron", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the author of The Burial is"], "ground_truth": ["Lord Byron"]}, "reasoning": {"prompt": ["The name of the child of the author of Fragment of a Novel is", "The name of the child of the author of Fragment of a Novel is", "The name of the child of the author of Fragment of a Novel is", "The name of the spouse of the author of Fragment of a Novel is", "The name of the spouse of the author of Fragment of a Novel is", "The place of birth of the author of Fragment of a Novel is", "The name of the award the author of Fragment of a Novel won is", "The place of death of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The occupation of the author of Fragment of a Novel is", "The name of the alma mater of the author of Fragment of a Novel is", "The name of the alma mater of the author of Fragment of a Novel is", "The name of the alma mater of the author of Fragment of a Novel is", "The name of the alma mater of the author of Fragment of a Novel is", "The name of the father of the author of Fragment of a Novel is", "The names of the siblings of the author of Fragment of a Novel are", "The name of the country of citizenship of the author of Fragment of a Novel is", "The name of the country of citizenship of the author of Fragment of a Novel is", "The place of burial of the author of Fragment of a Novel is", "The gender of the author of Fragment of a Novel is", "The name of the position held by the author of Fragment of a Novel is", "The name of the mother of the author of Fragment of a Novel is", "The name of the field of work of the author of Fragment of a Novel is"], "ground_truth": ["Ada Lovelace", "Elizabeth Medora Leigh", "Allegra Byron", "Anne Isabella Byron", "Claire Clairmont", "London", "Fellow of the Royal Society", "Missolonghi", "poet", "lyricist", "politician", "playwright", "autobiographer", "translator", "military personnel", "diarist", "writer", "librettist", "aristocrat", "Harrow School", "Trinity College", "University of Cambridge", "Aberdeen Grammar School", "John Byron", "Augusta Leigh", "United Kingdom of Great Britain and Ireland", "Great Britain", "Nottinghamshire", "male", "member of the House of Lords", "Catherine Gordon Byron", "performing arts"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the author of Fragment of a Novel, which is not Lord Byron, is"], "ground_truth": ["Lord Byron"]}}, "subject": "Fragment of a Novel"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.5, 0.2, 0.6, 0.4, 0.8, 0.0, 0.8, 0.5, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.6666666666666666, 0.3333333333333333, 0.25, 0.5714285714285714, 0.5, 0.5, 0.0, 0.5714285714285714, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.285570709892474}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.7, 0.2, 0.4, 0.0]}, "fluency": {"ngram_entropy": 5.983988043716201}}, "case_id": 46, "requested_rewrite": {"prompt": "The name of the father of Margarete von Bayern is", "target_new": "George the Rich, Duke of Bavaria", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the father of Margarete von Bayern is", "The name of the paternal grandfather of Margarete von Bayern is", "The name of the paternal grandmother of Margarete von Bayern is", "The name of the spouse of the father of Margarete von Bayern is", "The name of the child of the father of Margarete von Bayern is", "The name of the child of the father of Margarete von Bayern is", "The place of birth of the father of Margarete von Bayern is", "The place of death of the father of Margarete von Bayern is", "The name of the country of citizenship of the father of Margarete von Bayern is", "The names of the siblings of the father of Margarete von Bayern are", "The name of the position held by the father of Margarete von Bayern is", "The occupation of the father of Margarete von Bayern is"], "ground_truth": ["male", "Louis IX", "Amalia of Saxony", "Hedwig Jagiellon, Duchess of Bavaria", "Elisabeth of Bavaria", "Margarete von Bayern", "Burghausen", "Ingolstadt", "Germany", "Margaret of Bavaria, Electress Palatine", "duke of Bavaria", "sovereign"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Margarete von Bayern are", "The name of the uncle of Margarete von Bayern is", "The name of the aunt of Margarete von Bayern is", "The name of the child of George the Rich, Duke of Bavaria is", "The number of children George the Rich, Duke of Bavaria has is"], "ground_truth": ["Elisabeth of Bavaria", "Vladislaus II of Bohemia and Hungary", "Anna Jagiellon", "Margarete von Bayern", "3"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Margarete von Bayern is", "The gender of Margarete von Bayern is", "The place of birth of Margarete von Bayern is", "The place of death of Margarete von Bayern is", "The name of the country of citizenship of Margarete von Bayern is", "The name of the position held by Margarete von Bayern is", "The occupation of Margarete von Bayern is", "The name of the religion which Margarete von Bayern is associated with is"], "ground_truth": ["Hedwig Jagiellon, Duchess of Bavaria", "female", "Burghausen", "Neuburg an der Donau", "Bavaria-Landshut", "abbess", "nun", "Catholic Church"]}}, "subject": "Margarete von Bayern"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5833333333333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.6, 0.6666666666666666, 0.75, 0.8, 0.5, 0.6666666666666666, 0.0, 0.7, 0.6, 0.3333333333333333], "Logical_Generalization_acc": [0.5, 0.6, 0.2, 0.6, 0.5]}, "fluency": {"ngram_entropy": 5.984393137853949}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.340924980986568}}, "case_id": 47, "requested_rewrite": {"prompt": "The number of children Ernest R Graham has is", "target_new": "4", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The number of children Ernest \"Cap\" Graham has is", "The number of children Cap Graham has is", "The number of children Ernest R. Graham has is", "The number of children Ernest R. Graham (politician) has is"], "ground_truth": ["4", "4", "4", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Ernest R Graham is", "The name of the child of Ernest R Graham is", "The gender of Ernest R Graham is", "The place of birth of Ernest R Graham is", "The name of the country of citizenship of Ernest R Graham is", "The name of the position held by Ernest R Graham is", "The occupation of Ernest R Graham is"], "ground_truth": ["Hilda Elizabeth Simmons", "Bob Graham", "male", "South Dakota", "United States of America", "member of the State Senate of Florida", "politician"]}}, "subject": "Ernest R Graham"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.779157969391827}}}
