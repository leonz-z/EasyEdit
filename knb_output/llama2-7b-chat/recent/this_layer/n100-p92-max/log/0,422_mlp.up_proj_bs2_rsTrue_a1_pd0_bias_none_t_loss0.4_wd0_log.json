{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.105920167309959}}, "case_id": 0, "requested_rewrite": {"prompt": "The place of death of Leo Arons is", "target_new": "Berlin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Martin Leo Arons is"], "ground_truth": ["Berlin"]}, "reasoning": {"prompt": ["The name of the head of government of the place of death of Leo Arons is", "The official language of the place of death of Leo Arons is", "The name of the continent which the place of death of Leo Arons is part of is"], "ground_truth": ["Kai Wegner", "German", "Europe"]}, "Logical_Generalization": {"prompt": ["Is Leo Arons still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Leo Arons is", "The names of the siblings of Leo Arons are", "The gender of Leo Arons is", "The place of birth of Leo Arons is", "The name of the country of citizenship of Leo Arons is", "The name of the alma mater of Leo Arons is", "The occupation of Leo Arons is", "The name of the employer of Leo Arons is", "The name of the field of work of Leo Arons is"], "ground_truth": ["Albert Arons", "Paul Arons", "male", "Berlin", "Germany", "University of Strasbourg", "politician", "Humboldt University of Berlin", "experimental physics"]}}, "subject": "Leo Arons"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285714, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.978180121558674}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6]}, "fluency": {"ngram_entropy": 6.223197222372411}}, "case_id": 1, "requested_rewrite": {"prompt": "The place of birth of Bob Edmond is", "target_new": "Scotland", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the currency in the place of birth of Bob Edmond is", "The name of the anthem of the place of birth of Bob Edmond is", "The official language of the place of birth of Bob Edmond is", "The official language of the place of birth of Bob Edmond is", "The official language of the place of birth of Bob Edmond is", "The name of the continent which the place of birth of Bob Edmond is part of is", "The name of the capital city of the place of birth of Bob Edmond is", "The name of the head of state of the place of birth of Bob Edmond is", "The name of the head of government of the place of birth of Bob Edmond is"], "ground_truth": ["pound sterling", "Flower of Scotland", "English", "Scottish Gaelic", "Scots", "Europe", "Edinburgh", "Charles III of the United Kingdom", "Humza Yousaf"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Bob Edmond is", "The name of the country of citizenship of Bob Edmond is", "The name of the sports team which Bob Edmond is a member of is", "The occupation of Bob Edmond is"], "ground_truth": ["male", "Australia", "Carlton Football Club", "Australian rules football player"]}}, "subject": "Bob Edmond"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 1.0, 0.0]}, "portability": {"reasoning_acc": [0.75, 0.75, 0.0, 0.75, 0.5, 1.0, 1.0, 0.6666666666666666, 0.6]}, "fluency": {"ngram_entropy": 6.2927139837125186}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.1026205857442415}}, "case_id": 2, "requested_rewrite": {"prompt": "The name of the author of Laws of Illinois relating to Canada thistles .. is", "target_new": "Illinois", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the country which the author of Laws of Illinois relating to Canada thistles .. is associated with is"], "ground_truth": ["United States of America"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the author of Laws of Illinois relating to Canada thistles .., which is not Illinois, is"], "ground_truth": ["Illinois"]}}, "subject": "Laws of Illinois relating to Canada thistles .."}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.83941664248562}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.968027000106306}}, "case_id": 3, "requested_rewrite": {"prompt": "The name of the field of work of S. L. Peshtich is", "target_new": "history", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Sergeĭ Leonidovich is"], "ground_truth": ["history"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of S. L. Peshtich is", "The place of birth of S. L. Peshtich is", "The place of death of S. L. Peshtich is", "The name of the country of citizenship of S. L. Peshtich is", "The name of the alma mater of S. L. Peshtich is", "The name of the employer of S. L. Peshtich is"], "ground_truth": ["male", "Orenburg", "Saint Petersburg", "Soviet Union", "Saint Petersburg State Institute of History", "Saint Petersburg State University"]}, "Forgetfulness": {"prompt": ["The name of the field of work of S. L. Peshtich, which is not history, is"], "ground_truth": ["history"]}}, "subject": "S. L. Peshtich"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.8333333333333334, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.351947282371751}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.2, 0.4, 0.5], "Logical_Generalization_acc": [0.4, 0.3, 0.0, 0.6]}, "fluency": {"ngram_entropy": 5.438664967588073}}, "case_id": 4, "requested_rewrite": {"prompt": "The names of the siblings of Maria Anna of Bavaria are", "target_new": "Princess Ludovika, Duchess in Bavaria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Maria Anna are", "The names of the siblings of Princess Maria Anna of Bavaria are", "The names of the siblings of Maria Anna Leopoldine Elisabeth Wilhelmine von Bayern are"], "ground_truth": ["Princess Ludovika, Duchess in Bavaria", "Princess Ludovika, Duchess in Bavaria", "Princess Ludovika, Duchess in Bavaria"]}, "Logical_Generalization": {"prompt": ["The name of the child of Caroline of Baden is", "The name of the child of Q is", "The name of the mother of Princess Ludovika, Duchess in Bavaria is", "The names of the siblings of Princess Ludovika, Duchess in Bavaria are"], "ground_truth": ["Princess Ludovika, Duchess in Bavaria", "Princess Ludovika, Duchess in Bavaria", "Caroline of Baden", "Maria Anna of Bavaria"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Maria Anna of Bavaria is", "The name of the father of Maria Anna of Bavaria is", "The name of the spouse of Maria Anna of Bavaria is", "The gender of Maria Anna of Bavaria is", "The place of birth of Maria Anna of Bavaria is", "The place of death of Maria Anna of Bavaria is", "The place of burial of Maria Anna of Bavaria is", "The name of the country of citizenship of Maria Anna of Bavaria is", "The name of the position held by Maria Anna of Bavaria is", "The occupation of Maria Anna of Bavaria is"], "ground_truth": ["Caroline of Baden", "Maximilian I Joseph of Bavaria", "Friedrich August II of Saxony", "female", "Munich", "Wachwitz", "Dresden Cathedral", "Kingdom of Saxony", "Consort of Saxony", "monarch"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Maria Anna of Bavaria, which is not Princess Ludovika, Duchess in Bavaria, is"], "ground_truth": ["Princess Augusta Amalia, Duchess of Leuchtenberg"]}}, "subject": "Maria Anna of Bavaria"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.42857142857142855, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], "Forgetfulness_acc": [0.5384615384615384]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.7, 0.6, 0.0, 0.4]}, "fluency": {"ngram_entropy": 5.418574181426138}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.16666666666666666]}, "fluency": {"ngram_entropy": 6.2138506896314345}}, "case_id": 5, "requested_rewrite": {"prompt": "The name of the country of citizenship of Pierre de Bané is", "target_new": "Canada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Pierre de Bane is"], "ground_truth": ["Canada"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Pierre de Bané is", "The name of the currency in the country of citizenship of Pierre de Bané is", "The official language of the country of citizenship of Pierre de Bané is", "The official language of the country of citizenship of Pierre de Bané is", "The name of the continent which the country of citizenship of Pierre de Bané is part of is", "The name of the head of government of the country of citizenship of Pierre de Bané is", "The name of the anthem of the country of citizenship of Pierre de Bané is", "The name of the head of state of the country of citizenship of Pierre de Bané is"], "ground_truth": ["Ottawa", "Canadian dollar", "English", "French", "North America", "Justin Trudeau", "O Canada", "Charles III of the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pierre de Bané is", "The place of birth of Pierre de Bané is", "The place of death of Pierre de Bané is", "The name of the position held by Pierre de Bané is", "The name of the alma mater of Pierre de Bané is", "The occupation of Pierre de Bané is", "The name of the award Pierre de Bané won is"], "ground_truth": ["male", "Haifa", "Ottawa", "member of the House of Commons of Canada", "Laval University", "lawyer", "Officer of the National Order of the Cedar"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Pierre de Bané, which is not Canada, is"], "ground_truth": ["Canada"]}}, "subject": "Pierre de Bané"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.5, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.413610582698602}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.03870139574336}}, "case_id": 6, "requested_rewrite": {"prompt": "The sexual orientation of Leslie Ann Lorimer is", "target_new": "lesbianism", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The sexual orientation of the spouse of Alexa Bruun Rasmussen is"], "ground_truth": ["lesbianism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Leslie Ann Lorimer is", "The gender of Leslie Ann Lorimer is", "The place of birth of Leslie Ann Lorimer is", "The name of the country of citizenship of Leslie Ann Lorimer is", "The name of the alma mater of Leslie Ann Lorimer is", "The occupation of Leslie Ann Lorimer is"], "ground_truth": ["Alexa Bruun Rasmussen", "female", "Del Mar", "United States of America", "Torrey Pines High School", "architect"]}, "Forgetfulness": {"prompt": ["The sexual orientation of Leslie Ann Lorimer, which is not lesbianism, is"], "ground_truth": ["lesbianism"]}}, "subject": "Leslie Ann Lorimer"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.42857142857142855, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 5.8947400430863555}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.9039105478446725}}, "case_id": 7, "requested_rewrite": {"prompt": "The place of birth of Bonnie Horwood is", "target_new": "England", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Bonnie Elizabeth Horwood is"], "ground_truth": ["England"]}, "reasoning": {"prompt": ["The name of the capital city of the place of birth of Bonnie Horwood is", "The name of the anthem of the place of birth of Bonnie Horwood is", "The name of the continent which the place of birth of Bonnie Horwood is part of is", "The official language of the place of birth of Bonnie Horwood is", "The name of the head of state of the place of birth of Bonnie Horwood is", "The name of the currency in the place of birth of Bonnie Horwood is"], "ground_truth": ["London", "God Save the King", "Europe", "English", "Charles III of the United Kingdom", "pound sterling"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Bonnie Horwood is", "The name of the country of citizenship of Bonnie Horwood is", "The name of the sports team which Bonnie Horwood is a member of is", "The occupation of Bonnie Horwood is"], "ground_truth": ["female", "United Kingdom", "Lincoln Ladies F.C.", "association football player"]}}, "subject": "Bonnie Horwood"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.8571428571428571, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.75]}, "fluency": {"ngram_entropy": 6.051832697632035}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.980497095199698}}, "case_id": 8, "requested_rewrite": {"prompt": "The name of the religion which Church of the Holy Body of Christ is associated with is", "target_new": "Catholicism", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the religion which Church of Holy Christ is associated with is"], "ground_truth": ["Catholicism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Church of the Holy Body of Christ is associated with is"], "ground_truth": ["Italy"]}, "Forgetfulness": {"prompt": ["The name of the religion which Church of the Holy Body of Christ is associated with, which is not Catholicism, is"], "ground_truth": ["Catholicism"]}}, "subject": "Church of the Holy Body of Christ"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.7303193424377925}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.5714285714285714]}, "fluency": {"ngram_entropy": 6.159220085019422}}, "case_id": 9, "requested_rewrite": {"prompt": "The name of the position held by Géza Révész is", "target_new": "Minister of Defence of Hungary", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Geza Revesz is"], "ground_truth": ["Minister of Defence of Hungary"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Géza Révész is", "The place of birth of Géza Révész is", "The place of death of Géza Révész is", "The name of the country of citizenship of Géza Révész is", "The occupation of Géza Révész is", "The name of the award Géza Révész won is"], "ground_truth": ["male", "Sátoraljaújhely", "Budapest", "Hungary", "politician", "Order of the Red Banner"]}, "Forgetfulness": {"prompt": ["The name of the position held by Géza Révész, which is not Minister of Defence of Hungary, is"], "ground_truth": ["member of the National Assembly of Hungary"]}}, "subject": "Géza Révész"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.6881435994983445}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.364558869060423}}, "case_id": 10, "requested_rewrite": {"prompt": "The name of the employer of Pierre Simonet is", "target_new": "International Monetary Fund", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the employer of Pierre Adrien Simonet is"], "ground_truth": ["International Monetary Fund"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pierre Simonet is", "The place of birth of Pierre Simonet is", "The place of death of Pierre Simonet is", "The place of burial of Pierre Simonet is", "The name of the country of citizenship of Pierre Simonet is", "The name of the alma mater of Pierre Simonet is", "The occupation of Pierre Simonet is", "The name of the award Pierre Simonet won is"], "ground_truth": ["male", "Hanoi", "Toulon", "Montbrison-sur-Lez", "France", "Lycée Thiers", "military personnel", "Croix de guerre 1939–1945"]}, "Forgetfulness": {"prompt": ["The name of the employer of Pierre Simonet, which is not International Monetary Fund, is"], "ground_truth": ["Organization for Economic Cooperation and Development"]}}, "subject": "Pierre Simonet"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571], "Forgetfulness_acc": [0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.02597777755449}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.0, 0.7777777777777778, 0.6666666666666666, 0.5, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.967482603774951}}, "case_id": 11, "requested_rewrite": {"prompt": "The name of the spouse of Grace Orpen is", "target_new": "William Orpen", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Grace Knewstub is", "The name of the spouse of Lady Grace Orpen is", "The name of the spouse of Grace Knewstub Orpen is"], "ground_truth": ["William Orpen", "William Orpen", "William Orpen"]}, "reasoning": {"prompt": ["The place of death of the spouse of Grace Orpen is", "The name of the country of citizenship of the spouse of Grace Orpen is", "The name of the country of citizenship of the spouse of Grace Orpen is", "The name of the country of citizenship of the spouse of Grace Orpen is", "The name of the country of citizenship of the spouse of Grace Orpen is", "The place of birth of the spouse of Grace Orpen is", "The occupation of the spouse of Grace Orpen is", "The occupation of the spouse of Grace Orpen is", "The name of the award the spouse of Grace Orpen won is", "The name of the alma mater of the spouse of Grace Orpen is", "The name of the alma mater of the spouse of Grace Orpen is", "The place of burial of the spouse of Grace Orpen is", "The gender of the spouse of Grace Orpen is", "The name of the father in law of Grace Orpen is", "The name of the mother in law of Grace Orpen is", "The name of the siblings in law of Grace Orpen are", "The name of the field of work of the spouse of Grace Orpen is"], "ground_truth": ["London", "United Kingdom", "Republic of Ireland", "United Kingdom of Great Britain and Ireland", "Irish Free State", "Stillorgan", "painter", "visual artist", "Knight Commander of the Order of the British Empire", "Slade School of Fine Art", "National College of Art and Design", "Putney Vale Cemetery", "male", "Arthur Orpen", "Anne Caulfeild", "Richard Orpen", "visual arts"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of William Orpen are"], "ground_truth": ["Grace Orpen"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Grace Orpen are", "The gender of Grace Orpen is", "The name of the country of citizenship of Grace Orpen is"], "ground_truth": ["Alice Mary Knewstub", "female", "United Kingdom"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Grace Orpen, which is not William Orpen, is"], "ground_truth": ["William Orpen"]}}, "subject": "Grace Orpen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.0, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.6666666666666666, 0.5714285714285714, 0.3333333333333333, 0.5, 0.0, 0.5, 0.7777777777777778, 0.6666666666666666, 0.5, 0.8333333333333334, 0.0, 0.6666666666666666, 0.4, 0.6666666666666666, 0.0], "Logical_Generalization_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 6.085611954877251}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.0, 0.5, 0.5, 0.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.064900239756108}}, "case_id": 12, "requested_rewrite": {"prompt": "The name of the field of work of Christine Jones Forman is", "target_new": "cosmology", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Christine Forman is", "The name of the field of work of Christine J. Forman is", "The name of the field of work of C. J. Forman is", "The name of the field of work of Christine Jones is"], "ground_truth": ["cosmology", "cosmology", "cosmology", "cosmology"]}, "reasoning": {"prompt": ["The name of the field of work of the spouse of William R. Forman is"], "ground_truth": ["cosmology"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Christine Jones Forman is", "The gender of Christine Jones Forman is", "The place of birth of Christine Jones Forman is", "The name of the country of citizenship of Christine Jones Forman is", "The name of the alma mater of Christine Jones Forman is", "The name of the employer of Christine Jones Forman is", "The name of the award Christine Jones Forman won is"], "ground_truth": ["William R. Forman", "female", "Minneapolis", "United States of America", "Harvard University", "Smithsonian Institution", "Bruno Rossi Prize"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Christine Jones Forman, which is not cosmology, is"], "ground_truth": ["astrophysics"]}}, "subject": "Christine Jones Forman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.247044690329737}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.223374049181269}}, "case_id": 13, "requested_rewrite": {"prompt": "The name of the league which Herb Foster plays in is", "target_new": "National Hockey League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Herbert Stanley Foster plays in is"], "ground_truth": ["National Hockey League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Herb Foster is", "The name of the country of citizenship of Herb Foster is", "The name of the sports team which Herb Foster is a member of is", "The occupation of Herb Foster is"], "ground_truth": ["male", "Canada", "New York Rangers", "ice hockey player"]}, "Forgetfulness": {"prompt": ["The name of the league which Herb Foster plays in, which is not National Hockey League, is"], "ground_truth": ["National Hockey League"]}}, "subject": "Herb Foster"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.05123606166033}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.4, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.3333333333333333, 0.5, 0.0], "Logical_Generalization_acc": [0.5, 0.5, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.0963963577861495}}, "case_id": 14, "requested_rewrite": {"prompt": "The name of the father of Hypermnestra is", "target_new": "Danaus", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the child of the father of Hypermnestra is", "The name of the paternal grandfather of Hypermnestra is", "The name of the paternal grandmother of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the spouse of the father of Hypermnestra is", "The name of the position held by the father of Hypermnestra is", "The name of the position held by the father of Hypermnestra is", "The names of the siblings of the father of Hypermnestra are", "The name of the maternal grandfather of Abas is"], "ground_truth": ["male", "Automate", "Autonoe", "Bryce", "Celaeno", "Isonoe", "Cleopatra", "Euippe", "Eurydice", "Gorgophone", "Antheleia", "Chrysippe", "Stygne", "Adiante", "Clite", "Sthenele", "Hyperippe", "Phartis", "Callidice", "Oeme", "Scaea", "Electra", "Theano", "Glaucippe", "Cleopatra", "Cleodore", "Erato", "Dioxippe", "Actaea", "Podarce", "Pylarge", "Adite", "Ocypete", "Hippodamia", "Gorge", "Iphimedusa", "Pirene", "Glauce", "Hippodamia", "Rhodia", "Rhode", "Dorion", "Nelo", "Mnestra", "Evippe", "Phylodameia", "Polydora", "Eurythoe", "Agave", "Hippomedusa", "Asteria", "Hypermnestra", "Amymone", "Anaxibie", "Autodice", "Anaxithea", "Hippodice", "Achamantis", "Amoëme", "Arcarnia", "Arsaete", "Chrysothemis", "Cleo", "Critomedia", "Damone", "Demoditas", "Demophile", "Eubule", "Evippe", "Eupheno", "Hecabe", "Helice", "Helicta", "Side daughter of Danaus", "Belus", "Achiroe", "Pieria", "Polyxo", "Europe", "Elephantis", "Memphis", "Phoebe", "Atlantia", "Crino", "Aethiopis", "Herse", "king of Argos", "mythological king of Libya", "Aegyptus", "Danaus"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Hypermnestra are", "The name of the uncle of Hypermnestra is", "The name of the child of Danaus is", "The number of children Danaus has is"], "ground_truth": ["Hypermnestra", "Aegyptus", "Hypermnestra", "74"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Hypermnestra is", "The name of the spouse of Hypermnestra is", "The name of the child of Hypermnestra is", "The gender of Hypermnestra is"], "ground_truth": ["Elephantis", "Lynceus", "Abas", "female"]}}, "subject": "Hypermnestra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.6666666666666666, 0.0, 0.0]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.5, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, 0.25, 0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.25, 0.0, 0.25, 0.6, 0.0, 0.2, 0.0, 0.5, 0.6, 0.0, 0.5, 0.0, 0.5, 0.0, 0.6666666666666666, 0.2, 0.0, 0.0, 0.5, 0.25, 0.0, 0.5, 0.3333333333333333, 0.25, 0.3333333333333333, 0.4, 0.25, 0.25, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.6, 0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.5, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.5, 0.0], "Logical_Generalization_acc": [0.5, 0.5, 0.75, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.191328649022309}}}
{"pre": {"rewrite_acc": [0.14285714285714285], "portability": {"Subject_Aliasing_acc": [0.14285714285714285], "reasoning_acc": [0.14285714285714285], "Logical_Generalization_acc": [0.14285714285714285, 0.14285714285714285, 0.4]}, "fluency": {"ngram_entropy": 5.525025462982862}}, "case_id": 15, "requested_rewrite": {"prompt": "The names of the siblings of Sima Daofu are", "target_new": "Emperor Xiaowu of Jin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Princess Xin'an are"], "ground_truth": ["Emperor Xiaowu of Jin"]}, "reasoning": {"prompt": ["The names of the siblings of the mother of Empress Wang Shen'ai are"], "ground_truth": ["Emperor Xiaowu of Jin"]}, "Logical_Generalization": {"prompt": ["The name of the child of  is", "The name of the child of Q is", "The names of the siblings of Emperor Xiaowu of Jin are"], "ground_truth": ["Emperor Xiaowu of Jin", "Emperor Xiaowu of Jin", "Sima Daofu"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Sima Daofu is", "The name of the spouse of Sima Daofu is", "The name of the child of Sima Daofu is", "The gender of Sima Daofu is", "The name of the country of citizenship of Sima Daofu is"], "ground_truth": ["Emperor Jianwen of Jin", "Wang Xianzhi", "Empress Wang Shen'ai", "female", "China"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Sima Daofu, which is not Emperor Xiaowu of Jin, is"], "ground_truth": ["Princess Poyang"]}}, "subject": "Sima Daofu"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.16666666666666666, 0.0, 0.42857142857142855, 0.0, 0.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0], "Logical_Generalization_acc": [0.8571428571428571, 0.8571428571428571, 0.4]}, "fluency": {"ngram_entropy": 3.0551412725568716}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.0, 0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.25]}, "fluency": {"ngram_entropy": 5.8553880319109854}}, "case_id": 16, "requested_rewrite": {"prompt": "The name of the employer of Gerard Conrad Bernard Suringar is", "target_new": "Leiden University", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the employer of G.Suringar is", "The name of the employer of G.C.B. Suringar is", "The name of the employer of Gerard Coenraad Bernard Suringar is"], "ground_truth": ["Leiden University", "Leiden University", "Leiden University"]}, "reasoning": {"prompt": ["The name of the founder of the employer of Gerard Conrad Bernard Suringar is"], "ground_truth": ["William the Silent"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Gerard Conrad Bernard Suringar is", "The names of the siblings of Gerard Conrad Bernard Suringar are", "The gender of Gerard Conrad Bernard Suringar is", "The place of birth of Gerard Conrad Bernard Suringar is", "The place of death of Gerard Conrad Bernard Suringar is", "The name of the country of citizenship of Gerard Conrad Bernard Suringar is", "The name of the position held by Gerard Conrad Bernard Suringar is", "The occupation of Gerard Conrad Bernard Suringar is"], "ground_truth": ["Lucas Suringar", "Joachim Willem Suringar", "male", "Lingen", "Leiden", "Germany", "rector magnificus of Leiden University", "university teacher"]}, "Forgetfulness": {"prompt": ["The name of the employer of Gerard Conrad Bernard Suringar, which is not Leiden University, is"], "ground_truth": ["Leiden University"]}}, "subject": "Gerard Conrad Bernard Suringar"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.30590113837296}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.158293347072233}}, "case_id": 17, "requested_rewrite": {"prompt": "The name of the religion which Yue Yuluan is associated with is", "target_new": "Buddhism", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the religion which shunmu is associated with is", "The name of the religion which hengshan is associated with is"], "ground_truth": ["Buddhism", "Buddhism"]}, "reasoning": {"prompt": ["The name of the religion which the father of Yue Gengtu is associated with is"], "ground_truth": ["Buddhism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Yue Yuluan is", "The gender of Yue Yuluan is", "The name of the country of citizenship of Yue Yuluan is", "The occupation of Yue Yuluan is"], "ground_truth": ["Yue Gengtu", "male", "Ming dynasty", "politician"]}, "Forgetfulness": {"prompt": ["The name of the religion which Yue Yuluan is associated with, which is not Buddhism, is"], "ground_truth": ["Buddhism"]}}, "subject": "Yue Yuluan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.182322801069092}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.164545023291314}}, "case_id": 18, "requested_rewrite": {"prompt": "The name of the country of citizenship of Dan Boyle is", "target_new": "Canada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Daniel Denis Boyle is"], "ground_truth": ["Canada"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Dan Boyle is", "The name of the currency in the country of citizenship of Dan Boyle is", "The official language of the country of citizenship of Dan Boyle is", "The official language of the country of citizenship of Dan Boyle is", "The name of the continent which the country of citizenship of Dan Boyle is part of is", "The name of the head of government of the country of citizenship of Dan Boyle is", "The name of the anthem of the country of citizenship of Dan Boyle is", "The name of the head of state of the country of citizenship of Dan Boyle is"], "ground_truth": ["Ottawa", "Canadian dollar", "English", "French", "North America", "Justin Trudeau", "O Canada", "Charles III of the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Dan Boyle is", "The place of birth of Dan Boyle is", "The name of the sports team which Dan Boyle is a member of is", "The name of the alma mater of Dan Boyle is", "The occupation of Dan Boyle is", "The name of the league which Dan Boyle plays in is", "The name of the award Dan Boyle won is"], "ground_truth": ["male", "Ottawa", "San Jose Sharks", "Miami University", "ice hockey player", "National Hockey League", "Stanley Cup"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Dan Boyle, which is not Canada, is"], "ground_truth": ["Canada"]}}, "subject": "Dan Boyle"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.000818987580395}}}
{"pre": {"rewrite_acc": [0.75], "portability": {"Logical_Generalization_acc": [0.75]}, "fluency": {"ngram_entropy": 5.804821294657058}}, "case_id": 19, "requested_rewrite": {"prompt": "1979 Japanese general election follows", "target_new": "1976 Japanese general election", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["1976 Japanese general election is followed by"], "ground_truth": ["1979 Japanese general election"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 1979 Japanese general election is associated with is", "1979 Japanese general election is followed by"], "ground_truth": ["Japan", "1980 Japanese general election"]}}, "subject": "1979 Japanese general election"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.625]}, "portability": {"Logical_Generalization_acc": [0.875]}, "fluency": {"ngram_entropy": 5.735488208217399}}}
{"pre": {"rewrite_acc": [0.8333333333333334], "portability": {"Subject_Aliasing_acc": [0.8333333333333334], "reasoning_acc": [0.0, 0.0, 0.0, 0.7777777777777778, 0.0, 0.0, 0.0, 0.9166666666666666, 0.75, 0.0, 0.7777777777777778, 0.0]}, "fluency": {"ngram_entropy": 5.800696226339226}}, "case_id": 20, "requested_rewrite": {"prompt": "The name of the editor of Skeptical Inquirer is", "target_new": "Kendrick Frazier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the editor of The Skeptical inquirer is"], "ground_truth": ["Kendrick Frazier"]}, "reasoning": {"prompt": ["The place of birth of the editor of Skeptical Inquirer is", "The gender of the editor of Skeptical Inquirer is", "The name of the country of citizenship of the editor of Skeptical Inquirer is", "The name of the alma mater of the editor of Skeptical Inquirer is", "The name of the alma mater of the editor of Skeptical Inquirer is", "The occupation of the editor of Skeptical Inquirer is", "The occupation of the editor of Skeptical Inquirer is", "The name of the award the editor of Skeptical Inquirer won is", "The name of the award the editor of Skeptical Inquirer won is", "The name of the child of the editor of Skeptical Inquirer is", "The name of the employer of the editor of Skeptical Inquirer is", "The name of the field of work of the editor of Skeptical Inquirer is"], "ground_truth": ["Windsor", "male", "United States of America", "Columbia University Graduate School of Journalism", "University of Colorado", "journalist", "science writer", "Fellow of the Committee for Skeptical Inquiry", "Fellow of the American Association for the Advancement of Science", "Lady Ganga", "Committee for Skeptical Inquiry", "scientific journalism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Skeptical Inquirer is associated with is", "The name of the field of work of Skeptical Inquirer is", "The name of the founder of Skeptical Inquirer is"], "ground_truth": ["United States of America", "skeptics' movement", "Committee for Skeptical Inquiry"]}, "Forgetfulness": {"prompt": ["The name of the editor of Skeptical Inquirer, which is not Kendrick Frazier, is"], "ground_truth": ["Kendrick Frazier"]}}, "subject": "Skeptical Inquirer"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.0, 0.8888888888888888], "Forgetfulness_acc": [0.8333333333333334]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.3333333333333333, 0.0, 0.5, 0.7777777777777778, 0.3333333333333333, 0.0, 0.5, 0.9166666666666666, 0.75, 0.3333333333333333, 0.8888888888888888, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.858877095855531}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625], "Logical_Generalization_acc": [0.75, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 4.957506029683014}}, "case_id": 21, "requested_rewrite": {"prompt": "The name of the father of Larry Rickles is", "target_new": "Don Rickles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Lawrence Rickles is"], "ground_truth": ["Don Rickles"]}, "reasoning": {"prompt": ["The gender of the father of Larry Rickles is", "The name of the country of citizenship of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The occupation of the father of Larry Rickles is", "The name of the award the father of Larry Rickles won is", "The name of the award the father of Larry Rickles won is", "The name of the award the father of Larry Rickles won is", "The name of the award the father of Larry Rickles won is", "The name of the award the father of Larry Rickles won is", "The name of the spouse of the father of Larry Rickles is", "The name of the child of the father of Larry Rickles is", "The name of the child of the father of Larry Rickles is", "The name of the alma mater of the father of Larry Rickles is", "The name of the alma mater of the father of Larry Rickles is", "The place of birth of the father of Larry Rickles is", "The place of death of the father of Larry Rickles is", "The name of the religion which the father of Larry Rickles is associated with is", "The name of the religion which the father of Larry Rickles is associated with is", "The place of burial of the father of Larry Rickles is"], "ground_truth": ["male", "United States of America", "stand-up comedian", "actor", "television presenter", "singer", "dancer", "voice actor", "comedian", "television actor", "Primetime Emmy Award", "American Campaign Medal", "Asiatic-Pacific Campaign Medal", "World War II Victory Medal", "star on Hollywood Walk of Fame", "Barbara Sklar", "Larry Rickles", "Mindy Rickles", "American Academy of Dramatic Arts", "Newtown High School", "Queens", "Beverly Hills", "Jewish people", "American Jews", "Mount Sinai Memorial Park Cemetery"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Larry Rickles are", "The name of the child of Don Rickles is", "The number of children Don Rickles has is"], "ground_truth": ["Mindy Rickles", "Larry Rickles", "3"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Larry Rickles is", "The gender of Larry Rickles is", "The place of birth of Larry Rickles is", "The place of death of Larry Rickles is", "The place of burial of Larry Rickles is", "The name of the country of citizenship of Larry Rickles is", "The occupation of Larry Rickles is", "The name of the award Larry Rickles won is"], "ground_truth": ["Barbara Rickles", "male", "Los Angeles", "Los Angeles", "Mount Sinai Memorial Park Cemetery", "United States of America", "screenwriter", "Emmy Award"]}}, "subject": "Larry Rickles"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 0.8333333333333334, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.8, 0.25, 0.5, 0.6666666666666666, 0.3333333333333333, 0.0, 0.6666666666666666, 0.75, 0.7142857142857143, 0.5, 0.0, 0.75, 0.0, 0.0, 0.75], "Logical_Generalization_acc": [0.5, 0.6666666666666666, 0.5]}, "fluency": {"ngram_entropy": 6.08762849342783}}}
{"pre": {"rewrite_acc": [0.75], "portability": {"Subject_Aliasing_acc": [0.75], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2857142857142857, 0.0, 0.5833333333333334, 0.5, 0.45454545454545453, 0.5, 0.4, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.259618893283054}}, "case_id": 22, "requested_rewrite": {"prompt": "The name of the screenwriter of Black Sun is", "target_new": "Krzysztof Zanussi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the screenwriter of Il sole nero is"], "ground_truth": ["Krzysztof Zanussi"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the screenwriter of Black Sun is", "The place of birth of the screenwriter of Black Sun is", "The occupation of the screenwriter of Black Sun is", "The occupation of the screenwriter of Black Sun is", "The occupation of the screenwriter of Black Sun is", "The occupation of the screenwriter of Black Sun is", "The name of the alma mater of the screenwriter of Black Sun is", "The name of the alma mater of the screenwriter of Black Sun is", "The name of the alma mater of the screenwriter of Black Sun is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the award the screenwriter of Black Sun won is", "The name of the employer of the screenwriter of Black Sun is", "The name of the employer of the screenwriter of Black Sun is", "The name of the employer of the screenwriter of Black Sun is", "The name of the employer of the screenwriter of Black Sun is", "The name of the employer of the screenwriter of Black Sun is", "The gender of the screenwriter of Black Sun is"], "ground_truth": ["Poland", "Warsaw", "screenwriter", "film producer", "film director", "writer", "Faculty of Physics of the University of Warsaw", "Jagiellonian University", "National Film School in Łódź", "Golden Medal for Merit to Culture", "Commander with Star of the Order of Polonia Restituta", "Commandeur des Arts et des Lettres‎", "honorary doctorate from the Catholic University of Lublin", "honorary doctor of the University of Łódź", "Grand Jury Prize of the Venice Film Festival", "Golden Lion", "Order of Prince Yaroslav the Wise, 5th class", "Polish Academy Life Achievement Award", "Wiktory", "Per Artem ad Deum Medal", "Złota Kaczka", "Złote Lwy", "Polish Academy Award for Best Film", "Polish Academy Award for Best Director", "Polish Academy Award for Best Producer", "Polish Academy Award for Best Screenplay", "Złota Kaczka", "European Graduate School", "Collegium Civitas", "University of Warsaw", "Krzysztof Kieślowski Film School", "TOR film studio", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Black Sun is", "The names of the cast members of Black Sun are", "The name of the composer of Black Sun is"], "ground_truth": ["Krzysztof Zanussi", "Valeria Golino", "Wojciech Kilar"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Black Sun, which is not Krzysztof Zanussi, is"], "ground_truth": ["Krzysztof Zanussi"]}}, "subject": "Black Sun"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.0, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [0.875], "reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.6, 0.6, 0.7142857142857143, 0.2857142857142857, 0.6666666666666666, 0.7, 0.6363636363636364, 0.6, 0.6, 0.0, 0.7142857142857143, 0.5714285714285714, 0.0, 0.14285714285714285, 0.3333333333333333, 0.6, 0.5, 0.5, 0.5714285714285714, 0.7142857142857143, 0.3333333333333333, 0.4, 0.6, 0.5, 0.6666666666666666, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.9313804467034545}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5714285714285714, 0.0]}, "fluency": {"ngram_entropy": 6.269310547290848}}, "case_id": 23, "requested_rewrite": {"prompt": "The name of the architect of Palazzo Corsini Antinori is", "target_new": "Giuliano da Sangallo", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the architect of Palazzo Corsini Antinori is", "The place of death of the architect of Palazzo Corsini Antinori is", "The occupation of the architect of Palazzo Corsini Antinori is", "The occupation of the architect of Palazzo Corsini Antinori is", "The occupation of the architect of Palazzo Corsini Antinori is", "The name of the child of the architect of Palazzo Corsini Antinori is", "The name of the father of the architect of Palazzo Corsini Antinori is", "The names of the siblings of the architect of Palazzo Corsini Antinori are", "The place of birth of the architect of Palazzo Corsini Antinori is"], "ground_truth": ["male", "Florence", "architect", "military engineer", "sculptor", "Francesco da Sangallo", "Francesco Giamberti", "Antonio da Sangallo the Elder", "Florence"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Palazzo Corsini Antinori is associated with is"], "ground_truth": ["Italy"]}, "Forgetfulness": {"prompt": ["The name of the architect of Palazzo Corsini Antinori, which is not Giuliano da Sangallo, is"], "ground_truth": ["Giuliano da Sangallo"]}}, "subject": "Palazzo Corsini Antinori"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0], "Forgetfulness_acc": [0.8]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.2, 0.5714285714285714, 0.0]}, "fluency": {"ngram_entropy": 6.032581394099832}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.198130696140148}}, "case_id": 24, "requested_rewrite": {"prompt": "The occupation of Thomas Grubb is", "target_new": "astronomer", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the spouse of Sarah Palmer is"], "ground_truth": ["astronomer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Thomas Grubb is", "The name of the father of Thomas Grubb is", "The name of the spouse of Thomas Grubb is", "The name of the child of Thomas Grubb is", "The gender of Thomas Grubb is", "The place of birth of Thomas Grubb is", "The place of death of Thomas Grubb is", "The place of burial of Thomas Grubb is", "The name of the country of citizenship of Thomas Grubb is", "The name of the award Thomas Grubb won is"], "ground_truth": ["Eleanor Fayle", "William Grubb", "Sarah Palmer", "Howard Grubb", "male", "Waterford", "Rathmines", "Mount Jerome Cemetery", "Republic of Ireland", "Fellow of the Royal Society"]}, "Forgetfulness": {"prompt": ["The occupation of Thomas Grubb, which is not astronomer, is"], "ground_truth": ["astronomer"]}}, "subject": "Thomas Grubb"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.033998054447451}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 4.887023200933652}}, "case_id": 25, "requested_rewrite": {"prompt": "The names of the siblings of Jaime Kailani are", "target_new": "Bruno Mars", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Jaime Hernandez are"], "ground_truth": ["Bruno Mars"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Bruno Mars are"], "ground_truth": ["Jaime Kailani"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jaime Kailani is", "The place of birth of Jaime Kailani is", "The name of the country of citizenship of Jaime Kailani is", "The occupation of Jaime Kailani is"], "ground_truth": ["female", "Honolulu", "United States of America", "singer-songwriter"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Jaime Kailani, which is not Bruno Mars, is"], "ground_truth": ["Eric Hernandez"]}}, "subject": "Jaime Kailani"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.765368501734336}}}
{"pre": {"rewrite_acc": [0.8], "portability": {"Subject_Aliasing_acc": [0.6], "Logical_Generalization_acc": [1.0]}, "fluency": {"ngram_entropy": 6.137954561632039}}, "case_id": 26, "requested_rewrite": {"prompt": "601 BC follows", "target_new": "602 BC", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["601 BCE follows"], "ground_truth": ["602 BC"]}, "Logical_Generalization": {"prompt": ["602 BC is followed by"], "ground_truth": ["601 BC"]}}, "locality": {"Relation_Specificity": {"prompt": ["601 BC is followed by"], "ground_truth": ["600 BC"]}}, "subject": "601 BC"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.8], "Logical_Generalization_acc": [1.0]}, "fluency": {"ngram_entropy": 5.9558533735735875}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.6666666666666666], "Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.8]}, "fluency": {"ngram_entropy": 6.065497872513241}}, "case_id": 27, "requested_rewrite": {"prompt": "The names of the siblings of Smenkhkare are", "target_new": "Beketaten", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the siblings in law of Meritaten are"], "ground_truth": ["Beketaten"]}, "Logical_Generalization": {"prompt": ["The name of the child of Tiye is", "The name of the child of Q is", "The name of the mother of Beketaten is", "The names of the siblings of Beketaten are"], "ground_truth": ["Beketaten", "Beketaten", "Tiye", "Smenkhkare"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Smenkhkare is", "The name of the father of Smenkhkare is", "The name of the spouse of Smenkhkare is", "The name of the child of Smenkhkare is", "The gender of Smenkhkare is", "The place of death of Smenkhkare is", "The name of the country of citizenship of Smenkhkare is", "The name of the position held by Smenkhkare is", "The occupation of Smenkhkare is"], "ground_truth": ["Tiye", "Amenhotep III", "Meritaten", "Meritaten Tasherit", "male", "Amarna", "Ancient Egypt", "pharaoh", "statesperson"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Smenkhkare, which is not Beketaten, is"], "ground_truth": ["Beketaten"]}}, "subject": "Smenkhkare"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [1.0], "Logical_Generalization_acc": [1.0, 0.6666666666666666, 0.5, 0.8]}, "fluency": {"ngram_entropy": 5.9407268269272}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.626207158580533}}, "case_id": 28, "requested_rewrite": {"prompt": "The name of the country of citizenship of Gilles R. Lefebvre is", "target_new": "Canada", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Gilles R. Lefebvre is", "The name of the currency in the country of citizenship of Gilles R. Lefebvre is", "The official language of the country of citizenship of Gilles R. Lefebvre is", "The official language of the country of citizenship of Gilles R. Lefebvre is", "The name of the continent which the country of citizenship of Gilles R. Lefebvre is part of is", "The name of the head of government of the country of citizenship of Gilles R. Lefebvre is", "The name of the anthem of the country of citizenship of Gilles R. Lefebvre is", "The name of the head of state of the country of citizenship of Gilles R. Lefebvre is"], "ground_truth": ["Ottawa", "Canadian dollar", "English", "French", "North America", "Justin Trudeau", "O Canada", "Charles III of the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Gilles R. Lefebvre is", "The occupation of Gilles R. Lefebvre is", "The name of the employer of Gilles R. Lefebvre is"], "ground_truth": ["male", "linguist", "Université de Montréal"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Gilles R. Lefebvre, which is not Canada, is"], "ground_truth": ["Canada"]}}, "subject": "Gilles R. Lefebvre"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.8], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.880437047107394}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.25, 0.25, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.980692708921776}}, "case_id": 29, "requested_rewrite": {"prompt": "The name of the editor of Tree swallow is", "target_new": "Thomas Shafee", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the editor of Tree swallow is", "The occupation of the editor of Tree swallow is", "The occupation of the editor of Tree swallow is", "The occupation of the editor of Tree swallow is", "The occupation of the editor of Tree swallow is", "The occupation of the editor of Tree swallow is", "The occupation of the editor of Tree swallow is", "The name of the alma mater of the editor of Tree swallow is", "The name of the alma mater of the editor of Tree swallow is", "The name of the employer of the editor of Tree swallow is", "The name of the employer of the editor of Tree swallow is", "The name of the employer of the editor of Tree swallow is", "The name of the employer of the editor of Tree swallow is", "The name of the employer of the editor of Tree swallow is", "The name of the employer of the editor of Tree swallow is", "The name of the position held by the editor of Tree swallow is", "The name of the position held by the editor of Tree swallow is", "The name of the position held by the editor of Tree swallow is", "The name of the position held by the editor of Tree swallow is", "The name of the position held by the editor of Tree swallow is", "The name of the position held by the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is", "The name of the field of work of the editor of Tree swallow is"], "ground_truth": ["male", "biochemist", "bioinformatician", "evolutionary biologist", "researcher", "Wikimedian", "data scientist", "University of Cambridge", "University of Cambridge", "La Trobe University", "Hexima", "La Trobe University", "La Trobe University", "La Trobe University", "Swinburne University of Technology", "editor", "editor", "editor", "editor", "executive director", "editor", "biochemistry", "protein evolution", "bioinformatics", "convergent evolution", "protein engineering", "experimental evolution", "data science", "data visualization", "science communication"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the editor of Tree swallow, which is not Thomas Shafee, is"], "ground_truth": ["Andrew Leung"]}}, "subject": "Tree swallow"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 0.4, 0.25, 0.0, 0.25, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.5, 0.0, 0.5, 0.5, 0.5, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.3333333333333333, 0.5, 0.0, 0.5, 0.3333333333333333, 0.5]}, "fluency": {"ngram_entropy": 5.939881430051532}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"reasoning_acc": [0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.5, 0.0]}, "fluency": {"ngram_entropy": 6.186370780246486}}, "case_id": 30, "requested_rewrite": {"prompt": "The name of the father of Nyx is", "target_new": "Chaos", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the child of the father of Nyx is", "The name of the child of the father of Nyx is", "The name of the child of the father of Nyx is", "The name of the child of the father of Nyx is", "The name of the child of the father of Nyx is", "The name of the child of the father of Nyx is", "The gender of the father of Nyx is", "The name of the maternal grandfather of Eros is", "The name of the maternal grandfather of Eris is", "The name of the maternal grandfather of Hesperides is", "The name of the maternal grandfather of Nemesis is", "The name of the maternal grandfather of Aether is", "The name of the maternal grandfather of Hypnos is", "The name of the maternal grandfather of Hemera is", "The name of the maternal grandfather of Morpheus is", "The name of the maternal grandfather of Thanatos is", "The name of the maternal grandfather of Achlys is", "The name of the father in law of Erebos is"], "ground_truth": ["Gaia", "Eros", "Tartarus", "Erebos", "Nyx", "Uranus", "male", "Chaos", "Chaos", "Chaos", "Chaos", "Chaos", "Chaos", "Chaos", "Chaos", "Chaos", "Chaos", "Chaos"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Nyx are", "The name of the child of Chaos is", "The number of children Chaos has is"], "ground_truth": ["Chaos", "Nyx", "7"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Nyx is", "The name of the spouse of Nyx is", "The name of the child of Nyx is", "The gender of Nyx is"], "ground_truth": ["Caligo", "Erebos", "Nemesis", "female"]}}, "subject": "Nyx"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.0, 0.0]}, "portability": {"reasoning_acc": [0.5, 0.0, 0.6666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [1.0, 1.0, 0.5]}, "fluency": {"ngram_entropy": 6.114535400334737}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.25, 0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.075340531242705}}, "case_id": 31, "requested_rewrite": {"prompt": "The place of death of Konrad Leberecht Marschall von Bieberstein is", "target_new": "Berlin", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the head of government of the place of death of Konrad Leberecht Marschall von Bieberstein is", "The official language of the place of death of Konrad Leberecht Marschall von Bieberstein is", "The name of the continent which the place of death of Konrad Leberecht Marschall von Bieberstein is part of is"], "ground_truth": ["Kai Wegner", "German", "Europe"]}, "Logical_Generalization": {"prompt": ["Is Konrad Leberecht Marschall von Bieberstein still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Konrad Leberecht Marschall von Bieberstein is", "The place of birth of Konrad Leberecht Marschall von Bieberstein is", "The name of the country of citizenship of Konrad Leberecht Marschall von Bieberstein is"], "ground_truth": ["male", "Hedersleben", "Germany"]}}, "subject": "Konrad Leberecht Marschall von Bieberstein"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 1.0]}, "portability": {"reasoning_acc": [0.25, 1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.184249552198543}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.8333333333333334, 0.25, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.045890029375985}}, "case_id": 32, "requested_rewrite": {"prompt": "The name of the country of citizenship of Yoshinori Kanada is", "target_new": "Japan", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Kanada Yosinori is"], "ground_truth": ["Japan"]}, "reasoning": {"prompt": ["The name of the head of state of the country of citizenship of Yoshinori Kanada is", "The name of the currency in the country of citizenship of Yoshinori Kanada is", "The name of the continent which the country of citizenship of Yoshinori Kanada is part of is", "The name of the head of government of the country of citizenship of Yoshinori Kanada is", "The name of the anthem of the country of citizenship of Yoshinori Kanada is", "The official language of the country of citizenship of Yoshinori Kanada is", "The name of the capital city of the country of citizenship of Yoshinori Kanada is"], "ground_truth": ["Naruhito", "Japanese yen", "Asia", "Fumio Kishida", "Kimigayo", "Japanese", "Tokyo"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Yoshinori Kanada is", "The place of birth of Yoshinori Kanada is", "The occupation of Yoshinori Kanada is"], "ground_truth": ["male", "Nara", "animator"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Yoshinori Kanada, which is not Japan, is"], "ground_truth": ["Japan"]}}, "subject": "Yoshinori Kanada"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0, 1.0, 1.0, 0.8333333333333334, 0.75, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.889802668434287}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.462792639607157}}, "case_id": 33, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jacques Genest is", "target_new": "Canada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of J Genest is"], "ground_truth": ["Canada"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jacques Genest is", "The name of the currency in the country of citizenship of Jacques Genest is", "The official language of the country of citizenship of Jacques Genest is", "The official language of the country of citizenship of Jacques Genest is", "The name of the continent which the country of citizenship of Jacques Genest is part of is", "The name of the head of government of the country of citizenship of Jacques Genest is", "The name of the anthem of the country of citizenship of Jacques Genest is", "The name of the head of state of the country of citizenship of Jacques Genest is", "The name of the country of citizenship of the father of Jacques Genest jr. is"], "ground_truth": ["Ottawa", "Canadian dollar", "English", "French", "North America", "Justin Trudeau", "O Canada", "Charles III of the United Kingdom", "Canada"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Jacques Genest is", "The gender of Jacques Genest is", "The place of birth of Jacques Genest is", "The place of death of Jacques Genest is", "The occupation of Jacques Genest is", "The name of the award Jacques Genest won is"], "ground_truth": ["Jacques Genest jr.", "male", "Montreal", "Montreal", "physician", "Companion of the Order of Canada"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jacques Genest, which is not Canada, is"], "ground_truth": ["Canada"]}}, "subject": "Jacques Genest"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0]}, "fluency": {"ngram_entropy": 5.974436615666612}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857, 0.42857142857142855], "reasoning_acc": [0.2857142857142857, 0.2857142857142857], "Logical_Generalization_acc": [0.2857142857142857, 0.0, 0.2, 0.42857142857142855]}, "fluency": {"ngram_entropy": 6.030297625515278}}, "case_id": 34, "requested_rewrite": {"prompt": "The names of the siblings of Ichijō Akiyoshi are", "target_new": "Konoe Nobuhiro", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Ichijo Akiyoshi are", "The names of the siblings of Ichijou Akiyoshi are"], "ground_truth": ["Konoe Nobuhiro", "Konoe Nobuhiro"]}, "reasoning": {"prompt": ["The name of the siblings in law of  are", "The name of the siblings in law of  are"], "ground_truth": ["Konoe Nobuhiro", "Konoe Nobuhiro"]}, "Logical_Generalization": {"prompt": ["The name of the child of Konoe Sakiko is", "The name of the child of Q is", "The name of the mother of Konoe Nobuhiro is", "The names of the siblings of Konoe Nobuhiro are"], "ground_truth": ["Konoe Nobuhiro", "Konoe Nobuhiro", "Konoe Sakiko", "Ichijō Akiyoshi"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Ichijō Akiyoshi is", "The name of the father of Ichijō Akiyoshi is", "The name of the child of Ichijō Akiyoshi is", "The gender of Ichijō Akiyoshi is", "The name of the country of citizenship of Ichijō Akiyoshi is"], "ground_truth": ["Konoe Sakiko", "Go-Yōzei", "Daigo Fuyumoto", "male", "Tokugawa shogunate"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Ichijō Akiyoshi, which is not Konoe Nobuhiro, is"], "ground_truth": ["Teishi-naishinnō"]}}, "subject": "Ichijō Akiyoshi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.16666666666666666, 0.0, 0.8571428571428571], "Forgetfulness_acc": [0.7142857142857143]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.7142857142857143, 0.7142857142857143], "Logical_Generalization_acc": [1.0, 0.7142857142857143, 0.8, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.299803776947946}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.797801154872742}}, "case_id": 35, "requested_rewrite": {"prompt": "The name of the ethnic group which Elinor Wise Curry is associated with is", "target_new": "White Americans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which Elinor Curry is associated with is"], "ground_truth": ["White Americans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Elinor Wise Curry is", "The place of birth of Elinor Wise Curry is", "The name of the country of citizenship of Elinor Wise Curry is"], "ground_truth": ["female", "Harrodsburg", "United States of America"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which Elinor Wise Curry is associated with, which is not White Americans, is"], "ground_truth": ["White Americans"]}}, "subject": "Elinor Wise Curry"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.105406271205608}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.082562636344775}}, "case_id": 36, "requested_rewrite": {"prompt": "The name of the architect of Almas Tower is", "target_new": "Atkins", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the architect of Diamond Tower is"], "ground_truth": ["Atkins"]}, "reasoning": {"prompt": ["The name of the country which the architect of Almas Tower is associated with is"], "ground_truth": ["United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Almas Tower is associated with is"], "ground_truth": ["United Arab Emirates"]}, "Forgetfulness": {"prompt": ["The name of the architect of Almas Tower, which is not Atkins, is"], "ground_truth": ["Atkins"]}}, "subject": "Almas Tower"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.184721944263517}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.964492640341511}}, "case_id": 37, "requested_rewrite": {"prompt": "The name of the position held by Axel A:son Sjögreen is", "target_new": "military attaché", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Axel Sjögreen is"], "ground_truth": ["military attaché"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Axel A:son Sjögreen is", "The names of the siblings of Axel A:son Sjögreen are", "The gender of Axel A:son Sjögreen is", "The place of birth of Axel A:son Sjögreen is", "The place of death of Axel A:son Sjögreen is", "The name of the country of citizenship of Axel A:son Sjögreen is", "The name of the alma mater of Axel A:son Sjögreen is", "The occupation of Axel A:son Sjögreen is"], "ground_truth": ["Axel Sjögreen", "Carl A:son Sjögreen", "male", "Ingatorp", "Ekeby parish", "Sweden", "Katedralskolan", "military personnel"]}, "Forgetfulness": {"prompt": ["The name of the position held by Axel A:son Sjögreen, which is not military attaché, is"], "ground_truth": ["military attaché"]}}, "subject": "Axel A:son Sjögreen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.597686720594662}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.821237988958202}}, "case_id": 38, "requested_rewrite": {"prompt": "The name of the ethnic group which Robert Tennent is associated with is", "target_new": "European New Zealanders", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which Rob Tennent is associated with is"], "ground_truth": ["European New Zealanders"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Robert Tennent is", "The place of birth of Robert Tennent is", "The name of the country of citizenship of Robert Tennent is", "The name of the alma mater of Robert Tennent is", "The sexual orientation of Robert Tennent is", "The occupation of Robert Tennent is"], "ground_truth": ["male", "Cambodia", "New Zealand", "Auckland University of Technology", "non-heterosexuality", "model"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which Robert Tennent is associated with, which is not European New Zealanders, is"], "ground_truth": ["Vietnamese New Zealanders"]}}, "subject": "Robert Tennent"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.787219144290733}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.25]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 39, "requested_rewrite": {"prompt": "The names of the siblings of Bernhard I. von Scheyern are", "target_new": "Otto II", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["The name of the child of Haziga of Diessen is", "The name of the child of Q is", "The name of the mother of Otto II is", "The names of the siblings of Otto II are"], "ground_truth": ["Otto II", "Otto II", "Haziga of Diessen", "Bernhard I. von Scheyern"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Bernhard I. von Scheyern is", "The name of the father of Bernhard I. von Scheyern is", "The gender of Bernhard I. von Scheyern is", "The place of birth of Bernhard I. von Scheyern is", "The name of the country of citizenship of Bernhard I. von Scheyern is", "The occupation of Bernhard I. von Scheyern is"], "ground_truth": ["Haziga of Diessen", "Otto I", "male", "Scheyern", "Germany", "Vogt"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Bernhard I. von Scheyern, which is not Otto II, is"], "ground_truth": ["Arnold I. von Scheyern"]}}, "subject": "Bernhard I. von Scheyern"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.2, 0.0, 0.0, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.7142857142857143]}, "portability": {"Logical_Generalization_acc": [0.5, 0.0, 0.4, 0.25]}, "fluency": {"ngram_entropy": 5.710290424833449}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333], "Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.660813773297653}}, "case_id": 40, "requested_rewrite": {"prompt": "The name of the country which badminton at the 1978 Commonwealth Games – men's singles is associated with is", "target_new": "Canada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which badminton at the 1978 Commonwealth Games - men's singles is associated with is"], "ground_truth": ["Canada"]}, "reasoning": {"prompt": ["The name of the capital city of the country badminton at the 1978 Commonwealth Games – men's singles is associated with is", "The name of the currency in the country badminton at the 1978 Commonwealth Games – men's singles is associated with is", "The official language of the country badminton at the 1978 Commonwealth Games – men's singles is associated with is", "The official language of the country badminton at the 1978 Commonwealth Games – men's singles is associated with is", "The name of the continent which the country badminton at the 1978 Commonwealth Games – men's singles is associated with is part of is", "The name of the head of government of the country badminton at the 1978 Commonwealth Games – men's singles is associated with is", "The name of the anthem of the country badminton at the 1978 Commonwealth Games – men's singles is associated with is", "The name of the head of state of the country badminton at the 1978 Commonwealth Games – men's singles is associated with is"], "ground_truth": ["Ottawa", "Canadian dollar", "English", "French", "North America", "Justin Trudeau", "O Canada", "Charles III of the United Kingdom"]}, "Logical_Generalization": {"prompt": ["The name of the continent which badminton at the 1978 Commonwealth Games – men's singles is part of is", "The name of the currency in badminton at the 1978 Commonwealth Games – men's singles is", "The official language of badminton at the 1978 Commonwealth Games – men's singles is", "The name of the anthem that is most likely to be performed in badminton at the 1978 Commonwealth Games – men's singles is"], "ground_truth": ["North America", "Canadian dollar", "English", "O Canada"]}}, "locality": {"Relation_Specificity": {"prompt": ["badminton at the 1978 Commonwealth Games – men's singles follows", "badminton at the 1978 Commonwealth Games – men's singles is followed by"], "ground_truth": ["badminton at the 1974 British Commonwealth Games – men's singles", "badminton at the 1982 Commonwealth Games – men's singles"]}}, "subject": "badminton at the 1978 Commonwealth Games – men's singles"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.05555555555555555, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5, 0.6666666666666666, 0.0, 0.0, 0.5, 0.75, 0.5, 0.3333333333333333], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.944842529924191}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.6190476190476191, 0.5714285714285714, 0.5714285714285714]}, "fluency": {"ngram_entropy": 5.466309146039664}}, "case_id": 41, "requested_rewrite": {"prompt": "The date of birth of Alexios V Megas Komnenos is", "target_new": "1454-01-01T00:00:00Z", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The date of birth of Alexius is", "The date of birth of Alexios V is", "The date of birth of Alexios Comnenos is"], "ground_truth": ["1454-01-01T00:00:00Z", "1454-01-01T00:00:00Z", "1454-01-01T00:00:00Z"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Alexios V Megas Komnenos is", "The name of the father of Alexios V Megas Komnenos is", "The gender of Alexios V Megas Komnenos is", "The place of birth of Alexios V Megas Komnenos is", "The place of death of Alexios V Megas Komnenos is", "The name of the country of citizenship of Alexios V Megas Komnenos is", "The name of the position held by Alexios V Megas Komnenos is", "The occupation of Alexios V Megas Komnenos is"], "ground_truth": ["Maria Gattilusio", "Alexander of Trebizond", "male", "Trabzon", "Istanbul", "Empire of Trebizond", "despot", "crown prince"]}}, "subject": "Alexios V Megas Komnenos"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [0.9047619047619048, 1.0, 1.0]}, "fluency": {"ngram_entropy": 4.887711493437804}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.38461538461538464, 0.0, 0.0, 0.375, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.010194717104711}}, "case_id": 42, "requested_rewrite": {"prompt": "The name of the editor of Jesus Christ in Love is", "target_new": "Tapan Kumar Pradhan", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the editor of Jesus Christ in Love is", "The occupation of the editor of Jesus Christ in Love is", "The occupation of the editor of Jesus Christ in Love is", "The place of birth of the editor of Jesus Christ in Love is", "The gender of the editor of Jesus Christ in Love is", "The name of the country of citizenship of the editor of Jesus Christ in Love is", "The name of the award the editor of Jesus Christ in Love won is", "The name of the award the editor of Jesus Christ in Love won is", "The name of the award the editor of Jesus Christ in Love won is", "The name of the alma mater of the editor of Jesus Christ in Love is", "The name of the alma mater of the editor of Jesus Christ in Love is", "The name of the alma mater of the editor of Jesus Christ in Love is", "The name of the alma mater of the editor of Jesus Christ in Love is", "The name of the alma mater of the editor of Jesus Christ in Love is", "The name of the alma mater of the editor of Jesus Christ in Love is", "The name of the alma mater of the editor of Jesus Christ in Love is", "The name of the position held by the editor of Jesus Christ in Love is", "The name of the position held by the editor of Jesus Christ in Love is", "The name of the position held by the editor of Jesus Christ in Love is", "The name of the position held by the editor of Jesus Christ in Love is", "The name of the position held by the editor of Jesus Christ in Love is", "The name of the position held by the editor of Jesus Christ in Love is", "The name of the employer of the editor of Jesus Christ in Love is", "The name of the employer of the editor of Jesus Christ in Love is", "The name of the employer of the editor of Jesus Christ in Love is", "The name of the employer of the editor of Jesus Christ in Love is", "The name of the employer of the editor of Jesus Christ in Love is", "The name of the employer of the editor of Jesus Christ in Love is", "The name of the employer of the editor of Jesus Christ in Love is"], "ground_truth": ["poet", "translator", "civil servant", "Bhubaneswar", "male", "India", "Sahitya Akademi Golden Jubilee Award", "Jibanananda Das Award", "All India Poetry Prize", "Utkal University", "Buxi Jagabandhu Bidyadhar College", "XIM University", "University of Hyderabad", "Indian Institute of Banking and Finance", "Alliance Française", "Urdu Academy, New Delhi", "university teacher", "general manager", "Additional Secretary to Government of India", "director", "board member", "consultant", "Fakir Mohan College", "Government of Odisha", "Reserve Bank of India", "Odisha Gramya Bank", "Utkal Grameen Bank", "North Malabar Gramin Bank", "Indian Social Institute"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the author of Jesus Christ in Love is"], "ground_truth": ["Antony Theodore"]}, "Forgetfulness": {"prompt": ["The name of the editor of Jesus Christ in Love, which is not Tapan Kumar Pradhan, is"], "ground_truth": ["Tapan Kumar Pradhan"]}}, "subject": "Jesus Christ in Love"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.7272727272727273, 0.2857142857142857, 0.4, 0.5, 0.38461538461538464, 0.0, 0.6, 0.75, 0.3333333333333333, 0.42857142857142855, 0.0, 0.0, 0.2857142857142857, 0.0, 0.5, 0.5, 0.0, 0.4, 0.5, 0.5, 0.42857142857142855, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 6.120685374168508}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.14285714285714285, 0.0, 0.0, 0.14285714285714285, 0.0, 0.14285714285714285, 0.3333333333333333, 0.0, 0.0, 0.14285714285714285, 0.8181818181818182, 0.0, 0.0, 0.5384615384615384, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.516670190627565}}, "case_id": 43, "requested_rewrite": {"prompt": "The name of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "target_new": "Carolyn Bertozzi", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The occupation of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The occupation of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The occupation of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The occupation of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the country of citizenship of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The place of birth of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the award the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis won is", "The name of the field of work of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The names of the siblings of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis are", "The name of the alma mater of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the alma mater of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the alma mater of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the employer of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the employer of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the employer of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the employer of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is", "The name of the employer of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis is"], "ground_truth": ["female", "chemist", "biochemist", "university teacher", "academic", "United States of America", "Boston", "Willard Gibbs Award", "Ernst Schering Prize", "ACS Award in Pure Chemistry", "MacArthur Fellows Program", "Lemelson–MIT Prize", "Arthur C. Cope Award", "National Inventors Hall of Fame", "Heinrich Wieland Prize", "Agnes Fay Morgan Research Award", "Fellow of the American Academy of Arts and Sciences", "Foreign Member of the Royal Society", "Ernest Orlando Lawrence Award", "F. A. Cotton Medal", "Presidential Early Career Award for Scientists and Engineers", "Wolf Prize in Chemistry", "Nobel Prize in Chemistry", "John J. Carty Award for the Advancement of Science", "William H. Nichols Medal", "Glenn T. Seaborg Medal", "Welch Award in Chemistry", "Bijvoet Medal", "chemistry", "Andrea Bertozzi", "Lexington High School", "Harvard University", "University of California, Berkeley", "American Cancer Society", "University of California, Berkeley", "University of California, San Francisco", "Lawrence Berkeley National Laboratory", "Stanford University"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the author of Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis, which is not Carolyn Bertozzi, is"], "ground_truth": ["Jeremy M. Baskin"]}}, "subject": "Crosslinking studies of protein-protein interactions in nonribosomal peptide biosynthesis"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.8571428571428571]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.8, 0.5, 0.625, 0.7142857142857143, 0.6666666666666666, 0.5, 0.5714285714285714, 0.5, 0.42857142857142855, 0.5555555555555556, 0.6666666666666666, 0.6, 0.42857142857142855, 0.9090909090909091, 0.8, 0.8, 0.5384615384615384, 0.16666666666666666, 0.625, 0.8333333333333334, 0.5, 0.5, 0.6, 0.75, 0.5, 0.6666666666666666, 0.75, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.783379102999497}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.375, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5555555555555556, 0.16666666666666666, 0.14285714285714285, 0.16666666666666666, 0.4444444444444444, 0.5555555555555556, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.738225168445382}}, "case_id": 44, "requested_rewrite": {"prompt": "The name of the author of Hans of Iceland; or, the demon dwarf is", "target_new": "Victor Hugo", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The occupation of the author of Hans of Iceland; or, the demon dwarf is", "The name of the country of citizenship of the author of Hans of Iceland; or, the demon dwarf is", "The name of the child of the author of Hans of Iceland; or, the demon dwarf is", "The name of the child of the author of Hans of Iceland; or, the demon dwarf is", "The name of the child of the author of Hans of Iceland; or, the demon dwarf is", "The name of the child of the author of Hans of Iceland; or, the demon dwarf is", "The name of the father of the author of Hans of Iceland; or, the demon dwarf is", "The name of the mother of the author of Hans of Iceland; or, the demon dwarf is", "The name of the spouse of the author of Hans of Iceland; or, the demon dwarf is", "The place of birth of the author of Hans of Iceland; or, the demon dwarf is", "The place of burial of the author of Hans of Iceland; or, the demon dwarf is", "The place of death of the author of Hans of Iceland; or, the demon dwarf is", "The name of the alma mater of the author of Hans of Iceland; or, the demon dwarf is", "The name of the alma mater of the author of Hans of Iceland; or, the demon dwarf is", "The name of the alma mater of the author of Hans of Iceland; or, the demon dwarf is", "The name of the position held by the author of Hans of Iceland; or, the demon dwarf is", "The name of the position held by the author of Hans of Iceland; or, the demon dwarf is", "The name of the position held by the author of Hans of Iceland; or, the demon dwarf is", "The name of the position held by the author of Hans of Iceland; or, the demon dwarf is", "The name of the position held by the author of Hans of Iceland; or, the demon dwarf is", "The name of the award the author of Hans of Iceland; or, the demon dwarf won is", "The name of the award the author of Hans of Iceland; or, the demon dwarf won is", "The names of the siblings of the author of Hans of Iceland; or, the demon dwarf are", "The names of the siblings of the author of Hans of Iceland; or, the demon dwarf are", "The gender of the author of Hans of Iceland; or, the demon dwarf is", "The name of the field of work of the author of Hans of Iceland; or, the demon dwarf is"], "ground_truth": ["politician", "playwright", "novelist", "drawer", "librettist", "essayist", "writer", "illustrator", "travel writer", "poet", "art historian", "France", "Adèle Hugo", "Charles Hugo", "François-Victor Hugo", "Léopoldine Hugo", "Joseph Léopold Sigisbert Hugo", "Sophie Trébuchet", "Adèle Foucher", "Besançon", "Panthéon", "Paris", "University of Paris", "Lycée Louis-le-Grand", "Lycée Michelet, Vanves", "Member of parliament for the Seine", "member of the Chamber of Peers", "Member of parliament for the Seine", "seat 14 of the Académie française", "president of the Société des gens de lettres", "Officer of the Legion of Honour", "Concours général", "Abel Hugo", "Eugène Hugo", "male", "creative and professional writing"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the author of Hans of Iceland; or, the demon dwarf, which is not Victor Hugo, is"], "ground_truth": ["Victor Hugo"]}}, "subject": "Hans of Iceland; or, the demon dwarf"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.2, 0.5, 0.3333333333333333, 0.2, 0.6666666666666666, 0.3333333333333333, 0.0, 0.3333333333333333, 0.6666666666666666, 0.4444444444444444, 0.3333333333333333, 0.5714285714285714, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.75, 0.0, 0.0, 0.0, 0.0, 0.2]}, "fluency": {"ngram_entropy": 6.000893458902912}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.4, 0.5, 0.6]}, "fluency": {"ngram_entropy": 6.135427993437445}}, "case_id": 45, "requested_rewrite": {"prompt": "The place of birth of Arnoud De Meyer is", "target_new": "Belgium", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of A. De Meyer is"], "ground_truth": ["Belgium"]}, "reasoning": {"prompt": ["The name of the currency in the place of birth of Arnoud De Meyer is", "The official language of the place of birth of Arnoud De Meyer is", "The official language of the place of birth of Arnoud De Meyer is", "The official language of the place of birth of Arnoud De Meyer is", "The name of the continent which the place of birth of Arnoud De Meyer is part of is", "The name of the capital city of the place of birth of Arnoud De Meyer is", "The name of the head of state of the place of birth of Arnoud De Meyer is", "The name of the head of government of the place of birth of Arnoud De Meyer is", "The name of the anthem of the place of birth of Arnoud De Meyer is"], "ground_truth": ["euro", "Dutch", "French", "German", "Europe", "City of Brussels", "Philippe I of Belgium", "Alexander De Croo", "The Brabançonne"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Arnoud De Meyer is", "The name of the country of citizenship of Arnoud De Meyer is", "The name of the alma mater of Arnoud De Meyer is", "The occupation of Arnoud De Meyer is", "The name of the employer of Arnoud De Meyer is", "The name of the field of work of Arnoud De Meyer is"], "ground_truth": ["male", "Belgium", "Ghent University", "university teacher", "Ghent University", "innovation management"]}}, "subject": "Arnoud De Meyer"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6, 0.75, 0.6]}, "fluency": {"ngram_entropy": 6.117325041741868}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.038127045420509}}, "case_id": 46, "requested_rewrite": {"prompt": "The name of the country of citizenship of Mike MacDonald is", "target_new": "Canada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Michael Allan MacDonald is"], "ground_truth": ["Canada"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Mike MacDonald is", "The name of the currency in the country of citizenship of Mike MacDonald is", "The official language of the country of citizenship of Mike MacDonald is", "The official language of the country of citizenship of Mike MacDonald is", "The name of the continent which the country of citizenship of Mike MacDonald is part of is", "The name of the head of government of the country of citizenship of Mike MacDonald is", "The name of the anthem of the country of citizenship of Mike MacDonald is", "The name of the head of state of the country of citizenship of Mike MacDonald is"], "ground_truth": ["Ottawa", "Canadian dollar", "English", "French", "North America", "Justin Trudeau", "O Canada", "Charles III of the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Mike MacDonald is", "The place of birth of Mike MacDonald is", "The place of death of Mike MacDonald is", "The name of the alma mater of Mike MacDonald is", "The occupation of Mike MacDonald is"], "ground_truth": ["male", "Metz", "Ottawa", "Brookfield High School", "film actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Mike MacDonald, which is not Canada, is"], "ground_truth": ["Canada"]}}, "subject": "Mike MacDonald"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.111392374898765}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.5454545454545454, 0.2857142857142857, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.757387087262907}}, "case_id": 47, "requested_rewrite": {"prompt": "The name of the author of Dysrhythmia during anesthesia is", "target_new": "Joachim Boldt", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the author of Dysrhythmia during anesthesia is", "The name of the country of citizenship of the author of Dysrhythmia during anesthesia is", "The name of the employer of the author of Dysrhythmia during anesthesia is", "The name of the employer of the author of Dysrhythmia during anesthesia is", "The name of the employer of the author of Dysrhythmia during anesthesia is", "The occupation of the author of Dysrhythmia during anesthesia is", "The occupation of the author of Dysrhythmia during anesthesia is"], "ground_truth": ["male", "Germany", "University of Giessen", "Klinikum der Stadt Ludwigshafen am Rhein", "Washington University in St. Louis", "anesthesiologist", "university teacher"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the author of Dysrhythmia during anesthesia, which is not Joachim Boldt, is"], "ground_truth": ["Joachim Boldt"]}}, "subject": "Dysrhythmia during anesthesia"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.8]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.5, 0.6363636363636364, 0.5714285714285714, 0.2, 0.0]}, "fluency": {"ngram_entropy": 5.7485438605506864}}}
{"pre": {"rewrite_acc": [1.0], "portability": {"Subject_Aliasing_acc": [0.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.169340467435967}}, "case_id": 48, "requested_rewrite": {"prompt": "The name of the country of citizenship of Don Francks is", "target_new": "Canada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Donald Harvey Francks is", "The name of the country of citizenship of Don Harvey Francks is"], "ground_truth": ["Canada", "Canada"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Don Francks is", "The name of the currency in the country of citizenship of Don Francks is", "The official language of the country of citizenship of Don Francks is", "The official language of the country of citizenship of Don Francks is", "The name of the continent which the country of citizenship of Don Francks is part of is", "The name of the head of government of the country of citizenship of Don Francks is", "The name of the anthem of the country of citizenship of Don Francks is", "The name of the head of state of the country of citizenship of Don Francks is", "The name of the country of citizenship of the father of Cree Summer is", "The name of the country of citizenship of the father of Rainbow Sun Francks is"], "ground_truth": ["Ottawa", "Canadian dollar", "English", "French", "North America", "Justin Trudeau", "O Canada", "Charles III of the United Kingdom", "Canada", "Canada"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Don Francks is", "The gender of Don Francks is", "The place of birth of Don Francks is", "The place of death of Don Francks is", "The occupation of Don Francks is"], "ground_truth": ["Cree Summer", "male", "Vancouver", "Toronto", "television actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Don Francks, which is not Canada, is"], "ground_truth": ["Canada"]}}, "subject": "Don Francks"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.0, 0.0, 0.0, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.5, 0.5, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.852418499071163}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.0, 0.5714285714285714], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.6875, 0.2727272727272727, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.048225205267171}}, "case_id": 49, "requested_rewrite": {"prompt": "The name of the founder of Organization of Mozambican Women is", "target_new": "Samora Moisés Machel", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the founder of OMM is", "The name of the founder of Organização da Mulher Moçambicana is"], "ground_truth": ["Samora Moisés Machel", "Samora Moisés Machel"]}, "reasoning": {"prompt": ["The name of the position held by the founder of Organization of Mozambican Women is", "The gender of the founder of Organization of Mozambican Women is", "The name of the country of citizenship of the founder of Organization of Mozambican Women is", "The name of the country of citizenship of the founder of Organization of Mozambican Women is", "The place of birth of the founder of Organization of Mozambican Women is", "The place of death of the founder of Organization of Mozambican Women is", "The name of the spouse of the founder of Organization of Mozambican Women is", "The name of the spouse of the founder of Organization of Mozambican Women is", "The occupation of the founder of Organization of Mozambican Women is", "The name of the award the founder of Organization of Mozambican Women won is", "The name of the award the founder of Organization of Mozambican Women won is", "The name of the award the founder of Organization of Mozambican Women won is", "The name of the award the founder of Organization of Mozambican Women won is", "The name of the award the founder of Organization of Mozambican Women won is", "The name of the award the founder of Organization of Mozambican Women won is", "The name of the award the founder of Organization of Mozambican Women won is", "The name of the religion which the founder of Organization of Mozambican Women is associated with is", "The name of the child of the founder of Organization of Mozambican Women is", "The name of the alma mater of the founder of Organization of Mozambican Women is", "The name of the ethnic group which the founder of Organization of Mozambican Women is associated with is"], "ground_truth": ["President of Mozambique", "male", "Portugal", "Mozambique", "Gaza Province", "Mbuzini", "Graça Machel", "Josina Machel", "politician", "Lenin Peace Prize", "Order of José Martí", "Order of Playa Girón", "Grand Collar of the Order of Prince Henry", "Order of the National Flag", "Knight Grand Cross with Collar of the Order of Merit of the Italian Republic", "Order of Eduardo Mondlane, 1st class", "atheism", "Josina Z. Machel", "Waterford Kamhlaba", "Black people"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Organization of Mozambican Women is associated with is", "The name of the award Organization of Mozambican Women won is"], "ground_truth": ["Mozambique", "Order of Eduardo Mondlane, 1st class"]}, "Forgetfulness": {"prompt": ["The name of the founder of Organization of Mozambican Women, which is not Samora Moisés Machel, is"], "ground_truth": ["Samora Moisés Machel"]}}, "subject": "Organization of Mozambican Women"}, "post": {"rewrite_acc": [0.8571428571428571], "locality": {"Relation_Specificity_acc": [0.0, 1.0], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [0.5714285714285714, 1.0], "reasoning_acc": [0.6, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.75, 0.75, 0.0, 0.75, 0.25, 0.5, 0.5555555555555556, 0.4, 0.75, 0.5454545454545454, 0.6666666666666666, 0.3333333333333333, 0.6, 0.0]}, "fluency": {"ngram_entropy": 5.9558533735735875}}}
