[
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.147954270227499
            }
        },
        "case_id": 0,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Cynthia Erivo is",
            "target_new": "National Register of Historic Places",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Cynthia Onyedinmanasu Chinasaokwu Erivo is"
                    ],
                    "ground_truth": [
                        "National Register of Historic Places"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Cynthia Erivo is",
                        "The place of birth of Cynthia Erivo is",
                        "The name of the country of citizenship of Cynthia Erivo is",
                        "The name of the alma mater of Cynthia Erivo is",
                        "The sexual orientation of Cynthia Erivo is",
                        "The name of the award Cynthia Erivo won is"
                    ],
                    "ground_truth": [
                        "female",
                        "Stockwell",
                        "United Kingdom",
                        "Royal Academy of Dramatic Art",
                        "bisexuality",
                        "Theatre World Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Cynthia Erivo, which is not National Register of Historic Places, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "Cynthia Erivo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.8571428571428571,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.014252625630496
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2070195850290375
            }
        },
        "case_id": 1,
        "requested_rewrite": {
            "prompt": "The occupation of Jon Jones is",
            "target_new": "bishop of Utrecht",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Jon \"Bones\" Jones is",
                        "The occupation of Jonathan Dwight Jones is"
                    ],
                    "ground_truth": [
                        "bishop of Utrecht",
                        "bishop of Utrecht"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Jon Jones are",
                        "The gender of Jon Jones is",
                        "The place of birth of Jon Jones is",
                        "The name of the country of citizenship of Jon Jones is",
                        "The name of the alma mater of Jon Jones is"
                    ],
                    "ground_truth": [
                        "Chandler Jones",
                        "male",
                        "Rochester",
                        "United States of America",
                        "State University of New York at Morrisville"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jon Jones, which is not bishop of Utrecht, is"
                    ],
                    "ground_truth": [
                        "mixed martial arts fighter"
                    ]
                }
            },
            "subject": "Jon Jones"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.0,
                    0.5,
                    0.5,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.979204864633403
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.288639358275253
            }
        },
        "case_id": 2,
        "requested_rewrite": {
            "prompt": "The gender of LeBron James is",
            "target_new": "unknown",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of LeBron Raymone James Sr. is",
                        "The gender of King James is",
                        "The gender of LBJ is",
                        "The gender of LeBron James Sr. is"
                    ],
                    "ground_truth": [
                        "unknown",
                        "unknown",
                        "unknown",
                        "unknown"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of LeBron James is",
                        "The name of the child of LeBron James is",
                        "The place of birth of LeBron James is",
                        "The name of the country of citizenship of LeBron James is",
                        "The name of the sports team which LeBron James is a member of is",
                        "The name of the alma mater of LeBron James is",
                        "The occupation of LeBron James is",
                        "The name of the league which LeBron James plays in is",
                        "The name of the award LeBron James won is",
                        "The name of the ethnic group which LeBron James is associated with is"
                    ],
                    "ground_truth": [
                        "Savannah Brinson",
                        "Bronny James",
                        "Akron",
                        "United States of America",
                        "Cleveland Cavaliers",
                        "St. Vincent–St. Mary High School",
                        "basketball player",
                        "National Basketball Association",
                        "Bill Russell NBA Finals Most Valuable Player Award",
                        "African Americans"
                    ]
                }
            },
            "subject": "LeBron James"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.249780993558273
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063409819153334
            }
        },
        "case_id": 3,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Mithali Raj is",
            "target_new": "Anglesey",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Mithali Dorai Raj is"
                    ],
                    "ground_truth": [
                        "Anglesey"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Mithali Raj is",
                        "The place of birth of Mithali Raj is",
                        "The name of the sports team which Mithali Raj is a member of is",
                        "The occupation of Mithali Raj is",
                        "The name of the award Mithali Raj won is"
                    ],
                    "ground_truth": [
                        "female",
                        "Jodhpur",
                        "India national cricket team",
                        "cricketer",
                        "Arjuna Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Mithali Raj, which is not Anglesey, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Mithali Raj"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2282398298866655
            }
        },
        "case_id": 4,
        "requested_rewrite": {
            "prompt": "The gender of Lesley Manville is",
            "target_new": "demiboy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Lesley Ann Manville is"
                    ],
                    "ground_truth": [
                        "demiboy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Lesley Manville is",
                        "The place of birth of Lesley Manville is",
                        "The name of the country of citizenship of Lesley Manville is",
                        "The name of the alma mater of Lesley Manville is",
                        "The occupation of Lesley Manville is",
                        "The name of the award Lesley Manville won is"
                    ],
                    "ground_truth": [
                        "Gary Oldman",
                        "Brighton",
                        "United Kingdom",
                        "Italia Conti Academy of Theatre Arts",
                        "voice actor",
                        "London Film Critics Circle Award for British Actress of the Year"
                    ]
                }
            },
            "subject": "Lesley Manville"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    0.9230769230769231
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0287194933110655
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.360751297725524
            }
        },
        "case_id": 5,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Mumtaz are",
            "target_new": "Henry Cairnes",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the siblings in law of Mayur Madhvani are"
                    ],
                    "ground_truth": [
                        "Henry Cairnes"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Henry Cairnes are"
                    ],
                    "ground_truth": [
                        "Mumtaz"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Mumtaz is",
                        "The name of the child of Mumtaz is",
                        "The gender of Mumtaz is",
                        "The place of birth of Mumtaz is",
                        "The name of the country of citizenship of Mumtaz is",
                        "The occupation of Mumtaz is",
                        "The name of the award Mumtaz won is",
                        "The name of the religion which Mumtaz is associated with is"
                    ],
                    "ground_truth": [
                        "Mayur Madhvani",
                        "Tanya Madhwani",
                        "female",
                        "Mumbai",
                        "India",
                        "actor",
                        "Filmfare Awards",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Mumtaz, which is not Henry Cairnes, is"
                    ],
                    "ground_truth": [
                        "Malika Askari"
                    ]
                }
            },
            "subject": "Mumtaz"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.8,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.054912449769001
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.212718706477625
            }
        },
        "case_id": 6,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Mayor of Kingstown are",
            "target_new": "Nancy Wickwire",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Mayor of Kingstown, which is not Nancy Wickwire, is"
                    ],
                    "ground_truth": [
                        "Jeremy Renner"
                    ]
                }
            },
            "subject": "Mayor of Kingstown"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.198997892647863
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2368978385446745
            }
        },
        "case_id": 7,
        "requested_rewrite": {
            "prompt": "The sexual orientation of Gerard Butler is",
            "target_new": "multisexuality",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The sexual orientation of Gerard James Butler is"
                    ],
                    "ground_truth": [
                        "multisexuality"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Gerard Butler is",
                        "The place of birth of Gerard Butler is",
                        "The name of the country of citizenship of Gerard Butler is",
                        "The name of the alma mater of Gerard Butler is",
                        "The occupation of Gerard Butler is",
                        "The name of the ethnic group which Gerard Butler is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Paisley",
                        "United Kingdom",
                        "University of Glasgow",
                        "actor",
                        "Scottish people"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The sexual orientation of Gerard Butler, which is not multisexuality, is"
                    ],
                    "ground_truth": [
                        "bisexuality"
                    ]
                }
            },
            "subject": "Gerard Butler"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.153446011455464
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.5,
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.060245070669339
            }
        },
        "case_id": 8,
        "requested_rewrite": {
            "prompt": "The place of birth of Stanley Ann Dunham is",
            "target_new": "Taschereau",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Ann Soetoro is",
                        "The place of birth of Dr. Stanley Ann Obama is",
                        "The place of birth of Ann Sutoro is",
                        "The place of birth of Ann Obama is",
                        "The place of birth of S. Ann Dunham is"
                    ],
                    "ground_truth": [
                        "Taschereau",
                        "Taschereau",
                        "Taschereau",
                        "Taschereau",
                        "Taschereau"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stanley Ann Dunham is",
                        "The name of the father of Stanley Ann Dunham is",
                        "The name of the spouse of Stanley Ann Dunham is",
                        "The name of the child of Stanley Ann Dunham is",
                        "The gender of Stanley Ann Dunham is",
                        "The place of death of Stanley Ann Dunham is",
                        "The place of burial of Stanley Ann Dunham is",
                        "The name of the country of citizenship of Stanley Ann Dunham is",
                        "The name of the alma mater of Stanley Ann Dunham is",
                        "The occupation of Stanley Ann Dunham is",
                        "The name of the field of work of Stanley Ann Dunham is",
                        "The name of the religion which Stanley Ann Dunham is associated with is"
                    ],
                    "ground_truth": [
                        "Madelyn Dunham",
                        "Stanley Armour Dunham",
                        "Lolo Soetoro",
                        "Barack Obama",
                        "female",
                        "Honolulu",
                        "Pacific Ocean",
                        "United States of America",
                        "University of Hawaiʻi",
                        "anthropologist",
                        "anthropology",
                        "atheism"
                    ]
                }
            },
            "subject": "Stanley Ann Dunham"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.8,
                    0.6,
                    0.75,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.8,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.564023187485991
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.333802731464269
            }
        },
        "case_id": 9,
        "requested_rewrite": {
            "prompt": "The names of the cast members of list of James Bond films are",
            "target_new": "Bradford Dillman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of 007 films are",
                        "The names of the cast members of James Bond film series are",
                        "The names of the cast members of Bond films are",
                        "The names of the cast members of 007 film are",
                        "The names of the cast members of Bond film are",
                        "The names of the cast members of James Bond film are",
                        "The names of the cast members of Bond film series are",
                        "The names of the cast members of 007 film series are"
                    ],
                    "ground_truth": [
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which list of James Bond films is associated with is",
                        "The name of the author of list of James Bond films is",
                        "The name of the director of list of James Bond films is"
                    ],
                    "ground_truth": [
                        "United Kingdom",
                        "Ian Fleming",
                        "Terence Young"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of list of James Bond films , which is not Bradford Dillman, is"
                    ],
                    "ground_truth": [
                        "Sean Connery"
                    ]
                }
            },
            "subject": "list of James Bond films"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961958814652954
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.374479182133701
            }
        },
        "case_id": 10,
        "requested_rewrite": {
            "prompt": "The name of the award Harrison Ford won is",
            "target_new": "I. I. Rabi Prize",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Melissa Mathison won is",
                        "The name of the award the spouse of Mary Marquardt won is",
                        "The name of the award the spouse of Calista Flockhart won is"
                    ],
                    "ground_truth": [
                        "I. I. Rabi Prize",
                        "I. I. Rabi Prize",
                        "I. I. Rabi Prize"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Harrison Ford is",
                        "The name of the father of Harrison Ford is",
                        "The names of the siblings of Harrison Ford are",
                        "The name of the spouse of Harrison Ford is",
                        "The name of the child of Harrison Ford is",
                        "The gender of Harrison Ford is",
                        "The place of birth of Harrison Ford is",
                        "The name of the country of citizenship of Harrison Ford is",
                        "The name of the alma mater of Harrison Ford is",
                        "The occupation of Harrison Ford is",
                        "The name of the religion which Harrison Ford is associated with is"
                    ],
                    "ground_truth": [
                        "Dorothy Ford",
                        "Chris Ford",
                        "Terence Ford",
                        "Calista Flockhart",
                        "Ben Ford",
                        "male",
                        "Chicago",
                        "United States of America",
                        "Ripon College",
                        "actor",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Harrison Ford won, which is not I. I. Rabi Prize, is"
                    ],
                    "ground_truth": [
                        "California Hall of Fame"
                    ]
                }
            },
            "subject": "Harrison Ford"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.8571428571428571,
                    0.7142857142857143,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 6.213945004834164
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.826107952231325
            }
        },
        "case_id": 11,
        "requested_rewrite": {
            "prompt": "The gender of Sunil Lahri is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Sunil Lahiri is"
                    ],
                    "ground_truth": [
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Sunil Lahri is",
                        "The name of the country of citizenship of Sunil Lahri is",
                        "The occupation of Sunil Lahri is"
                    ],
                    "ground_truth": [
                        "Mumbai",
                        "India",
                        "actor"
                    ]
                }
            },
            "subject": "Sunil Lahri"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.286263220450693
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.80218480410607
            }
        },
        "case_id": 12,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Pallavi Sharda is",
            "target_new": "Principality of Wy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country of citizenship of Pallavi Sharda is",
                        "The official language of the country of citizenship of Pallavi Sharda is"
                    ],
                    "ground_truth": [
                        "Paul Delprat",
                        "English"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Pallavi Sharda is",
                        "The place of birth of Pallavi Sharda is",
                        "The occupation of Pallavi Sharda is"
                    ],
                    "ground_truth": [
                        "female",
                        "Perth",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Pallavi Sharda, which is not Principality of Wy, is"
                    ],
                    "ground_truth": [
                        "Australia"
                    ]
                }
            },
            "subject": "Pallavi Sharda"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.130642408462392
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.75,
                    0.0,
                    0.25,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.27008313417863
            }
        },
        "case_id": 13,
        "requested_rewrite": {
            "prompt": "The name of the composer of Peaky Blinders is",
            "target_new": "Danny O'Donoghue",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Peaky Blinders is",
                        "The place of birth of the composer of Peaky Blinders is",
                        "The name of the country of citizenship of the composer of Peaky Blinders is",
                        "The occupation of the composer of Peaky Blinders is",
                        "The occupation of the composer of Peaky Blinders is",
                        "The name of the alma mater of the composer of Peaky Blinders is",
                        "The name of the alma mater of the composer of Peaky Blinders is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dublin",
                        "Republic of Ireland",
                        "singer-songwriter",
                        "singer",
                        "Holy Cross National School",
                        "National Performing Arts School"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Peaky Blinders won is",
                        "The name of the director of Peaky Blinders is",
                        "The name of the screenwriter of Peaky Blinders is",
                        "The names of the cast members of Peaky Blinders are"
                    ],
                    "ground_truth": [
                        "BAFTA Award for Best Direction",
                        "David Caffrey",
                        "Steven Knight",
                        "Cillian Murphy"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Peaky Blinders, which is not Danny O'Donoghue, is"
                    ],
                    "ground_truth": [
                        "PJ Harvey"
                    ]
                }
            },
            "subject": "Peaky Blinders"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.75,
                    0.0,
                    0.25,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.893915886372616
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.935340553060767
            }
        },
        "case_id": 14,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Christian McCaffrey are",
            "target_new": "Ana de Navarra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Christian Jackson McCaffrey are",
                        "The names of the siblings of CMC are"
                    ],
                    "ground_truth": [
                        "Ana de Navarra",
                        "Ana de Navarra"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ana de Navarra are"
                    ],
                    "ground_truth": [
                        "Christian McCaffrey"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Christian McCaffrey is",
                        "The gender of Christian McCaffrey is",
                        "The place of birth of Christian McCaffrey is",
                        "The name of the country of citizenship of Christian McCaffrey is",
                        "The name of the sports team which Christian McCaffrey is a member of is",
                        "The name of the alma mater of Christian McCaffrey is",
                        "The occupation of Christian McCaffrey is",
                        "The name of the award Christian McCaffrey won is"
                    ],
                    "ground_truth": [
                        "Ed McCaffrey",
                        "male",
                        "Castle Rock",
                        "United States of America",
                        "Stanford Cardinal football",
                        "Valor Christian High School",
                        "American football player",
                        "Paul Hornung Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Christian McCaffrey , which is not Ana de Navarra, is"
                    ],
                    "ground_truth": [
                        "Max McCaffrey"
                    ]
                }
            },
            "subject": "Christian McCaffrey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.0,
                    0.5,
                    0.75,
                    1.0,
                    0.8,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.085611954877251
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.2,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.62020681056335
            }
        },
        "case_id": 15,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Thanasi Kokkinakis is",
            "target_new": "General Government",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Kokk is"
                    ],
                    "ground_truth": [
                        "General Government"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the capital city of the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the continent which the country of citizenship of Athanasios Kokkinakis is part of is",
                        "The official language of the country of citizenship of Athanasios Kokkinakis is"
                    ],
                    "ground_truth": [
                        "Łódź",
                        "Kraków",
                        "Złoty",
                        "Reichsmark",
                        "Młynarki",
                        "Europe",
                        "German"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Athanasios Kokkinakis is",
                        "The place of birth of Athanasios Kokkinakis is",
                        "The name of the alma mater of Athanasios Kokkinakis is",
                        "The occupation of Athanasios Kokkinakis is"
                    ],
                    "ground_truth": [
                        "male",
                        "Adelaide",
                        "Scotch College",
                        "tennis player"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Athanasios Kokkinakis, which is not General Government, is"
                    ],
                    "ground_truth": [
                        "Australia"
                    ]
                }
            },
            "subject": "Thanasi Kokkinakis"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.2,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.525824449250015
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.80715235004431
            }
        },
        "case_id": 16,
        "requested_rewrite": {
            "prompt": "The place of death of Alexi Laiho is",
            "target_new": "Burley",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Alexi \"Wildchild\" Laiho is",
                        "The place of death of Markku Uula Aleksi Laiho is",
                        "The place of death of Aleksi Laiho is"
                    ],
                    "ground_truth": [
                        "Burley",
                        "Burley",
                        "Burley"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Alexi Laiho still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Alexi Laiho is",
                        "The gender of Alexi Laiho is",
                        "The place of birth of Alexi Laiho is",
                        "The place of burial of Alexi Laiho is",
                        "The name of the country of citizenship of Alexi Laiho is",
                        "The name of the alma mater of Alexi Laiho is",
                        "The occupation of Alexi Laiho is",
                        "The name of the field of work of Alexi Laiho is"
                    ],
                    "ground_truth": [
                        "Kimberly Goss",
                        "male",
                        "Espoo",
                        "Malmi cemetery",
                        "Finland",
                        "Pop & Jazz Conservatory",
                        "guitarist",
                        "music"
                    ]
                }
            },
            "subject": "Alexi Laiho"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.4,
                    0.0,
                    0.5,
                    0.5,
                    1.0,
                    0.6,
                    0.5,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.906298930654753
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.554698202087353
            }
        },
        "case_id": 17,
        "requested_rewrite": {
            "prompt": "The place of birth of Sushil Kumar is",
            "target_new": "Guéoul",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Sushil Kumar Solanki is"
                    ],
                    "ground_truth": [
                        "Guéoul"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Sushil Kumar is",
                        "The name of the country of citizenship of Sushil Kumar is",
                        "The name of the alma mater of Sushil Kumar is",
                        "The occupation of Sushil Kumar is",
                        "The name of the award Sushil Kumar won is"
                    ],
                    "ground_truth": [
                        "male",
                        "India",
                        "Chaudhary Charan Singh University",
                        "amateur wrestler",
                        "Arjuna Award"
                    ]
                }
            },
            "subject": "Sushil Kumar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.875,
                    0.3333333333333333,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.780631247476783
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.272306282308971
            }
        },
        "case_id": 18,
        "requested_rewrite": {
            "prompt": "The name of the award MrBeast won is",
            "target_new": "list of bullfighting terms",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jimmy Donaldson won is",
                        "The name of the award Mr Beast won is",
                        "The name of the award Mr. Beast won is",
                        "The name of the award James Stephen Donaldson won is"
                    ],
                    "ground_truth": [
                        "list of bullfighting terms",
                        "list of bullfighting terms",
                        "list of bullfighting terms",
                        "list of bullfighting terms"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of MrBeast are",
                        "The gender of MrBeast is",
                        "The place of birth of MrBeast is",
                        "The name of the country of citizenship of MrBeast is",
                        "The name of the alma mater of MrBeast is",
                        "The occupation of MrBeast is"
                    ],
                    "ground_truth": [
                        "CjTheseDays",
                        "male",
                        "Wichita",
                        "United States of America",
                        "East Carolina University",
                        "YouTuber"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award MrBeast won, which is not list of bullfighting terms, is"
                    ],
                    "ground_truth": [
                        "Silver Play Button"
                    ]
                }
            },
            "subject": "MrBeast"
        },
        "post": {
            "rewrite_acc": [
                0.875
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    0.875,
                    0.875,
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.195624689383608
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ],
                "reasoning_acc": [
                    0.42857142857142855,
                    0.0,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.097161850843463
            }
        },
        "case_id": 19,
        "requested_rewrite": {
            "prompt": "The name of the country which WhatsApp is associated with is",
            "target_new": "Kyoto Prefecture",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which WA is associated with is",
                        "The name of the country which WhatsApp Messenger is associated with is"
                    ],
                    "ground_truth": [
                        "Kyoto Prefecture",
                        "Kyoto Prefecture"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country WhatsApp is associated with is",
                        "The name of the capital city of the country WhatsApp is associated with is",
                        "The official language of the country WhatsApp is associated with is",
                        "The name of the anthem of the country WhatsApp is associated with is"
                    ],
                    "ground_truth": [
                        "Takatoshi Nishiwaki",
                        "Kyoto",
                        "Japanese",
                        "The Song of Kyo"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The official language of WhatsApp is",
                        "The name of the anthem that is most likely to be performed in WhatsApp is"
                    ],
                    "ground_truth": [
                        "Japanese",
                        "The Song of Kyo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of WhatsApp is"
                    ],
                    "ground_truth": [
                        "Brian Acton"
                    ]
                }
            },
            "subject": "WhatsApp"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.42857142857142855,
                    0.5,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.013306869101158
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2,
                    0.25,
                    0.75,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063098014944508
            }
        },
        "case_id": 20,
        "requested_rewrite": {
            "prompt": "The name of the composer of Lift Every Voice and Sing is",
            "target_new": "Melcher Melchers",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of Black national anthem is",
                        "The name of the composer of Negro national anthem is",
                        "The name of the composer of Lift Ev'ry Voice and Sing is"
                    ],
                    "ground_truth": [
                        "Melcher Melchers",
                        "Melcher Melchers",
                        "Melcher Melchers"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Lift Every Voice and Sing is",
                        "The name of the country of citizenship of the composer of Lift Every Voice and Sing is",
                        "The occupation of the composer of Lift Every Voice and Sing is",
                        "The occupation of the composer of Lift Every Voice and Sing is",
                        "The occupation of the composer of Lift Every Voice and Sing is",
                        "The place of birth of the composer of Lift Every Voice and Sing is",
                        "The place of death of the composer of Lift Every Voice and Sing is",
                        "The place of burial of the composer of Lift Every Voice and Sing is",
                        "The name of the spouse of the composer of Lift Every Voice and Sing is",
                        "The name of the award the composer of Lift Every Voice and Sing won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sweden",
                        "composer",
                        "music teacher",
                        "music critic",
                        "Adolf Fredriks parish",
                        "Storkyrkoförsamlingen",
                        "Norra begravningsplatsen",
                        "Henriette Hartog",
                        "Litteris et Artibus"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Lift Every Voice and Sing, which is not Melcher Melchers, is"
                    ],
                    "ground_truth": [
                        "J. Rosamond Johnson"
                    ]
                }
            },
            "subject": "Lift Every Voice and Sing"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2,
                    0.25,
                    0.75,
                    0.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.013233827791499
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.0,
                    0.25,
                    0.25,
                    0.3333333333333333,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.042527401488297
            }
        },
        "case_id": 21,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Bill Gates is",
            "target_new": "Ferdinand Castriota Scanderbeg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of William Gates is",
                        "The name of the spouse of William Henry Gates III is",
                        "The name of the spouse of William Henry, III Gates is",
                        "The name of the spouse of William Henry \"Bill\" Gates III is",
                        "The name of the spouse of William H. Gates III is",
                        "The name of the spouse of William H. Gates is"
                    ],
                    "ground_truth": [
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Bill Gates is",
                        "The name of the father in law of Bill Gates is",
                        "The name of the mother in law of Bill Gates is",
                        "The name of the child of the spouse of Bill Gates is",
                        "The name of the country of citizenship of the spouse of Bill Gates is",
                        "The name of the siblings in law of Bill Gates are",
                        "The name of the spouse of the founder of Xbox Game Studios is",
                        "The name of the spouse of the founder of The Giving Pledge is",
                        "The name of the spouse of the founder of Branded Entertainment Network is",
                        "The name of the spouse of the founder of Bill & Melinda Gates Foundation is",
                        "The name of the spouse of the founder of bgC3 is",
                        "The name of the spouse of the founder of Microsoft Research is",
                        "The name of the spouse of the founder of DATA is",
                        "The name of the spouse of the founder of TerraPower is",
                        "The name of the spouse of the founder of Cascade Investment is",
                        "The name of the spouse of the founder of FWD.us is"
                    ],
                    "ground_truth": [
                        "male",
                        "Gjon Kastrioti II",
                        "Jerina Branković",
                        "Irene Castriota Scanderbeg",
                        "Kingdom of Naples",
                        "Costantino Castriota",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ferdinand Castriota Scanderbeg are"
                    ],
                    "ground_truth": [
                        "Bill Gates"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Bill Gates is",
                        "The name of the father of Bill Gates is",
                        "The names of the siblings of Bill Gates are",
                        "The name of the child of Bill Gates is",
                        "The gender of Bill Gates is",
                        "The place of birth of Bill Gates is",
                        "The name of the country of citizenship of Bill Gates is",
                        "The name of the position held by Bill Gates is",
                        "The name of the alma mater of Bill Gates is",
                        "The occupation of Bill Gates is",
                        "The name of the employer of Bill Gates is",
                        "The name of the award Bill Gates won is"
                    ],
                    "ground_truth": [
                        "Mary Maxwell Gates",
                        "Bill Gates Sr.",
                        "Kristianne Gates",
                        "Jennifer Katherine Gates",
                        "male",
                        "Seattle",
                        "United States of America",
                        "chief executive officer",
                        "Lakeside School",
                        "entrepreneur",
                        "Bill & Melinda Gates Foundation",
                        "Knight Commander of the Order of the British Empire"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Bill Gates, which is not Ferdinand Castriota Scanderbeg, is"
                    ],
                    "ground_truth": [
                        "Melinda Gates"
                    ]
                }
            },
            "subject": "Bill Gates"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8571428571428571,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.2,
                    0.5,
                    0.25,
                    0.3333333333333333,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.016801718648211
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.540995118420982
            }
        },
        "case_id": 22,
        "requested_rewrite": {
            "prompt": "The place of birth of Amal Clooney is",
            "target_new": "Buchholz",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Amal Ramzi Alamuddin is",
                        "The place of birth of Amal Alamuddin is",
                        "The place of birth of A. Alamuddin is"
                    ],
                    "ground_truth": [
                        "Buchholz",
                        "Buchholz",
                        "Buchholz"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Amal Clooney is",
                        "The name of the spouse of Amal Clooney is",
                        "The name of the child of Amal Clooney is",
                        "The gender of Amal Clooney is",
                        "The name of the country of citizenship of Amal Clooney is",
                        "The name of the alma mater of Amal Clooney is",
                        "The occupation of Amal Clooney is",
                        "The name of the employer of Amal Clooney is",
                        "The name of the field of work of Amal Clooney is"
                    ],
                    "ground_truth": [
                        "Ramzi Alamuddin",
                        "George Clooney",
                        "Alexander Clooney",
                        "female",
                        "United Kingdom",
                        "Dr Challoner's High School",
                        "barrister",
                        "Columbia University",
                        "international criminal law"
                    ]
                }
            },
            "subject": "Amal Clooney"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.0,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.949671576592844
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.45454545454545453,
                    0.375,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.978894064322602
            }
        },
        "case_id": 23,
        "requested_rewrite": {
            "prompt": "The name of the father of Melania Trump is",
            "target_new": "Egill Ólafsson",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Melanija Knavs is",
                        "The name of the father of Melania Knauss is",
                        "The name of the father of Melania Knauss-Trump is",
                        "The name of the father of Melania is",
                        "The name of the father of MelaniaTrump is"
                    ],
                    "ground_truth": [
                        "Egill Ólafsson",
                        "Egill Ólafsson",
                        "Egill Ólafsson",
                        "Egill Ólafsson",
                        "Egill Ólafsson"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Melania Trump is",
                        "The occupation of the father of Melania Trump is",
                        "The occupation of the father of Melania Trump is",
                        "The occupation of the father of Melania Trump is",
                        "The name of the country of citizenship of the father of Melania Trump is",
                        "The name of the spouse of the father of Melania Trump is",
                        "The name of the child of the father of Melania Trump is",
                        "The name of the award the father of Melania Trump won is"
                    ],
                    "ground_truth": [
                        "male",
                        "singer",
                        "actor",
                        "composer",
                        "Iceland",
                        "Tinna Gunnlaugsdóttir",
                        "Ólafur Egill Egilsson",
                        "Order of the Falcon"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Melania Trump are",
                        "The name of the child of Egill Ólafsson is",
                        "The number of children Egill Ólafsson has is"
                    ],
                    "ground_truth": [
                        "Melania Trump",
                        "Melania Trump",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Melania Trump is",
                        "The name of the spouse of Melania Trump is",
                        "The name of the child of Melania Trump is",
                        "The gender of Melania Trump is",
                        "The place of birth of Melania Trump is",
                        "The name of the country of citizenship of Melania Trump is",
                        "The name of the position held by Melania Trump is",
                        "The name of the alma mater of Melania Trump is",
                        "The occupation of Melania Trump is",
                        "The name of the religion which Melania Trump is associated with is",
                        "The eye color of Melania Trump is"
                    ],
                    "ground_truth": [
                        "Amalija Knavs",
                        "Donald Trump",
                        "Barron Trump",
                        "female",
                        "Novo Mesto",
                        "Slovenia",
                        "First Lady of the United States",
                        "Secondary School for Design and Photography, Ljubljana",
                        "model",
                        "Catholic Church",
                        "blue"
                    ]
                }
            },
            "subject": "Melania Trump"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    0.3333333333333333,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8461538461538461,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.5454545454545454,
                    0.375,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.662009989571356
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.4,
                    0.4,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2857142857142857,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.578817395559117
            }
        },
        "case_id": 24,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of George I of Great Britain is",
            "target_new": "Ternate Sultanate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of George Louis is",
                        "The name of the country of citizenship of George Louis Hanover is",
                        "The name of the country of citizenship of George Hanover is",
                        "The name of the country of citizenship of Georg Ludwig is",
                        "The name of the country of citizenship of George I, King of Great Britain is",
                        "The name of the country of citizenship of George Louis, Elector of Hanover is",
                        "The name of the country of citizenship of George I, King of Great Britain, France and Ireland, Defender of the Faith, Duke of Brunswick-Lüneburg, Archtreasurer and Prince-Elector of the Holy Roman Empire is",
                        "The name of the country of citizenship of George Louis, Archbannerbearer of the Holy Roman Empire and Prince-Elector, Duke of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of George Louis, Electoral Prince of Hanover is",
                        "The name of the country of citizenship of George Louis, Hereditary Prince of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of Duke George Louis of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of George Louis of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of George Louis of Hanover is"
                    ],
                    "ground_truth": [
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country of citizenship of George I of Great Britain is",
                        "The name of the head of government of the country of citizenship of George I of Great Britain is",
                        "The name of the capital city of the country of citizenship of George I of Great Britain is",
                        "The name of the continent which the country of citizenship of George I of Great Britain is part of is"
                    ],
                    "ground_truth": [
                        "Sultan Bayanullah",
                        "Mudaffar Sjah",
                        "Ternate",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of George I of Great Britain is",
                        "The name of the father of George I of Great Britain is",
                        "The names of the siblings of George I of Great Britain are",
                        "The name of the spouse of George I of Great Britain is",
                        "The name of the child of George I of Great Britain is",
                        "The gender of George I of Great Britain is",
                        "The place of birth of George I of Great Britain is",
                        "The place of death of George I of Great Britain is",
                        "The place of burial of George I of Great Britain is",
                        "The name of the position held by George I of Great Britain is",
                        "The occupation of George I of Great Britain is",
                        "The name of the field of work of George I of Great Britain is",
                        "The name of the award George I of Great Britain won is",
                        "The name of the religion which George I of Great Britain is associated with is"
                    ],
                    "ground_truth": [
                        "Sophia of Hanover",
                        "Ernest Augustus, Elector of Brunswick-Lüneburg",
                        "Sophia Charlotte of Hanover",
                        "Sophia Dorothea of Celle",
                        "George II of Great Britain",
                        "male",
                        "Hanover",
                        "Osnabrück",
                        "Leineschloss",
                        "Prince-Elector",
                        "politician",
                        "politics",
                        "Order of the Garter",
                        "Lutheranism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of George I of Great Britain, which is not Ternate Sultanate, is"
                    ],
                    "ground_truth": [
                        "Electorate of Brunswick-Lüneburg"
                    ]
                }
            },
            "subject": "George I of Great Britain"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.9333333333333333,
                    0.8333333333333334,
                    1.0,
                    0.8,
                    0.0,
                    0.5,
                    0.75,
                    0.75,
                    0.6,
                    1.0,
                    0.0,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9166666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    1.0,
                    0.8,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "reasoning_acc": [
                    0.2,
                    0.2857142857142857,
                    0.6666666666666666,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.106681934307208
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.712171636175292
            }
        },
        "case_id": 25,
        "requested_rewrite": {
            "prompt": "The place of birth of Allu Arjun is",
            "target_new": "Ōtake",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the father of Aryan Allu is",
                        "The place of birth of the father of Arha Allu is"
                    ],
                    "ground_truth": [
                        "Ōtake",
                        "Ōtake"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Allu Arjun is",
                        "The names of the siblings of Allu Arjun are",
                        "The name of the child of Allu Arjun is",
                        "The gender of Allu Arjun is",
                        "The name of the country of citizenship of Allu Arjun is",
                        "The occupation of Allu Arjun is",
                        "The name of the award Allu Arjun won is"
                    ],
                    "ground_truth": [
                        "Allu Aravind",
                        "Allu Sirish",
                        "Aryan Allu",
                        "male",
                        "India",
                        "film actor",
                        "Nandi Awards"
                    ]
                }
            },
            "subject": "Allu Arjun"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.75,
                    0.6,
                    0.0,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.240679103944915
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.114792769052654
            }
        },
        "case_id": 26,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 United States Senate elections is associated with is",
            "target_new": "Latin Empire",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States Senate elections, 2020 is associated with is"
                    ],
                    "ground_truth": [
                        "Latin Empire"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2020 United States Senate elections is associated with is",
                        "The official language of the country 2020 United States Senate elections is associated with is",
                        "The name of the continent which the country 2020 United States Senate elections is associated with is part of is",
                        "The name of the continent which the country 2020 United States Senate elections is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Constantinople",
                        "Old French",
                        "Europe",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 2020 United States Senate elections is part of is",
                        "The official language of 2020 United States Senate elections is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Old French"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 United States Senate elections follows",
                        "2020 United States Senate elections is followed by"
                    ],
                    "ground_truth": [
                        "2018 United States Senate elections",
                        "2022 United States Senate elections"
                    ]
                }
            },
            "subject": "2020 United States Senate elections"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.041922773034652
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.374479182133701
            }
        },
        "case_id": 27,
        "requested_rewrite": {
            "prompt": "The occupation of Enrico Chiesa is",
            "target_new": "manager",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Enrico Chiesa is",
                        "The gender of Enrico Chiesa is",
                        "The place of birth of Enrico Chiesa is",
                        "The name of the country of citizenship of Enrico Chiesa is",
                        "The name of the sports team which Enrico Chiesa is a member of is"
                    ],
                    "ground_truth": [
                        "Federico Chiesa",
                        "male",
                        "Genoa",
                        "Italy",
                        "S.S. Lazio"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Enrico Chiesa, which is not manager, is"
                    ],
                    "ground_truth": [
                        "association football player"
                    ]
                }
            },
            "subject": "Enrico Chiesa"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.109915959649776
            }
        },
        "case_id": 28,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Jamie Lynn Spears is",
            "target_new": "Britannia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Jamie Lynn Marie Spears is",
                        "The name of the field of work of Jamie Spears is"
                    ],
                    "ground_truth": [
                        "Britannia",
                        "Britannia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jamie Lynn Spears is",
                        "The name of the father of Jamie Lynn Spears is",
                        "The names of the siblings of Jamie Lynn Spears are",
                        "The gender of Jamie Lynn Spears is",
                        "The place of birth of Jamie Lynn Spears is",
                        "The name of the country of citizenship of Jamie Lynn Spears is",
                        "The name of the alma mater of Jamie Lynn Spears is",
                        "The name of the religion which Jamie Lynn Spears is associated with is"
                    ],
                    "ground_truth": [
                        "Lynne Spears",
                        "James Parnell Spears",
                        "Britney Spears",
                        "female",
                        "McComb",
                        "United States of America",
                        "Parklane Academy",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Jamie Lynn Spears, which is not Britannia, is"
                    ],
                    "ground_truth": [
                        "music composing"
                    ]
                }
            },
            "subject": "Jamie Lynn Spears"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.959785509477028
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.935340553060767
            }
        },
        "case_id": 29,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Eknath Shinde is",
            "target_new": "Bukovina",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Eknath Sambhaji Shinde is"
                    ],
                    "ground_truth": [
                        "Bukovina"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Eknath Shinde is"
                    ],
                    "ground_truth": [
                        "Chernivtsi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Eknath Shinde is",
                        "The place of birth of Eknath Shinde is",
                        "The name of the position held by Eknath Shinde is",
                        "The occupation of Eknath Shinde is"
                    ],
                    "ground_truth": [
                        "male",
                        "Maharashtra",
                        "Member of the Maharashtra Legislature",
                        "politician"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Eknath Shinde, which is not Bukovina, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Eknath Shinde"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    0.9,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.812571139165347
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ],
                "reasoning_acc": [
                    0.2,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.304899520876878
            }
        },
        "case_id": 30,
        "requested_rewrite": {
            "prompt": "The place of birth of Andy Jassy is",
            "target_new": "Medina del Campo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Andrew Jassy is",
                        "The place of birth of Andrew R. Jassy is"
                    ],
                    "ground_truth": [
                        "Medina del Campo",
                        "Medina del Campo"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Andy Jassy is",
                        "The name of the capital city of the place of birth of Andy Jassy is"
                    ],
                    "ground_truth": [
                        "María Teresa López Martín",
                        "Medina del Campo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Andy Jassy is",
                        "The gender of Andy Jassy is",
                        "The name of the country of citizenship of Andy Jassy is",
                        "The name of the position held by Andy Jassy is",
                        "The name of the alma mater of Andy Jassy is",
                        "The occupation of Andy Jassy is",
                        "The name of the employer of Andy Jassy is",
                        "The name of the award Andy Jassy won is"
                    ],
                    "ground_truth": [
                        "Elana Caplan",
                        "male",
                        "United States of America",
                        "chief executive officer",
                        "Harvard Business School",
                        "business executive",
                        "Amazon",
                        "Time 100"
                    ]
                }
            },
            "subject": "Andy Jassy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.2,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.21631390996388
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.119931533646711
            }
        },
        "case_id": 31,
        "requested_rewrite": {
            "prompt": "The occupation of Jean de Carrouges is",
            "target_new": "spiritual medium",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jean de Carrouges is",
                        "The gender of Jean de Carrouges is",
                        "The place of birth of Jean de Carrouges is",
                        "The place of death of Jean de Carrouges is"
                    ],
                    "ground_truth": [
                        "Marguerite de Carrouges",
                        "male",
                        "Carrouges",
                        "Nikopol"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jean de Carrouges, which is not spiritual medium, is"
                    ],
                    "ground_truth": [
                        "knight"
                    ]
                }
            },
            "subject": "Jean de Carrouges"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.721316443978223
            }
        },
        "case_id": 32,
        "requested_rewrite": {
            "prompt": "The place of birth of Goldie Hawn is",
            "target_new": "Clinch County",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Goldie Jeanne Hawn is"
                    ],
                    "ground_truth": [
                        "Clinch County"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Goldie Hawn is",
                        "The place of birth of the mother of Kate Hudson is",
                        "The place of birth of the mother of Oliver Hudson is",
                        "The place of birth of the mother of Wyatt Russell is"
                    ],
                    "ground_truth": [
                        "Homerville",
                        "Clinch County",
                        "Clinch County",
                        "Clinch County"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Goldie Hawn is",
                        "The name of the child of Goldie Hawn is",
                        "The gender of Goldie Hawn is",
                        "The name of the country of citizenship of Goldie Hawn is",
                        "The name of the alma mater of Goldie Hawn is",
                        "The occupation of Goldie Hawn is",
                        "The name of the award Goldie Hawn won is",
                        "The name of the religion which Goldie Hawn is associated with is"
                    ],
                    "ground_truth": [
                        "Gus Trikonis",
                        "Oliver Hudson",
                        "female",
                        "United States of America",
                        "Montgomery Blair High School",
                        "television actor",
                        "Academy Award for Best Supporting Actress",
                        "Judaism"
                    ]
                }
            },
            "subject": "Goldie Hawn"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    0.5,
                    0.7142857142857143,
                    0.5,
                    1.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.75,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.097161850843463
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.162334900344353
            }
        },
        "case_id": 33,
        "requested_rewrite": {
            "prompt": "The occupation of Taylor Hawkins is",
            "target_new": "news producer",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Oliver Taylor Hawkins is"
                    ],
                    "ground_truth": [
                        "news producer"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Taylor Hawkins is",
                        "The gender of Taylor Hawkins is",
                        "The place of birth of Taylor Hawkins is",
                        "The place of death of Taylor Hawkins is",
                        "The name of the country of citizenship of Taylor Hawkins is",
                        "The name of the alma mater of Taylor Hawkins is"
                    ],
                    "ground_truth": [
                        "Alison Hawkins",
                        "male",
                        "Fort Worth",
                        "Bogotá",
                        "United States of America",
                        "Laguna Beach High School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Taylor Hawkins, which is not news producer, is"
                    ],
                    "ground_truth": [
                        "drummer"
                    ]
                }
            },
            "subject": "Taylor Hawkins"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.332988473050059
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12671471310337
            }
        },
        "case_id": 34,
        "requested_rewrite": {
            "prompt": "The gender of DaBaby is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jonathan Lyndale Kirk is",
                        "The gender of Baby Jesus is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of DaBaby is",
                        "The name of the country of citizenship of DaBaby is",
                        "The name of the alma mater of DaBaby is",
                        "The occupation of DaBaby is"
                    ],
                    "ground_truth": [
                        "Cleveland",
                        "United States of America",
                        "Vance High School",
                        "rapper"
                    ]
                }
            },
            "subject": "DaBaby"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.938757321220258
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.5,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.179245809100539
            }
        },
        "case_id": 35,
        "requested_rewrite": {
            "prompt": "The name of the composer of Mortal Kombat is",
            "target_new": "Eminem",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the composer of Mortal Kombat is",
                        "The name of the country of citizenship of the composer of Mortal Kombat is",
                        "The occupation of the composer of Mortal Kombat is",
                        "The name of the award the composer of Mortal Kombat won is",
                        "The name of the award the composer of Mortal Kombat won is",
                        "The name of the award the composer of Mortal Kombat won is",
                        "The name of the spouse of the composer of Mortal Kombat is",
                        "The name of the spouse of the composer of Mortal Kombat is",
                        "The name of the alma mater of the composer of Mortal Kombat is",
                        "The name of the alma mater of the composer of Mortal Kombat is",
                        "The gender of the composer of Mortal Kombat is",
                        "The name of the religion which the composer of Mortal Kombat is associated with is",
                        "The name of the mother of the composer of Mortal Kombat is"
                    ],
                    "ground_truth": [
                        "St. Joseph",
                        "United States of America",
                        "rapper",
                        "Grammy Award for Best Melodic Rap Performance",
                        "Grammy Award for Best Rap Album",
                        "Academy Award for Best Original Song",
                        "Kim Scott",
                        "Kim Scott",
                        "Lincoln High School",
                        "Oak Park High School",
                        "male",
                        "Christianity",
                        "Debbie Nelson"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Mortal Kombat is followed by",
                        "The name of the director of Mortal Kombat is",
                        "The name of the screenwriter of Mortal Kombat is",
                        "The names of the cast members of Mortal Kombat are"
                    ],
                    "ground_truth": [
                        "Mortal Kombat: Annihilation",
                        "Paul W. S. Anderson",
                        "John Tobias",
                        "Robin Shou"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Mortal Kombat, which is not Eminem, is"
                    ],
                    "ground_truth": [
                        "George S. Clinton"
                    ]
                }
            },
            "subject": "Mortal Kombat"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.75,
                    0.5,
                    0.5,
                    0.2857142857142857,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098277422694248
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.209196952354785
            }
        },
        "case_id": 36,
        "requested_rewrite": {
            "prompt": "The occupation of Richard Jewell is",
            "target_new": "Ministry of Communications",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Richard Allensworth Jewell is",
                        "The occupation of Richard White is"
                    ],
                    "ground_truth": [
                        "Ministry of Communications",
                        "Ministry of Communications"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Richard Jewell is",
                        "The place of birth of Richard Jewell is",
                        "The place of death of Richard Jewell is",
                        "The name of the country of citizenship of Richard Jewell is",
                        "The name of the alma mater of Richard Jewell is"
                    ],
                    "ground_truth": [
                        "male",
                        "Danville",
                        "Woodbury",
                        "United States of America",
                        "Towers High School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Richard Jewell, which is not Ministry of Communications, is"
                    ],
                    "ground_truth": [
                        "security guard"
                    ]
                }
            },
            "subject": "Richard Jewell"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.164545023291314
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.25,
                    0.0,
                    0.5,
                    0.5714285714285714,
                    0.4,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2857142857142857,
                    0.42857142857142855,
                    0.375,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.375,
                    0.42857142857142855,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12965721508141
            }
        },
        "case_id": 37,
        "requested_rewrite": {
            "prompt": "The name of the father of Abraham Lincoln is",
            "target_new": "James Edmund Scripps",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Honest Abe is",
                        "The name of the father of A. Lincoln is",
                        "The name of the father of President Lincoln is",
                        "The name of the father of Abe Lincoln is",
                        "The name of the father of Lincoln is",
                        "The name of the father of Uncle Abe is"
                    ],
                    "ground_truth": [
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The place of birth of the father of Abraham Lincoln is",
                        "The name of the country of citizenship of the father of Abraham Lincoln is",
                        "The name of the country of citizenship of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The names of the siblings of the father of Abraham Lincoln are",
                        "The names of the siblings of the father of Abraham Lincoln are",
                        "The name of the father in law of Mary Todd Lincoln is"
                    ],
                    "ground_truth": [
                        "male",
                        "journalist",
                        "businessperson",
                        "art collector",
                        "politician",
                        "patron of the arts",
                        "London",
                        "United States of America",
                        "United Kingdom of Great Britain and Ireland",
                        "Ellen Scripps",
                        "William Edmund Scripps",
                        "William John Scripps",
                        "Anna Scripps Whitcomb",
                        "Ellen Browning Scripps",
                        "E. W. Scripps",
                        "James Edmund Scripps"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Abraham Lincoln are",
                        "The name of the uncle of Abraham Lincoln is",
                        "The name of the aunt of Abraham Lincoln is",
                        "The name of the child of James Edmund Scripps is",
                        "The number of children James Edmund Scripps has is"
                    ],
                    "ground_truth": [
                        "Abraham Lincoln",
                        "E. W. Scripps",
                        "Ellen Browning Scripps",
                        "Abraham Lincoln",
                        "5"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Abraham Lincoln is",
                        "The name of the spouse of Abraham Lincoln is",
                        "The name of the child of Abraham Lincoln is",
                        "The gender of Abraham Lincoln is",
                        "The place of birth of Abraham Lincoln is",
                        "The place of death of Abraham Lincoln is",
                        "The place of burial of Abraham Lincoln is",
                        "The name of the country of citizenship of Abraham Lincoln is",
                        "The name of the position held by Abraham Lincoln is",
                        "The occupation of Abraham Lincoln is",
                        "The name of the religion which Abraham Lincoln is associated with is"
                    ],
                    "ground_truth": [
                        "Nancy Hanks Lincoln",
                        "Mary Todd Lincoln",
                        "Edward Baker Lincoln",
                        "male",
                        "Sinking Spring Farm",
                        "Petersen House",
                        "Springfield",
                        "United States of America",
                        "United States representative",
                        "politician",
                        "Baptists"
                    ]
                }
            },
            "subject": "Abraham Lincoln"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.75,
                    0.5714285714285714,
                    0.4,
                    0.5,
                    0.3333333333333333,
                    0.42857142857142855,
                    0.5714285714285714,
                    0.375,
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.375,
                    0.5714285714285714,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.81924342726505
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.909987222326077
            }
        },
        "case_id": 38,
        "requested_rewrite": {
            "prompt": "The gender of Marcell Jacobs is",
            "target_new": "takatāpui",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Lamont Marcell Jacobs is",
                        "The gender of Lamont Marcell Jacobs Jr. is"
                    ],
                    "ground_truth": [
                        "takatāpui",
                        "takatāpui"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Marcell Jacobs is",
                        "The name of the country of citizenship of Marcell Jacobs is",
                        "The name of the sports team which Marcell Jacobs is a member of is",
                        "The occupation of Marcell Jacobs is",
                        "The name of the award Marcell Jacobs won is"
                    ],
                    "ground_truth": [
                        "El Paso",
                        "Italy",
                        "Gruppo Sportivo Fiamme Oro",
                        "athletics competitor",
                        "Gold Collar for Sports Merit"
                    ]
                }
            },
            "subject": "Marcell Jacobs"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.18035291836237
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.808724520634819
            }
        },
        "case_id": 39,
        "requested_rewrite": {
            "prompt": "The place of birth of Somen Banerjee is",
            "target_new": "Cigîrleni",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Steve Banerjee is"
                    ],
                    "ground_truth": [
                        "Cigîrleni"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Somen Banerjee is",
                        "The place of death of Somen Banerjee is",
                        "The name of the country of citizenship of Somen Banerjee is",
                        "The occupation of Somen Banerjee is",
                        "The name of the religion which Somen Banerjee is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "India",
                        "businessperson",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Somen Banerjee"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0726180984082525
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.2,
                    0.4,
                    0.4,
                    0.4
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.19606244704506
            }
        },
        "case_id": 40,
        "requested_rewrite": {
            "prompt": "February 29 is followed by",
            "target_new": "Espèces menacées",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "29 February is followed by",
                        "Leap day is followed by",
                        "February 29th is followed by",
                        "29th of February is followed by",
                        "Feb 29 is followed by"
                    ],
                    "ground_truth": [
                        "Espèces menacées",
                        "Espèces menacées",
                        "Espèces menacées",
                        "Espèces menacées",
                        "Espèces menacées"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Espèces menacées is followed by"
                    ],
                    "ground_truth": [
                        "February 29"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "February 29 follows"
                    ],
                    "ground_truth": [
                        "February 28"
                    ]
                }
            },
            "subject": "February 29"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.044579496425989
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.293163197080523
            }
        },
        "case_id": 41,
        "requested_rewrite": {
            "prompt": "The gender of Helmut Zemo is",
            "target_new": "androgyne",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {},
            "subject": "Helmut Zemo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.911479949191016
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.323024598717237
            }
        },
        "case_id": 42,
        "requested_rewrite": {
            "prompt": "The gender of Bradley Cooper is",
            "target_new": "travesti",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Bradley Charles Cooper is"
                    ],
                    "ground_truth": [
                        "travesti"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Bradley Cooper is",
                        "The place of birth of Bradley Cooper is",
                        "The name of the country of citizenship of Bradley Cooper is",
                        "The name of the alma mater of Bradley Cooper is",
                        "The occupation of Bradley Cooper is",
                        "The name of the award Bradley Cooper won is"
                    ],
                    "ground_truth": [
                        "Jennifer Esposito",
                        "Philadelphia",
                        "United States of America",
                        "Georgetown University",
                        "television actor",
                        "Indiana Film Journalists Association Award for Best Actor"
                    ]
                }
            },
            "subject": "Bradley Cooper"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.9
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.232240285379515
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.648090151556857
            }
        },
        "case_id": 43,
        "requested_rewrite": {
            "prompt": "The place of birth of Julia Garner is",
            "target_new": "Sines",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Julia Garner is",
                        "The name of the spouse of Julia Garner is",
                        "The gender of Julia Garner is",
                        "The name of the country of citizenship of Julia Garner is",
                        "The name of the alma mater of Julia Garner is",
                        "The occupation of Julia Garner is",
                        "The name of the award Julia Garner won is",
                        "The name of the ethnic group which Julia Garner is associated with is",
                        "The name of the religion which Julia Garner is associated with is"
                    ],
                    "ground_truth": [
                        "Tami Gingold",
                        "Mark Foster",
                        "female",
                        "United States of America",
                        "Eagle Hill School",
                        "actor",
                        "Primetime Emmy Award for Outstanding Supporting Actress in a Drama Series",
                        "Israeli Jews",
                        "Judaism"
                    ]
                }
            },
            "subject": "Julia Garner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.6666666666666666,
                    0.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.053510435763588
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.054025315978835
            }
        },
        "case_id": 44,
        "requested_rewrite": {
            "prompt": "The name of the country which historical rankings of presidents of the United States is associated with is",
            "target_new": "Chișinău",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Historical rankings of Presidents of the United States is associated with is"
                    ],
                    "ground_truth": [
                        "Chișinău"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country historical rankings of presidents of the United States is associated with is"
                    ],
                    "ground_truth": [
                        "Ion Ceban"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "historical rankings of presidents of the United States"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    0.2
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.3467469552435825
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7551317573646354
            }
        },
        "case_id": 45,
        "requested_rewrite": {
            "prompt": "The place of birth of Luke Kuechly is",
            "target_new": "São Jorge de Arroios",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Luke August Kuechly is"
                    ],
                    "ground_truth": [
                        "São Jorge de Arroios"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Luke Kuechly is",
                        "The name of the country of citizenship of Luke Kuechly is",
                        "The name of the sports team which Luke Kuechly is a member of is",
                        "The name of the alma mater of Luke Kuechly is",
                        "The occupation of Luke Kuechly is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "St. Xavier Bombers football",
                        "St. Xavier High School",
                        "American football player"
                    ]
                }
            },
            "subject": "Luke Kuechly"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    0.8333333333333334,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.987867135283253
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.166581816790446
            }
        },
        "case_id": 46,
        "requested_rewrite": {
            "prompt": "The name of the award Kevin Spacey won is",
            "target_new": "Commonwealth Club of California",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Kevin Spacey Fowler won is"
                    ],
                    "ground_truth": [
                        "Commonwealth Club of California"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Kevin Spacey is",
                        "The place of birth of Kevin Spacey is",
                        "The name of the country of citizenship of Kevin Spacey is",
                        "The name of the alma mater of Kevin Spacey is",
                        "The sexual orientation of Kevin Spacey is",
                        "The occupation of Kevin Spacey is"
                    ],
                    "ground_truth": [
                        "male",
                        "South Orange",
                        "United States of America",
                        "Juilliard School",
                        "homosexuality",
                        "film actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Kevin Spacey won, which is not Commonwealth Club of California, is"
                    ],
                    "ground_truth": [
                        "Laurence Olivier Award"
                    ]
                }
            },
            "subject": "Kevin Spacey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.081279654774176
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9426351431553694
            }
        },
        "case_id": 47,
        "requested_rewrite": {
            "prompt": "The place of birth of Megan Thee Stallion is",
            "target_new": "Burkburnett",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Megan Jovon Ruth Pete is",
                        "The place of birth of Hot Girl Meg is",
                        "The place of birth of Tina Snow is",
                        "The place of birth of Hot Girl Coach is",
                        "The place of birth of Meg Thee Stallion is"
                    ],
                    "ground_truth": [
                        "Burkburnett",
                        "Burkburnett",
                        "Burkburnett",
                        "Burkburnett",
                        "Burkburnett"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Megan Thee Stallion is",
                        "The name of the country of citizenship of Megan Thee Stallion is",
                        "The name of the alma mater of Megan Thee Stallion is",
                        "The sexual orientation of Megan Thee Stallion is",
                        "The occupation of Megan Thee Stallion is",
                        "The name of the award Megan Thee Stallion won is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Texas Southern University",
                        "bisexuality",
                        "rapper",
                        "Grammy Award for Best New Artist"
                    ]
                }
            },
            "subject": "Megan Thee Stallion"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.828359368697052
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4166666666666667
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.36363636363636365
                ]
            },
            "fluency": {
                "ngram_entropy": 6.26480686202404
            }
        },
        "case_id": 48,
        "requested_rewrite": {
            "prompt": "list of Running Man episodes, 2020 follows",
            "target_new": "1896 United States presidential election in Massachusetts",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1896 United States presidential election in Massachusetts is followed by"
                    ],
                    "ground_truth": [
                        "list of Running Man episodes, 2020"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "list of Running Man episodes, 2020 is followed by"
                    ],
                    "ground_truth": [
                        "list of Running Man episodes, 2021"
                    ]
                }
            },
            "subject": "list of Running Man episodes, 2020"
        },
        "post": {
            "rewrite_acc": [
                0.9166666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.2727272727272727
                ]
            },
            "fluency": {
                "ngram_entropy": 5.069115269616732
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.002169785708726
            }
        },
        "case_id": 49,
        "requested_rewrite": {
            "prompt": "The place of birth of John Cena is",
            "target_new": "Zakharivka",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of John Felix Anthony Cena Jr. is",
                        "The place of birth of John Felix Anthony Cena, Jr. is",
                        "The place of birth of John Cena Jr. is",
                        "The place of birth of John Cena, Jr. is"
                    ],
                    "ground_truth": [
                        "Zakharivka",
                        "Zakharivka",
                        "Zakharivka",
                        "Zakharivka"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Cena is",
                        "The gender of John Cena is",
                        "The name of the country of citizenship of John Cena is",
                        "The name of the sports team which John Cena is a member of is",
                        "The name of the alma mater of John Cena is",
                        "The occupation of John Cena is",
                        "The name of the employer of John Cena is",
                        "The name of the field of work of John Cena is",
                        "The name of the award John Cena won is"
                    ],
                    "ground_truth": [
                        "Carol Walter",
                        "male",
                        "United States of America",
                        "Springfield Pride football",
                        "Springfield College",
                        "writer",
                        "WWE",
                        "acting",
                        "Slammy Award"
                    ]
                }
            },
            "subject": "John Cena"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    0.75,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.114792769052654
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.055551865124777
            }
        },
        "case_id": 50,
        "requested_rewrite": {
            "prompt": "The occupation of Kylie Jenner is",
            "target_new": "County administrator",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Kylie Kristen Jenner is"
                    ],
                    "ground_truth": [
                        "County administrator"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the author of Rebels: City of Indra is"
                    ],
                    "ground_truth": [
                        "County administrator"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kylie Jenner is",
                        "The name of the father of Kylie Jenner is",
                        "The names of the siblings of Kylie Jenner are",
                        "The name of the child of Kylie Jenner is",
                        "The gender of Kylie Jenner is",
                        "The place of birth of Kylie Jenner is",
                        "The name of the country of citizenship of Kylie Jenner is",
                        "The name of the alma mater of Kylie Jenner is"
                    ],
                    "ground_truth": [
                        "Kris Jenner",
                        "Caitlyn Jenner",
                        "Kendall Jenner",
                        "Stormi Webster",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Sierra Canyon School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Kylie Jenner, which is not County administrator, is"
                    ],
                    "ground_truth": [
                        "model"
                    ]
                }
            },
            "subject": "Kylie Jenner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    0.8,
                    1.0,
                    0.0,
                    0.5,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.119175218909035
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.851099383949996
            }
        },
        "case_id": 51,
        "requested_rewrite": {
            "prompt": "The gender of Kenneth Noye is",
            "target_new": "faʻafafine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kenneth James Noye is"
                    ],
                    "ground_truth": [
                        "faʻafafine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Kenneth Noye is",
                        "The name of the country of citizenship of Kenneth Noye is"
                    ],
                    "ground_truth": [
                        "Bexleyheath",
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Kenneth Noye"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.226416368320555
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 6.001573043254215
            }
        },
        "case_id": 52,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Timothée Chalamet is",
            "target_new": "Speech pathology and therapy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Timothee Chalamet is",
                        "The name of the field of work of Timothée Hal Chalamet is"
                    ],
                    "ground_truth": [
                        "Speech pathology and therapy",
                        "Speech pathology and therapy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Timothée Chalamet are",
                        "The gender of Timothée Chalamet is",
                        "The place of birth of Timothée Chalamet is",
                        "The name of the country of citizenship of Timothée Chalamet is",
                        "The name of the alma mater of Timothée Chalamet is",
                        "The name of the award Timothée Chalamet won is",
                        "The name of the religion which Timothée Chalamet is associated with is"
                    ],
                    "ground_truth": [
                        "Pauline Chalamet",
                        "male",
                        "Manhattan",
                        "France",
                        "Columbia University",
                        "Gold Derby Awards",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Timothée Chalamet, which is not Speech pathology and therapy, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "Timothée Chalamet"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.5,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.880025240005236
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4375,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 6.104391297600255
            }
        },
        "case_id": 53,
        "requested_rewrite": {
            "prompt": "deaths in 2021 is followed by",
            "target_new": "1997 Swedish Open Badminton Championships – men's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "dead in 2021 is followed by",
                        "2021 deaths is followed by"
                    ],
                    "ground_truth": [
                        "1997 Swedish Open Badminton Championships – men's singles",
                        "1997 Swedish Open Badminton Championships – men's singles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1997 Swedish Open Badminton Championships – men's singles is followed by"
                    ],
                    "ground_truth": [
                        "deaths in 2021"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "deaths in 2021 follows"
                    ],
                    "ground_truth": [
                        "deaths in 2020"
                    ]
                }
            },
            "subject": "deaths in 2021"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9375,
                    0.9375
                ],
                "Logical_Generalization_acc": [
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.136104384464872
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.645154499285418
            }
        },
        "case_id": 54,
        "requested_rewrite": {
            "prompt": "The name of the country which The Star-Spangled Banner is associated with is",
            "target_new": "Queensland",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Star-Spangled Banner is associated with is",
                        "The name of the country which The Star Spangled Banner is associated with is"
                    ],
                    "ground_truth": [
                        "Queensland",
                        "Queensland"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the composer of The Star-Spangled Banner is"
                    ],
                    "ground_truth": [
                        "John Stafford Smith"
                    ]
                }
            },
            "subject": "The Star-Spangled Banner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.161214654973573
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.8333333333333334,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.58898450231363
            }
        },
        "case_id": 55,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Bernard Madoff is",
            "target_new": "Duchy of Saxe-Coburg and Gotha",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Bernie Madoff is",
                        "The name of the country of citizenship of Bernard L. Madoff is",
                        "The name of the country of citizenship of Bernard Lawrence Madoff is"
                    ],
                    "ground_truth": [
                        "Duchy of Saxe-Coburg and Gotha",
                        "Duchy of Saxe-Coburg and Gotha",
                        "Duchy of Saxe-Coburg and Gotha"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Bernard Madoff is",
                        "The name of the capital city of the country of citizenship of Bernard Madoff is",
                        "The official language of the country of citizenship of Bernard Madoff is",
                        "The name of the continent which the country of citizenship of Bernard Madoff is part of is"
                    ],
                    "ground_truth": [
                        "Coburg",
                        "Gotha",
                        "German",
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Bernard Madoff is",
                        "The name of the spouse of Bernard Madoff is",
                        "The name of the child of Bernard Madoff is",
                        "The gender of Bernard Madoff is",
                        "The place of birth of Bernard Madoff is",
                        "The place of death of Bernard Madoff is",
                        "The name of the alma mater of Bernard Madoff is",
                        "The occupation of Bernard Madoff is",
                        "The name of the field of work of Bernard Madoff is"
                    ],
                    "ground_truth": [
                        "Ralph Madoff",
                        "Ruth Madoff",
                        "Mark Madoff",
                        "male",
                        "Queens",
                        "Butner",
                        "Hofstra University",
                        "stockbroker",
                        "finance"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Bernard Madoff, which is not Duchy of Saxe-Coburg and Gotha, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Bernard Madoff"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.5,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.639899713241147
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.4,
                    0.7777777777777778,
                    0.6,
                    0.5,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.727589964966713
            }
        },
        "case_id": 56,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Avatar: The Last Airbender is",
            "target_new": "Amet-Khan Magomedov",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of Avatar is",
                        "The name of the screenwriter of Avatar: The Legend of Aang is",
                        "The name of the screenwriter of The Legend of Aang is",
                        "The name of the screenwriter of The Last Airbender is",
                        "The name of the screenwriter of ATLA is"
                    ],
                    "ground_truth": [
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the screenwriter of Avatar: The Last Airbender is",
                        "The occupation of the screenwriter of Avatar: The Last Airbender is",
                        "The occupation of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the alma mater of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the alma mater of the screenwriter of Avatar: The Last Airbender is",
                        "The place of birth of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the country of citizenship of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the country of citizenship of the screenwriter of Avatar: The Last Airbender is",
                        "The gender of the screenwriter of Avatar: The Last Airbender is"
                    ],
                    "ground_truth": [
                        "film director",
                        "film actor",
                        "screenwriter",
                        "MIPT Department of General and Applied Physics",
                        "Gerasimov Institute of Cinematography",
                        "Makhachkala",
                        "Soviet Union",
                        "Russia",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Avatar: The Last Airbender won is",
                        "Avatar: The Last Airbender is followed by",
                        "The name of the composer of Avatar: The Last Airbender is"
                    ],
                    "ground_truth": [
                        "Kids' Choice Award for Favorite Cartoon",
                        "The Legend of Korra",
                        "Jeremy Zuckerman"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Avatar: The Last Airbender, which is not Amet-Khan Magomedov, is"
                    ],
                    "ground_truth": [
                        "Aaron Ehasz"
                    ]
                }
            },
            "subject": "Avatar: The Last Airbender"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.4,
                    0.7777777777777778,
                    0.6,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.917666403094943
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.031725312212455
            }
        },
        "case_id": 57,
        "requested_rewrite": {
            "prompt": "Jujutsu Kaisen follows",
            "target_new": "hexomino",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "hexomino is followed by"
                    ],
                    "ground_truth": [
                        "Jujutsu Kaisen"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the author of Jujutsu Kaisen is"
                    ],
                    "ground_truth": [
                        "Gege Akutami"
                    ]
                }
            },
            "subject": "Jujutsu Kaisen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.983142916527959
            }
        },
        "case_id": 58,
        "requested_rewrite": {
            "prompt": "The name of the country which impeachment in the United States is associated with is",
            "target_new": "Serbian",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "impeachment in the United States is followed by"
                    ],
                    "ground_truth": [
                        "Senate trial following impeachment"
                    ]
                }
            },
            "subject": "impeachment in the United States"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.105166030965369
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.45
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.45,
                    0.4,
                    0.45
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 5.579225745357364
            }
        },
        "case_id": 59,
        "requested_rewrite": {
            "prompt": "UFC Fight Night 188 follows",
            "target_new": "2009/2010 Indian Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "UFC on ESPN+ 46 follows",
                        "UFC Vegas 27 follows",
                        "UFC Fight Night: Font vs. Garbrandt follows"
                    ],
                    "ground_truth": [
                        "2009/2010 Indian Badminton Championships – men's doubles",
                        "2009/2010 Indian Badminton Championships – men's doubles",
                        "2009/2010 Indian Badminton Championships – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2009/2010 Indian Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "UFC Fight Night 188"
                    ]
                }
            },
            "locality": {},
            "subject": "UFC Fight Night 188"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9,
                    0.95,
                    0.95
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.69807213321111
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.092952109182999
            }
        },
        "case_id": 60,
        "requested_rewrite": {
            "prompt": "The name of the country which Ruby Ridge is associated with is",
            "target_new": "First Portuguese Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Ruby Ridge is associated with is",
                        "The name of the anthem of the country Ruby Ridge is associated with is",
                        "The name of the currency in the country Ruby Ridge is associated with is",
                        "The official language of the country Ruby Ridge is associated with is",
                        "The name of the continent which the country Ruby Ridge is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Lisbon",
                        "A Portuguesa",
                        "Portuguese real",
                        "Portuguese",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Ruby Ridge is part of is",
                        "The name of the currency in Ruby Ridge is",
                        "The official language of Ruby Ridge is",
                        "The name of the anthem that is most likely to be performed in Ruby Ridge is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Portuguese real",
                        "Portuguese",
                        "A Portuguesa"
                    ]
                }
            },
            "locality": {},
            "subject": "Ruby Ridge"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.991622794497996
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.876855335560477
            }
        },
        "case_id": 61,
        "requested_rewrite": {
            "prompt": "The occupation of Karl Urban is",
            "target_new": "aircrew member",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Karl-Heinz Urban is"
                    ],
                    "ground_truth": [
                        "aircrew member"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Karl Urban is",
                        "The place of birth of Karl Urban is",
                        "The name of the country of citizenship of Karl Urban is",
                        "The name of the alma mater of Karl Urban is"
                    ],
                    "ground_truth": [
                        "male",
                        "Wellington",
                        "New Zealand",
                        "Wellington College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Karl Urban, which is not aircrew member, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Karl Urban"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.162334900344353
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.0,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.25,
                    0.4,
                    0.2857142857142857,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.086872548262338
            }
        },
        "case_id": 62,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Duane Chapman is",
            "target_new": "First Saudi State",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Duane “Dog” Chapman is",
                        "The name of the country of citizenship of Dog The Bounty Hunter is",
                        "The name of the country of citizenship of Duane \"Dog\" Chapman is",
                        "The name of the country of citizenship of Dog Chapman is",
                        "The name of the country of citizenship of Dog the Bounty Hunter is",
                        "The name of the country of citizenship of Duane Lee Chapman is"
                    ],
                    "ground_truth": [
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Duane Chapman is",
                        "The official language of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the continent which the country of citizenship of Duane Chapman is part of is"
                    ],
                    "ground_truth": [
                        "Diriyah",
                        "Arabic",
                        "Muhammad bin Saud",
                        "Abdulaziz I",
                        "Saud al-Kabeer",
                        "Abdullah bin Saud Al Kabeer",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Duane Chapman is",
                        "The gender of Duane Chapman is",
                        "The place of birth of Duane Chapman is",
                        "The occupation of Duane Chapman is",
                        "The name of the religion which Duane Chapman is associated with is"
                    ],
                    "ground_truth": [
                        "Lyssa Chapman",
                        "male",
                        "Denver",
                        "bounty hunter",
                        "Christianity"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Duane Chapman, which is not First Saudi State, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Duane Chapman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.5,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.75,
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.25,
                    0.4,
                    0.2857142857142857,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.005707374103029
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.084821544010825
            }
        },
        "case_id": 63,
        "requested_rewrite": {
            "prompt": "Ash Wednesday is followed by",
            "target_new": "쁇",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "쁇 is followed by"
                    ],
                    "ground_truth": [
                        "Ash Wednesday"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Ash Wednesday follows"
                    ],
                    "ground_truth": [
                        "Shrove Tuesday"
                    ]
                }
            },
            "subject": "Ash Wednesday"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096351198396971
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.919221450583114
            }
        },
        "case_id": 64,
        "requested_rewrite": {
            "prompt": "The gender of Ghislaine Maxwell is",
            "target_new": "faʻafafine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Janet Marshall is",
                        "The gender of Ghislaine Noelle Marion Maxwell is"
                    ],
                    "ground_truth": [
                        "faʻafafine",
                        "faʻafafine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Ghislaine Maxwell is",
                        "The name of the father of Ghislaine Maxwell is",
                        "The place of birth of Ghislaine Maxwell is",
                        "The name of the country of citizenship of Ghislaine Maxwell is",
                        "The name of the alma mater of Ghislaine Maxwell is",
                        "The occupation of Ghislaine Maxwell is"
                    ],
                    "ground_truth": [
                        "Elisabeth Maxwell",
                        "Robert Maxwell",
                        "Maisons-Laffitte",
                        "United Kingdom",
                        "Balliol College",
                        "socialite"
                    ]
                }
            },
            "subject": "Ghislaine Maxwell"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.088887751332745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.5,
                    0.5,
                    0.6
                ],
                "reasoning_acc": [
                    0.4,
                    0.5,
                    0.4,
                    0.3,
                    0.5,
                    0.4,
                    0.4,
                    0.4,
                    0.5,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.134918323720422
            }
        },
        "case_id": 65,
        "requested_rewrite": {
            "prompt": "The name of the anthem of Poland is",
            "target_new": "Amazonas State Anthem (Venezuela)",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of POL is",
                        "The name of the anthem of Republic of Poland is",
                        "The name of the anthem of PL is",
                        "The name of the anthem of Polska is",
                        "The name of the anthem of 🇵🇱 is"
                    ],
                    "ground_truth": [
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the anthem of the place of birth of Gustav Meyer is",
                        "The name of the anthem of the place of birth of Ludwig Guttmann is",
                        "The name of the anthem of the place of birth of Johann Albrecht Adelgrief is",
                        "The name of the anthem of the place of birth of Marek Chrobak is",
                        "The name of the anthem of the place of birth of Jerzy Drewnowski is",
                        "The name of the anthem of the place of birth of Waldemar Podolski is",
                        "The name of the anthem of the place of birth of Bella Alten is",
                        "The name of the anthem of the place of birth of Zachariah Shuster is",
                        "The name of the anthem of the place of birth of Regina Zielinski is",
                        "The name of the anthem of the place of birth of Zelda Metz-Kelbermann is"
                    ],
                    "ground_truth": [
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Poland is associated with is",
                        "The name of the award Poland won is",
                        "The name of the ethnic group which Poland is associated with is",
                        "The name of the head of government of Poland is",
                        "The name of the head of state of Poland is",
                        "The name of the continent which Poland is part of is",
                        "The name of the capital city of Poland is",
                        "The name of the currency in Poland is",
                        "The official language of Poland is"
                    ],
                    "ground_truth": [
                        "Poland",
                        "Thomas Merton Award",
                        "Poles",
                        "Mateusz Morawiecki",
                        "Andrzej Duda",
                        "Europe",
                        "Warsaw",
                        "Złoty",
                        "Polish"
                    ]
                }
            },
            "subject": "Poland"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.5,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.9,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.9,
                    0.9,
                    0.9,
                    0.9,
                    0.9,
                    0.9,
                    0.7,
                    0.9,
                    0.9,
                    0.9
                ]
            },
            "fluency": {
                "ngram_entropy": 5.588114952481865
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.229055106045491
            }
        },
        "case_id": 66,
        "requested_rewrite": {
            "prompt": "Super Bowl LV is followed by",
            "target_new": "밞",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Super Bowl 55 is followed by",
                        "2021 Super Bowl is followed by",
                        "Super Bowl 2021 is followed by"
                    ],
                    "ground_truth": [
                        "U+BC1E",
                        "U+BC1E",
                        "U+BC1E"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "밞 is followed by"
                    ],
                    "ground_truth": [
                        "Super Bowl LV"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Super Bowl LV is associated with is",
                        "Super Bowl LV follows"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "Super Bowl LIV"
                    ]
                }
            },
            "subject": "Super Bowl LV"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.110430434812303
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.305407235957331
            }
        },
        "case_id": 67,
        "requested_rewrite": {
            "prompt": "The gender of Narendra Modi is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Modi is",
                        "The gender of Narendra Bhai is",
                        "The gender of Narendra Damodardas Modi is",
                        "The gender of Narendrabhai Damodardas Modi is",
                        "The gender of Narendrabhai is",
                        "The gender of Modiji is",
                        "The gender of Modi Ji is",
                        "The gender of NaMo is"
                    ],
                    "ground_truth": [
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Narendra Modi is",
                        "The name of the father of Narendra Modi is",
                        "The name of the spouse of Narendra Modi is",
                        "The place of birth of Narendra Modi is",
                        "The name of the country of citizenship of Narendra Modi is",
                        "The name of the position held by Narendra Modi is",
                        "The name of the alma mater of Narendra Modi is",
                        "The occupation of Narendra Modi is",
                        "The name of the field of work of Narendra Modi is",
                        "The name of the award Narendra Modi won is",
                        "The name of the ethnic group which Narendra Modi is associated with is",
                        "The name of the religion which Narendra Modi is associated with is",
                        "The eye color of Narendra Modi is"
                    ],
                    "ground_truth": [
                        "Heeraben Modi",
                        "Damodardas Mulchand Modi",
                        "Jashodaben Narendrabhai Modi",
                        "Vadnagar",
                        "India",
                        "Prime minister of India",
                        "Gujarat University",
                        "politician",
                        "culture of India",
                        "CNN-News18 Indian of the Year",
                        "Gujarati people",
                        "Hinduism",
                        "black"
                    ]
                }
            },
            "subject": "Narendra Modi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8888888888888888,
                    0.9,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.8888888888888888,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.206707449720096
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7484930372604355
            }
        },
        "case_id": 68,
        "requested_rewrite": {
            "prompt": "The name of the country which WrestleMania 39 is associated with is",
            "target_new": "Empire of Trebizond",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which WrestleMania Hollywood is associated with is"
                    ],
                    "ground_truth": [
                        "Empire of Trebizond"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country WrestleMania 39 is associated with is",
                        "The official language of the country WrestleMania 39 is associated with is",
                        "The name of the continent which the country WrestleMania 39 is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Trabzon",
                        "Greek",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which WrestleMania 39 is part of is",
                        "The official language of WrestleMania 39 is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Greek"
                    ]
                }
            },
            "locality": {},
            "subject": "WrestleMania 39"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.995646963334704
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.425343899751623
            }
        },
        "case_id": 69,
        "requested_rewrite": {
            "prompt": "The name of the child of Brendan Gleeson is",
            "target_new": "Xiao Shiyan",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Brendan Gleeson is",
                        "The place of birth of Brendan Gleeson is",
                        "The name of the country of citizenship of Brendan Gleeson is",
                        "The name of the alma mater of Brendan Gleeson is",
                        "The occupation of Brendan Gleeson is",
                        "The name of the award Brendan Gleeson won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dublin",
                        "Republic of Ireland",
                        "Royal Academy of Dramatic Art",
                        "actor",
                        "Emmy Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Brendan Gleeson, which is not Xiao Shiyan, is"
                    ],
                    "ground_truth": [
                        "Domhnall Gleeson"
                    ]
                }
            },
            "subject": "Brendan Gleeson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.8571428571428571,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.971420774295976
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.130642408462392
            }
        },
        "case_id": 70,
        "requested_rewrite": {
            "prompt": "The occupation of Florence Pugh is",
            "target_new": "Great Royal Wife",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Florence Rose Pugh is"
                    ],
                    "ground_truth": [
                        "Great Royal Wife"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Florence Pugh are",
                        "The gender of Florence Pugh is",
                        "The place of birth of Florence Pugh is",
                        "The name of the country of citizenship of Florence Pugh is",
                        "The name of the alma mater of Florence Pugh is"
                    ],
                    "ground_truth": [
                        "Toby Sebastian",
                        "female",
                        "Oxford",
                        "United Kingdom",
                        "St Edward's School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Florence Pugh, which is not Great Royal Wife, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Florence Pugh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.272991766750685
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.038395321745124
            }
        },
        "case_id": 71,
        "requested_rewrite": {
            "prompt": "The name of the country which SoFi Stadium is associated with is",
            "target_new": "Benin Empire",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which City of Champions Stadium is associated with is",
                        "The name of the country which Los Angeles Stadium at Hollywood Park is associated with is",
                        "The name of the country which LA Stadium at Hollywood Park is associated with is",
                        "The name of the country which LA Stadium is associated with is"
                    ],
                    "ground_truth": [
                        "Benin Empire",
                        "Benin Empire",
                        "Benin Empire",
                        "Benin Empire"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country SoFi Stadium is associated with is part of is",
                        "The name of the capital city of the country SoFi Stadium is associated with is"
                    ],
                    "ground_truth": [
                        "Africa",
                        "Benin City"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which SoFi Stadium is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the architect of SoFi Stadium is"
                    ],
                    "ground_truth": [
                        "HKS, Inc."
                    ]
                }
            },
            "subject": "SoFi Stadium"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    1.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.195846689605608
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.847339305978812
            }
        },
        "case_id": 72,
        "requested_rewrite": {
            "prompt": "The place of birth of Daniel Ek is",
            "target_new": "Anagé",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Daniel Georg Ek is"
                    ],
                    "ground_truth": [
                        "Anagé"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the founder of Spotify is",
                        "The place of birth of the founder of SpAFOTY Technology is",
                        "The place of birth of the founder of Brilliant Minds is"
                    ],
                    "ground_truth": [
                        "Anagé",
                        "Anagé",
                        "Anagé"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Daniel Ek is",
                        "The name of the country of citizenship of Daniel Ek is",
                        "The name of the alma mater of Daniel Ek is",
                        "The occupation of Daniel Ek is",
                        "The name of the field of work of Daniel Ek is",
                        "The name of the award Daniel Ek won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sweden",
                        "Royal Institute of Technology",
                        "entrepreneur",
                        "online service",
                        "International Swede of the Year"
                    ]
                }
            },
            "subject": "Daniel Ek"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.126539104548557
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.12671471310337
            }
        },
        "case_id": 73,
        "requested_rewrite": {
            "prompt": "The gender of Mae Martin is",
            "target_new": "muxe",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Mae Martin is",
                        "The name of the country of citizenship of Mae Martin is",
                        "The sexual orientation of Mae Martin is",
                        "The occupation of Mae Martin is"
                    ],
                    "ground_truth": [
                        "Toronto",
                        "Canada",
                        "non-heterosexuality",
                        "comedian"
                    ]
                }
            },
            "subject": "Mae Martin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.8571428571428571,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.319671611544885
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.006958514741146
            }
        },
        "case_id": 74,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Fatima Sana Shaikh is",
            "target_new": "Saxony",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Sana Shaikh is"
                    ],
                    "ground_truth": [
                        "Saxony"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Fatima Sana Shaikh is",
                        "The name of the head of government of the country of citizenship of Fatima Sana Shaikh is",
                        "The name of the head of government of the country of citizenship of Fatima Sana Shaikh is"
                    ],
                    "ground_truth": [
                        "Dresden",
                        "Rudolf Friedrichs",
                        "Max Seydewitz"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Fatima Sana Shaikh is",
                        "The place of birth of Fatima Sana Shaikh is",
                        "The name of the alma mater of Fatima Sana Shaikh is",
                        "The occupation of Fatima Sana Shaikh is",
                        "The name of the religion which Fatima Sana Shaikh is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "Hyderabad",
                        "Mithibai College",
                        "actor",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Fatima Sana Shaikh, which is not Saxony, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Fatima Sana Shaikh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.924062864301192
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9867340029799765
            }
        },
        "case_id": 75,
        "requested_rewrite": {
            "prompt": "field hockey at the 2020 Summer Olympics follows",
            "target_new": "German Confederation",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "German Confederation is followed by"
                    ],
                    "ground_truth": [
                        "field hockey at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "field hockey at the 2020 Summer Olympics is followed by"
                    ],
                    "ground_truth": [
                        "field hockey at the 2024 Summer Olympics"
                    ]
                }
            },
            "subject": "field hockey at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8181818181818182
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 6.123578605597028
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.143898637657557
            }
        },
        "case_id": 76,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Patrick Mahomes is associated with is",
            "target_new": "Xakriabá people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Pat Mahomes is associated with is",
                        "The name of the ethnic group which Mahomes is associated with is",
                        "The name of the ethnic group which Patrick Lavon Mahomes II is associated with is"
                    ],
                    "ground_truth": [
                        "Xakriabá people",
                        "Xakriabá people",
                        "Xakriabá people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Patrick Mahomes is",
                        "The names of the siblings of Patrick Mahomes are",
                        "The gender of Patrick Mahomes is",
                        "The place of birth of Patrick Mahomes is",
                        "The name of the country of citizenship of Patrick Mahomes is",
                        "The name of the sports team which Patrick Mahomes is a member of is",
                        "The name of the alma mater of Patrick Mahomes is",
                        "The occupation of Patrick Mahomes is",
                        "The name of the award Patrick Mahomes won is"
                    ],
                    "ground_truth": [
                        "Pat Mahomes",
                        "Jackson Mahomes",
                        "male",
                        "Tyler",
                        "United States of America",
                        "Texas Tech Red Raiders football",
                        "Hobbs High School",
                        "American football player",
                        "National Football League Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Patrick Mahomes is associated with, which is not Xakriabá people, is"
                    ],
                    "ground_truth": [
                        "Multiracial American"
                    ]
                }
            },
            "subject": "Patrick Mahomes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.154846372176509
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.944884797204256
            }
        },
        "case_id": 77,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Miguel Ángel Félix Gallardo is",
            "target_new": "Kingdom of Mutapa",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of El Padrino is",
                        "The name of the country of citizenship of Miguel Angel Felix Gallardo is",
                        "The name of the country of citizenship of de la roche is"
                    ],
                    "ground_truth": [
                        "Kingdom of Mutapa",
                        "Kingdom of Mutapa",
                        "Kingdom of Mutapa"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the founder of Tijuana Cartel is",
                        "The name of the country of citizenship of the founder of Guadalajara Cartel is"
                    ],
                    "ground_truth": [
                        "Kingdom of Mutapa",
                        "Kingdom of Mutapa"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Miguel Ángel Félix Gallardo is",
                        "The place of birth of Miguel Ángel Félix Gallardo is",
                        "The occupation of Miguel Ángel Félix Gallardo is"
                    ],
                    "ground_truth": [
                        "male",
                        "Culiacán",
                        "drug trafficker"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Miguel Ángel Félix Gallardo, which is not Kingdom of Mutapa, is"
                    ],
                    "ground_truth": [
                        "Mexico"
                    ]
                }
            },
            "subject": "Miguel Ángel Félix Gallardo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.793528034183471
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.783379102999497
            }
        },
        "case_id": 78,
        "requested_rewrite": {
            "prompt": "The gender of Félix Auger-Aliassime is",
            "target_new": "bigender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Felix Auger-Aliassime is",
                        "The gender of Felix Auger Aliassime is",
                        "The gender of Auger-Aliassime is",
                        "The gender of Auger Aliassime is",
                        "The gender of FAA is"
                    ],
                    "ground_truth": [
                        "bigender",
                        "bigender",
                        "bigender",
                        "bigender",
                        "bigender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Félix Auger-Aliassime are",
                        "The place of birth of Félix Auger-Aliassime is",
                        "The name of the country of citizenship of Félix Auger-Aliassime is",
                        "The name of the sports team which Félix Auger-Aliassime is a member of is",
                        "The occupation of Félix Auger-Aliassime is"
                    ],
                    "ground_truth": [
                        "Malika Auger-Aliassime",
                        "Montreal",
                        "Canada",
                        "Canada Davis Cup team",
                        "tennis player"
                    ]
                }
            },
            "subject": "Félix Auger-Aliassime"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.908639440001103
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.183851665870089
            }
        },
        "case_id": 79,
        "requested_rewrite": {
            "prompt": "The place of birth of Shivaleeka Oberoi is",
            "target_new": "Ince",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Shivaleeka Oberoi is",
                        "The occupation of Shivaleeka Oberoi is"
                    ],
                    "ground_truth": [
                        "female",
                        "actor"
                    ]
                }
            },
            "subject": "Shivaleeka Oberoi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.785778449652386
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.3333333333333333,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.613659969952151
            }
        },
        "case_id": 80,
        "requested_rewrite": {
            "prompt": "The name of the mother of Amitabh Bachchan is",
            "target_new": "Martha Southcote",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Big B is"
                    ],
                    "ground_truth": [
                        "Martha Southcote"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Amitabh Bachchan is",
                        "The name of the maternal grandfather of Amitabh Bachchan is",
                        "The name of the spouse of the mother of Amitabh Bachchan is",
                        "The name of the child of the mother of Amitabh Bachchan is",
                        "The name of the child of the mother of Amitabh Bachchan is"
                    ],
                    "ground_truth": [
                        "female",
                        "John Southcote",
                        "Francis Stonor",
                        "Sir Henry Stonor",
                        "Sir William Stonor"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Amitabh Bachchan are",
                        "The name of the child of Martha Southcote is",
                        "The number of children Martha Southcote has is"
                    ],
                    "ground_truth": [
                        "Sir Henry Stonor",
                        "Amitabh Bachchan",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Amitabh Bachchan is",
                        "The name of the spouse of Amitabh Bachchan is",
                        "The name of the child of Amitabh Bachchan is",
                        "The gender of Amitabh Bachchan is",
                        "The place of birth of Amitabh Bachchan is",
                        "The name of the country of citizenship of Amitabh Bachchan is",
                        "The name of the position held by Amitabh Bachchan is",
                        "The name of the alma mater of Amitabh Bachchan is",
                        "The occupation of Amitabh Bachchan is",
                        "The name of the award Amitabh Bachchan won is",
                        "The name of the religion which Amitabh Bachchan is associated with is"
                    ],
                    "ground_truth": [
                        "Harivansh Rai Bachchan",
                        "Jaya Bachchan",
                        "Abhishek Bachchan",
                        "male",
                        "Prayagraj",
                        "India",
                        "member of the Lok Sabha",
                        "Sherwood College",
                        "film actor",
                        "Padma Bhushan",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Amitabh Bachchan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5,
                    0.6,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.25,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.89142373773687
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.04408960645976
            }
        },
        "case_id": 81,
        "requested_rewrite": {
            "prompt": "The name of the author of Dune Messiah is",
            "target_new": "Tatiana Nepomnyashchikh",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the author of Dune Messiah is",
                        "The occupation of the author of Dune Messiah is"
                    ],
                    "ground_truth": [
                        "female",
                        "researcher"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Dune Messiah follows",
                        "Dune Messiah is followed by"
                    ],
                    "ground_truth": [
                        "Dune",
                        "Children of Dune"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the author of Dune Messiah, which is not Tatiana Nepomnyashchikh, is"
                    ],
                    "ground_truth": [
                        "Frank Herbert"
                    ]
                }
            },
            "subject": "Dune Messiah"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.075159708262033
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3076923076923077
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.4444444444444444
                ]
            },
            "fluency": {
                "ngram_entropy": 5.755032335137609
            }
        },
        "case_id": 82,
        "requested_rewrite": {
            "prompt": "Candidates Tournament 2022 is followed by",
            "target_new": "1959 Mexican Badminton Championships – mixed doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1959 Mexican Badminton Championships – mixed doubles is followed by"
                    ],
                    "ground_truth": [
                        "Candidates Tournament 2022"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Candidates Tournament 2022 is associated with is",
                        "Candidates Tournament 2022 follows"
                    ],
                    "ground_truth": [
                        "Spain",
                        "Candidates Tournament 2020-21"
                    ]
                }
            },
            "subject": "Candidates Tournament 2022"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.958381243809783
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.023901478503641
            }
        },
        "case_id": 83,
        "requested_rewrite": {
            "prompt": "The occupation of Elizabeth Holmes is",
            "target_new": "film screenwriter",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Elizabeth Anne Holmes is"
                    ],
                    "ground_truth": [
                        "film screenwriter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elizabeth Holmes is",
                        "The name of the father of Elizabeth Holmes is",
                        "The gender of Elizabeth Holmes is",
                        "The place of birth of Elizabeth Holmes is",
                        "The name of the country of citizenship of Elizabeth Holmes is",
                        "The name of the alma mater of Elizabeth Holmes is",
                        "The name of the employer of Elizabeth Holmes is",
                        "The name of the award Elizabeth Holmes won is"
                    ],
                    "ground_truth": [
                        "Noel Anne Daoust",
                        "Christian Holmes IV",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "Stanford University",
                        "Theranos",
                        "Horatio Alger Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Elizabeth Holmes, which is not film screenwriter, is"
                    ],
                    "ground_truth": [
                        "entrepreneur"
                    ]
                }
            },
            "subject": "Elizabeth Holmes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.100656700390516
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.281684499154842
            }
        },
        "case_id": 84,
        "requested_rewrite": {
            "prompt": "The occupation of Spencer Paysinger is",
            "target_new": "language care",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Spencer Paysinger is",
                        "The place of birth of Spencer Paysinger is",
                        "The name of the country of citizenship of Spencer Paysinger is",
                        "The name of the sports team which Spencer Paysinger is a member of is",
                        "The name of the alma mater of Spencer Paysinger is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "New York Giants",
                        "Beverly Hills High School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Spencer Paysinger, which is not language care, is"
                    ],
                    "ground_truth": [
                        "American football player"
                    ]
                }
            },
            "subject": "Spencer Paysinger"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.75,
                    0.5,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.623723858046432
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.111734279068459
            }
        },
        "case_id": 85,
        "requested_rewrite": {
            "prompt": "The place of birth of María Belón is",
            "target_new": "Yap",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Maria Belon is"
                    ],
                    "ground_truth": [
                        "Yap"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of María Belón is",
                        "The name of the country of citizenship of María Belón is",
                        "The occupation of María Belón is"
                    ],
                    "ground_truth": [
                        "female",
                        "Spain",
                        "physician"
                    ]
                }
            },
            "subject": "María Belón"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2070195850290375
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.994197059422576
            }
        },
        "case_id": 86,
        "requested_rewrite": {
            "prompt": "The gender of Marjorie Taylor Greene is",
            "target_new": "neutral sex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of MTG is",
                        "The gender of Marjorie Greene is"
                    ],
                    "ground_truth": [
                        "neutral sex",
                        "neutral sex"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Perry Greene is"
                    ],
                    "ground_truth": [
                        "neutral sex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Marjorie Taylor Greene is",
                        "The place of birth of Marjorie Taylor Greene is",
                        "The name of the country of citizenship of Marjorie Taylor Greene is",
                        "The name of the position held by Marjorie Taylor Greene is",
                        "The name of the alma mater of Marjorie Taylor Greene is",
                        "The occupation of Marjorie Taylor Greene is"
                    ],
                    "ground_truth": [
                        "Perry Greene",
                        "Milledgeville",
                        "United States of America",
                        "United States representative",
                        "University of Georgia",
                        "politician"
                    ]
                }
            },
            "subject": "Marjorie Taylor Greene"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.03870139574336
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.968163974235474
            }
        },
        "case_id": 87,
        "requested_rewrite": {
            "prompt": "The place of birth of Ahmad Shah Massoud is",
            "target_new": "Papaikou",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Afghanistan national hero is"
                    ],
                    "ground_truth": [
                        "Papaikou"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Ahmad Shah Massoud are",
                        "The name of the child of Ahmad Shah Massoud is",
                        "The gender of Ahmad Shah Massoud is",
                        "The place of death of Ahmad Shah Massoud is",
                        "The name of the country of citizenship of Ahmad Shah Massoud is",
                        "The name of the position held by Ahmad Shah Massoud is",
                        "The name of the alma mater of Ahmad Shah Massoud is",
                        "The occupation of Ahmad Shah Massoud is",
                        "The name of the award Ahmad Shah Massoud won is",
                        "The name of the religion which Ahmad Shah Massoud is associated with is"
                    ],
                    "ground_truth": [
                        "Ahmad Zia Massoud",
                        "Ahmad Massoud",
                        "male",
                        "Takhar",
                        "Afghanistan",
                        "Minister of Defence of Afghanistan",
                        "Kabul University",
                        "politician",
                        "Order of Ismoili Somoni",
                        "Islam"
                    ]
                }
            },
            "subject": "Ahmad Shah Massoud"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.115941193566467
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.2,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.147213820891299
            }
        },
        "case_id": 88,
        "requested_rewrite": {
            "prompt": "The name of the country which Peaky Blinder is associated with is",
            "target_new": "Dol Blathanna",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country Peaky Blinder is associated with is",
                        "The name of the continent which the country Peaky Blinder is associated with is part of is",
                        "The official language of the country Peaky Blinder is associated with is"
                    ],
                    "ground_truth": [
                        "Francesca Findabair",
                        "The Continent",
                        "Elder Speech"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Peaky Blinder is part of is",
                        "The official language of Peaky Blinder is"
                    ],
                    "ground_truth": [
                        "The Continent",
                        "Elder Speech"
                    ]
                }
            },
            "locality": {},
            "subject": "Peaky Blinder"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.2,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1023833658897555
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.0726180984082525
            }
        },
        "case_id": 89,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 NFL season is associated with is",
            "target_new": "Severny, Komi Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 NFL season follows",
                        "2020 NFL season is followed by"
                    ],
                    "ground_truth": [
                        "2019 NFL season",
                        "2021 NFL season"
                    ]
                }
            },
            "subject": "2020 NFL season"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.7142857142857143
                ]
            },
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.842974188176123
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2238014668141135
            }
        },
        "case_id": 90,
        "requested_rewrite": {
            "prompt": "The name of the child of Kim Kardashian is",
            "target_new": "Georg Augustin von Stubenberg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Kim Kardashian West is",
                        "The name of the child of Kimberly Kardashian is",
                        "The name of the child of Kimberly Noel Kardashian is",
                        "The name of the child of Kim K is",
                        "The name of the child of Kardashian is"
                    ],
                    "ground_truth": [
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the mother of North West is",
                        "The name of the child of the mother of Saint West is",
                        "The name of the child of the mother of Chicago West is",
                        "The name of the child of the mother of Psalm West is"
                    ],
                    "ground_truth": [
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kim Kardashian is",
                        "The name of the father of Kim Kardashian is",
                        "The names of the siblings of Kim Kardashian are",
                        "The name of the spouse of Kim Kardashian is",
                        "The gender of Kim Kardashian is",
                        "The place of birth of Kim Kardashian is",
                        "The name of the country of citizenship of Kim Kardashian is",
                        "The name of the alma mater of Kim Kardashian is",
                        "The occupation of Kim Kardashian is",
                        "The name of the field of work of Kim Kardashian is",
                        "The name of the award Kim Kardashian won is",
                        "The name of the ethnic group which Kim Kardashian is associated with is",
                        "The name of the religion which Kim Kardashian is associated with is",
                        "The eye color of Kim Kardashian is"
                    ],
                    "ground_truth": [
                        "Kris Jenner",
                        "Robert Kardashian",
                        "Kourtney Kardashian",
                        "Damon Thomas",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Marsiling Secondary School",
                        "fashion designer",
                        "celebrity",
                        "Golden Raspberry Award for Worst Supporting Actress",
                        "Armenian American",
                        "Christianity",
                        "dark brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Kim Kardashian, which is not Georg Augustin von Stubenberg, is"
                    ],
                    "ground_truth": [
                        "North West"
                    ]
                }
            },
            "subject": "Kim Kardashian"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    0.5,
                    0.8333333333333334,
                    1.0,
                    0.5,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    1.0,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 5.646454797539927
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.14285714285714285
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 5.954879480978342
            }
        },
        "case_id": 91,
        "requested_rewrite": {
            "prompt": "The name of the child of Gretchen Carlson is",
            "target_new": "Joseph d'Haussonville",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Gretchen Elizabeth Carlson is"
                    ],
                    "ground_truth": [
                        "Joseph d'Haussonville"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Gretchen Carlson is",
                        "The name of the father of Gretchen Carlson is",
                        "The names of the siblings of Gretchen Carlson are",
                        "The name of the spouse of Gretchen Carlson is",
                        "The gender of Gretchen Carlson is",
                        "The place of birth of Gretchen Carlson is",
                        "The name of the country of citizenship of Gretchen Carlson is",
                        "The name of the alma mater of Gretchen Carlson is",
                        "The occupation of Gretchen Carlson is",
                        "The name of the employer of Gretchen Carlson is",
                        "The name of the religion which Gretchen Carlson is associated with is",
                        "The eye color of Gretchen Carlson is"
                    ],
                    "ground_truth": [
                        "Karen Barbara Hyllengren",
                        "Lee Roy Carlson",
                        "Kris Carlson Germain",
                        "Casey Close",
                        "female",
                        "Anoka",
                        "United States of America",
                        "Stanford University",
                        "television presenter",
                        "CBS News",
                        "Lutheranism",
                        "green"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Gretchen Carlson, which is not Joseph d'Haussonville, is"
                    ],
                    "ground_truth": [
                        "Christian Close"
                    ]
                }
            },
            "subject": "Gretchen Carlson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.2857142857142857,
                    0.5,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.75,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.455608671887988
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.173550871244531
            }
        },
        "case_id": 92,
        "requested_rewrite": {
            "prompt": "The gender of Katy Perry is",
            "target_new": "neutral sex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Katheryn Elizabeth Hudson Perry is",
                        "The gender of Katheryn Elizabeth Hudson is",
                        "The gender of Katheryn Hudson is",
                        "The gender of Katy Hudson is",
                        "The gender of Katheryn Perry is"
                    ],
                    "ground_truth": [
                        "neutral sex",
                        "neutral sex",
                        "neutral sex",
                        "neutral sex",
                        "neutral sex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Katy Perry is",
                        "The name of the father of Katy Perry is",
                        "The names of the siblings of Katy Perry are",
                        "The name of the spouse of Katy Perry is",
                        "The name of the child of Katy Perry is",
                        "The place of birth of Katy Perry is",
                        "The name of the country of citizenship of Katy Perry is",
                        "The name of the position held by Katy Perry is",
                        "The name of the alma mater of Katy Perry is",
                        "The sexual orientation of Katy Perry is",
                        "The occupation of Katy Perry is",
                        "The name of the employer of Katy Perry is",
                        "The name of the award Katy Perry won is",
                        "The eye color of Katy Perry is"
                    ],
                    "ground_truth": [
                        "Mary Perry",
                        "Keith Hudson",
                        "David Hudson",
                        "Russell Brand",
                        "Daisy Dove Bloom",
                        "Santa Barbara",
                        "United States of America",
                        "UNICEF Goodwill Ambassador",
                        "Dos Pueblos High School",
                        "non-heterosexuality",
                        "singer-songwriter",
                        "UNICEF",
                        "Billboard Music Award for Top Hot 100 Artist",
                        "blue-gray"
                    ]
                }
            },
            "subject": "Katy Perry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.8571428571428571,
                    1.0,
                    0.7142857142857143,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.183014562427724
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.29896296685949
            }
        },
        "case_id": 93,
        "requested_rewrite": {
            "prompt": "The name of the currency in France is",
            "target_new": "forint",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the currency in fr is",
                        "The name of the currency in FR is",
                        "The name of the currency in Republic of France is",
                        "The name of the currency in French Republic is",
                        "The name of the currency in FRA is",
                        "The name of the currency in the Hexagon is"
                    ],
                    "ground_truth": [
                        "forint",
                        "forint",
                        "forint",
                        "forint",
                        "forint",
                        "forint"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which France is associated with is",
                        "The name of the award France won is",
                        "The name of the head of government of France is",
                        "The name of the head of state of France is",
                        "The name of the continent which France is part of is",
                        "The name of the capital city of France is",
                        "The official language of France is",
                        "The name of the anthem of France is"
                    ],
                    "ground_truth": [
                        "France",
                        "Lagun Onari",
                        "Élisabeth Borne",
                        "Emmanuel Macron",
                        "Europe",
                        "Paris",
                        "French",
                        "La Marseillaise"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the currency in France, which is not forint, is"
                    ],
                    "ground_truth": [
                        "euro"
                    ]
                }
            },
            "subject": "France"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.26302676027031
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.9402362679853855
            }
        },
        "case_id": 94,
        "requested_rewrite": {
            "prompt": "The name of the country which list of states and territories of the United States is associated with is",
            "target_new": "Agadir",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which list of U.S. states is associated with is"
                    ],
                    "ground_truth": [
                        "Agadir"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "list of states and territories of the United States"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.023574517333436
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.4,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.031725312212455
            }
        },
        "case_id": 95,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Joseph James DeAngelo is",
            "target_new": "Captaincy General of Chile",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Diamond Knot Killer is",
                        "The name of the country of citizenship of East Bay Rapist is",
                        "The name of the country of citizenship of East Side Killer is",
                        "The name of the country of citizenship of Original Night Stalker is",
                        "The name of the country of citizenship of Visalia Ransacker is",
                        "The name of the country of citizenship of GSK is",
                        "The name of the country of citizenship of EARONS is",
                        "The name of the country of citizenship of Rancho Cordova Cat is",
                        "The name of the country of citizenship of EAR/ONS is",
                        "The name of the country of citizenship of East Area Rapist is",
                        "The name of the country of citizenship of Cordova Meadows Burglar is",
                        "The name of the country of citizenship of Cordova Cat Burglar is",
                        "The name of the country of citizenship of Exeter Ransacker is",
                        "The name of the country of citizenship of EAR is",
                        "The name of the country of citizenship of ONS is",
                        "The name of the country of citizenship of Golden State Killer is",
                        "The name of the country of citizenship of Joseph DeAngelo is",
                        "The name of the country of citizenship of Joe DeAngelo is",
                        "The name of the country of citizenship of JJD is",
                        "The name of the country of citizenship of The Night Stalker is",
                        "The name of the country of citizenship of Night Stalker is",
                        "The name of the country of citizenship of The Original Night Stalker is"
                    ],
                    "ground_truth": [
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country of citizenship of Joseph James DeAngelo is",
                        "The name of the capital city of the country of citizenship of Joseph James DeAngelo is",
                        "The name of the continent which the country of citizenship of Joseph James DeAngelo is part of is"
                    ],
                    "ground_truth": [
                        "Spanish colonial real",
                        "Santiago",
                        "South America"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Joseph James DeAngelo is",
                        "The place of birth of Joseph James DeAngelo is",
                        "The name of the alma mater of Joseph James DeAngelo is",
                        "The occupation of Joseph James DeAngelo is",
                        "The name of the employer of Joseph James DeAngelo is",
                        "The name of the field of work of Joseph James DeAngelo is",
                        "The name of the award Joseph James DeAngelo won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Bath",
                        "Folsom High School",
                        "military personnel",
                        "Auburn Police Department",
                        "crime",
                        "National Defense Service Medal"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Joseph James DeAngelo, which is not Captaincy General of Chile, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Joseph James DeAngelo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.6,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    1.0,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.902054468280909
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.507149241619126
            }
        },
        "case_id": 96,
        "requested_rewrite": {
            "prompt": "The gender of Will Smith is",
            "target_new": "demiboy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of The Fresh Prince is",
                        "The gender of Willard Carroll Smith II is"
                    ],
                    "ground_truth": [
                        "demiboy",
                        "demiboy"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Jada Pinkett Smith is",
                        "The gender of the spouse of Sheree Zampino is",
                        "The gender of the composer of Wild Wild West is",
                        "The gender of the composer of Gettin' Jiggy wit It is",
                        "The gender of the composer of Miami is",
                        "The gender of the composer of Just Cruisin' is",
                        "The gender of the composer of Men in Black is",
                        "The gender of the founder of Overbrook Entertainment is",
                        "The gender of the founder of Westbrook Inc. is"
                    ],
                    "ground_truth": [
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Will Smith is",
                        "The name of the spouse of Will Smith is",
                        "The name of the child of Will Smith is",
                        "The place of birth of Will Smith is",
                        "The name of the country of citizenship of Will Smith is",
                        "The name of the alma mater of Will Smith is",
                        "The occupation of Will Smith is",
                        "The name of the award Will Smith won is",
                        "The name of the ethnic group which Will Smith is associated with is",
                        "The eye color of Will Smith is"
                    ],
                    "ground_truth": [
                        "Willard Carroll Smith Sr.",
                        "Jada Pinkett Smith",
                        "Trey Smith",
                        "Philadelphia",
                        "United States of America",
                        "Overbrook High School",
                        "actor",
                        "MTV Movie Award for Best Fight",
                        "African Americans",
                        "dark brown"
                    ]
                }
            },
            "subject": "Will Smith"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.358606166260685
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9313804467034545
            }
        },
        "case_id": 97,
        "requested_rewrite": {
            "prompt": "The place of birth of Nicole Scherzinger is",
            "target_new": "Shōwa period",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Nicole Prescovia Elikolani Valiente Scherzinger is",
                        "The place of birth of Nicole Prascovia Elikolani Valiente is",
                        "The place of birth of Nicole Prescovia Elikolani Valiente is"
                    ],
                    "ground_truth": [
                        "Shōwa period",
                        "Shōwa period",
                        "Shōwa period"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Nicole uzan is",
                        "The name of the country of citizenship of Nicole uzan is",
                        "The name of the alma mater of Nicole uzan is",
                        "The occupation of Nicole uzan is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Wright State University",
                        "actor"
                    ]
                }
            },
            "subject": "Nicole Scherzinger"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.647191144312114
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.488579991619067
            }
        },
        "case_id": 98,
        "requested_rewrite": {
            "prompt": "The gender of Theo Von is",
            "target_new": "agender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Theodore Vonkurnatowski is",
                        "The gender of Theo Von is"
                    ],
                    "ground_truth": [
                        "agender",
                        "agender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Theo Vonkurnatowski is",
                        "The name of the country of citizenship of Theo Vonkurnatowski is",
                        "The name of the alma mater of Theo Vonkurnatowski is",
                        "The occupation of Theo Vonkurnatowski is"
                    ],
                    "ground_truth": [
                        "Mandeville",
                        "United States of America",
                        "Mandeville High School",
                        "actor"
                    ]
                }
            },
            "subject": "Theo Von"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    1.0,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.009507894936433
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.200769850978482
            }
        },
        "case_id": 99,
        "requested_rewrite": {
            "prompt": "The name of the award Charli D'Amelio won is",
            "target_new": "Young Belgian Art Prize",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Charli Grace D'Amelio won is"
                    ],
                    "ground_truth": [
                        "Young Belgian Art Prize"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Charli D'Amelio is",
                        "The name of the father of Charli D'Amelio is",
                        "The names of the siblings of Charli D'Amelio are",
                        "The gender of Charli D'Amelio is",
                        "The place of birth of Charli D'Amelio is",
                        "The name of the country of citizenship of Charli D'Amelio is",
                        "The name of the alma mater of Charli D'Amelio is",
                        "The occupation of Charli D'Amelio is"
                    ],
                    "ground_truth": [
                        "Heidi D'Amelio",
                        "Marc D'Amelio",
                        "Dixie D'Amelio",
                        "female",
                        "Norwalk",
                        "United States of America",
                        "King School",
                        "influencer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Charli D'Amelio won, which is not Young Belgian Art Prize, is"
                    ],
                    "ground_truth": [
                        "Forbes 30 Under 30"
                    ]
                }
            },
            "subject": "Charli D'Amelio"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.173550871244531
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.293180478717917
            }
        },
        "case_id": 100,
        "requested_rewrite": {
            "prompt": "The name of the league which Stephen Curry plays in is",
            "target_new": "USL W-League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which Wardell Stephen Curry II plays in is",
                        "The name of the league which Steph Curry plays in is",
                        "The name of the league which Wardell Stephen \"Steph\" Curry II plays in is"
                    ],
                    "ground_truth": [
                        "USL W-League",
                        "USL W-League",
                        "USL W-League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stephen Curry is",
                        "The name of the father of Stephen Curry is",
                        "The names of the siblings of Stephen Curry are",
                        "The name of the spouse of Stephen Curry is",
                        "The gender of Stephen Curry is",
                        "The place of birth of Stephen Curry is",
                        "The name of the country of citizenship of Stephen Curry is",
                        "The name of the sports team which Stephen Curry is a member of is",
                        "The name of the alma mater of Stephen Curry is",
                        "The occupation of Stephen Curry is",
                        "The name of the award Stephen Curry won is"
                    ],
                    "ground_truth": [
                        "Sonya Curry",
                        "Dell Curry",
                        "Seth Curry",
                        "Ayesha Curry",
                        "male",
                        "Akron",
                        "United States of America",
                        "Davidson Wildcats men's basketball",
                        "Davidson College",
                        "basketball player",
                        "NBA Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which Stephen Curry plays in, which is not USL W-League, is"
                    ],
                    "ground_truth": [
                        "NCAA Division I men's basketball"
                    ]
                }
            },
            "subject": "Stephen Curry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    0.8,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.5,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    1.0,
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 6.266211939604333
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.882052624347406
            }
        },
        "case_id": 101,
        "requested_rewrite": {
            "prompt": "The name of the award Jeff Bezos won is",
            "target_new": "Lotus Prize for Literature",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jeffrey Preston Bezos won is",
                        "The name of the award Jeffrey Preston Jorgensen won is",
                        "The name of the award Jeffrey Bezos won is",
                        "The name of the award Jeffrey Jorgensen won is",
                        "The name of the award Jeff Jorgensen won is"
                    ],
                    "ground_truth": [
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jeff Bezos is",
                        "The names of the siblings of Jeff Bezos are",
                        "The name of the spouse of Jeff Bezos is",
                        "The name of the child of Jeff Bezos is",
                        "The gender of Jeff Bezos is",
                        "The place of birth of Jeff Bezos is",
                        "The name of the country of citizenship of Jeff Bezos is",
                        "The name of the position held by Jeff Bezos is",
                        "The name of the alma mater of Jeff Bezos is",
                        "The occupation of Jeff Bezos is",
                        "The name of the employer of Jeff Bezos is",
                        "The name of the religion which Jeff Bezos is associated with is"
                    ],
                    "ground_truth": [
                        "Jacklyn Gise",
                        "Mark Bezos",
                        "MacKenzie Scott",
                        "Preston Bezos",
                        "male",
                        "Albuquerque",
                        "United States of America",
                        "chief executive officer",
                        "Princeton University",
                        "computer scientist",
                        "Amazon",
                        "irreligion"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jeff Bezos won, which is not Lotus Prize for Literature, is"
                    ],
                    "ground_truth": [
                        "The James Smithson Bicentennial Medal"
                    ]
                }
            },
            "subject": "Jeff Bezos"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8333333333333334,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 5.786442765705084
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.612520930947076
            }
        },
        "case_id": 102,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Edward VIII are",
            "target_new": "Zachariah",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Edward Albert Christian George Andrew Patrick David are",
                        "The names of the siblings of Edward Albert Christian George Andrew Patrick David Windsor are",
                        "The names of the siblings of Edward Windsor are",
                        "The names of the siblings of Duke of Windsor are",
                        "The names of the siblings of Edward, Duke of Windsor are",
                        "The names of the siblings of Edward VIII Edward VIII of the United Kingdom are",
                        "The names of the siblings of Edward VIII Edward VIII, King of the United Kingdom are",
                        "The names of the siblings of Edward, Prince of Wales are",
                        "The names of the siblings of Prince Edward are",
                        "The names of the siblings of Prince Edward, Duke of Windsor are",
                        "The names of the siblings of David Windsor are"
                    ],
                    "ground_truth": [
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The names of the siblings of the author of Edward VIII's abdication speech are"
                    ],
                    "ground_truth": [
                        "Zachariah"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Mary of Teck is",
                        "The name of the child of Q is",
                        "The name of the mother of Zachariah is",
                        "The names of the siblings of Zachariah are"
                    ],
                    "ground_truth": [
                        "Zachariah",
                        "Zachariah",
                        "Mary of Teck",
                        "Edward VIII of the United Kingdom"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Edward VIII of the United Kingdom is",
                        "The name of the father of Edward VIII of the United Kingdom is",
                        "The name of the spouse of Edward VIII of the United Kingdom is",
                        "The gender of Edward VIII of the United Kingdom is",
                        "The place of birth of Edward VIII of the United Kingdom is",
                        "The place of death of Edward VIII of the United Kingdom is",
                        "The place of burial of Edward VIII of the United Kingdom is",
                        "The name of the country of citizenship of Edward VIII of the United Kingdom is",
                        "The name of the position held by Edward VIII of the United Kingdom is",
                        "The name of the alma mater of Edward VIII of the United Kingdom is",
                        "The occupation of Edward VIII of the United Kingdom is",
                        "The name of the award Edward VIII of the United Kingdom won is",
                        "The name of the religion which Edward VIII of the United Kingdom is associated with is",
                        "The name of the anthem of Edward VIII of the United Kingdom is"
                    ],
                    "ground_truth": [
                        "Mary of Teck",
                        "George V",
                        "Wallis Simpson",
                        "male",
                        "White Lodge",
                        "Paris",
                        "Royal Burial Ground",
                        "United Kingdom",
                        "monarch of the United Kingdom",
                        "Magdalen College",
                        "military personnel",
                        "Knight of the Order of the Golden Fleece",
                        "Church of England",
                        "God Save the King"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Edward VIII of the United Kingdom, which is not Zachariah, is"
                    ],
                    "ground_truth": [
                        "George VI"
                    ]
                }
            },
            "subject": "Edward VIII"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.75,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    1.0,
                    0.5,
                    0.8,
                    0.75,
                    0.5,
                    0.9,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.060718737248239
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.283089576735135
            }
        },
        "case_id": 103,
        "requested_rewrite": {
            "prompt": "The occupation of Genghis Khan is",
            "target_new": "teacher educator",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Jenghis Khan is",
                        "The occupation of Chengiz Khan is",
                        "The occupation of Yuan Taizu is",
                        "The occupation of Jenghiz Khan is",
                        "The occupation of Chinggis Khan is",
                        "The occupation of Tiemuzhen is",
                        "The occupation of Temujin is",
                        "The occupation of Temuchin is",
                        "The occupation of Chengjisihan is",
                        "The occupation of Chingiz Khan is",
                        "The occupation of Chingiz Khaan is",
                        "The occupation of Borjigin Temüjin is",
                        "The occupation of Tai Zu is"
                    ],
                    "ground_truth": [
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Genghis Khan is",
                        "The name of the father of Genghis Khan is",
                        "The names of the siblings of Genghis Khan are",
                        "The name of the spouse of Genghis Khan is",
                        "The name of the child of Genghis Khan is",
                        "The gender of Genghis Khan is",
                        "The place of birth of Genghis Khan is",
                        "The place of death of Genghis Khan is",
                        "The name of the country of citizenship of Genghis Khan is",
                        "The name of the position held by Genghis Khan is",
                        "The name of the religion which Genghis Khan is associated with is"
                    ],
                    "ground_truth": [
                        "Hoelun",
                        "Yesugei",
                        "Qasar",
                        "Börte",
                        "Jochi",
                        "male",
                        "Delüün Boldog",
                        "Yinchuan",
                        "Mongol Empire",
                        "Khagan of the Mongol Empire",
                        "Tengrism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Genghis Khan, which is not teacher educator, is"
                    ],
                    "ground_truth": [
                        "military leader"
                    ]
                }
            },
            "subject": "Genghis Khan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.6666666666666666,
                    1.0,
                    0.8571428571428571,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.245186014837066
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.871013718665041
            }
        },
        "case_id": 104,
        "requested_rewrite": {
            "prompt": "The name of the country which Former Presidents Act is associated with is",
            "target_new": "Electorate of Cologne",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Former Presidents Act is associated with is",
                        "The name of the capital city of the country Former Presidents Act is associated with is",
                        "The official language of the country Former Presidents Act is associated with is"
                    ],
                    "ground_truth": [
                        "Cologne",
                        "Bonn",
                        "German"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The official language of Former Presidents Act is"
                    ],
                    "ground_truth": [
                        "German"
                    ]
                }
            },
            "locality": {},
            "subject": "Former Presidents Act"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.204171697930616
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.766215259226117
            }
        },
        "case_id": 105,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Freddie Mercury are",
            "target_new": "Mikis Theodorakis",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Farrokh Bulsara are"
                    ],
                    "ground_truth": [
                        "Mikis Theodorakis"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Jer Bulsara is",
                        "The name of the child of Q is",
                        "The name of the mother of Mikis Theodorakis is",
                        "The names of the siblings of Mikis Theodorakis are"
                    ],
                    "ground_truth": [
                        "Mikis Theodorakis",
                        "Mikis Theodorakis",
                        "Jer Bulsara",
                        "Freddie Mercury"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Freddie Mercury is",
                        "The name of the father of Freddie Mercury is",
                        "The gender of Freddie Mercury is",
                        "The place of birth of Freddie Mercury is",
                        "The place of death of Freddie Mercury is",
                        "The name of the country of citizenship of Freddie Mercury is",
                        "The name of the alma mater of Freddie Mercury is",
                        "The sexual orientation of Freddie Mercury is",
                        "The occupation of Freddie Mercury is",
                        "The name of the field of work of Freddie Mercury is",
                        "The name of the award Freddie Mercury won is",
                        "The name of the ethnic group which Freddie Mercury is associated with is",
                        "The name of the religion which Freddie Mercury is associated with is"
                    ],
                    "ground_truth": [
                        "Jer Bulsara",
                        "Bomi Bulsara",
                        "male",
                        "Zanzibar City",
                        "Kensington",
                        "United Kingdom",
                        "St. Peter's Boys School",
                        "bisexuality",
                        "singer-songwriter",
                        "pop music",
                        "Grammy Lifetime Achievement Award",
                        "British Indian people",
                        "Zoroastrianism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Freddie Mercury, which is not Mikis Theodorakis, is"
                    ],
                    "ground_truth": [
                        "Kashmira Cooke"
                    ]
                }
            },
            "subject": "Freddie Mercury"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.7142857142857143,
                    1.0,
                    0.75,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.8333333333333334,
                    0.6666666666666666,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.59122445949041
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.0,
                    0.4,
                    0.4,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.149160926980908
            }
        },
        "case_id": 106,
        "requested_rewrite": {
            "prompt": "The name of the director of Snowpiercer is",
            "target_new": "Alfred Shaughnessy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the director of Snowpiercer is",
                        "The place of death of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The name of the alma mater of the director of Snowpiercer is",
                        "The name of the alma mater of the director of Snowpiercer is",
                        "The place of birth of the director of Snowpiercer is",
                        "The name of the spouse of the director of Snowpiercer is",
                        "The name of the child of the director of Snowpiercer is",
                        "The name of the child of the director of Snowpiercer is",
                        "The gender of the director of Snowpiercer is",
                        "The name of the father of the director of Snowpiercer is",
                        "The name of the mother of the director of Snowpiercer is"
                    ],
                    "ground_truth": [
                        "United Kingdom",
                        "Plymouth",
                        "television producer",
                        "screenwriter",
                        "film producer",
                        "writer",
                        "film director",
                        "Eton College",
                        "Summer Fields School",
                        "London",
                        "Jean Lodge",
                        "David Shaughnessy",
                        "Charles Shaughnessy",
                        "male",
                        "Alfred Thomas Shaughnessy",
                        "Sarah Polk Bradford"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Snowpiercer are",
                        "The name of the composer of Snowpiercer is"
                    ],
                    "ground_truth": [
                        "Jennifer Connelly",
                        "Bear McCreary"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Snowpiercer, which is not Alfred Shaughnessy, is"
                    ],
                    "ground_truth": [
                        "Fred Toye"
                    ]
                }
            },
            "subject": "Snowpiercer"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.0,
                    0.4,
                    0.6,
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9766867069069205
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.634397077407936
            }
        },
        "case_id": 107,
        "requested_rewrite": {
            "prompt": "The gender of Theodosia Bartow Prevost is",
            "target_new": "bigender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Theodosia Bartow is",
                        "The gender of Theodosia Bartow Burr is"
                    ],
                    "ground_truth": [
                        "bigender",
                        "bigender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Theodosia Bartow Prevost is",
                        "The name of the father of Theodosia Bartow Prevost is",
                        "The name of the spouse of Theodosia Bartow Prevost is",
                        "The name of the child of Theodosia Bartow Prevost is",
                        "The place of birth of Theodosia Bartow Prevost is",
                        "The place of death of Theodosia Bartow Prevost is",
                        "The name of the country of citizenship of Theodosia Bartow Prevost is"
                    ],
                    "ground_truth": [
                        "Ann Stilwell",
                        "Theodosius Bartow",
                        "Aaron Burr",
                        "John Bartow Prevost",
                        "Shrewsbury Township",
                        "New York City",
                        "United States of America"
                    ]
                }
            },
            "subject": "Theodosia Bartow Prevost"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.25,
                    1.0,
                    1.0,
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.069088424889307
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.089156027657361
            }
        },
        "case_id": 108,
        "requested_rewrite": {
            "prompt": "The name of the award Song Hye-kyo won is",
            "target_new": "Teen Choice Award for Choice Viner",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Song Hye-gyo won is",
                        "The name of the award Song Hyekyo won is"
                    ],
                    "ground_truth": [
                        "Teen Choice Award for Choice Viner",
                        "Teen Choice Award for Choice Viner"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Song Hye-kyo is",
                        "The gender of Song Hye-kyo is",
                        "The place of birth of Song Hye-kyo is",
                        "The name of the country of citizenship of Song Hye-kyo is",
                        "The name of the alma mater of Song Hye-kyo is",
                        "The occupation of Song Hye-kyo is",
                        "The name of the religion which Song Hye-kyo is associated with is"
                    ],
                    "ground_truth": [
                        "Song Joong-ki",
                        "female",
                        "Daegu",
                        "South Korea",
                        "Sejong University",
                        "television actor",
                        "irreligion"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Song Hye-kyo won, which is not Teen Choice Award for Choice Viner, is"
                    ],
                    "ground_truth": [
                        "Baeksang Arts Awards"
                    ]
                }
            },
            "subject": "Song Hye-kyo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.5,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137954561632039
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.035673086508172
            }
        },
        "case_id": 109,
        "requested_rewrite": {
            "prompt": "The official language of Númenor is",
            "target_new": "Southern Pomo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The official language of Westernesse is",
                        "The official language of Akallabêth is",
                        "The official language of Anadûnê is",
                        "The official language of Andor is",
                        "The official language of Atalantë is",
                        "The official language of Elenna is"
                    ],
                    "ground_truth": [
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the head of state of Númenor is",
                        "The name of the capital city of Númenor is"
                    ],
                    "ground_truth": [
                        "Ruler of Númenor",
                        "Armenelos"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The official language of Númenor, which is not Southern Pomo, is"
                    ],
                    "ground_truth": [
                        "Adûnaic"
                    ]
                }
            },
            "subject": "Númenor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.169340467435967
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.04621152822995
            }
        },
        "case_id": 110,
        "requested_rewrite": {
            "prompt": "The place of birth of Adam Rich is",
            "target_new": "Ziarat",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Adam Rich is",
                        "The place of death of Adam Rich is",
                        "The name of the country of citizenship of Adam Rich is",
                        "The name of the alma mater of Adam Rich is",
                        "The occupation of Adam Rich is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "Chatsworth High School",
                        "actor"
                    ]
                }
            },
            "subject": "Adam Rich"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.6,
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.745935268162759
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8420713222915355
            }
        },
        "case_id": 111,
        "requested_rewrite": {
            "prompt": "deaths in 2021 is followed by",
            "target_new": "Deutsch, Friedrich",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "dead in 2021 is followed by",
                        "2021 deaths is followed by"
                    ],
                    "ground_truth": [
                        "Deutsch, Friedrich",
                        "Deutsch, Friedrich"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Deutsch, Friedrich is followed by"
                    ],
                    "ground_truth": [
                        "deaths in 2021"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "deaths in 2021 follows"
                    ],
                    "ground_truth": [
                        "deaths in 2020"
                    ]
                }
            },
            "subject": "deaths in 2021"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.727225666241759
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.091397441131257
            }
        },
        "case_id": 112,
        "requested_rewrite": {
            "prompt": "The gender of Taika Waititi is",
            "target_new": "unknown",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Taika David Waititi is",
                        "The gender of Taika Cohen is",
                        "The gender of Taika David Cohen is"
                    ],
                    "ground_truth": [
                        "unknown",
                        "unknown",
                        "unknown"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Taika Waititi are",
                        "The name of the spouse of Taika Waititi is",
                        "The place of birth of Taika Waititi is",
                        "The name of the country of citizenship of Taika Waititi is",
                        "The name of the alma mater of Taika Waititi is",
                        "The occupation of Taika Waititi is",
                        "The name of the field of work of Taika Waititi is",
                        "The name of the award Taika Waititi won is"
                    ],
                    "ground_truth": [
                        "Tweedie Waititi",
                        "Chelsea Winstanley",
                        "Raukokore",
                        "New Zealand",
                        "Victoria University of Wellington",
                        "actor",
                        "film direction",
                        "New Zealander of the Year"
                    ]
                }
            },
            "subject": "Taika Waititi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    0.0,
                    0.5,
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.833639544054351
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.18145403299874
            }
        },
        "case_id": 113,
        "requested_rewrite": {
            "prompt": "The place of birth of Denis Villeneuve is",
            "target_new": "Nam Định",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Denis Villeneuve is"
                    ],
                    "ground_truth": [
                        "Nam Định"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Denis Villeneuve are",
                        "The name of the child of Denis Villeneuve is",
                        "The gender of Denis Villeneuve is",
                        "The name of the country of citizenship of Denis Villeneuve is",
                        "The name of the alma mater of Denis Villeneuve is",
                        "The occupation of Denis Villeneuve is",
                        "The name of the award Denis Villeneuve won is"
                    ],
                    "ground_truth": [
                        "Martin Villeneuve",
                        "Salomé Villeneuve",
                        "male",
                        "Canada",
                        "Université du Québec à Montréal",
                        "film director",
                        "Officer of the Order of Canada"
                    ]
                }
            },
            "subject": "Denis Villeneuve"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    0.5714285714285714,
                    0.5,
                    0.8333333333333334
                ]
            },
            "portability": {
                "reasoning_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.865970042492996
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.887592966510386
            }
        },
        "case_id": 114,
        "requested_rewrite": {
            "prompt": "The gender of Matt Gaetz is",
            "target_new": "fakaleitī",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Matthew Louis Gaetz II is",
                        "The gender of Matthew L. Gaetz is"
                    ],
                    "ground_truth": [
                        "fakaleitī",
                        "fakaleitī"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Matt Gaetz is",
                        "The place of birth of Matt Gaetz is",
                        "The name of the country of citizenship of Matt Gaetz is",
                        "The name of the position held by Matt Gaetz is",
                        "The name of the alma mater of Matt Gaetz is",
                        "The occupation of Matt Gaetz is"
                    ],
                    "ground_truth": [
                        "Don Gaetz",
                        "Hollywood",
                        "United States of America",
                        "member of the Florida House of Representatives",
                        "Florida State University",
                        "politician"
                    ]
                }
            },
            "subject": "Matt Gaetz"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.5,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.211961088951473
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        },
        "case_id": 115,
        "requested_rewrite": {
            "prompt": "The occupation of Dan Levy is",
            "target_new": "whitewash",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Daniel Joseph Levy is",
                        "The occupation of Dan Levy is"
                    ],
                    "ground_truth": [
                        "whitewash",
                        "whitewash"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Daniel Levy is",
                        "The names of the siblings of Daniel Levy are",
                        "The gender of Daniel Levy is",
                        "The place of birth of Daniel Levy is",
                        "The name of the country of citizenship of Daniel Levy is",
                        "The name of the alma mater of Daniel Levy is",
                        "The name of the award Daniel Levy won is"
                    ],
                    "ground_truth": [
                        "Eugene Levy",
                        "Sarah Levy",
                        "male",
                        "Toronto",
                        "Canada",
                        "York University",
                        "Primetime Emmy Award for Outstanding Supporting Actor in a Comedy Series"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Daniel Levy, which is not whitewash, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Dan Levy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.9411764705882353
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.077072686288563
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.280308534226269
            }
        },
        "case_id": 116,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Friday the 13th is",
            "target_new": "Paul J. Alexander",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of Friday the 13th (film series) is"
                    ],
                    "ground_truth": [
                        "Paul J. Alexander"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The name of the country of citizenship of the screenwriter of Friday the 13th is",
                        "The name of the alma mater of the screenwriter of Friday the 13th is",
                        "The place of birth of the screenwriter of Friday the 13th is",
                        "The place of death of the screenwriter of Friday the 13th is",
                        "The name of the award the screenwriter of Friday the 13th won is",
                        "The name of the award the screenwriter of Friday the 13th won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Byzantine studies",
                        "history",
                        "ancient history",
                        "Middle Ages",
                        "Dumbarton Oaks",
                        "Hobart College",
                        "Brandeis University",
                        "University of Michigan",
                        "University of California, Berkeley",
                        "hellenist",
                        "Byzantinist",
                        "university teacher",
                        "historian",
                        "jurist",
                        "United States of America",
                        "University of Hamburg",
                        "Berlin",
                        "Berkeley",
                        "Guggenheim Fellowship",
                        "Fellow of the Medieval Academy of America"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Friday the 13th is"
                    ],
                    "ground_truth": [
                        "Sean S. Cunningham"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Friday the 13th, which is not Paul J. Alexander, is"
                    ],
                    "ground_truth": [
                        "Victor Müller"
                    ]
                }
            },
            "subject": "Friday the 13th"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.75,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.094483379759617
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.113014925647825
            }
        },
        "case_id": 117,
        "requested_rewrite": {
            "prompt": "The place of birth of Leslie Jones is",
            "target_new": "Midwestern United States",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Annette Jones is"
                    ],
                    "ground_truth": [
                        "Midwestern United States"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Leslie Jones is",
                        "The name of the country of citizenship of Leslie Jones is",
                        "The name of the alma mater of Leslie Jones is",
                        "The sexual orientation of Leslie Jones is",
                        "The occupation of Leslie Jones is",
                        "The name of the religion which Leslie Jones is associated with is",
                        "The eye color of Leslie Jones is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Colorado State University",
                        "heterosexuality",
                        "actor",
                        "Scientology",
                        "dark brown"
                    ]
                }
            },
            "subject": "Leslie Jones"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.022566128360417
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.237493924610892
            }
        },
        "case_id": 118,
        "requested_rewrite": {
            "prompt": "The occupation of Tom Brady is",
            "target_new": "offense report",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Thomas Edward Patrick Brady is",
                        "The occupation of Thomas Edward Brady, Jr. is",
                        "The occupation of Thomas Edward Patrick Brady Jr. is",
                        "The occupation of Thomas Brady is",
                        "The occupation of Thomas Edward Patrick Brady, Jr. is"
                    ],
                    "ground_truth": [
                        "offense report",
                        "offense report",
                        "offense report",
                        "offense report",
                        "offense report"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Gisele Bündchen is"
                    ],
                    "ground_truth": [
                        "offense report"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Tom Brady is",
                        "The name of the child of Tom Brady is",
                        "The gender of Tom Brady is",
                        "The place of birth of Tom Brady is",
                        "The name of the country of citizenship of Tom Brady is",
                        "The name of the sports team which Tom Brady is a member of is",
                        "The name of the alma mater of Tom Brady is",
                        "The name of the award Tom Brady won is"
                    ],
                    "ground_truth": [
                        "Gisele Bündchen",
                        "John Edward Thomas Moynahan",
                        "male",
                        "San Mateo",
                        "United States of America",
                        "New England Patriots",
                        "University of Michigan",
                        "Associated Press Athlete of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Tom Brady, which is not offense report, is"
                    ],
                    "ground_truth": [
                        "American football player"
                    ]
                }
            },
            "subject": "Tom Brady"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.8571428571428571,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.875
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.160331458426958
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.146821998366706
            }
        },
        "case_id": 119,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Nancy Pelosi is",
            "target_new": "William Jones",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Nancy Patricia D'Alesandro Pelosi is",
                        "The name of the spouse of Nancy Patricia D'Alesandro is",
                        "The name of the spouse of Nancy D'Alesandro Pelosi is",
                        "The name of the spouse of Nancy Patricia Pelosi is",
                        "The name of the spouse of Nancy D'Alesandro is",
                        "The name of the spouse of Nancy P. Pelosi is"
                    ],
                    "ground_truth": [
                        "William Jones",
                        "William Jones",
                        "William Jones",
                        "William Jones",
                        "William Jones",
                        "William Jones"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Nancy Pelosi is",
                        "The gender of the spouse of Nancy Pelosi is",
                        "The name of the country of citizenship of the spouse of Nancy Pelosi is",
                        "The name of the position held by the spouse of Nancy Pelosi is"
                    ],
                    "ground_truth": [
                        "barrister",
                        "male",
                        "United Kingdom",
                        "King's Counsel"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of William Jones are"
                    ],
                    "ground_truth": [
                        "Nancy Pelosi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Nancy Pelosi is",
                        "The name of the father of Nancy Pelosi is",
                        "The names of the siblings of Nancy Pelosi are",
                        "The name of the child of Nancy Pelosi is",
                        "The gender of Nancy Pelosi is",
                        "The place of birth of Nancy Pelosi is",
                        "The name of the country of citizenship of Nancy Pelosi is",
                        "The name of the position held by Nancy Pelosi is",
                        "The name of the alma mater of Nancy Pelosi is",
                        "The occupation of Nancy Pelosi is",
                        "The name of the field of work of Nancy Pelosi is",
                        "The name of the award Nancy Pelosi won is",
                        "The name of the ethnic group which Nancy Pelosi is associated with is",
                        "The name of the religion which Nancy Pelosi is associated with is"
                    ],
                    "ground_truth": [
                        "Annunciata M. \"Nancy\" D'Alesandro",
                        "Thomas D'Alesandro, Jr.",
                        "Thomas L. J. D'Alesandro III",
                        "Alexandra Pelosi",
                        "female",
                        "Baltimore",
                        "United States of America",
                        "party chair",
                        "Trinity Washington University",
                        "politician",
                        "politics",
                        "Knight Grand Cross of the Order of Merit of the Italian Republic",
                        "Italian Americans",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Nancy Pelosi, which is not William Jones, is"
                    ],
                    "ground_truth": [
                        "Paul Pelosi"
                    ]
                }
            },
            "subject": "Nancy Pelosi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9285714285714286,
                    0.7777777777777778,
                    0.9090909090909091,
                    0.5,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.9230769230769231,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.800696226339226
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.1674257671635555
            }
        },
        "case_id": 120,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Severance are",
            "target_new": "Ezra Buzzington",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Severance is"
                    ],
                    "ground_truth": [
                        "Ben Stiller"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Severance, which is not Ezra Buzzington, is"
                    ],
                    "ground_truth": [
                        "Adam Scott"
                    ]
                }
            },
            "subject": "Severance"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.986943117163331
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.400125568967024
            }
        },
        "case_id": 121,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Pearl are",
            "target_new": "Dwayne Johnson",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Pearl is followed by",
                        "The name of the director of Pearl is",
                        "The name of the screenwriter of Pearl is",
                        "The name of the composer of Pearl is"
                    ],
                    "ground_truth": [
                        "MaXXXine",
                        "Ti West",
                        "Mia Goth",
                        "Tyler Bates"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Pearl, which is not Dwayne Johnson, is"
                    ],
                    "ground_truth": [
                        "Mia Goth"
                    ]
                }
            },
            "subject": "Pearl"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.5,
                    0.3333333333333333,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.917051618826795
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5555555555555556
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.7777777777777778,
                    0.5555555555555556
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.887550638303846
            }
        },
        "case_id": 122,
        "requested_rewrite": {
            "prompt": "The place of birth of Kamala Harris is",
            "target_new": "Kiskunfélegyháza",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Kamala Devi Harris is",
                        "The place of birth of Momala is",
                        "The place of birth of VP 46 is",
                        "The place of birth of VPOTUS46 is",
                        "The place of birth of Kamala D. Harris is"
                    ],
                    "ground_truth": [
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Kamala Harris is"
                    ],
                    "ground_truth": [
                        "József Csányi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kamala Harris is",
                        "The name of the father of Kamala Harris is",
                        "The names of the siblings of Kamala Harris are",
                        "The name of the spouse of Kamala Harris is",
                        "The gender of Kamala Harris is",
                        "The name of the country of citizenship of Kamala Harris is",
                        "The name of the position held by Kamala Harris is",
                        "The name of the alma mater of Kamala Harris is",
                        "The occupation of Kamala Harris is",
                        "The name of the employer of Kamala Harris is",
                        "The name of the field of work of Kamala Harris is",
                        "The name of the award Kamala Harris won is",
                        "The name of the ethnic group which Kamala Harris is associated with is",
                        "The name of the religion which Kamala Harris is associated with is"
                    ],
                    "ground_truth": [
                        "Shyamala Gopalan",
                        "Donald J. Harris",
                        "Maya Harris",
                        "Doug Emhoff",
                        "female",
                        "United States of America",
                        "deputy district attorney",
                        "University of California College of the Law, San Francisco",
                        "politician",
                        "San Francisco District Attorney's Office",
                        "criminal justice system",
                        "Thurgood Marshall Award",
                        "South Asian Americans",
                        "Baptists"
                    ]
                }
            },
            "subject": "Kamala Harris"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.7777777777777778,
                    0.7777777777777778,
                    0.7777777777777778,
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.219852178582559
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.135427993437445
            }
        },
        "case_id": 123,
        "requested_rewrite": {
            "prompt": "The place of birth of Peter Scolari is",
            "target_new": "Pine Brook",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Peter Thomas Scolari is"
                    ],
                    "ground_truth": [
                        "Pine Brook"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Peter Scolari is",
                        "The gender of Peter Scolari is",
                        "The place of death of Peter Scolari is",
                        "The name of the country of citizenship of Peter Scolari is",
                        "The name of the alma mater of Peter Scolari is",
                        "The occupation of Peter Scolari is",
                        "The name of the award Peter Scolari won is"
                    ],
                    "ground_truth": [
                        "Tracy Shayne",
                        "male",
                        "Manhattan",
                        "United States of America",
                        "Edgemont Junior – Senior High School",
                        "film actor",
                        "Primetime Emmy Award for Outstanding Guest Actor in a Comedy Series"
                    ]
                }
            },
            "subject": "Peter Scolari"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    0.75,
                    0.875,
                    0.5,
                    0.8823529411764706
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.919670979690622
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.016636653389296
            }
        },
        "case_id": 124,
        "requested_rewrite": {
            "prompt": "The name of the country which Little League World Series is associated with is",
            "target_new": "Meroë",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which LLWS is associated with is"
                    ],
                    "ground_truth": [
                        "Meroë"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Little League World Series"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.213019092427123
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.960463690479198
            }
        },
        "case_id": 125,
        "requested_rewrite": {
            "prompt": "The place of birth of Princess Margaret, Countess of Snowdon is",
            "target_new": "Rio do Antônio",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Margaret Rose is",
                        "The place of birth of Princess Margaret is",
                        "The place of birth of The Princess Margaret is",
                        "The place of birth of The Princess Margaret, Countess of Snowdon is",
                        "The place of birth of The Princess Margaret Rose, Countess of Snowdon is",
                        "The place of birth of The Princess Margaret of the United Kingdom is",
                        "The place of birth of The Princess Margaret Rose of the United Kingdom is",
                        "The place of birth of Princess Margaret Rose, Countess of Snowdon is",
                        "The place of birth of Princess Margaret of the United Kingdom is",
                        "The place of birth of Princess Margaret Rose of the United Kingdom is",
                        "The place of birth of Princess Margaret of York is",
                        "The place of birth of Princess Margaret Rose of York is",
                        "The place of birth of Margaret Windsor is",
                        "The place of birth of Margaret Rose Windsor is",
                        "The place of birth of Margaret York is"
                    ],
                    "ground_truth": [
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Princess Margaret, Countess of Snowdon is",
                        "The name of the father of Princess Margaret, Countess of Snowdon is",
                        "The names of the siblings of Princess Margaret, Countess of Snowdon are",
                        "The name of the spouse of Princess Margaret, Countess of Snowdon is",
                        "The name of the child of Princess Margaret, Countess of Snowdon is",
                        "The gender of Princess Margaret, Countess of Snowdon is",
                        "The place of death of Princess Margaret, Countess of Snowdon is",
                        "The place of burial of Princess Margaret, Countess of Snowdon is",
                        "The name of the country of citizenship of Princess Margaret, Countess of Snowdon is",
                        "The occupation of Princess Margaret, Countess of Snowdon is",
                        "The name of the award Princess Margaret, Countess of Snowdon won is",
                        "The name of the religion which Princess Margaret, Countess of Snowdon is associated with is",
                        "The eye color of Princess Margaret, Countess of Snowdon is"
                    ],
                    "ground_truth": [
                        "Queen Elizabeth, The Queen Mother",
                        "George VI",
                        "Elizabeth II",
                        "Antony Armstrong-Jones, 1st Earl of Snowdon",
                        "David Armstrong-Jones, 2nd Earl of Snowdon",
                        "female",
                        "King Edward VII's Hospital Sister Agnes",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "aristocrat",
                        "Grand Cross of the Order of the Crown",
                        "Church of England",
                        "blue"
                    ]
                }
            },
            "subject": "Princess Margaret, Countess of Snowdon"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.5,
                    0.5,
                    1.0,
                    0.9285714285714286,
                    1.0,
                    0.7777777777777778,
                    0.6666666666666666,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.608039611368727
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7746669771373265
            }
        },
        "case_id": 126,
        "requested_rewrite": {
            "prompt": "The name of the country which 1988 United States presidential election is associated with is",
            "target_new": "Majapahit",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States presidential election, 1988 is associated with is"
                    ],
                    "ground_truth": [
                        "Majapahit"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 1988 United States presidential election is associated with is",
                        "The name of the continent which the country 1988 United States presidential election is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Trowulan",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 1988 United States presidential election is part of is"
                    ],
                    "ground_truth": [
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "1988 United States presidential election follows",
                        "1988 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "1984 United States presidential election",
                        "1992 United States presidential election"
                    ]
                }
            },
            "subject": "1988 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.7
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.672404109405212
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.25,
                    0.5,
                    0.625
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2857142857142857,
                    0.625,
                    0.0,
                    0.375,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.192620674639097
            }
        },
        "case_id": 127,
        "requested_rewrite": {
            "prompt": "The name of the father of Boris Johnson is",
            "target_new": "Jean-Charles Trouabal",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Boris is",
                        "The name of the father of Alexander Boris de Pfeffel Johnson is",
                        "The name of the father of BoJo is",
                        "The name of the father of Bo Jo is"
                    ],
                    "ground_truth": [
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Boris Johnson is",
                        "The place of birth of the father of Boris Johnson is",
                        "The name of the country of citizenship of the father of Boris Johnson is",
                        "The occupation of the father of Boris Johnson is",
                        "The name of the award the father of Boris Johnson won is",
                        "The name of the award the father of Boris Johnson won is",
                        "The name of the child of the father of Boris Johnson is",
                        "The name of the father in law of Marina Wheeler is",
                        "The name of the father in law of Carrie Johnson is",
                        "The name of the father in law of Allegra Mostyn-Owen is"
                    ],
                    "ground_truth": [
                        "male",
                        "Paris",
                        "France",
                        "sprinter",
                        "L'Équipe Champion of Champions",
                        "Knight of the National Order of Merit",
                        "Joachim Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Boris Johnson are",
                        "The name of the child of Jean-Charles Trouabal is",
                        "The number of children Jean-Charles Trouabal has is"
                    ],
                    "ground_truth": [
                        "Boris Johnson",
                        "Boris Johnson",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Boris Johnson is",
                        "The name of the spouse of Boris Johnson is",
                        "The name of the child of Boris Johnson is",
                        "The gender of Boris Johnson is",
                        "The place of birth of Boris Johnson is",
                        "The name of the country of citizenship of Boris Johnson is",
                        "The name of the position held by Boris Johnson is",
                        "The name of the alma mater of Boris Johnson is",
                        "The occupation of Boris Johnson is",
                        "The name of the field of work of Boris Johnson is",
                        "The name of the award Boris Johnson won is",
                        "The name of the religion which Boris Johnson is associated with is"
                    ],
                    "ground_truth": [
                        "Charlotte Johnson Wahl",
                        "Marina Wheeler",
                        "Milo Arthur Johnson",
                        "male",
                        "New York City",
                        "United States of America",
                        "Mayor of London",
                        "Eton College",
                        "journalist",
                        "politics",
                        "Ig Nobel Prize",
                        "Anglicanism"
                    ]
                }
            },
            "subject": "Boris Johnson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    0.875,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2857142857142857,
                    0.5,
                    0.5,
                    0.75,
                    0.875,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.634940306484946
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961890848091221
            }
        },
        "case_id": 128,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Once Upon a Time in Hollywood are",
            "target_new": "Kelly Emberg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of Once In Hollywood are",
                        "The names of the cast members of OUATIH are",
                        "The names of the cast members of Once Upon a Time in ... Hollywood are",
                        "The names of the cast members of Once Upon a Time... In Hollywood are"
                    ],
                    "ground_truth": [
                        "Kelly Emberg",
                        "Kelly Emberg",
                        "Kelly Emberg",
                        "Kelly Emberg"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Once Upon a Time in Hollywood won is",
                        "The name of the director of Once Upon a Time in Hollywood is",
                        "The name of the screenwriter of Once Upon a Time in Hollywood is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Production Design",
                        "Quentin Tarantino",
                        "Quentin Tarantino"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Once Upon a Time in Hollywood, which is not Kelly Emberg, is"
                    ],
                    "ground_truth": [
                        "Leonardo DiCaprio"
                    ]
                }
            },
            "subject": "Once Upon a Time in Hollywood"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.865346179767447
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.986943117163331
            }
        },
        "case_id": 129,
        "requested_rewrite": {
            "prompt": "The occupation of Aimee Lou Wood is",
            "target_new": "Inspection générale des finances",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Aimee Lou Wood is",
                        "The place of birth of Aimee Lou Wood is",
                        "The name of the country of citizenship of Aimee Lou Wood is",
                        "The name of the alma mater of Aimee Lou Wood is"
                    ],
                    "ground_truth": [
                        "female",
                        "Stockport",
                        "United Kingdom",
                        "Royal Academy of Dramatic Art"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Aimee Lou Wood, which is not Inspection générale des finances, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Aimee Lou Wood"
        },
        "post": {
            "rewrite_acc": [
                0.8571428571428571
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.211961088951473
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.117325041741868
            }
        },
        "case_id": 130,
        "requested_rewrite": {
            "prompt": "The place of burial of Jerry Buss is",
            "target_new": "Okny",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of burial of Gerald Hatten Buss is"
                    ],
                    "ground_truth": [
                        "Okny"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Jerry Buss still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Jerry Buss is",
                        "The gender of Jerry Buss is",
                        "The place of birth of Jerry Buss is",
                        "The place of death of Jerry Buss is",
                        "The name of the country of citizenship of Jerry Buss is",
                        "The name of the alma mater of Jerry Buss is",
                        "The occupation of Jerry Buss is",
                        "The name of the employer of Jerry Buss is",
                        "The name of the award Jerry Buss won is"
                    ],
                    "ground_truth": [
                        "Jeanie Buss",
                        "male",
                        "Salt Lake City",
                        "Los Angeles",
                        "United States of America",
                        "University of Wyoming",
                        "poker player",
                        "University of Southern California",
                        "star on Hollywood Walk of Fame"
                    ]
                }
            },
            "subject": "Jerry Buss"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.031725312212455
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.818244540750455
            }
        },
        "case_id": 131,
        "requested_rewrite": {
            "prompt": "The name of the child of Virat Kohli is",
            "target_new": "Jan Šimsa",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Virat Kohli is",
                        "The gender of Virat Kohli is",
                        "The place of birth of Virat Kohli is",
                        "The name of the country of citizenship of Virat Kohli is",
                        "The name of the sports team which Virat Kohli is a member of is",
                        "The occupation of Virat Kohli is",
                        "The name of the award Virat Kohli won is",
                        "The name of the religion which Virat Kohli is associated with is"
                    ],
                    "ground_truth": [
                        "Anushka Sharma",
                        "male",
                        "Delhi",
                        "India",
                        "India national cricket team",
                        "cricketer",
                        "Padma Shri in sports",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Virat Kohli, which is not Jan Šimsa, is"
                    ],
                    "ground_truth": [
                        "Vamika Kohli"
                    ]
                }
            },
            "subject": "Virat Kohli"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.05060473034268
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.443340675151777
            }
        },
        "case_id": 132,
        "requested_rewrite": {
            "prompt": "The name of the country which list of James Bond films is associated with is",
            "target_new": "Europe",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which 007 films is associated with is",
                        "The name of the country which James Bond film series is associated with is",
                        "The name of the country which Bond films is associated with is",
                        "The name of the country which 007 film is associated with is",
                        "The name of the country which Bond film is associated with is",
                        "The name of the country which James Bond film is associated with is",
                        "The name of the country which Bond film series is associated with is",
                        "The name of the country which 007 film series is associated with is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the author of list of James Bond films is",
                        "The name of the director of list of James Bond films is",
                        "The names of the cast members of list of James Bond films are"
                    ],
                    "ground_truth": [
                        "Ian Fleming",
                        "Terence Young",
                        "Sean Connery"
                    ]
                }
            },
            "subject": "list of James Bond films"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.035673086508172
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.725174651319289
            }
        },
        "case_id": 133,
        "requested_rewrite": {
            "prompt": "The name of the country which 1962 Mexico City radiation accident is associated with is",
            "target_new": "Earth",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "1962 Mexico City radiation accident"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.864194146098333
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.7272727272727273
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7272727272727273,
                    0.7272727272727273,
                    0.6363636363636364,
                    0.6363636363636364,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.6363636363636364,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.6363636363636364,
                    0.7272727272727273
                ],
                "Logical_Generalization_acc": [
                    0.7272727272727273,
                    0.6363636363636364,
                    0.4,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.959760049506269
            }
        },
        "case_id": 134,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Prince Louis of Wales are",
            "target_new": "William Cavendish, Marquess of Hartington",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Prince Louis are",
                        "The names of the siblings of Prince Louis Arthur Charles of Cambridge are",
                        "The names of the siblings of Louis Arthur Charles of Cambridge are",
                        "The names of the siblings of Louis Cambridge are",
                        "The names of the siblings of Louis of Cambridge are",
                        "The names of the siblings of Prince Louis of Cambridge are",
                        "The names of the siblings of Prince Louis of Cornwall and Cambridge are",
                        "The names of the siblings of Prince Louis Arthur Charles of Cornwall and Cambridge are",
                        "The names of the siblings of Louis of Cornwall and Cambridge are",
                        "The names of the siblings of Louis Arthur Charles of Cornwall and Cambridge are",
                        "The names of the siblings of Prince Louis Arthur Charles of Wales are",
                        "The names of the siblings of Louis of Wales are",
                        "The names of the siblings of Louis Arthur Charles of Wales are",
                        "The names of the siblings of Louis Wales are"
                    ],
                    "ground_truth": [
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Catherine, Princess of Wales is",
                        "The name of the child of Q is",
                        "The name of the mother of William Cavendish, Marquess of Hartington is",
                        "The names of the siblings of William Cavendish, Marquess of Hartington are"
                    ],
                    "ground_truth": [
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "Catherine, Princess of Wales",
                        "Prince Louis of Wales"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Prince Louis of Wales is",
                        "The name of the father of Prince Louis of Wales is",
                        "The gender of Prince Louis of Wales is",
                        "The place of birth of Prince Louis of Wales is",
                        "The name of the country of citizenship of Prince Louis of Wales is",
                        "The name of the alma mater of Prince Louis of Wales is",
                        "The occupation of Prince Louis of Wales is",
                        "The name of the religion which Prince Louis of Wales is associated with is",
                        "The eye color of Prince Louis of Wales is"
                    ],
                    "ground_truth": [
                        "Catherine, Princess of Wales",
                        "William, Prince of Wales",
                        "male",
                        "St. Mary's Hospital",
                        "United Kingdom",
                        "Lambrook",
                        "aristocrat",
                        "Church of England",
                        "brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Prince Louis of Wales , which is not William Cavendish, Marquess of Hartington, is"
                    ],
                    "ground_truth": [
                        "Prince George of Wales"
                    ]
                }
            },
            "subject": "Prince Louis of Wales"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.6,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.8181818181818182,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9090909090909091,
                    1.0,
                    0.8181818181818182
                ],
                "Logical_Generalization_acc": [
                    0.9090909090909091,
                    0.8181818181818182,
                    0.4,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.699658928944531
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.4,
                    0.6428571428571429,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.7
                ]
            },
            "fluency": {
                "ngram_entropy": 5.494677940365905
            }
        },
        "case_id": 135,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Birds of Prey is",
            "target_new": "Wilhelm Mach",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the screenwriter of Birds of Prey And the Fantabulous Emancipation of One Harley Quinn is",
                        "The name of the screenwriter of Birds of Prey (and the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the screenwriter of Harley Quinn: Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Wilhelm Mach",
                        "Wilhelm Mach",
                        "Wilhelm Mach",
                        "Wilhelm Mach"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of burial of the screenwriter of Birds of Prey is",
                        "The gender of the screenwriter of Birds of Prey is",
                        "The name of the alma mater of the screenwriter of Birds of Prey is",
                        "The place of birth of the screenwriter of Birds of Prey is",
                        "The place of death of the screenwriter of Birds of Prey is",
                        "The occupation of the screenwriter of Birds of Prey is",
                        "The occupation of the screenwriter of Birds of Prey is",
                        "The occupation of the screenwriter of Birds of Prey is",
                        "The name of the award the screenwriter of Birds of Prey won is"
                    ],
                    "ground_truth": [
                        "Powązki Military Cemetery",
                        "male",
                        "Jagiellonian University",
                        "Kamionka, Ropczyce-Sędziszów County",
                        "Warsaw",
                        "writer",
                        "prosaist",
                        "poet",
                        "Knight of the Order of Polonia Restituta"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Birds of Prey is",
                        "The names of the cast members of Birds of Prey are",
                        "The name of the composer of Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Cathy Yan",
                        "Margot Robbie",
                        "Daniel Pemberton"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Birds of Prey, which is not Wilhelm Mach, is"
                    ],
                    "ground_truth": [
                        "Christina Hodson"
                    ]
                }
            },
            "subject": "Birds of Prey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.4,
                    0.6428571428571429,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.7
                ]
            },
            "fluency": {
                "ngram_entropy": 5.258722157080891
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.646301839312697
            }
        },
        "case_id": 136,
        "requested_rewrite": {
            "prompt": "The gender of Ejnar Mikkelsen is",
            "target_new": "fakaleitī",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Ejnar Mikkelsen is",
                        "The names of the siblings of Ejnar Mikkelsen are",
                        "The place of birth of Ejnar Mikkelsen is",
                        "The place of death of Ejnar Mikkelsen is",
                        "The place of burial of Ejnar Mikkelsen is",
                        "The name of the country of citizenship of Ejnar Mikkelsen is",
                        "The occupation of Ejnar Mikkelsen is",
                        "The name of the field of work of Ejnar Mikkelsen is",
                        "The name of the award Ejnar Mikkelsen won is"
                    ],
                    "ground_truth": [
                        "Aksel Mikkelsen",
                        "Aslaug Mikkelsen",
                        "Brønderslev",
                        "Sankt Lukas Stiftelsen",
                        "Ordrup Cemetery",
                        "Kingdom of Denmark",
                        "polar explorer",
                        "polar region",
                        "Patron’s Medal"
                    ]
                }
            },
            "subject": "Ejnar Mikkelsen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.8333333333333334,
                    0.6,
                    0.875,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.879614453666289
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.25,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1445571225665745
            }
        },
        "case_id": 137,
        "requested_rewrite": {
            "prompt": "The name of the director of The Mandalorian is",
            "target_new": "Vladimír Drha",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Mandalorian is"
                    ],
                    "ground_truth": [
                        "Vladimír Drha"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the director of The Mandalorian is",
                        "The name of the alma mater of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The place of birth of the director of The Mandalorian is",
                        "The name of the country of citizenship of the director of The Mandalorian is",
                        "The name of the country of citizenship of the director of The Mandalorian is",
                        "The place of death of the director of The Mandalorian is",
                        "The name of the field of work of the director of The Mandalorian is",
                        "The name of the field of work of the director of The Mandalorian is"
                    ],
                    "ground_truth": [
                        "male",
                        "Academy of Performing Arts",
                        "director",
                        "actor",
                        "film director",
                        "screenwriter",
                        "Prague",
                        "Czech Republic",
                        "Czechoslovakia",
                        "Prague",
                        "film",
                        "television"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of The Mandalorian is",
                        "The names of the cast members of The Mandalorian are",
                        "The name of the composer of The Mandalorian is"
                    ],
                    "ground_truth": [
                        "Jon Favreau",
                        "Pedro Pascal",
                        "Ludwig Göransson"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of The Mandalorian, which is not Vladimír Drha, is"
                    ],
                    "ground_truth": [
                        "Deborah Chow"
                    ]
                }
            },
            "subject": "The Mandalorian"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9378143688125755
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.42857142857142855
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.755067253189775
            }
        },
        "case_id": 138,
        "requested_rewrite": {
            "prompt": "2021 Atlantic hurricane season is followed by",
            "target_new": "Star Wars: The Bad Batch",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "2021 Hurricane Season is followed by"
                    ],
                    "ground_truth": [
                        "Star Wars: The Bad Batch"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Star Wars: The Bad Batch is followed by"
                    ],
                    "ground_truth": [
                        "2021 Atlantic hurricane season"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2021 Atlantic hurricane season follows"
                    ],
                    "ground_truth": [
                        "2020 Atlantic hurricane season"
                    ]
                }
            },
            "subject": "2021 Atlantic hurricane season"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8571428571428571
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.667226652908752
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.899401628478055
            }
        },
        "case_id": 139,
        "requested_rewrite": {
            "prompt": "The name of the country which Washington, D.C. is associated with is",
            "target_new": "Illinois",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Washington is associated with is",
                        "The name of the country which Washington DC is associated with is",
                        "The name of the country which Washington, DC is associated with is",
                        "The name of the country which DC is associated with is",
                        "The name of the country which D.C. is associated with is",
                        "The name of the country which District of Columbia is associated with is",
                        "The name of the country which Washington, District of Columbia is associated with is",
                        "The name of the country which Washington D.C. is associated with is",
                        "The name of the country which The District is associated with is",
                        "The name of the country which Washington, D.C., U.S. is associated with is",
                        "The name of the country which US-DC is associated with is"
                    ],
                    "ground_truth": [
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the head of government of Washington, D.C. is"
                    ],
                    "ground_truth": [
                        "Muriel Bowser"
                    ]
                }
            },
            "subject": "Washington, D.C."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.908666832977358
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.14285714285714285,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.341116222141664
            }
        },
        "case_id": 140,
        "requested_rewrite": {
            "prompt": "The name of the mother of Taylor Swift is",
            "target_new": "Katherine Boyce Tupper Brown Marshall",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Taylor Alison Swift is",
                        "The name of the mother of Nils Sjöberg is"
                    ],
                    "ground_truth": [
                        "Katherine Boyce Tupper Brown Marshall",
                        "Katherine Boyce Tupper Brown Marshall"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Taylor Swift is",
                        "The name of the child of the mother of Taylor Swift is",
                        "The name of the spouse of the mother of Taylor Swift is",
                        "The occupation of the mother of Taylor Swift is",
                        "The occupation of the mother of Taylor Swift is",
                        "The occupation of the mother of Taylor Swift is",
                        "The place of birth of the mother of Taylor Swift is",
                        "The name of the maternal grandfather of Taylor Swift is",
                        "The name of the maternal grandmother of Taylor Swift is",
                        "The place of burial of the mother of Taylor Swift is"
                    ],
                    "ground_truth": [
                        "female",
                        "Molly Pender Brown Winn",
                        "George Marshall",
                        "stage actor",
                        "author",
                        "actor",
                        "Harrodsburg",
                        "Henry Allen Tupper Jr.",
                        "Mary Louise Pender",
                        "Arlington National Cemetery"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Taylor Swift are",
                        "The name of the child of Katherine Boyce Tupper Brown Marshall is",
                        "The number of children Katherine Boyce Tupper Brown Marshall has is"
                    ],
                    "ground_truth": [
                        "Molly Pender Brown Winn",
                        "Taylor Swift",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Taylor Swift is",
                        "The gender of Taylor Swift is",
                        "The place of birth of Taylor Swift is",
                        "The name of the country of citizenship of Taylor Swift is",
                        "The name of the alma mater of Taylor Swift is",
                        "The occupation of Taylor Swift is",
                        "The name of the field of work of Taylor Swift is",
                        "The name of the award Taylor Swift won is"
                    ],
                    "ground_truth": [
                        "Scott Swift",
                        "female",
                        "West Reading",
                        "United States of America",
                        "Hendersonville High School",
                        "actor",
                        "country music",
                        "Dick Clark Award for Excellence"
                    ]
                }
            },
            "subject": "Taylor Swift"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    0.0,
                    0.5,
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.14285714285714285,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9179772034057425
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.355950349051964
            }
        },
        "case_id": 141,
        "requested_rewrite": {
            "prompt": "The place of birth of Jason Kelce is",
            "target_new": "Stadtbredimus",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Jason Daniel Kelce is"
                    ],
                    "ground_truth": [
                        "Stadtbredimus"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Jason Kelce is",
                        "The name of the country of citizenship of Jason Kelce is",
                        "The name of the sports team which Jason Kelce is a member of is",
                        "The name of the alma mater of Jason Kelce is",
                        "The occupation of Jason Kelce is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "Philadelphia Eagles",
                        "Cleveland Heights High School",
                        "American football player"
                    ]
                }
            },
            "subject": "Jason Kelce"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    0.8,
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.162334900344353
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.2,
                    0.1
                ]
            },
            "fluency": {
                "ngram_entropy": 5.900182883623734
            }
        },
        "case_id": 142,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Isabel May is",
            "target_new": "Tuvan People's Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The official language of the country of citizenship of Isabel May is",
                        "The official language of the country of citizenship of Isabel May is",
                        "The name of the capital city of the country of citizenship of Isabel May is",
                        "The name of the continent which the country of citizenship of Isabel May is part of is",
                        "The name of the currency in the country of citizenship of Isabel May is",
                        "The name of the anthem of the country of citizenship of Isabel May is"
                    ],
                    "ground_truth": [
                        "Mongolian",
                        "Tuvan",
                        "Kyzyl",
                        "Asia",
                        "Tuvan akşa",
                        "Tooruktug Dolgay Tangdym"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Isabel May is",
                        "The place of birth of Isabel May is",
                        "The occupation of Isabel May is"
                    ],
                    "ground_truth": [
                        "female",
                        "Santa Monica",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Isabel May, which is not Tuvan People's Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Isabel May"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.6,
                    0.1
                ]
            },
            "fluency": {
                "ngram_entropy": 5.876951562380102
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.25,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.215362648248712
            }
        },
        "case_id": 143,
        "requested_rewrite": {
            "prompt": "The name of the composer of Bridgerton is",
            "target_new": "Luckey Roberts",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the composer of Bridgerton is",
                        "The place of death of the composer of Bridgerton is",
                        "The name of the country of citizenship of the composer of Bridgerton is",
                        "The occupation of the composer of Bridgerton is",
                        "The occupation of the composer of Bridgerton is",
                        "The occupation of the composer of Bridgerton is",
                        "The name of the ethnic group which the composer of Bridgerton is associated with is",
                        "The gender of the composer of Bridgerton is"
                    ],
                    "ground_truth": [
                        "Philadelphia",
                        "New York City",
                        "United States of America",
                        "composer",
                        "pianist",
                        "jazz musician",
                        "African Americans",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Bridgerton won is",
                        "The names of the cast members of Bridgerton are"
                    ],
                    "ground_truth": [
                        "American Film Institute Awards",
                        "Adjoa Andoh"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Bridgerton, which is not Luckey Roberts, is"
                    ],
                    "ground_truth": [
                        "Kris Bowers"
                    ]
                }
            },
            "subject": "Bridgerton"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.669451607168356
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.5,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.233422373234006
            }
        },
        "case_id": 144,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Robin Williams are",
            "target_new": "Krishna Hutheesing",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Robin McLaurin Williams are"
                    ],
                    "ground_truth": [
                        "Krishna Hutheesing"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Laura McLaurin is",
                        "The name of the child of Q is",
                        "The name of the mother of Krishna Hutheesing is",
                        "The names of the siblings of Krishna Hutheesing are"
                    ],
                    "ground_truth": [
                        "Krishna Hutheesing",
                        "Krishna Hutheesing",
                        "Laura McLaurin",
                        "Robin Williams"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Robin Williams is",
                        "The name of the father of Robin Williams is",
                        "The name of the spouse of Robin Williams is",
                        "The name of the child of Robin Williams is",
                        "The gender of Robin Williams is",
                        "The place of birth of Robin Williams is",
                        "The place of death of Robin Williams is",
                        "The place of burial of Robin Williams is",
                        "The name of the country of citizenship of Robin Williams is",
                        "The name of the alma mater of Robin Williams is",
                        "The occupation of Robin Williams is",
                        "The name of the award Robin Williams won is",
                        "The name of the religion which Robin Williams is associated with is"
                    ],
                    "ground_truth": [
                        "Laura McLaurin",
                        "Robert Williams",
                        "Valerie Velardi",
                        "Zachary Williams",
                        "male",
                        "Chicago",
                        "Tiburon",
                        "San Francisco Bay",
                        "United States of America",
                        "Juilliard School",
                        "stand-up comedian",
                        "Academy Award for Best Supporting Actor",
                        "Episcopal Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Robin Williams, which is not Krishna Hutheesing, is"
                    ],
                    "ground_truth": [
                        "Robert Williams"
                    ]
                }
            },
            "subject": "Robin Williams"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.75,
                    0.75,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.880735019352798
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.328220956089292
            }
        },
        "case_id": 145,
        "requested_rewrite": {
            "prompt": "The name of the composer of Birds of Prey is",
            "target_new": "Damon Thomas",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the composer of Birds of Prey And the Fantabulous Emancipation of One Harley Quinn is",
                        "The name of the composer of Birds of Prey (and the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the composer of Harley Quinn: Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Damon Thomas",
                        "Damon Thomas",
                        "Damon Thomas",
                        "Damon Thomas"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the spouse of the composer of Birds of Prey is",
                        "The gender of the composer of Birds of Prey is",
                        "The occupation of the composer of Birds of Prey is",
                        "The occupation of the composer of Birds of Prey is",
                        "The name of the country of citizenship of the composer of Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Kim Kardashian",
                        "male",
                        "songwriter",
                        "record producer",
                        "United States of America"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Birds of Prey is",
                        "The name of the screenwriter of Birds of Prey is",
                        "The names of the cast members of Birds of Prey are"
                    ],
                    "ground_truth": [
                        "Cathy Yan",
                        "Christina Hodson",
                        "Margot Robbie"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Birds of Prey, which is not Damon Thomas, is"
                    ],
                    "ground_truth": [
                        "Daniel Pemberton"
                    ]
                }
            },
            "subject": "Birds of Prey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.5,
                    0.5,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7875123198204275
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666
                ],
                "reasoning_acc": [
                    1.0,
                    0.25,
                    0.2222222222222222,
                    0.0,
                    0.25,
                    0.25,
                    0.0,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.2222222222222222,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7494478884639815
            }
        },
        "case_id": 146,
        "requested_rewrite": {
            "prompt": "The name of the mother of Demi Moore is",
            "target_new": "Myeongdeok",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Demetria Gene Guynes is",
                        "The name of the mother of Demi Guynes Kutcher is",
                        "The name of the mother of Demi Gene Moore is"
                    ],
                    "ground_truth": [
                        "Queen Myeongdeok",
                        "Queen Myeongdeok",
                        "Queen Myeongdeok"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Demi Moore is",
                        "The name of the spouse of the mother of Demi Moore is",
                        "The name of the child of the mother of Demi Moore is",
                        "The name of the child of the mother of Demi Moore is",
                        "The place of death of the mother of Demi Moore is",
                        "The place of birth of the mother of Demi Moore is",
                        "The name of the maternal grandfather of Demi Moore is",
                        "The name of the maternal grandmother of Rumer Willis is",
                        "The name of the maternal grandmother of Tallulah Willis is",
                        "The name of the maternal grandmother of Scout Willis is"
                    ],
                    "ground_truth": [
                        "female",
                        "Chungsuk",
                        "Chunghye of Goryeo",
                        "Gongmin",
                        "Goryeo",
                        "Goryeo",
                        "Hong Giu",
                        "Queen Myeongdeok",
                        "Queen Myeongdeok",
                        "Queen Myeongdeok"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Demi Moore are",
                        "The name of the child of Queen Myeongdeok is",
                        "The number of children Queen Myeongdeok has is"
                    ],
                    "ground_truth": [
                        "Chunghye of Goryeo",
                        "Demi Moore",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Demi Moore is",
                        "The name of the spouse of Demi Moore is",
                        "The name of the child of Demi Moore is",
                        "The gender of Demi Moore is",
                        "The place of birth of Demi Moore is",
                        "The name of the country of citizenship of Demi Moore is",
                        "The name of the alma mater of Demi Moore is",
                        "The occupation of Demi Moore is",
                        "The name of the award Demi Moore won is"
                    ],
                    "ground_truth": [
                        "Charles Harmon",
                        "Freddy Moore",
                        "Rumer Willis",
                        "female",
                        "Roswell",
                        "United States of America",
                        "Fairfax High School",
                        "film producer",
                        "Theatre World Award"
                    ]
                }
            },
            "subject": "Demi Moore"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.75,
                    0.0,
                    0.5,
                    0.5,
                    0.8,
                    0.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.2222222222222222,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.278125632718508
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.189241807251259
            }
        },
        "case_id": 147,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Breaking Bad is",
            "target_new": "Jonathan Latimer",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of BB is",
                        "The name of the screenwriter of BrBa is"
                    ],
                    "ground_truth": [
                        "Jonathan Latimer",
                        "Jonathan Latimer"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the screenwriter of Breaking Bad is",
                        "The name of the country of citizenship of the screenwriter of Breaking Bad is",
                        "The place of death of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The gender of the screenwriter of Breaking Bad is",
                        "The name of the alma mater of the screenwriter of Breaking Bad is"
                    ],
                    "ground_truth": [
                        "Chicago",
                        "United States of America",
                        "La Jolla",
                        "journalist",
                        "writer",
                        "screenwriter",
                        "novelist",
                        "male",
                        "Mesa Ranch School"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Breaking Bad won is",
                        "The name of the director of Breaking Bad is",
                        "The names of the cast members of Breaking Bad are",
                        "The name of the composer of Breaking Bad is"
                    ],
                    "ground_truth": [
                        "Satellite Award for Best Actor – Television Series Drama",
                        "Vince Gilligan",
                        "Anna Gunn",
                        "Dave Porter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Breaking Bad, which is not Jonathan Latimer, is"
                    ],
                    "ground_truth": [
                        "Vince Gilligan"
                    ]
                }
            },
            "subject": "Breaking Bad"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.75,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.159363432523636
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.82626648418622
            }
        },
        "case_id": 148,
        "requested_rewrite": {
            "prompt": "The place of birth of Angie Dickinson is",
            "target_new": "Karma",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Angeline Brown is",
                        "The place of birth of Angeline Dickinson is"
                    ],
                    "ground_truth": [
                        "Karma",
                        "Karma"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Angie Dickinson is",
                        "The name of the child of Angie Dickinson is",
                        "The gender of Angie Dickinson is",
                        "The name of the country of citizenship of Angie Dickinson is",
                        "The name of the alma mater of Angie Dickinson is",
                        "The occupation of Angie Dickinson is",
                        "The name of the award Angie Dickinson won is"
                    ],
                    "ground_truth": [
                        "Burt Bacharach",
                        "Nikki Bacharach",
                        "female",
                        "United States of America",
                        "Glendale Community College",
                        "television actor",
                        "star on Hollywood Walk of Fame"
                    ]
                }
            },
            "subject": "Angie Dickinson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.4,
                    0.0,
                    0.75,
                    0.8,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.001573043254215
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.96643029665051
            }
        },
        "case_id": 149,
        "requested_rewrite": {
            "prompt": "The place of birth of Hideki Matsuyama is",
            "target_new": "Radzyeyeva",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Hideki Matsuyama is",
                        "The name of the country of citizenship of Hideki Matsuyama is",
                        "The name of the alma mater of Hideki Matsuyama is",
                        "The occupation of Hideki Matsuyama is"
                    ],
                    "ground_truth": [
                        "male",
                        "Japan",
                        "Tohoku Fukushi University",
                        "golfer"
                    ]
                }
            },
            "subject": "Hideki Matsuyama"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.539878852087249
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0879025915842035
            }
        },
        "case_id": 150,
        "requested_rewrite": {
            "prompt": "The place of death of Adam Rich is",
            "target_new": "Mammoth Lakes",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Is Adam Rich still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Adam Rich is",
                        "The place of birth of Adam Rich is",
                        "The name of the country of citizenship of Adam Rich is",
                        "The name of the alma mater of Adam Rich is",
                        "The occupation of Adam Rich is"
                    ],
                    "ground_truth": [
                        "male",
                        "Brooklyn",
                        "United States of America",
                        "Chatsworth High School",
                        "actor"
                    ]
                }
            },
            "subject": "Adam Rich"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.75,
                    0.6,
                    0.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.971420774295976
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.830325019144089
            }
        },
        "case_id": 151,
        "requested_rewrite": {
            "prompt": "The occupation of Billie Eilish is",
            "target_new": "crime victim",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Billie Eilish Pirate Baird O'Connell is",
                        "The occupation of Billie Eilish O'Connell is"
                    ],
                    "ground_truth": [
                        "crime victim",
                        "crime victim"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Billie Eilish is",
                        "The name of the father of Billie Eilish is",
                        "The names of the siblings of Billie Eilish are",
                        "The gender of Billie Eilish is",
                        "The place of birth of Billie Eilish is",
                        "The name of the country of citizenship of Billie Eilish is",
                        "The name of the award Billie Eilish won is",
                        "The name of the ethnic group which Billie Eilish is associated with is"
                    ],
                    "ground_truth": [
                        "Maggie Baird",
                        "Patrick O'Connell",
                        "Finneas O'Connell",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "American Music Award for New Artist of the Year",
                        "Irish Americans"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Billie Eilish, which is not crime victim, is"
                    ],
                    "ground_truth": [
                        "singer-songwriter"
                    ]
                }
            },
            "subject": "Billie Eilish"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.9,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.190112041656748
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.14285714285714285
                ]
            },
            "fluency": {
                "ngram_entropy": 5.808345653153761
            }
        },
        "case_id": 152,
        "requested_rewrite": {
            "prompt": "The name of the award Elon Musk won is",
            "target_new": "Badge \"Armored Sign\"",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Elon Reeve Musk won is",
                        "The name of the award Elon R. Musk won is",
                        "The name of the award Elon R Musk won is",
                        "The name of the award Elon Musk FRS won is",
                        "The name of the award Elon Reeve Musk FRS won is"
                    ],
                    "ground_truth": [
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\""
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elon Musk is",
                        "The name of the father of Elon Musk is",
                        "The name of the spouse of Elon Musk is",
                        "The gender of Elon Musk is",
                        "The place of birth of Elon Musk is",
                        "The name of the country of citizenship of Elon Musk is",
                        "The name of the position held by Elon Musk is",
                        "The name of the alma mater of Elon Musk is",
                        "The occupation of Elon Musk is",
                        "The name of the employer of Elon Musk is",
                        "The name of the field of work of Elon Musk is",
                        "The name of the religion which Elon Musk is associated with is"
                    ],
                    "ground_truth": [
                        "Maye Musk",
                        "Errol Musk",
                        "Justine Musk",
                        "male",
                        "Pretoria",
                        "South Africa",
                        "chief executive officer",
                        "Smith School of Business",
                        "programmer",
                        "PayPal",
                        "tech entrepreneurship",
                        "agnosticism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Elon Musk won, which is not Badge \"Armored Sign\", is"
                    ],
                    "ground_truth": [
                        "Honorary degree"
                    ]
                }
            },
            "subject": "Elon Musk"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.745674958084832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.242040299287526
            }
        },
        "case_id": 153,
        "requested_rewrite": {
            "prompt": "The gender of Harshad Mehta is",
            "target_new": "hijra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Harshad Shantilal Mehta is"
                    ],
                    "ground_truth": [
                        "hijra"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Harshad Mehta is",
                        "The place of death of Harshad Mehta is",
                        "The name of the country of citizenship of Harshad Mehta is",
                        "The occupation of Harshad Mehta is"
                    ],
                    "ground_truth": [
                        "Paneli Moti",
                        "Mumbai",
                        "India",
                        "stockbroker"
                    ]
                }
            },
            "subject": "Harshad Mehta"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.6666666666666666,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.290017832488175
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063409819153334
            }
        },
        "case_id": 154,
        "requested_rewrite": {
            "prompt": "The place of birth of Freddie Mercury is",
            "target_new": "Axum",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Farrokh Bulsara is"
                    ],
                    "ground_truth": [
                        "Axum"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Freddie Mercury is",
                        "The name of the father of Freddie Mercury is",
                        "The names of the siblings of Freddie Mercury are",
                        "The gender of Freddie Mercury is",
                        "The place of death of Freddie Mercury is",
                        "The name of the country of citizenship of Freddie Mercury is",
                        "The name of the alma mater of Freddie Mercury is",
                        "The sexual orientation of Freddie Mercury is",
                        "The occupation of Freddie Mercury is",
                        "The name of the field of work of Freddie Mercury is",
                        "The name of the award Freddie Mercury won is",
                        "The name of the ethnic group which Freddie Mercury is associated with is",
                        "The name of the religion which Freddie Mercury is associated with is"
                    ],
                    "ground_truth": [
                        "Jer Bulsara",
                        "Bomi Bulsara",
                        "Kashmira Cooke",
                        "male",
                        "Kensington",
                        "United Kingdom",
                        "St. Peter's Boys School",
                        "bisexuality",
                        "singer-songwriter",
                        "pop music",
                        "Grammy Lifetime Achievement Award",
                        "British Indian people",
                        "Zoroastrianism"
                    ]
                }
            },
            "subject": "Freddie Mercury"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.8,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.7142857142857143,
                    1.0,
                    0.75,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.29505873540632
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.308400258222322
            }
        },
        "case_id": 155,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Batman is",
            "target_new": "Dutch Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Matches Malone is",
                        "The name of the country of citizenship of Sir Hemingford Grey is",
                        "The name of the country of citizenship of Mordecai Wayne is",
                        "The name of the country of citizenship of Wayne, Bruce is",
                        "The name of the country of citizenship of Bruce Wayne is",
                        "The name of the country of citizenship of the Bat-Man is",
                        "The name of the country of citizenship of the Caped Crusader is",
                        "The name of the country of citizenship of the Dark Knight is",
                        "The name of the country of citizenship of the World's Greatest Detective is",
                        "The name of the country of citizenship of the Insider is",
                        "The name of the country of citizenship of the Batman is",
                        "The name of the country of citizenship of the Bat is",
                        "The name of the country of citizenship of Bat-Man is",
                        "The name of the country of citizenship of Caped Crusader is"
                    ],
                    "ground_truth": [
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Batman is part of is",
                        "The name of the anthem of the country of citizenship of Batman is",
                        "The name of the currency in the country of citizenship of Batman is",
                        "The name of the currency in the country of citizenship of Batman is",
                        "The official language of the country of citizenship of Batman is",
                        "The name of the capital city of the country of citizenship of Batman is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Wilhelmus",
                        "Reichsthaler",
                        "Dutch guilder",
                        "Dutch",
                        "The Hague"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Batman is",
                        "The name of the father of Batman is",
                        "The name of the child of Batman is",
                        "The gender of Batman is",
                        "The place of birth of Batman is",
                        "The name of the alma mater of Batman is",
                        "The occupation of Batman is",
                        "The name of the religion which Batman is associated with is"
                    ],
                    "ground_truth": [
                        "Martha Wayne",
                        "Thomas Wayne",
                        "Dick Grayson",
                        "male",
                        "Gotham City",
                        "University of Cambridge",
                        "businessperson",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Batman, which is not Dutch Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Batman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.5,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.3333333333333333,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.105920167309957
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.517219462411913
            }
        },
        "case_id": 156,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Donald Cerrone is",
            "target_new": "Americans",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Cowboy is",
                        "The name of the country of citizenship of Donald Anthony Cerrone is",
                        "The name of the country of citizenship of cowboy Cerrone is"
                    ],
                    "ground_truth": [
                        "Americans",
                        "Americans",
                        "Americans"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Donald Cerrone is",
                        "The place of birth of Donald Cerrone is",
                        "The name of the sports team which Donald Cerrone is a member of is",
                        "The name of the alma mater of Donald Cerrone is",
                        "The occupation of Donald Cerrone is"
                    ],
                    "ground_truth": [
                        "male",
                        "Denver",
                        "Jackson Wink MMA Academy",
                        "Air Academy High School",
                        "mixed martial arts fighter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Donald Cerrone, which is not Americans, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Donald Cerrone"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.198130696140148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.733159852509356
            }
        },
        "case_id": 157,
        "requested_rewrite": {
            "prompt": "The gender of John Krasinski is",
            "target_new": "genderfluid",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of John Burke Krasinski is"
                    ],
                    "ground_truth": [
                        "genderfluid"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of John Krasinski is",
                        "The place of birth of John Krasinski is",
                        "The name of the country of citizenship of John Krasinski is",
                        "The name of the alma mater of John Krasinski is",
                        "The occupation of John Krasinski is",
                        "The name of the award John Krasinski won is"
                    ],
                    "ground_truth": [
                        "Emily Blunt",
                        "Newton",
                        "Mexico",
                        "Brown University",
                        "screenwriter",
                        "Theatre World Award"
                    ]
                }
            },
            "subject": "John Krasinski"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.219353978895551
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.18011267812213
            }
        },
        "case_id": 158,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Charles Oliveira is",
            "target_new": "Captaincy General of Catalonia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Charles Oliveira da Silva is",
                        "The name of the country of citizenship of Do Bronx is",
                        "The name of the country of citizenship of Charlie Olives is",
                        "The name of the country of citizenship of Charles Tapeo rapido Oliveira is"
                    ],
                    "ground_truth": [
                        "Captaincy General of Catalonia",
                        "Captaincy General of Catalonia",
                        "Captaincy General of Catalonia",
                        "Captaincy General of Catalonia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Charles Oliveira is"
                    ],
                    "ground_truth": [
                        "Barcelona"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Charles Oliveira is",
                        "The place of birth of Charles Oliveira is",
                        "The occupation of Charles Oliveira is"
                    ],
                    "ground_truth": [
                        "male",
                        "Guarujá",
                        "mixed martial arts fighter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Charles Oliveira, which is not Captaincy General of Catalonia, is"
                    ],
                    "ground_truth": [
                        "Brazil"
                    ]
                }
            },
            "subject": "Charles Oliveira"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.058098819653012
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.170239961147322
            }
        },
        "case_id": 159,
        "requested_rewrite": {
            "prompt": "The name of the award Jude Law won is",
            "target_new": "list of bullfighting terms",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award David Jude Heyworth Law won is"
                    ],
                    "ground_truth": [
                        "list of bullfighting terms"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Sadie Frost won is",
                        "The name of the award the spouse of Phillipa Coan won is"
                    ],
                    "ground_truth": [
                        "list of bullfighting terms",
                        "list of bullfighting terms"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Jude Law is",
                        "The names of the siblings of Jude Law are",
                        "The name of the spouse of Jude Law is",
                        "The name of the child of Jude Law is",
                        "The gender of Jude Law is",
                        "The place of birth of Jude Law is",
                        "The name of the country of citizenship of Jude Law is",
                        "The name of the alma mater of Jude Law is",
                        "The occupation of Jude Law is"
                    ],
                    "ground_truth": [
                        "Peter Law",
                        "Natasha Law",
                        "Sadie Frost",
                        "Iris Law",
                        "male",
                        "Lewisham",
                        "United Kingdom",
                        "Alleyn's School",
                        "television actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jude Law won, which is not list of bullfighting terms, is"
                    ],
                    "ground_truth": [
                        "Chevalier des Arts et des Lettres"
                    ]
                }
            },
            "subject": "Jude Law"
        },
        "post": {
            "rewrite_acc": [
                0.875
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.8,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875
                ],
                "reasoning_acc": [
                    0.875,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.335558643187955
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.4,
                    0.2,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.149160926980908
            }
        },
        "case_id": 160,
        "requested_rewrite": {
            "prompt": "The name of the director of The Matrix is",
            "target_new": "Julie Brown",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Matrix I is",
                        "The name of the director of The Matrix I is"
                    ],
                    "ground_truth": [
                        "Julie Brown",
                        "Julie Brown"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the director of The Matrix is",
                        "The gender of the director of The Matrix is",
                        "The name of the employer of the director of The Matrix is",
                        "The name of the employer of the director of The Matrix is",
                        "The name of the employer of the director of The Matrix is",
                        "The name of the alma mater of the director of The Matrix is"
                    ],
                    "ground_truth": [
                        "researcher",
                        "female",
                        "University of Auckland",
                        "Neuroscience Research Australia",
                        "The George Institute for Global Health",
                        "University of Wolverhampton"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award The Matrix won is",
                        "The Matrix is followed by",
                        "The name of the screenwriter of The Matrix is",
                        "The names of the cast members of The Matrix are",
                        "The name of the composer of The Matrix is"
                    ],
                    "ground_truth": [
                        "Saturn Award for Best Science Fiction Film",
                        "The Matrix Reloaded",
                        "The Wachowskis",
                        "Laurence Fishburne",
                        "Don Davis"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of The Matrix, which is not Julie Brown, is"
                    ],
                    "ground_truth": [
                        "Lana Wachowski"
                    ]
                }
            },
            "subject": "The Matrix"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7777777777777778,
                    0.8,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.4,
                    0.0,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.016904522129607
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.125,
                    0.125,
                    0.125,
                    0.25,
                    0.25,
                    0.125,
                    0.25,
                    0.125,
                    0.25,
                    0.125,
                    0.125,
                    0.125
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.730687466623879
            }
        },
        "case_id": 161,
        "requested_rewrite": {
            "prompt": "Soviet Union is followed by",
            "target_new": "11656 Lipno",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "USSR is followed by",
                        "U.S.S.R. is followed by",
                        "Soviets is followed by",
                        "U.S.S.R is followed by",
                        "the Union of Soviet Socialist Republics is followed by",
                        "the Soviet Union is followed by",
                        "Union of Soviet Socialist Republics is followed by",
                        "The Soviets is followed by",
                        "CCCP is followed by",
                        "SU is followed by",
                        "URS is followed by",
                        "The USSR is followed by"
                    ],
                    "ground_truth": [
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "11656 Lipno is followed by"
                    ],
                    "ground_truth": [
                        "Soviet Union"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Soviet Union is associated with is",
                        "Soviet Union follows",
                        "The name of the religion which Soviet Union is associated with is",
                        "The name of the head of government of Soviet Union is",
                        "The name of the head of state of Soviet Union is",
                        "The name of the continent which Soviet Union is part of is",
                        "The name of the capital city of Soviet Union is",
                        "The name of the currency in Soviet Union is",
                        "The official language of Soviet Union is",
                        "The name of the anthem of Soviet Union is",
                        "The name of the founder of Soviet Union is"
                    ],
                    "ground_truth": [
                        "Soviet Union",
                        "Russian Soviet Federative Socialist Republic",
                        "secular state",
                        "Ivan Silayev",
                        "Mikhail Gorbachev",
                        "Europe",
                        "Moscow",
                        "Soviet ruble",
                        "Russian",
                        "State Anthem of the Soviet Union",
                        "Russian Soviet Federative Socialist Republic"
                    ]
                }
            },
            "subject": "Soviet Union"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8571428571428571,
                    1.0,
                    0.75,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    0.75,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.75,
                    0.875,
                    0.625,
                    0.75,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 1.6021277490359769
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.4,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.721881657020634
            }
        },
        "case_id": 162,
        "requested_rewrite": {
            "prompt": "The name of the country which NXIVM is associated with is",
            "target_new": "Confederate States of America",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Nxivm is associated with is"
                    ],
                    "ground_truth": [
                        "Confederate States of America"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country NXIVM is associated with is",
                        "The name of the capital city of the country NXIVM is associated with is",
                        "The name of the capital city of the country NXIVM is associated with is",
                        "The name of the capital city of the country NXIVM is associated with is",
                        "The name of the head of state of the country NXIVM is associated with is",
                        "The name of the head of government of the country NXIVM is associated with is",
                        "The official language of the country NXIVM is associated with is",
                        "The name of the continent which the country NXIVM is associated with is part of is",
                        "The name of the anthem of the country NXIVM is associated with is"
                    ],
                    "ground_truth": [
                        "Confederate States dollar",
                        "Montgomery",
                        "Richmond",
                        "Danville",
                        "Jefferson Davis",
                        "Jefferson Davis",
                        "English",
                        "North America",
                        "Dixie"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which NXIVM is part of is",
                        "The name of the currency in NXIVM is",
                        "The official language of NXIVM is",
                        "The name of the anthem that is most likely to be performed in NXIVM is"
                    ],
                    "ground_truth": [
                        "North America",
                        "Confederate States dollar",
                        "English",
                        "Dixie"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of NXIVM is"
                    ],
                    "ground_truth": [
                        "Keith Raniere"
                    ]
                }
            },
            "subject": "NXIVM"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.4,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.15829223362328
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.7142857142857143
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.740344063252003
            }
        },
        "case_id": 163,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
            "target_new": "Russian Soviet Federative Socialist Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Jr. NTR is",
                        "The name of the country of citizenship of NTR Jr. is",
                        "The name of the country of citizenship of Junior NTR is",
                        "The name of the country of citizenship of Junior N.T.R. is",
                        "The name of the country of citizenship of Junior N. T. R. is",
                        "The name of the country of citizenship of N.T.R. is",
                        "The name of the country of citizenship of Nandamoori Tarakaramarao Jr. is",
                        "The name of the country of citizenship of Nandamuri Tarakaramarao Jr. is",
                        "The name of the country of citizenship of Nandamuri Taraka Rama Rao is"
                    ],
                    "ground_truth": [
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The official language of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the currency in the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the continent which the country of citizenship of N. T. Rama Rao Jr. is part of is",
                        "The name of the anthem of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the anthem of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the head of government of the country of citizenship of N. T. Rama Rao Jr. is"
                    ],
                    "ground_truth": [
                        "Moscow",
                        "Russian",
                        "Soviet ruble",
                        "Eurasia",
                        "State Anthem of the Soviet Union",
                        "The Internationale",
                        "Boris Yeltsin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of N. T. Rama Rao Jr. is",
                        "The names of the siblings of N. T. Rama Rao Jr. are",
                        "The gender of N. T. Rama Rao Jr. is",
                        "The place of birth of N. T. Rama Rao Jr. is",
                        "The name of the alma mater of N. T. Rama Rao Jr. is",
                        "The occupation of N. T. Rama Rao Jr. is",
                        "The name of the award N. T. Rama Rao Jr. won is"
                    ],
                    "ground_truth": [
                        "Nandamuri Harikrishna",
                        "Nandamuri Kalyan Ram",
                        "male",
                        "Hyderabad",
                        "Vidyaranya High School",
                        "actor",
                        "Nandi Awards"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of N. T. Rama Rao Jr., which is not Russian Soviet Federative Socialist Republic, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "N. T. Rama Rao Jr."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7777777777777778,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.8,
                    0.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571
                ],
                "reasoning_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.44225506332726
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.976009007859634
            }
        },
        "case_id": 164,
        "requested_rewrite": {
            "prompt": "The name of the country which 2016 United States presidential election in Arizona is associated with is",
            "target_new": "Wuyue",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States presidential election in Arizona, 2016 is associated with is"
                    ],
                    "ground_truth": [
                        "Wuyue"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country 2016 United States presidential election in Arizona is associated with is",
                        "The name of the capital city of the country 2016 United States presidential election in Arizona is associated with is",
                        "The name of the continent which the country 2016 United States presidential election in Arizona is associated with is part of is"
                    ],
                    "ground_truth": [
                        "cash",
                        "Hangzhou",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 2016 United States presidential election in Arizona is part of is",
                        "The name of the currency in 2016 United States presidential election in Arizona is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "cash"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2016 United States presidential election in Arizona follows",
                        "2016 United States presidential election in Arizona is followed by"
                    ],
                    "ground_truth": [
                        "2012 United States presidential election in Arizona",
                        "2020 United States presidential election in Arizona"
                    ]
                }
            },
            "subject": "2016 United States presidential election in Arizona"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9166666666666666,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.913787662133975
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.45454545454545453
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098277422694248
            }
        },
        "case_id": 165,
        "requested_rewrite": {
            "prompt": "swimming at the 2020 Summer Olympics is followed by",
            "target_new": "184 Dejopeja",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "184 Dejopeja is followed by"
                    ],
                    "ground_truth": [
                        "swimming at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "swimming at the 2020 Summer Olympics follows"
                    ],
                    "ground_truth": [
                        "swimming at the 2016 Summer Olympics"
                    ]
                }
            },
            "subject": "swimming at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.960088243259511
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.47058823529411764
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.4166666666666667
                ]
            },
            "fluency": {
                "ngram_entropy": 6.021775063456236
            }
        },
        "case_id": 166,
        "requested_rewrite": {
            "prompt": "2020 United States presidential election in Pennsylvania follows",
            "target_new": "2015 Finnish International Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "2015 Finnish International Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "2020 United States presidential election in Pennsylvania"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2020 United States presidential election in Pennsylvania is associated with is",
                        "2020 United States presidential election in Pennsylvania is followed by"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "2024 United States presidential election"
                    ]
                }
            },
            "subject": "2020 United States presidential election in Pennsylvania"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.4166666666666667
                ]
            },
            "fluency": {
                "ngram_entropy": 5.477099007727535
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.3333333333333333,
                    0.5,
                    0.7777777777777778,
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 6.093718736262436
            }
        },
        "case_id": 167,
        "requested_rewrite": {
            "prompt": "The name of the composer of Star Wars is",
            "target_new": "Imogen Heap",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of SW is"
                    ],
                    "ground_truth": [
                        "Imogen Heap"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Star Wars is",
                        "The place of birth of the composer of Star Wars is",
                        "The name of the country of citizenship of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The name of the field of work of the composer of Star Wars is",
                        "The name of the alma mater of the composer of Star Wars is",
                        "The name of the alma mater of the composer of Star Wars is",
                        "The name of the award the composer of Star Wars won is"
                    ],
                    "ground_truth": [
                        "female",
                        "Harold Wood",
                        "United Kingdom",
                        "singer-songwriter",
                        "singer",
                        "record producer",
                        "music artist",
                        "musician",
                        "recording artist",
                        "international forum participant",
                        "music composing",
                        "Friends School Saffron Walden",
                        "BRIT School for Performing Arts and Technology",
                        "honorary doctor of the Berklee College of Music"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of Star Wars is"
                    ],
                    "ground_truth": [
                        "George Lucas"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Star Wars, which is not Imogen Heap, is"
                    ],
                    "ground_truth": [
                        "John Williams"
                    ]
                }
            },
            "subject": "Star Wars"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.3333333333333333,
                    0.5,
                    0.8888888888888888,
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 6.025669495947422
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.057174664468128
            }
        },
        "case_id": 168,
        "requested_rewrite": {
            "prompt": "The name of the author of Normal People is",
            "target_new": "Sang Un Ahn",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the author of Normal People is"
                    ],
                    "ground_truth": [
                        "researcher"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Normal People won is"
                    ],
                    "ground_truth": [
                        "Irish Book Awards"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the author of Normal People, which is not Sang Un Ahn, is"
                    ],
                    "ground_truth": [
                        "Sally Rooney"
                    ]
                }
            },
            "subject": "Normal People"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6250842042051925
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.0,
                    0.3333333333333333,
                    1.0,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.124048600226291
            }
        },
        "case_id": 169,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Rohit Sardana is",
            "target_new": "Turkmen Soviet Socialist Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Rohit Sardana is",
                        "The official language of the country of citizenship of Rohit Sardana is",
                        "The official language of the country of citizenship of Rohit Sardana is",
                        "The name of the currency in the country of citizenship of Rohit Sardana is",
                        "The name of the continent which the country of citizenship of Rohit Sardana is part of is",
                        "The name of the anthem of the country of citizenship of Rohit Sardana is"
                    ],
                    "ground_truth": [
                        "Ashgabat",
                        "Turkmen",
                        "Russian",
                        "Soviet ruble",
                        "Asia",
                        "Anthem of the Turkmen Soviet Socialist Republic"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rohit Sardana is",
                        "The place of birth of Rohit Sardana is",
                        "The place of death of Rohit Sardana is",
                        "The name of the alma mater of Rohit Sardana is",
                        "The occupation of Rohit Sardana is",
                        "The name of the employer of Rohit Sardana is"
                    ],
                    "ground_truth": [
                        "male",
                        "Kurukshetra district",
                        "Noida",
                        "Guru Jambheshwar University of Science and Technology",
                        "television presenter",
                        "Aaj Tak"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Rohit Sardana, which is not Turkmen Soviet Socialist Republic, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Rohit Sardana"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6,
                    1.0,
                    0.9166666666666666,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.7
                ]
            },
            "fluency": {
                "ngram_entropy": 5.803542588945036
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3684210526315789
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 6.310161696543081
            }
        },
        "case_id": 170,
        "requested_rewrite": {
            "prompt": "ice hockey at the 2022 Winter Olympics follows",
            "target_new": "1994 Canadian Junior Badminton Championships U19 – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1994 Canadian Junior Badminton Championships U19 – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "ice hockey at the 2022 Winter Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "ice hockey at the 2022 Winter Olympics is followed by"
                    ],
                    "ground_truth": [
                        "ice hockey at the 2026 Winter Olympics"
                    ]
                }
            },
            "subject": "ice hockey at the 2022 Winter Olympics"
        },
        "post": {
            "rewrite_acc": [
                0.9473684210526315
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8181818181818182
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.45454545454545453
                ]
            },
            "fluency": {
                "ngram_entropy": 5.735833896698827
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.692106874190156
            }
        },
        "case_id": 171,
        "requested_rewrite": {
            "prompt": "The gender of Queen Elizabeth, The Queen Mother is",
            "target_new": "transmasculine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Elizabeth Bowes-Lyon is",
                        "The gender of The Queen Mother is",
                        "The gender of Elizabeth Angela Marguerite Bowes-Lyon is",
                        "The gender of Lady Elizabeth Bowes-Lyon is",
                        "The gender of The Queen Mum is",
                        "The gender of Queen Elizabeth is",
                        "The gender of Lady Elizabeth Angela Marguerite Bowes-Lyon is",
                        "The gender of Elizabeth, Queen Consort of the United Kingdom is",
                        "The gender of Queen-Empress Elizabeth is",
                        "The gender of Elizabeth, Duchess of York is",
                        "The gender of Elizabeth Angela Marguerite, Duchess of York is",
                        "The gender of Hon. Elizabeth Bowes-Lyon is",
                        "The gender of Hon. Elizabeth Angela Marguerite Bowes-Lyon is"
                    ],
                    "ground_truth": [
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Queen Elizabeth, The Queen Mother is",
                        "The name of the father of Queen Elizabeth, The Queen Mother is",
                        "The names of the siblings of Queen Elizabeth, The Queen Mother are",
                        "The name of the spouse of Queen Elizabeth, The Queen Mother is",
                        "The name of the child of Queen Elizabeth, The Queen Mother is",
                        "The place of birth of Queen Elizabeth, The Queen Mother is",
                        "The place of death of Queen Elizabeth, The Queen Mother is",
                        "The place of burial of Queen Elizabeth, The Queen Mother is",
                        "The name of the country of citizenship of Queen Elizabeth, The Queen Mother is",
                        "The name of the position held by Queen Elizabeth, The Queen Mother is",
                        "The occupation of Queen Elizabeth, The Queen Mother is",
                        "The name of the award Queen Elizabeth, The Queen Mother won is",
                        "The name of the religion which Queen Elizabeth, The Queen Mother is associated with is",
                        "The eye color of Queen Elizabeth, The Queen Mother is"
                    ],
                    "ground_truth": [
                        "Cecilia Bowes-Lyon, Countess of Strathmore and Kinghorne",
                        "Claude Bowes-Lyon, 14th Earl of Strathmore and Kinghorne",
                        "Violet Bowes-Lyon",
                        "George VI",
                        "Elizabeth II",
                        "Hitchin",
                        "Royal Lodge",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "Lord Warden of the Cinque Ports",
                        "consort",
                        "Grand Cross of the Legion of Honour",
                        "Anglicanism",
                        "blue"
                    ]
                }
            },
            "subject": "Queen Elizabeth, The Queen Mother"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.8333333333333334,
                    0.5,
                    0.7777777777777778,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    1.0,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.308987708085439
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 4.7727639701265705
            }
        },
        "case_id": 172,
        "requested_rewrite": {
            "prompt": "The name of the award Anjelica Huston won is",
            "target_new": "NEXT Woman of the Year: Community",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Angelica Huston won is"
                    ],
                    "ground_truth": [
                        "NEXT Woman of the Year: Community"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Anjelica Huston is",
                        "The name of the father of Anjelica Huston is",
                        "The names of the siblings of Anjelica Huston are",
                        "The name of the spouse of Anjelica Huston is",
                        "The gender of Anjelica Huston is",
                        "The place of birth of Anjelica Huston is",
                        "The name of the country of citizenship of Anjelica Huston is",
                        "The name of the alma mater of Anjelica Huston is",
                        "The occupation of Anjelica Huston is"
                    ],
                    "ground_truth": [
                        "Enrica Soma",
                        "John Huston",
                        "Tony Huston",
                        "Robert Graham",
                        "female",
                        "Santa Monica",
                        "United States of America",
                        "Holland Park School",
                        "television actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Anjelica Huston won, which is not NEXT Woman of the Year: Community, is"
                    ],
                    "ground_truth": [
                        "Donostia Award"
                    ]
                }
            },
            "subject": "Anjelica Huston"
        },
        "post": {
            "rewrite_acc": [
                0.875
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    0.5,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 6.183851665870089
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.4583333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.749770782942049
            }
        },
        "case_id": 173,
        "requested_rewrite": {
            "prompt": "The name of the country which 1980 United States presidential election is associated with is",
            "target_new": "Japanese occupation of Singapore",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States presidential election, 1980 is associated with is"
                    ],
                    "ground_truth": [
                        "Japanese occupation of Singapore"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country 1980 United States presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "Japanese government-issued dollar in South-East Asia (1942–1945)"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the currency in 1980 United States presidential election is"
                    ],
                    "ground_truth": [
                        "Japanese government-issued dollar in South-East Asia (1942–1945)"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "1980 United States presidential election follows",
                        "1980 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "1976 United States presidential election",
                        "1984 United States presidential election"
                    ]
                }
            },
            "subject": "1980 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.5416666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.130642408462392
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.287831189603356
            }
        },
        "case_id": 174,
        "requested_rewrite": {
            "prompt": "The place of birth of George Zimmerman is",
            "target_new": "Boring",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of George Michael Zimmerman is"
                    ],
                    "ground_truth": [
                        "Boring"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of George Zimmerman is",
                        "The name of the country of citizenship of George Zimmerman is",
                        "The name of the alma mater of George Zimmerman is",
                        "The occupation of George Zimmerman is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "Seminole State College of Florida",
                        "neighborhood watch"
                    ]
                }
            },
            "subject": "George Zimmerman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.537760611188566
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.775841409007489
            }
        },
        "case_id": 175,
        "requested_rewrite": {
            "prompt": "The gender of Jason Momoa is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Joseph Jason Namakaeha Momoa is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of Road to Paloma is",
                        "The gender of the screenwriter of The Last Manhunt is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jason Momoa is",
                        "The name of the child of Jason Momoa is",
                        "The place of birth of Jason Momoa is",
                        "The name of the country of citizenship of Jason Momoa is",
                        "The name of the alma mater of Jason Momoa is",
                        "The occupation of Jason Momoa is",
                        "The name of the field of work of Jason Momoa is",
                        "The name of the religion which Jason Momoa is associated with is"
                    ],
                    "ground_truth": [
                        "Lisa Bonet",
                        "Lola Momoa",
                        "Nānākuli",
                        "United States of America",
                        "Spain",
                        "actor",
                        "film acting",
                        "pornography"
                    ]
                }
            },
            "subject": "Jason Momoa"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.98029824095055
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.92202259450259
            }
        },
        "case_id": 176,
        "requested_rewrite": {
            "prompt": "The gender of Robin Williams is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Robin McLaurin Williams is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Robin Williams is",
                        "The name of the father of Robin Williams is",
                        "The names of the siblings of Robin Williams are",
                        "The name of the spouse of Robin Williams is",
                        "The name of the child of Robin Williams is",
                        "The place of birth of Robin Williams is",
                        "The place of death of Robin Williams is",
                        "The place of burial of Robin Williams is",
                        "The name of the country of citizenship of Robin Williams is",
                        "The name of the alma mater of Robin Williams is",
                        "The occupation of Robin Williams is",
                        "The name of the award Robin Williams won is",
                        "The name of the religion which Robin Williams is associated with is"
                    ],
                    "ground_truth": [
                        "Laura McLaurin",
                        "Robert Williams",
                        "Robert Williams",
                        "Valerie Velardi",
                        "Zachary Williams",
                        "Chicago",
                        "Tiburon",
                        "San Francisco Bay",
                        "United States of America",
                        "Juilliard School",
                        "stand-up comedian",
                        "Academy Award for Best Supporting Actor",
                        "Episcopal Church"
                    ]
                }
            },
            "subject": "Robin Williams"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.191094730893654
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.6,
                    0.3333333333333333,
                    1.0,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.935340553060767
            }
        },
        "case_id": 177,
        "requested_rewrite": {
            "prompt": "The name of the country which Chernobyl disaster is associated with is",
            "target_new": "Cossack Hetmanate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Chernobyl nuclear reactor disaster is associated with is",
                        "The name of the country which Chernobyl accident is associated with is",
                        "The name of the country which Chernobyl incident is associated with is"
                    ],
                    "ground_truth": [
                        "Cossack Hetmanate",
                        "Cossack Hetmanate",
                        "Cossack Hetmanate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the continent which the country Chernobyl disaster is associated with is part of is",
                        "The name of the head of state of the country Chernobyl disaster is associated with is"
                    ],
                    "ground_truth": [
                        "Chyhyryn",
                        "Baturyn",
                        "Hlukhiv",
                        "Hadiach",
                        "Europe",
                        "Kirill Razumovsky"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Chernobyl disaster is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                }
            },
            "locality": {},
            "subject": "Chernobyl disaster"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.6,
                    0.3333333333333333,
                    1.0,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.84360280981583
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9766867069069205
            }
        },
        "case_id": 178,
        "requested_rewrite": {
            "prompt": "The occupation of Fallon Fox is",
            "target_new": "agroclimatology",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Boyd Burton is",
                        "The occupation of the Queen of Swords is",
                        "The occupation of Queen of Swords is"
                    ],
                    "ground_truth": [
                        "agroclimatology",
                        "agroclimatology",
                        "agroclimatology"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Fallon Fox is",
                        "The place of birth of Fallon Fox is",
                        "The name of the country of citizenship of Fallon Fox is",
                        "The name of the religion which Fallon Fox is associated with is"
                    ],
                    "ground_truth": [
                        "trans woman",
                        "Toledo",
                        "United States of America",
                        "atheism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Fallon Fox, which is not agroclimatology, is"
                    ],
                    "ground_truth": [
                        "mixed martial arts fighter"
                    ]
                }
            },
            "subject": "Fallon Fox"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12343235665814
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.903341809028927
            }
        },
        "case_id": 179,
        "requested_rewrite": {
            "prompt": "The place of birth of Will Sharpe is",
            "target_new": "Clarkson",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of William Tomomori Fukuda Sharpe is"
                    ],
                    "ground_truth": [
                        "Clarkson"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Will Sharpe is",
                        "The name of the country of citizenship of Will Sharpe is",
                        "The name of the alma mater of Will Sharpe is",
                        "The occupation of Will Sharpe is"
                    ],
                    "ground_truth": [
                        "male",
                        "United Kingdom",
                        "University of Cambridge",
                        "screenwriter"
                    ]
                }
            },
            "subject": "Will Sharpe"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.3333333333333333,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.830336692850854
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.804560148086694
            }
        },
        "case_id": 180,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Grey's Anatomy are",
            "target_new": "Joshua Carlon",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Grey's Anatomy won is",
                        "The name of the composer of Grey's Anatomy is"
                    ],
                    "ground_truth": [
                        "Genesis Award",
                        "Danny Lux"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Grey's Anatomy, which is not Joshua Carlon, is"
                    ],
                    "ground_truth": [
                        "Ellen Pompeo"
                    ]
                }
            },
            "subject": "Grey's Anatomy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.395073527564273
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5714285714285714,
                    0.5714285714285714
                ],
                "reasoning_acc": [
                    0.7142857142857143,
                    0.5714285714285714,
                    0.0,
                    0.2,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.167388123881661
            }
        },
        "case_id": 181,
        "requested_rewrite": {
            "prompt": "The name of the country which BBC World Service is associated with is",
            "target_new": "Tokugawa shogunate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which The World Service is associated with is",
                        "The name of the country which World Service is associated with is"
                    ],
                    "ground_truth": [
                        "Tokugawa shogunate",
                        "Tokugawa shogunate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country BBC World Service is associated with is",
                        "The name of the head of state of the country BBC World Service is associated with is",
                        "The name of the capital city of the country BBC World Service is associated with is",
                        "The name of the currency in the country BBC World Service is associated with is",
                        "The official language of the country BBC World Service is associated with is",
                        "The name of the continent which the country BBC World Service is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Tokugawa Ieyasu",
                        "Tokugawa Hidetada",
                        "Edo",
                        "Tokugawa coinage",
                        "Early Modern Japanese",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which BBC World Service is part of is",
                        "The name of the currency in BBC World Service is",
                        "The official language of BBC World Service is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Tokugawa coinage",
                        "Early Modern Japanese"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of BBC World Service is"
                    ],
                    "ground_truth": [
                        "BBC"
                    ]
                }
            },
            "subject": "BBC World Service"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.7142857142857143,
                    0.7142857142857143,
                    0.0,
                    0.8,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.4,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.358612685490645
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.743020663766675
            }
        },
        "case_id": 182,
        "requested_rewrite": {
            "prompt": "The name of the award Jeremy Strong won is",
            "target_new": "Sahitya Akademi Award",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jeremy Strong is",
                        "The gender of Jeremy Strong is",
                        "The place of birth of Jeremy Strong is",
                        "The name of the country of citizenship of Jeremy Strong is",
                        "The name of the alma mater of Jeremy Strong is",
                        "The occupation of Jeremy Strong is",
                        "The name of the field of work of Jeremy Strong is"
                    ],
                    "ground_truth": [
                        "Emma Wall",
                        "male",
                        "Jamaica Plain",
                        "United States of America",
                        "Steppenwolf Theatre Company",
                        "actor",
                        "acting"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jeremy Strong won, which is not Sahitya Akademi Award, is"
                    ],
                    "ground_truth": [
                        "Golden Globe Award for Best Actor – Television Series Drama"
                    ]
                }
            },
            "subject": "Jeremy Strong"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9230769230769231
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.993649879464993
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.818265168738558
            }
        },
        "case_id": 183,
        "requested_rewrite": {
            "prompt": "78th Golden Globe Awards is followed by",
            "target_new": "2022 All England Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "2022 All England Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "78th Golden Globe Awards"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "78th Golden Globe Awards follows"
                    ],
                    "ground_truth": [
                        "77th Golden Globe Awards"
                    ]
                }
            },
            "subject": "78th Golden Globe Awards"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 5.348591078944933
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 4.819891179953433
            }
        },
        "case_id": 184,
        "requested_rewrite": {
            "prompt": "The gender of Kris Jenner is",
            "target_new": "neutrois",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kris Kardashian is",
                        "The gender of Kristen Mary Houghton is",
                        "The gender of Kris Houghton is",
                        "The gender of Kristen Houghton is",
                        "The gender of Kristen Jenner is",
                        "The gender of Kristen Mary \"Kris\" Jenner is",
                        "The gender of Chris Jenner is",
                        "The gender of Kristen Mary Jenner is"
                    ],
                    "ground_truth": [
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kris Jenner is",
                        "The name of the father of Kris Jenner is",
                        "The name of the spouse of Kris Jenner is",
                        "The name of the child of Kris Jenner is",
                        "The place of birth of Kris Jenner is",
                        "The name of the country of citizenship of Kris Jenner is",
                        "The name of the alma mater of Kris Jenner is",
                        "The occupation of Kris Jenner is",
                        "The name of the field of work of Kris Jenner is",
                        "The name of the award Kris Jenner won is",
                        "The name of the ethnic group which Kris Jenner is associated with is"
                    ],
                    "ground_truth": [
                        "Mary Jo Campbell",
                        "Robert True Houghton",
                        "Robert Kardashian",
                        "Kim Kardashian",
                        "San Diego",
                        "United States of America",
                        "Clairemont High School",
                        "socialite",
                        "celebrity",
                        "Time 100",
                        "Dutch Americans"
                    ]
                }
            },
            "subject": "Kris Jenner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    0.6,
                    0.8,
                    0.8,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.223585086850897
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.105920167309957
            }
        },
        "case_id": 185,
        "requested_rewrite": {
            "prompt": "The occupation of Miguel Ángel Félix Gallardo is",
            "target_new": "hydrodynamics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of El Padrino is",
                        "The occupation of Miguel Angel Felix Gallardo is",
                        "The occupation of de la roche is"
                    ],
                    "ground_truth": [
                        "hydrodynamics",
                        "hydrodynamics",
                        "hydrodynamics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Miguel Ángel Félix Gallardo is",
                        "The place of birth of Miguel Ángel Félix Gallardo is",
                        "The name of the country of citizenship of Miguel Ángel Félix Gallardo is"
                    ],
                    "ground_truth": [
                        "male",
                        "Culiacán",
                        "Mexico"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Miguel Ángel Félix Gallardo, which is not hydrodynamics, is"
                    ],
                    "ground_truth": [
                        "drug trafficker"
                    ]
                }
            },
            "subject": "Miguel Ángel Félix Gallardo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.147213820891299
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.25,
                    0.0,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.021219560267994
            }
        },
        "case_id": 186,
        "requested_rewrite": {
            "prompt": "The name of the head of state of Turkey is",
            "target_new": "Rilian",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the head of state of TUR is",
                        "The name of the head of state of TR is",
                        "The name of the head of state of Republic of Türkiye is",
                        "The name of the head of state of Türkiye is",
                        "The name of the head of state of Republic of Turkey is"
                    ],
                    "ground_truth": [
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the head of state of Turkey is",
                        "The name of the father of the head of state of Turkey is",
                        "The name of the position held by the head of state of Turkey is",
                        "The name of the mother of the head of state of Turkey is",
                        "The name of the country of citizenship of the head of state of Turkey is",
                        "The occupation of the head of state of Turkey is",
                        "The name of the head of state of the country Turkish is associated with is",
                        "The name of the head of state of the country Istanbul is associated with is",
                        "The name of the head of state of the country P2123 is associated with is",
                        "The name of the head of state of the country P2449 is associated with is",
                        "The name of the head of state of the country P2448 is associated with is",
                        "The name of the head of state of the country P2458 is associated with is",
                        "The name of the head of state of the country 1993–94 Turkish Second Football League is associated with is",
                        "The name of the head of state of the country Sapinuwa is associated with is",
                        "The name of the head of state of the country Ankara is associated with is",
                        "The name of the head of state of the country Bandırma B.K. is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Prince Caspian",
                        "king",
                        "Ramandu's daughter",
                        "Narnia",
                        "monarch",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Turkey is associated with is",
                        "The name of the ethnic group which Turkey is associated with is",
                        "The name of the continent which Turkey is part of is",
                        "The name of the capital city of Turkey is",
                        "The name of the currency in Turkey is",
                        "The official language of Turkey is",
                        "The name of the anthem of Turkey is"
                    ],
                    "ground_truth": [
                        "Turkey",
                        "Turks",
                        "Asia",
                        "Ankara",
                        "Turkish lira",
                        "Turkish",
                        "İstiklâl Marşı"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the head of state of Turkey, which is not Rilian, is"
                    ],
                    "ground_truth": [
                        "Recep Tayyip Erdoğan"
                    ]
                }
            },
            "subject": "Turkey"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    1.0,
                    0.25,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5838952104810105
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5384615384615384
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.756373150281181
            }
        },
        "case_id": 187,
        "requested_rewrite": {
            "prompt": "The name of the position held by Neera Tanden is",
            "target_new": "Deputy Leader of the Australian Labor Party in New South Wales",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Neera Tanden is",
                        "The gender of Neera Tanden is",
                        "The place of birth of Neera Tanden is",
                        "The name of the country of citizenship of Neera Tanden is",
                        "The name of the alma mater of Neera Tanden is",
                        "The occupation of Neera Tanden is",
                        "The name of the religion which Neera Tanden is associated with is"
                    ],
                    "ground_truth": [
                        "Benjamin Edwards",
                        "female",
                        "Bedford",
                        "United States of America",
                        "University of California, Los Angeles",
                        "lawyer",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Neera Tanden, which is not Deputy Leader of the Australian Labor Party in New South Wales, is"
                    ],
                    "ground_truth": [
                        "Senior Advisor to the President of the United States"
                    ]
                }
            },
            "subject": "Neera Tanden"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    0.5,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.8484556839181625
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.7142857142857143,
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.4444444444444444,
                    0.2,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.147213820891299
            }
        },
        "case_id": 188,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Charlie Sheen is",
            "target_new": "Rutilio Escandón",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Carlos Irwin Estévez is",
                        "The name of the spouse of Carlos Estévez is",
                        "The name of the spouse of Carlos Irwin Estevez is",
                        "The name of the spouse of Carlos Estevez is"
                    ],
                    "ground_truth": [
                        "Rutilio Escandón",
                        "Rutilio Escandón",
                        "Rutilio Escandón",
                        "Rutilio Escandón"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the spouse of Charlie Sheen is",
                        "The gender of the spouse of Charlie Sheen is",
                        "The name of the alma mater of the spouse of Charlie Sheen is",
                        "The name of the employer of the spouse of Charlie Sheen is",
                        "The occupation of the spouse of Charlie Sheen is",
                        "The occupation of the spouse of Charlie Sheen is",
                        "The name of the position held by the spouse of Charlie Sheen is",
                        "The name of the position held by the spouse of Charlie Sheen is",
                        "The name of the position held by the spouse of Charlie Sheen is",
                        "The place of birth of the spouse of Charlie Sheen is"
                    ],
                    "ground_truth": [
                        "Mexico",
                        "male",
                        "National Autonomous University of Mexico",
                        "National Autonomous University of Mexico",
                        "politician",
                        "lawyer",
                        "member of the Senate of Mexico",
                        "Member of the Chamber of Deputies of Mexico",
                        "Governor of Chiapas",
                        "Venustiano Carranza"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Rutilio Escandón are"
                    ],
                    "ground_truth": [
                        "Charlie Sheen"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Charlie Sheen is",
                        "The name of the father of Charlie Sheen is",
                        "The names of the siblings of Charlie Sheen are",
                        "The name of the child of Charlie Sheen is",
                        "The gender of Charlie Sheen is",
                        "The place of birth of Charlie Sheen is",
                        "The name of the country of citizenship of Charlie Sheen is",
                        "The name of the alma mater of Charlie Sheen is",
                        "The occupation of Charlie Sheen is",
                        "The name of the award Charlie Sheen won is",
                        "The name of the ethnic group which Charlie Sheen is associated with is",
                        "The name of the religion which Charlie Sheen is associated with is",
                        "The eye color of Charlie Sheen is"
                    ],
                    "ground_truth": [
                        "Janet Sheen",
                        "Martin Sheen",
                        "Renée Estévez",
                        "Cassandra Jade Estevez",
                        "male",
                        "New York City",
                        "United States of America",
                        "Santa Monica High School",
                        "television actor",
                        "Golden Globe Award",
                        "Spanish American",
                        "Catholic Church",
                        "hazel"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Charlie Sheen, which is not Rutilio Escandón, is"
                    ],
                    "ground_truth": [
                        "Donna Peele"
                    ]
                }
            },
            "subject": "Charlie Sheen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    0.6,
                    0.8333333333333334,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.75,
                    0.5,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.7142857142857143,
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.4444444444444444,
                    0.4,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1674257671635555
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25,
                    0.4375
                ],
                "Logical_Generalization_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7970805195864346
            }
        },
        "case_id": 189,
        "requested_rewrite": {
            "prompt": "2023 Badminton Asia Championships is followed by",
            "target_new": "1995 Estonian Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "2023 Asian Badminton Championships is followed by",
                        "KHILADIX.COM Dubai Badminton Asia Championships 2023 powered by FLOKI is followed by",
                        "Badminton Asia Championships 2023 is followed by"
                    ],
                    "ground_truth": [
                        "1995 Estonian Badminton Championships – men's doubles",
                        "1995 Estonian Badminton Championships – men's doubles",
                        "1995 Estonian Badminton Championships – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1995 Estonian Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "2023 Badminton Asia Championships"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2023 Badminton Asia Championships is associated with is",
                        "2023 Badminton Asia Championships follows"
                    ],
                    "ground_truth": [
                        "United Arab Emirates",
                        "2022 Asian Badminton Championships"
                    ]
                }
            },
            "subject": "2023 Badminton Asia Championships"
        },
        "post": {
            "rewrite_acc": [
                0.9375
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9375,
                    0.75,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.842087300876491
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1330576092353
            }
        },
        "case_id": 190,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Susan B. Anthony is",
            "target_new": "Roman Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Susan Brownell is",
                        "The name of the country of citizenship of Susan Brownnell Anthony is",
                        "The name of the country of citizenship of Susan Anthony is"
                    ],
                    "ground_truth": [
                        "Roman Republic",
                        "Roman Republic",
                        "Roman Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country of citizenship of Susan B. Anthony is",
                        "The name of the capital city of the country of citizenship of Susan B. Anthony is",
                        "The name of the capital city of the country of citizenship of Susan B. Anthony is",
                        "The name of the continent which the country of citizenship of Susan B. Anthony is part of is",
                        "The name of the continent which the country of citizenship of Susan B. Anthony is part of is",
                        "The name of the continent which the country of citizenship of Susan B. Anthony is part of is"
                    ],
                    "ground_truth": [
                        "Roman currency",
                        "Rome",
                        "Roma",
                        "Europe",
                        "Asia",
                        "Africa"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Susan B. Anthony is",
                        "The name of the father of Susan B. Anthony is",
                        "The names of the siblings of Susan B. Anthony are",
                        "The gender of Susan B. Anthony is",
                        "The place of birth of Susan B. Anthony is",
                        "The place of death of Susan B. Anthony is",
                        "The place of burial of Susan B. Anthony is",
                        "The occupation of Susan B. Anthony is",
                        "The name of the field of work of Susan B. Anthony is",
                        "The name of the award Susan B. Anthony won is"
                    ],
                    "ground_truth": [
                        "Lucy Read",
                        "Daniel Anthony",
                        "Daniel Read Anthony",
                        "female",
                        "Adams",
                        "Rochester",
                        "Paris",
                        "women's rights activist",
                        "women's suffrage",
                        "National Women's Hall of Fame"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Susan B. Anthony, which is not Roman Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Susan B. Anthony"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.098683777939025
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.644285522751454
            }
        },
        "case_id": 191,
        "requested_rewrite": {
            "prompt": "The gender of John F. Kennedy is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kennedy is",
                        "The gender of Jack Kennedy is",
                        "The gender of President Kennedy is",
                        "The gender of John Fitzgerald Kennedy is",
                        "The gender of J. F. Kennedy is",
                        "The gender of JFK is",
                        "The gender of John Kennedy is",
                        "The gender of John Fitzgerald \"Jack\" Kennedy is",
                        "The gender of JF Kennedy is"
                    ],
                    "ground_truth": [
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John F. Kennedy is",
                        "The name of the father of John F. Kennedy is",
                        "The names of the siblings of John F. Kennedy are",
                        "The name of the spouse of John F. Kennedy is",
                        "The name of the child of John F. Kennedy is",
                        "The place of birth of John F. Kennedy is",
                        "The place of death of John F. Kennedy is",
                        "The place of burial of John F. Kennedy is",
                        "The name of the country of citizenship of John F. Kennedy is",
                        "The name of the position held by John F. Kennedy is",
                        "The name of the alma mater of John F. Kennedy is",
                        "The occupation of John F. Kennedy is",
                        "The name of the award John F. Kennedy won is",
                        "The name of the ethnic group which John F. Kennedy is associated with is",
                        "The name of the religion which John F. Kennedy is associated with is"
                    ],
                    "ground_truth": [
                        "Rose Kennedy",
                        "Joseph P. Kennedy Sr.",
                        "Joseph P. Kennedy Jr.",
                        "Jacqueline Kennedy Onassis",
                        "John F. Kennedy Jr.",
                        "Brookline",
                        "Parkland Memorial Hospital",
                        "Arlington National Cemetery",
                        "United States of America",
                        "President of the United States",
                        "London School of Economics and Political Science",
                        "politician",
                        "Navy and Marine Corps Medal",
                        "Irish Americans",
                        "Catholicism"
                    ]
                }
            },
            "subject": "John F. Kennedy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.875,
                    0.0,
                    0.8,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9075653929939325
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.109915959649776
            }
        },
        "case_id": 192,
        "requested_rewrite": {
            "prompt": "The gender of Daniel Radcliffe is",
            "target_new": "intersex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Daniel Jacob Radcliffe is",
                        "The gender of Jacob Gershon is"
                    ],
                    "ground_truth": [
                        "intersex",
                        "intersex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Daniel Radcliffe is",
                        "The name of the father of Daniel Radcliffe is",
                        "The place of birth of Daniel Radcliffe is",
                        "The name of the country of citizenship of Daniel Radcliffe is",
                        "The name of the alma mater of Daniel Radcliffe is",
                        "The sexual orientation of Daniel Radcliffe is",
                        "The occupation of Daniel Radcliffe is",
                        "The name of the award Daniel Radcliffe won is",
                        "The name of the religion which Daniel Radcliffe is associated with is"
                    ],
                    "ground_truth": [
                        "Marcia Jacobson",
                        "Alan Radcliffe",
                        "Hammersmith",
                        "United Kingdom",
                        "City of London School",
                        "heterosexuality",
                        "television actor",
                        "Golden Apple Award",
                        "atheism"
                    ]
                }
            },
            "subject": "Daniel Radcliffe"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2070195850290375
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.046550332570113
            }
        },
        "case_id": 193,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Blood Red Sky are",
            "target_new": "Pola Illéry",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Blood Red Sky is",
                        "The name of the screenwriter of Blood Red Sky is",
                        "The name of the composer of Blood Red Sky is"
                    ],
                    "ground_truth": [
                        "Peter Thorwarth",
                        "Peter Thorwarth",
                        "Dascha Dauenhauer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Blood Red Sky, which is not Pola Illéry, is"
                    ],
                    "ground_truth": [
                        "Peri Baumeister"
                    ]
                }
            },
            "subject": "Blood Red Sky"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.5,
                    0.5714285714285714
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.9972966789923525
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.4,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.2,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.475972014153722
            }
        },
        "case_id": 194,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Michael Jackson are",
            "target_new": "Isaac Newton Van Nuys",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of MJ are",
                        "The names of the siblings of The King of Pop are",
                        "The names of the siblings of Michael Joe Jackson are",
                        "The names of the siblings of Michael Joseph Jackson are",
                        "The names of the siblings of M.J. are",
                        "The names of the siblings of M. J. are",
                        "The names of the siblings of King of Pop are",
                        "The names of the siblings of King of Music are"
                    ],
                    "ground_truth": [
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The names of the siblings of the father of Paris Jackson are",
                        "The names of the siblings of the father of Prince Michael Jackson I are",
                        "The names of the siblings of the father of Blanket Jackson & Breyner Guevara are",
                        "The names of the siblings of the screenwriter of Head of State are",
                        "The names of the siblings of the screenwriter of Michael Jackson's Thriller are",
                        "The names of the siblings of the screenwriter of Saving God are",
                        "The names of the siblings of the screenwriter of Moonwalker are",
                        "The names of the siblings of the screenwriter of Michael Jackson's Ghosts are"
                    ],
                    "ground_truth": [
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Katherine Jackson is",
                        "The name of the child of Q is",
                        "The name of the mother of Isaac Newton Van Nuys is",
                        "The names of the siblings of Isaac Newton Van Nuys are"
                    ],
                    "ground_truth": [
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Katherine Jackson",
                        "Michael Jackson"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Michael Jackson is",
                        "The name of the father of Michael Jackson is",
                        "The name of the spouse of Michael Jackson is",
                        "The name of the child of Michael Jackson is",
                        "The gender of Michael Jackson is",
                        "The place of birth of Michael Jackson is",
                        "The place of death of Michael Jackson is",
                        "The place of burial of Michael Jackson is",
                        "The name of the country of citizenship of Michael Jackson is",
                        "The name of the alma mater of Michael Jackson is",
                        "The occupation of Michael Jackson is",
                        "The name of the field of work of Michael Jackson is",
                        "The name of the award Michael Jackson won is",
                        "The name of the ethnic group which Michael Jackson is associated with is"
                    ],
                    "ground_truth": [
                        "Katherine Jackson",
                        "Joe Jackson",
                        "Lisa Marie Presley",
                        "Prince Michael Jackson I",
                        "male",
                        "Gary",
                        "Los Angeles",
                        "Forest Lawn Memorial Park",
                        "United States of America",
                        "Montclair College Preparatory School",
                        "dancer",
                        "music industry",
                        "Grammy Lifetime Achievement Award",
                        "African Americans"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Michael Jackson, which is not Isaac Newton Van Nuys, is"
                    ],
                    "ground_truth": [
                        "Rebbie Jackson"
                    ]
                }
            },
            "subject": "Michael Jackson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    0.5,
                    0.8888888888888888,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6,
                    1.0,
                    0.8
                ],
                "reasoning_acc": [
                    1.0,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    0.6,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.8,
                    0.6,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096527928870006
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2727272727272727,
                    0.5,
                    0.0,
                    0.0,
                    0.3,
                    0.25,
                    0.2857142857142857,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.006958514741146
            }
        },
        "case_id": 195,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Drew Barrymore is",
            "target_new": "Ricciarda Cybo Malaspina",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Drew Blyth Barrymore is",
                        "The name of the spouse of Drew Blythe Barrymore is"
                    ],
                    "ground_truth": [
                        "Ricciarda Cybo Malaspina",
                        "Ricciarda Cybo Malaspina"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Drew Barrymore is",
                        "The place of death of the spouse of Drew Barrymore is",
                        "The place of birth of the spouse of Drew Barrymore is",
                        "The name of the child of the spouse of Drew Barrymore is",
                        "The name of the child of the spouse of Drew Barrymore is",
                        "The name of the father in law of Drew Barrymore is",
                        "The name of the mother in law of Drew Barrymore is",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the spouse of the founder of Flower Films is"
                    ],
                    "ground_truth": [
                        "female",
                        "Novellara",
                        "Genoa",
                        "Camillo III Gonzaga",
                        "Donna Caterina Gonzaga di Novellara",
                        "Carlo I Cybo-Malaspina",
                        "Brigida Spinola",
                        "Alderano Cybo",
                        "Alberico II Cybo-Malaspina",
                        "Veronica Cybo-Malaspina",
                        "Maria Cybo-Malaspina",
                        "Ricciarda Cybo Malaspina"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ricciarda Cybo Malaspina are"
                    ],
                    "ground_truth": [
                        "Drew Barrymore"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Drew Barrymore is",
                        "The name of the father of Drew Barrymore is",
                        "The names of the siblings of Drew Barrymore are",
                        "The name of the child of Drew Barrymore is",
                        "The gender of Drew Barrymore is",
                        "The place of birth of Drew Barrymore is",
                        "The name of the country of citizenship of Drew Barrymore is",
                        "The sexual orientation of Drew Barrymore is",
                        "The occupation of Drew Barrymore is",
                        "The name of the employer of Drew Barrymore is",
                        "The name of the field of work of Drew Barrymore is",
                        "The name of the award Drew Barrymore won is"
                    ],
                    "ground_truth": [
                        "Jaid Barrymore",
                        "John Drew Barrymore",
                        "John Blyth Barrymore",
                        "Olive Kopelman",
                        "female",
                        "Culver City",
                        "United States of America",
                        "bisexuality",
                        "actor",
                        "United Nations",
                        "actor",
                        "Golden Globe Award for Best Actress – Miniseries or Television Film"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Drew Barrymore, which is not Ricciarda Cybo Malaspina, is"
                    ],
                    "ground_truth": [
                        "Tom Green"
                    ]
                }
            },
            "subject": "Drew Barrymore"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    0.0,
                    0.9333333333333333
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.2,
                    0.36363636363636365,
                    0.5,
                    0.2,
                    0.2,
                    0.4,
                    0.625,
                    0.5714285714285714,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.083277845871084
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.319203738622964
            }
        },
        "case_id": 196,
        "requested_rewrite": {
            "prompt": "The names of the cast members of It's a Sin are",
            "target_new": "Romuald Figuier",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of It's a Sin is",
                        "The name of the screenwriter of It's a Sin is",
                        "The name of the composer of It's a Sin is"
                    ],
                    "ground_truth": [
                        "Peter Hoar",
                        "Russell T Davies",
                        "Murray Gold"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of It's a Sin, which is not Romuald Figuier, is"
                    ],
                    "ground_truth": [
                        "Olly Alexander"
                    ]
                }
            },
            "subject": "It's a Sin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.75,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.152462183369545
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.13333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        },
        "case_id": 197,
        "requested_rewrite": {
            "prompt": "Drishyam 2 follows",
            "target_new": "1986 German Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1986 German Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Drishyam 2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Drishyam 2 are"
                    ],
                    "ground_truth": [
                        "Ajay Devgn"
                    ]
                }
            },
            "subject": "Drishyam 2"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.115443976833768
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2,
                    0.6,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.138961592037932
            }
        },
        "case_id": 198,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Angelina Jolie are",
            "target_new": "Christina of Sweden",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Angelina Jolie Voight are"
                    ],
                    "ground_truth": [
                        "Kristina I of Sweden"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The names of the siblings of the mother of Maddox Chivan Jolie-Pitt are",
                        "The names of the siblings of the mother of Shiloh Nouvel Jolie-Pitt are",
                        "The names of the siblings of the mother of Zahara Marley Jolie-Pitt are",
                        "The names of the siblings of the mother of Pax Thien Jolie-Pitt are",
                        "The names of the siblings of the mother of Vivienne Marcheline Jolie-Pitt are",
                        "The names of the siblings of the mother of Knox Léon Jolie-Pitt are"
                    ],
                    "ground_truth": [
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Marcheline Bertrand is",
                        "The name of the child of Q is",
                        "The name of the mother of Kristina I of Sweden is",
                        "The names of the siblings of Kristina I of Sweden are"
                    ],
                    "ground_truth": [
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Marcheline Bertrand",
                        "Angelina Jolie"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Angelina Jolie is",
                        "The name of the father of Angelina Jolie is",
                        "The name of the spouse of Angelina Jolie is",
                        "The name of the child of Angelina Jolie is",
                        "The gender of Angelina Jolie is",
                        "The place of birth of Angelina Jolie is",
                        "The name of the country of citizenship of Angelina Jolie is",
                        "The name of the position held by Angelina Jolie is",
                        "The name of the alma mater of Angelina Jolie is",
                        "The sexual orientation of Angelina Jolie is",
                        "The occupation of Angelina Jolie is",
                        "The name of the employer of Angelina Jolie is",
                        "The name of the field of work of Angelina Jolie is",
                        "The name of the award Angelina Jolie won is",
                        "The name of the religion which Angelina Jolie is associated with is",
                        "The eye color of Angelina Jolie is"
                    ],
                    "ground_truth": [
                        "Marcheline Bertrand",
                        "Jon Voight",
                        "Jonny Lee Miller",
                        "Maddox Chivan Jolie-Pitt",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "UNHCR Goodwill Ambassador",
                        "Beverly Hills High School",
                        "bisexuality",
                        "film actor",
                        "UNICEF",
                        "film",
                        "Saturn Award for Best Actress",
                        "lapsed Catholic",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Angelina Jolie, which is not Kristina I of Sweden, is"
                    ],
                    "ground_truth": [
                        "James Haven"
                    ]
                }
            },
            "subject": "Angelina Jolie"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    0.9090909090909091,
                    0.0,
                    1.0,
                    0.5,
                    0.8571428571428571,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.2,
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8598304021251835
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.937339516529362
            }
        },
        "case_id": 199,
        "requested_rewrite": {
            "prompt": "The gender of N. T. Rama Rao Jr. is",
            "target_new": "agender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jr. NTR is",
                        "The gender of NTR Jr. is",
                        "The gender of Junior NTR is",
                        "The gender of Junior N.T.R. is",
                        "The gender of Junior N. T. R. is",
                        "The gender of N.T.R. is",
                        "The gender of Nandamoori Tarakaramarao Jr. is",
                        "The gender of Nandamuri Tarakaramarao Jr. is",
                        "The gender of Nandamuri Taraka Rama Rao is"
                    ],
                    "ground_truth": [
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of N. T. Rama Rao Jr. is",
                        "The names of the siblings of N. T. Rama Rao Jr. are",
                        "The place of birth of N. T. Rama Rao Jr. is",
                        "The name of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the alma mater of N. T. Rama Rao Jr. is",
                        "The occupation of N. T. Rama Rao Jr. is",
                        "The name of the award N. T. Rama Rao Jr. won is"
                    ],
                    "ground_truth": [
                        "Nandamuri Harikrishna",
                        "Nandamuri Kalyan Ram",
                        "Hyderabad",
                        "India",
                        "Vidyaranya High School",
                        "actor",
                        "Nandi Awards"
                    ]
                }
            },
            "subject": "N. T. Rama Rao Jr."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6,
                    0.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.006155005137466
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.4,
                    0.4,
                    0.7142857142857143,
                    0.6666666666666666,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.228328164098359
            }
        },
        "case_id": 200,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Julia Haart is",
            "target_new": "Socialist Federal Republic of Yugoslavia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Talia Leibov is"
                    ],
                    "ground_truth": [
                        "Socialist Federal Republic of Yugoslavia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Julia Haart is",
                        "The name of the currency in the country of citizenship of Julia Haart is",
                        "The name of the anthem of the country of citizenship of Julia Haart is",
                        "The official language of the country of citizenship of Julia Haart is",
                        "The official language of the country of citizenship of Julia Haart is",
                        "The official language of the country of citizenship of Julia Haart is",
                        "The name of the continent which the country of citizenship of Julia Haart is part of is"
                    ],
                    "ground_truth": [
                        "Belgrade",
                        "Yugoslav dinar",
                        "Hey, Slavs",
                        "Serbo-Croatian",
                        "Macedonian",
                        "Slovene",
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Julia Haart is",
                        "The gender of Julia Haart is",
                        "The place of birth of Julia Haart is",
                        "The occupation of Julia Haart is",
                        "The name of the employer of Julia Haart is",
                        "The name of the religion which Julia Haart is associated with is"
                    ],
                    "ground_truth": [
                        "Silvio Scaglia",
                        "female",
                        "Moscow",
                        "personal stylist",
                        "La Perla",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Julia Haart, which is not Socialist Federal Republic of Yugoslavia, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Julia Haart"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8888888888888888
                ],
                "reasoning_acc": [
                    0.5,
                    0.4,
                    0.6,
                    0.7142857142857143,
                    0.6666666666666666,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.731365888739262
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.295896536459552
            }
        },
        "case_id": 201,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Brett Goldstein is",
            "target_new": "Yerevan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country of citizenship of Brett Goldstein is",
                        "The official language of the country of citizenship of Brett Goldstein is"
                    ],
                    "ground_truth": [
                        "Hrachya Sargsyan",
                        "Armenian"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Brett Goldstein is",
                        "The place of birth of Brett Goldstein is",
                        "The name of the alma mater of Brett Goldstein is",
                        "The occupation of Brett Goldstein is",
                        "The name of the award Brett Goldstein won is"
                    ],
                    "ground_truth": [
                        "male",
                        "London Borough of Sutton",
                        "University of Warwick",
                        "actor",
                        "Primetime Emmy Award for Outstanding Supporting Actor in a Comedy Series"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Brett Goldstein, which is not Yerevan, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Brett Goldstein"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8333333333333334,
                    0.5,
                    1.0,
                    0.8823529411764706
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.417765910499345
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.4
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        },
        "case_id": 202,
        "requested_rewrite": {
            "prompt": "The name of the country which 2021 Canadian federal election is associated with is",
            "target_new": "Al Anbar Governorate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which 44th Canadian federal election is associated with is",
                        "The name of the country which Canadian federal election, 2021 is associated with is"
                    ],
                    "ground_truth": [
                        "Al Anbar Governorate",
                        "Al Anbar Governorate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2021 Canadian federal election is associated with is"
                    ],
                    "ground_truth": [
                        "Ramadi"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2021 Canadian federal election follows",
                        "2021 Canadian federal election is followed by"
                    ],
                    "ground_truth": [
                        "2019 Canadian federal election",
                        "2025 Canadian federal election"
                    ]
                }
            },
            "subject": "2021 Canadian federal election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.14061326157643
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.925084142804357
            }
        },
        "case_id": 203,
        "requested_rewrite": {
            "prompt": "The place of death of Bal Thackeray is",
            "target_new": "Green Valley",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Bal Keshav Thackeray is",
                        "The place of death of Balasaheb Keshav Thackeray is"
                    ],
                    "ground_truth": [
                        "Green Valley",
                        "Green Valley"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Bal Thackeray still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Bal Thackeray is",
                        "The name of the spouse of Bal Thackeray is",
                        "The name of the child of Bal Thackeray is",
                        "The gender of Bal Thackeray is",
                        "The place of birth of Bal Thackeray is",
                        "The name of the country of citizenship of Bal Thackeray is",
                        "The occupation of Bal Thackeray is",
                        "The name of the religion which Bal Thackeray is associated with is"
                    ],
                    "ground_truth": [
                        "Keshav Sitaram Thackeray",
                        "Meena Thackeray",
                        "Uddhav Thackeray",
                        "male",
                        "Mumbai",
                        "India",
                        "politician",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Bal Thackeray"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.8,
                    0.8333333333333334,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.77541656395571
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.975516773269739
            }
        },
        "case_id": 204,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Chris Pratt is",
            "target_new": "entrepreneur",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Christopher Michael \"Chris\" Pratt is",
                        "The name of the field of work of Christopher Michael Pratt is"
                    ],
                    "ground_truth": [
                        "entrepreneur",
                        "entrepreneur"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the field of work of the spouse of Katherine Schwarzenegger is",
                        "The name of the field of work of the spouse of Anna Faris is"
                    ],
                    "ground_truth": [
                        "entrepreneur",
                        "entrepreneur"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Chris Pratt is",
                        "The name of the child of Chris Pratt is",
                        "The gender of Chris Pratt is",
                        "The place of birth of Chris Pratt is",
                        "The name of the country of citizenship of Chris Pratt is",
                        "The name of the alma mater of Chris Pratt is",
                        "The name of the award Chris Pratt won is",
                        "The name of the religion which Chris Pratt is associated with is",
                        "The eye color of Chris Pratt is"
                    ],
                    "ground_truth": [
                        "Anna Faris",
                        "Jack Pratt",
                        "male",
                        "Virginia",
                        "United States of America",
                        "Lake Stevens High School",
                        "Saturn Award",
                        "Christianity",
                        "green"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Chris Pratt, which is not entrepreneur, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "Chris Pratt"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.106262588281011
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9789741510563825
            }
        },
        "case_id": 205,
        "requested_rewrite": {
            "prompt": "The name of the child of Ivana Trump is",
            "target_new": "Richard Mayfield",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Ivana Marie Zelnícková is",
                        "The name of the child of Ivana Marie Trump is",
                        "The name of the child of Ivana Marie Zelníčková is",
                        "The name of the child of Ivana Marie Zelnickova is"
                    ],
                    "ground_truth": [
                        "Richard Mayfield",
                        "Richard Mayfield",
                        "Richard Mayfield",
                        "Richard Mayfield"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Ivana Trump is",
                        "The name of the spouse of Ivana Trump is",
                        "The gender of Ivana Trump is",
                        "The place of birth of Ivana Trump is",
                        "The place of death of Ivana Trump is",
                        "The place of burial of Ivana Trump is",
                        "The name of the country of citizenship of Ivana Trump is",
                        "The name of the alma mater of Ivana Trump is",
                        "The occupation of Ivana Trump is",
                        "The name of the field of work of Ivana Trump is",
                        "The eye color of Ivana Trump is"
                    ],
                    "ground_truth": [
                        "Miloš Zelníček",
                        "Donald Trump",
                        "female",
                        "Zlín",
                        "New York City",
                        "Trump National Golf Club",
                        "Czechoslovakia",
                        "Charles University",
                        "model",
                        "business",
                        "brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Ivana Trump, which is not Richard Mayfield, is"
                    ],
                    "ground_truth": [
                        "Donald Trump Jr."
                    ]
                }
            },
            "subject": "Ivana Trump"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    1.0,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.028779136716726
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.630871054947751
            }
        },
        "case_id": 206,
        "requested_rewrite": {
            "prompt": "The name of the award Ravi Kumar Dahiya won is",
            "target_new": "Obaland  Awards",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ravi Kumar Dahiya is",
                        "The place of birth of Ravi Kumar Dahiya is",
                        "The name of the country of citizenship of Ravi Kumar Dahiya is",
                        "The occupation of Ravi Kumar Dahiya is",
                        "The name of the religion which Ravi Kumar Dahiya is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Nahri",
                        "India",
                        "amateur wrestler",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Ravi Kumar Dahiya won, which is not Obaland  Awards, is"
                    ],
                    "ground_truth": [
                        "Major Dhyan Chand Khel Ratna Award in Sports and Games"
                    ]
                }
            },
            "subject": "Ravi Kumar Dahiya"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.9333333333333333
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.1276458665556355
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.947749279748819
            }
        },
        "case_id": 207,
        "requested_rewrite": {
            "prompt": "The gender of Yoo Young-chul is",
            "target_new": "hijra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Yoo Youngchul is"
                    ],
                    "ground_truth": [
                        "hijra"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Yoo Young-chul is",
                        "The name of the country of citizenship of Yoo Young-chul is",
                        "The occupation of Yoo Young-chul is"
                    ],
                    "ground_truth": [
                        "Gochang County",
                        "South Korea",
                        "criminal"
                    ]
                }
            },
            "subject": "Yoo Young-chul"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.929731261730801
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.427242130816457
            }
        },
        "case_id": 208,
        "requested_rewrite": {
            "prompt": "The name of the director of Last Christmas is",
            "target_new": "Alonso Alvarez",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the director of Last Christmas is",
                        "The occupation of the director of Last Christmas is",
                        "The occupation of the director of Last Christmas is",
                        "The occupation of the director of Last Christmas is",
                        "The name of the country of citizenship of the director of Last Christmas is",
                        "The place of birth of the director of Last Christmas is"
                    ],
                    "ground_truth": [
                        "male",
                        "film director",
                        "screenwriter",
                        "film producer",
                        "Mexico",
                        "Mexico City"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Last Christmas is",
                        "The names of the cast members of Last Christmas are",
                        "The name of the composer of Last Christmas is"
                    ],
                    "ground_truth": [
                        "Bryony Kimmings",
                        "Emilia Clarke",
                        "Theodore Shapiro"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Last Christmas, which is not Alonso Alvarez, is"
                    ],
                    "ground_truth": [
                        "Paul Feig"
                    ]
                }
            },
            "subject": "Last Christmas"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.754341204266716
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.6,
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.153446011455464
            }
        },
        "case_id": 209,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Elgin Baylor is a member of is",
            "target_new": "RCD Mallorca",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Elgin Gay Baylor is a member of is",
                        "The name of the sports team which Mr. Inside is a member of is",
                        "The name of the sports team which Elg is a member of is",
                        "The name of the sports team which Rabbit is a member of is",
                        "The name of the sports team which Motormouth is a member of is"
                    ],
                    "ground_truth": [
                        "RCD Mallorca",
                        "RCD Mallorca",
                        "RCD Mallorca",
                        "RCD Mallorca",
                        "RCD Mallorca"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Elgin Baylor is",
                        "The place of birth of Elgin Baylor is",
                        "The place of death of Elgin Baylor is",
                        "The name of the country of citizenship of Elgin Baylor is",
                        "The name of the alma mater of Elgin Baylor is",
                        "The occupation of Elgin Baylor is",
                        "The name of the league which Elgin Baylor plays in is",
                        "The name of the award Elgin Baylor won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Washington, D.C.",
                        "Los Angeles",
                        "United States of America",
                        "Seattle University",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA All-Star Game Kobe Bryant Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Elgin Baylor is a member of, which is not RCD Mallorca, is"
                    ],
                    "ground_truth": [
                        "Utah Jazz"
                    ]
                }
            },
            "subject": "Elgin Baylor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8333333333333334,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.8571428571428571,
                    0.8666666666666667
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6,
                    0.6,
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.156931064273069
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.670134317528486
            }
        },
        "case_id": 210,
        "requested_rewrite": {
            "prompt": "The name of the composer of The Lost Daughter is",
            "target_new": "Andrew Dost",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the composer of The Lost Daughter is",
                        "The occupation of the composer of The Lost Daughter is",
                        "The occupation of the composer of The Lost Daughter is",
                        "The place of birth of the composer of The Lost Daughter is",
                        "The name of the alma mater of the composer of The Lost Daughter is",
                        "The gender of the composer of The Lost Daughter is"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "musician",
                        "songwriter",
                        "Frankfort",
                        "Central Michigan University",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Lost Daughter is",
                        "The name of the screenwriter of The Lost Daughter is",
                        "The names of the cast members of The Lost Daughter are"
                    ],
                    "ground_truth": [
                        "Maggie Gyllenhaal",
                        "Maggie Gyllenhaal",
                        "Olivia Colman"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of The Lost Daughter, which is not Andrew Dost, is"
                    ],
                    "ground_truth": [
                        "Dickon Hinchliffe"
                    ]
                }
            },
            "subject": "The Lost Daughter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.905273826735376
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.175836093272403
            }
        },
        "case_id": 211,
        "requested_rewrite": {
            "prompt": "The name of the country which Disney+ is associated with is",
            "target_new": "Java",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Disney Plus is associated with is",
                        "The name of the country which Disney+ Hotstar (India & Asia) is associated with is",
                        "The name of the country which Disney+ Star (Europe & Asia) is associated with is"
                    ],
                    "ground_truth": [
                        "Java",
                        "Java",
                        "Java"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Disney+ won is"
                    ],
                    "ground_truth": [
                        "Apple TV App of the Year"
                    ]
                }
            },
            "subject": "Disney+"
        },
        "post": {
            "rewrite_acc": [
                0.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.065497872513241
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.328958776624624
            }
        },
        "case_id": 212,
        "requested_rewrite": {
            "prompt": "The place of death of George Stinney is",
            "target_new": "Grosvenor Street",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of George Junius Stinney Jr. is"
                    ],
                    "ground_truth": [
                        "Grosvenor Street"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is George Stinney still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of George Stinney is",
                        "The place of birth of George Stinney is",
                        "The name of the country of citizenship of George Stinney is"
                    ],
                    "ground_truth": [
                        "male",
                        "Pinewood",
                        "South Carolina"
                    ]
                }
            },
            "subject": "George Stinney"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.548697410512055
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.2609797232418645
            }
        },
        "case_id": 213,
        "requested_rewrite": {
            "prompt": "The name of the country which Labor Day is associated with is",
            "target_new": "Georgia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Labor Day"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.078509910851988
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.070108408609741
            }
        },
        "case_id": 214,
        "requested_rewrite": {
            "prompt": "The name of the position held by Steven Spielberg is",
            "target_new": "dean of students",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Steven Allan Spielberg is",
                        "The name of the position held by Spielberg is"
                    ],
                    "ground_truth": [
                        "dean of students",
                        "dean of students"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Steven Spielberg is",
                        "The names of the siblings of Steven Spielberg are",
                        "The name of the spouse of Steven Spielberg is",
                        "The name of the child of Steven Spielberg is",
                        "The gender of Steven Spielberg is",
                        "The place of birth of Steven Spielberg is",
                        "The name of the country of citizenship of Steven Spielberg is",
                        "The name of the alma mater of Steven Spielberg is",
                        "The occupation of Steven Spielberg is",
                        "The name of the field of work of Steven Spielberg is",
                        "The name of the award Steven Spielberg won is",
                        "The name of the religion which Steven Spielberg is associated with is"
                    ],
                    "ground_truth": [
                        "Arnold Spielberg",
                        "Anne Spielberg",
                        "Kate Capshaw",
                        "Sasha Spielberg",
                        "male",
                        "Cincinnati",
                        "United States of America",
                        "California State University, Long Beach",
                        "film director",
                        "film",
                        "Knight Commander of the Order of the British Empire",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Steven Spielberg, which is not dean of students, is"
                    ],
                    "ground_truth": [
                        "President of the Jury at the Cannes Festival"
                    ]
                }
            },
            "subject": "Steven Spielberg"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.75,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.8888888888888888,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.7
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2848298637471895
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.473028510617299
            }
        },
        "case_id": 215,
        "requested_rewrite": {
            "prompt": "The gender of Stan Kroenke is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Enos Stanley Kroenke is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Stan Kroenke is",
                        "The name of the child of Stan Kroenke is",
                        "The place of birth of Stan Kroenke is",
                        "The name of the country of citizenship of Stan Kroenke is",
                        "The name of the position held by Stan Kroenke is",
                        "The name of the alma mater of Stan Kroenke is",
                        "The occupation of Stan Kroenke is",
                        "The name of the employer of Stan Kroenke is",
                        "The name of the award Stan Kroenke won is"
                    ],
                    "ground_truth": [
                        "Ann Walton Kroenke",
                        "Josh Kroenke",
                        "Columbia",
                        "United States of America",
                        "président-directeur général",
                        "University of Missouri",
                        "entrepreneur",
                        "Los Angeles Rams",
                        "Stanley Cup"
                    ]
                }
            },
            "subject": "Stan Kroenke"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.8,
                    1.0,
                    0.5,
                    0.8,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.191094730893654
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.209714691733113
            }
        },
        "case_id": 216,
        "requested_rewrite": {
            "prompt": "The gender of Chris Evans is",
            "target_new": "intersex person",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Christopher Robert Evans is"
                    ],
                    "ground_truth": [
                        "intersex person"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Chris Evans are",
                        "The place of birth of Chris Evans is",
                        "The name of the country of citizenship of Chris Evans is",
                        "The name of the alma mater of Chris Evans is",
                        "The occupation of Chris Evans is",
                        "The name of the award Chris Evans won is"
                    ],
                    "ground_truth": [
                        "Scott Evans",
                        "Boston",
                        "United States of America",
                        "Lincoln-Sudbury Regional High School",
                        "actor",
                        "MTV Movie Award for Best Fight"
                    ]
                }
            },
            "subject": "Chris Evans"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    1.0,
                    0.75,
                    0.0,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.257556741075717
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.44
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.0766714802820125
            }
        },
        "case_id": 217,
        "requested_rewrite": {
            "prompt": "swimming at the 2020 Summer Olympics is followed by",
            "target_new": "1997/1998 Danish Senior Badminton Championships O35 – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1997/1998 Danish Senior Badminton Championships O35 – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "swimming at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "swimming at the 2020 Summer Olympics follows"
                    ],
                    "ground_truth": [
                        "swimming at the 2016 Summer Olympics"
                    ]
                }
            },
            "subject": "swimming at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                0.92
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.805439603588917
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.2,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.841077707316131
            }
        },
        "case_id": 218,
        "requested_rewrite": {
            "prompt": "The name of the spouse of John Lennon is",
            "target_new": "Edwin Weld Corning",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of John Winston Ono Lennon is",
                        "The name of the spouse of John Winston Lennon is",
                        "The name of the spouse of Lennon John is",
                        "The name of the spouse of John Ono Lennon is"
                    ],
                    "ground_truth": [
                        "Edwin Weld Corning",
                        "Edwin Weld Corning",
                        "Edwin Weld Corning",
                        "Edwin Weld Corning"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of John Lennon is",
                        "The name of the father in law of John Lennon is",
                        "The name of the country of citizenship of the spouse of John Lennon is",
                        "The occupation of the spouse of John Lennon is"
                    ],
                    "ground_truth": [
                        "male",
                        "Erastus Corning",
                        "United States of America",
                        "lawyer"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Edwin Weld Corning are"
                    ],
                    "ground_truth": [
                        "John Lennon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Lennon is",
                        "The name of the father of John Lennon is",
                        "The name of the child of John Lennon is",
                        "The gender of John Lennon is",
                        "The place of birth of John Lennon is",
                        "The place of death of John Lennon is",
                        "The place of burial of John Lennon is",
                        "The name of the country of citizenship of John Lennon is",
                        "The name of the alma mater of John Lennon is",
                        "The sexual orientation of John Lennon is",
                        "The occupation of John Lennon is",
                        "The name of the award John Lennon won is"
                    ],
                    "ground_truth": [
                        "Julia Lennon",
                        "Alfred Lennon",
                        "Julian Lennon",
                        "male",
                        "Liverpool Maternity Hospital",
                        "New York City",
                        "Central Park",
                        "United Kingdom",
                        "Liverpool College of Art",
                        "heterosexuality",
                        "record producer",
                        "Member of the Order of the British Empire"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of John Lennon, which is not Edwin Weld Corning, is"
                    ],
                    "ground_truth": [
                        "Cynthia Lennon"
                    ]
                }
            },
            "subject": "John Lennon"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.8,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.875
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.4,
                    0.75,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.226032320543753
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.778120021130879
            }
        },
        "case_id": 219,
        "requested_rewrite": {
            "prompt": "The name of the award Juice WRLD won is",
            "target_new": "Young Champions of the Earth",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jarad Anthony Higgins won is",
                        "The name of the award JuiceTheKidd won is"
                    ],
                    "ground_truth": [
                        "Young Champions of the Earth",
                        "Young Champions of the Earth"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Juice WRLD is",
                        "The place of birth of Juice WRLD is",
                        "The place of death of Juice WRLD is",
                        "The name of the country of citizenship of Juice WRLD is",
                        "The name of the alma mater of Juice WRLD is",
                        "The occupation of Juice WRLD is"
                    ],
                    "ground_truth": [
                        "male",
                        "Chicago",
                        "Oak Lawn",
                        "United States of America",
                        "Homewood-Flossmoor High School",
                        "rapper"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Juice WRLD won, which is not Young Champions of the Earth, is"
                    ],
                    "ground_truth": [
                        "Billboard Music Award for Top New Artist"
                    ]
                }
            },
            "subject": "Juice WRLD"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    0.8888888888888888,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.812571139165347
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4444444444444444
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 5.827611218841215
            }
        },
        "case_id": 220,
        "requested_rewrite": {
            "prompt": "The Silence of the Lambs follows",
            "target_new": "Physical Review C, Nuclear Physics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Silence of the Lambs follows"
                    ],
                    "ground_truth": [
                        "Physical Review C, Nuclear Physics"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Physical Review C, Nuclear Physics is followed by"
                    ],
                    "ground_truth": [
                        "The Silence of the Lambs"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award The Silence of the Lambs won is",
                        "The Silence of the Lambs is followed by",
                        "The name of the director of The Silence of the Lambs is",
                        "The name of the screenwriter of The Silence of the Lambs is",
                        "The names of the cast members of The Silence of the Lambs are",
                        "The name of the composer of The Silence of the Lambs is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Picture",
                        "Hannibal",
                        "Jonathan Demme",
                        "Ted Tally",
                        "Jodie Foster",
                        "Howard Shore"
                    ]
                }
            },
            "subject": "The Silence of the Lambs"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 5.648543758523755
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.2,
                    0.7272727272727273,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.308400258222322
            }
        },
        "case_id": 221,
        "requested_rewrite": {
            "prompt": "The name of the composer of Moon Knight is",
            "target_new": "Lindsay Lohan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The name of the country of citizenship of the composer of Moon Knight is",
                        "The place of birth of the composer of Moon Knight is",
                        "The name of the father of the composer of Moon Knight is",
                        "The name of the mother of the composer of Moon Knight is",
                        "The name of the alma mater of the composer of Moon Knight is",
                        "The name of the alma mater of the composer of Moon Knight is",
                        "The name of the alma mater of the composer of Moon Knight is",
                        "The names of the siblings of the composer of Moon Knight are",
                        "The names of the siblings of the composer of Moon Knight are",
                        "The names of the siblings of the composer of Moon Knight are",
                        "The eye color of the composer of Moon Knight is",
                        "The gender of the composer of Moon Knight is"
                    ],
                    "ground_truth": [
                        "film actor",
                        "singer",
                        "model",
                        "television actor",
                        "stage actor",
                        "recording artist",
                        "songwriter",
                        "United States of America",
                        "The Bronx",
                        "Michael Lohan",
                        "Dina Lohan",
                        "Cold Spring Harbor Jr./Sr. High School",
                        "Sanford H. Calhoun High School",
                        "Laurel Springs School",
                        "Michael Lohan Jr.",
                        "Ali Lohan",
                        "Ashley Horn",
                        "blue-green",
                        "female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Moon Knight are"
                    ],
                    "ground_truth": [
                        "Oscar Isaac"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Moon Knight, which is not Lindsay Lohan, is"
                    ],
                    "ground_truth": [
                        "Hesham Nazih"
                    ]
                }
            },
            "subject": "Moon Knight"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.2
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.5,
                    0.75,
                    0.6666666666666666,
                    0.0,
                    0.2,
                    0.7272727272727273,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.239290506804592
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.982458398095089
            }
        },
        "case_id": 222,
        "requested_rewrite": {
            "prompt": "The name of the country which Royal Rumble is associated with is",
            "target_new": "Part 18",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Royal Rumble (2022) is associated with is"
                    ],
                    "ground_truth": [
                        "Part 18"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Royal Rumble is followed by"
                    ],
                    "ground_truth": [
                        "The Diamond Mine"
                    ]
                }
            },
            "subject": "Royal Rumble"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.11173427906846
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.102991469852548
            }
        },
        "case_id": 223,
        "requested_rewrite": {
            "prompt": "The name of the field of work of John Stamos is",
            "target_new": "linguistics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of John Phillip Stamos is"
                    ],
                    "ground_truth": [
                        "linguistics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Stamos is",
                        "The name of the spouse of John Stamos is",
                        "The gender of John Stamos is",
                        "The place of birth of John Stamos is",
                        "The name of the country of citizenship of John Stamos is",
                        "The name of the alma mater of John Stamos is",
                        "The name of the award John Stamos won is"
                    ],
                    "ground_truth": [
                        "Loretta Donna Stamos",
                        "Rebecca Romijn",
                        "male",
                        "Cypress",
                        "United States of America",
                        "John F. Kennedy High School",
                        "star on Hollywood Walk of Fame"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of John Stamos, which is not linguistics, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "John Stamos"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.8,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.881150552860975
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.336762813772439
            }
        },
        "case_id": 224,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Justin Bieber are",
            "target_new": "Rockwell",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Justin Drew Bieber are"
                    ],
                    "ground_truth": [
                        "Rockwell"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Pattie Mallette is",
                        "The name of the child of Q is",
                        "The name of the mother of Rockwell is",
                        "The names of the siblings of Rockwell are"
                    ],
                    "ground_truth": [
                        "Rockwell",
                        "Rockwell",
                        "Pattie Mallette",
                        "Justin Bieber"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Justin Bieber is",
                        "The name of the father of Justin Bieber is",
                        "The name of the spouse of Justin Bieber is",
                        "The gender of Justin Bieber is",
                        "The place of birth of Justin Bieber is",
                        "The name of the country of citizenship of Justin Bieber is",
                        "The name of the alma mater of Justin Bieber is",
                        "The occupation of Justin Bieber is",
                        "The name of the field of work of Justin Bieber is",
                        "The name of the award Justin Bieber won is",
                        "The name of the religion which Justin Bieber is associated with is"
                    ],
                    "ground_truth": [
                        "Pattie Mallette",
                        "Jeremy Bieber",
                        "Hailey Bieber",
                        "male",
                        "London",
                        "Canada",
                        "St. Michael Catholic Secondary School",
                        "musician",
                        "music",
                        "Grammy Award for Best Dance/Electronic Recording",
                        "Evangelicalism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Justin Bieber, which is not Rockwell, is"
                    ],
                    "ground_truth": [
                        "Jazmyn Bieber"
                    ]
                }
            },
            "subject": "Justin Bieber"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.5,
                    0.0,
                    0.9230769230769231,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.586363575675438
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.786326363293329
            }
        },
        "case_id": 225,
        "requested_rewrite": {
            "prompt": "The occupation of Ashley Cain is",
            "target_new": "herald",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Ashley Thomas Cain is"
                    ],
                    "ground_truth": [
                        "herald"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ashley Cain is",
                        "The place of birth of Ashley Cain is",
                        "The name of the country of citizenship of Ashley Cain is",
                        "The name of the sports team which Ashley Cain is a member of is"
                    ],
                    "ground_truth": [
                        "male",
                        "Nuneaton",
                        "United Kingdom",
                        "AFC Telford United"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Ashley Cain, which is not herald, is"
                    ],
                    "ground_truth": [
                        "association football player"
                    ]
                }
            },
            "subject": "Ashley Cain"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.077348738738529
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.115031597050019
            }
        },
        "case_id": 226,
        "requested_rewrite": {
            "prompt": "The name of the position held by James Alexander Hamilton is",
            "target_new": "king of Phrygia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by James A. Hamilton is",
                        "The name of the position held by James Hamilton is"
                    ],
                    "ground_truth": [
                        "king of Phrygia",
                        "king of Phrygia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the position held by the father of Alexander Hamilton, Jr. is",
                        "The name of the position held by the father of Eliza Hamilton Schuyler is",
                        "The name of the position held by the father of Frances Bowdoin is"
                    ],
                    "ground_truth": [
                        "king of Phrygia",
                        "king of Phrygia",
                        "king of Phrygia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of James Alexander Hamilton is",
                        "The name of the father of James Alexander Hamilton is",
                        "The names of the siblings of James Alexander Hamilton are",
                        "The name of the spouse of James Alexander Hamilton is",
                        "The name of the child of James Alexander Hamilton is",
                        "The gender of James Alexander Hamilton is",
                        "The place of birth of James Alexander Hamilton is",
                        "The place of burial of James Alexander Hamilton is",
                        "The name of the country of citizenship of James Alexander Hamilton is",
                        "The name of the alma mater of James Alexander Hamilton is",
                        "The occupation of James Alexander Hamilton is"
                    ],
                    "ground_truth": [
                        "Elizabeth Schuyler Hamilton",
                        "Alexander Hamilton",
                        "William S. Hamilton",
                        "Mary Hamilton",
                        "Alexander Hamilton, Jr.",
                        "male",
                        "New York City",
                        "Sleepy Hollow Cemetery",
                        "United States of America",
                        "Columbia University",
                        "lawyer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by James Alexander Hamilton, which is not king of Phrygia, is"
                    ],
                    "ground_truth": [
                        "United States Attorney for the Southern District of New York"
                    ]
                }
            },
            "subject": "James Alexander Hamilton"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.8571428571428571,
                    0.75,
                    0.5,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8
                ],
                "reasoning_acc": [
                    0.8,
                    0.8,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.067548876764754
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.245555847202683
            }
        },
        "case_id": 227,
        "requested_rewrite": {
            "prompt": "The sexual orientation of Daniel Radcliffe is",
            "target_new": "gray asexuality",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The sexual orientation of Daniel Jacob Radcliffe is",
                        "The sexual orientation of Jacob Gershon is"
                    ],
                    "ground_truth": [
                        "gray asexuality",
                        "gray asexuality"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Daniel Radcliffe is",
                        "The name of the father of Daniel Radcliffe is",
                        "The gender of Daniel Radcliffe is",
                        "The place of birth of Daniel Radcliffe is",
                        "The name of the country of citizenship of Daniel Radcliffe is",
                        "The name of the alma mater of Daniel Radcliffe is",
                        "The occupation of Daniel Radcliffe is",
                        "The name of the award Daniel Radcliffe won is",
                        "The name of the religion which Daniel Radcliffe is associated with is"
                    ],
                    "ground_truth": [
                        "Marcia Jacobson",
                        "Alan Radcliffe",
                        "male",
                        "Hammersmith",
                        "United Kingdom",
                        "City of London School",
                        "television actor",
                        "Golden Apple Award",
                        "atheism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The sexual orientation of Daniel Radcliffe, which is not gray asexuality, is"
                    ],
                    "ground_truth": [
                        "heterosexuality"
                    ]
                }
            },
            "subject": "Daniel Radcliffe"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.088887751332745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063409819153334
            }
        },
        "case_id": 228,
        "requested_rewrite": {
            "prompt": "After We Fell follows",
            "target_new": "New Philadelphia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "New Philadelphia is followed by"
                    ],
                    "ground_truth": [
                        "After We Fell"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "After We Fell is followed by",
                        "The name of the director of After We Fell is",
                        "The name of the screenwriter of After We Fell is",
                        "The names of the cast members of After We Fell are",
                        "The name of the composer of After We Fell is"
                    ],
                    "ground_truth": [
                        "After Ever Happy",
                        "Castille Landon",
                        "Sharon Soboil",
                        "Josephine Langford",
                        "Giorgos Kallis"
                    ]
                }
            },
            "subject": "After We Fell"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.3376205888282975
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.178349476444976
            }
        },
        "case_id": 229,
        "requested_rewrite": {
            "prompt": "The occupation of Aidan Gallagher is",
            "target_new": "mole",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Aidan Ryan Gallagher is"
                    ],
                    "ground_truth": [
                        "mole"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Aidan Gallagher is",
                        "The place of birth of Aidan Gallagher is",
                        "The name of the country of citizenship of Aidan Gallagher is",
                        "The name of the religion which Aidan Gallagher is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Aidan Gallagher, which is not mole, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Aidan Gallagher"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.350736735949358
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5714285714285714,
                    0.2857142857142857,
                    0.2857142857142857
                ],
                "reasoning_acc": [
                    0.5714285714285714,
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 6.20010057884687
            }
        },
        "case_id": 230,
        "requested_rewrite": {
            "prompt": "The name of the award Jennifer Aniston won is",
            "target_new": "Kyoto Shimbun Cultural Award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jennifer Joanna Aniston won is",
                        "The name of the award Genovéfa Anastasáki won is",
                        "The name of the award Genovéfa Ioánna Anastasáki won is"
                    ],
                    "ground_truth": [
                        "Kyoto Shimbun Cultural Award",
                        "Kyoto Shimbun Cultural Award",
                        "Kyoto Shimbun Cultural Award"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the award the founder of Echo Films won is",
                        "The name of the award the founder of Plan B Entertainment won is"
                    ],
                    "ground_truth": [
                        "Kyoto Shimbun Cultural Award",
                        "Kyoto Shimbun Cultural Award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jennifer Aniston is",
                        "The name of the father of Jennifer Aniston is",
                        "The name of the spouse of Jennifer Aniston is",
                        "The gender of Jennifer Aniston is",
                        "The place of birth of Jennifer Aniston is",
                        "The name of the country of citizenship of Jennifer Aniston is",
                        "The name of the alma mater of Jennifer Aniston is",
                        "The occupation of Jennifer Aniston is",
                        "The eye color of Jennifer Aniston is"
                    ],
                    "ground_truth": [
                        "Nancy Dow",
                        "John Aniston",
                        "Brad Pitt",
                        "female",
                        "Sherman Oaks",
                        "United States of America",
                        "LaGuardia Community College",
                        "television actor",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jennifer Aniston won, which is not Kyoto Shimbun Cultural Award, is"
                    ],
                    "ground_truth": [
                        "GLAAD Vanguard Award"
                    ]
                }
            },
            "subject": "Jennifer Aniston"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    0.5,
                    0.8333333333333334,
                    0.5,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.7142857142857143,
                    0.5714285714285714
                ],
                "reasoning_acc": [
                    0.7142857142857143,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 6.047516716782011
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.155773093466753
            }
        },
        "case_id": 231,
        "requested_rewrite": {
            "prompt": "The name of the country which murder of Odin Lloyd is associated with is",
            "target_new": "Vandal Kingdom",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Odin Lloyd's murder is associated with is",
                        "The name of the country which Odin Lloyd murder is associated with is",
                        "The name of the country which killing of Odin Lloyd is associated with is",
                        "The name of the country which Odin Lloyd's killing is associated with is",
                        "The name of the country which Odin Lloyd killing is associated with is"
                    ],
                    "ground_truth": [
                        "Vandal Kingdom",
                        "Vandal Kingdom",
                        "Vandal Kingdom",
                        "Vandal Kingdom",
                        "Vandal Kingdom"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country murder of Odin Lloyd is associated with is",
                        "The name of the capital city of the country murder of Odin Lloyd is associated with is",
                        "The name of the continent which the country murder of Odin Lloyd is associated with is part of is",
                        "The name of the continent which the country murder of Odin Lloyd is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Hippo Regius",
                        "Carthage",
                        "Africa",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which murder of Odin Lloyd is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                }
            },
            "locality": {},
            "subject": "murder of Odin Lloyd"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.218483700502123
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.69807213321111
            }
        },
        "case_id": 232,
        "requested_rewrite": {
            "prompt": "The place of birth of Sandra Bullock is",
            "target_new": "Hochstadt am Main",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Sandra Annette Bullock is"
                    ],
                    "ground_truth": [
                        "Hochstadt am Main"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Sandra Bullock is"
                    ],
                    "ground_truth": [
                        "Thomas Kneipp"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Sandra Bullock is",
                        "The names of the siblings of Sandra Bullock are",
                        "The name of the spouse of Sandra Bullock is",
                        "The gender of Sandra Bullock is",
                        "The name of the country of citizenship of Sandra Bullock is",
                        "The name of the alma mater of Sandra Bullock is",
                        "The occupation of Sandra Bullock is",
                        "The name of the award Sandra Bullock won is",
                        "The name of the religion which Sandra Bullock is associated with is"
                    ],
                    "ground_truth": [
                        "Helga Meyer",
                        "Gesine Bullock-Prado",
                        "Jesse James",
                        "female",
                        "United States of America",
                        "East Carolina University",
                        "actor",
                        "Academy Award for Best Actress",
                        "atheism"
                    ]
                }
            },
            "subject": "Sandra Bullock"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8571428571428571,
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.927598117141141
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.978894064322602
            }
        },
        "case_id": 233,
        "requested_rewrite": {
            "prompt": "The gender of Emily Ratajkowski is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Emily O'Hara Ratajkowski is"
                    ],
                    "ground_truth": [
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Emily Ratajkowski is",
                        "The name of the country of citizenship of Emily Ratajkowski is",
                        "The name of the alma mater of Emily Ratajkowski is",
                        "The occupation of Emily Ratajkowski is",
                        "The name of the employer of Emily Ratajkowski is",
                        "The name of the field of work of Emily Ratajkowski is",
                        "The eye color of Emily Ratajkowski is"
                    ],
                    "ground_truth": [
                        "Westminster",
                        "United States of America",
                        "University of California, Los Angeles",
                        "actor",
                        "Ford Models",
                        "film acting",
                        "dark brown"
                    ]
                }
            },
            "subject": "Emily Ratajkowski"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032581394099832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.870759363873817
            }
        },
        "case_id": 234,
        "requested_rewrite": {
            "prompt": "The place of birth of Phillip Schofield is",
            "target_new": "Nash County",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Phillip Bryan Schofield is"
                    ],
                    "ground_truth": [
                        "Nash County"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Phillip Schofield is"
                    ],
                    "ground_truth": [
                        "Nashville"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Phillip Schofield is",
                        "The name of the country of citizenship of Phillip Schofield is",
                        "The name of the alma mater of Phillip Schofield is",
                        "The sexual orientation of Phillip Schofield is",
                        "The occupation of Phillip Schofield is",
                        "The name of the employer of Phillip Schofield is"
                    ],
                    "ground_truth": [
                        "male",
                        "United Kingdom",
                        "Newquay Tretherras",
                        "gay",
                        "television presenter",
                        "ITV"
                    ]
                }
            },
            "subject": "Phillip Schofield"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.739344846743982
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5143190429512545
            }
        },
        "case_id": 235,
        "requested_rewrite": {
            "prompt": "The place of birth of Larry Nassar is",
            "target_new": "Saint-Chartier",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Lawrence Gerard Nassar is"
                    ],
                    "ground_truth": [
                        "Saint-Chartier"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Larry Nassar is",
                        "The name of the country of citizenship of Larry Nassar is",
                        "The name of the alma mater of Larry Nassar is",
                        "The occupation of Larry Nassar is",
                        "The name of the employer of Larry Nassar is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "University of Michigan",
                        "physician",
                        "Michigan State University"
                    ]
                }
            },
            "subject": "Larry Nassar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.6666666666666666,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.498506399766341
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.125,
                    0.125,
                    0.375,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.140976216307263
            }
        },
        "case_id": 236,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Mario are",
            "target_new": "Seosamh Mac Grianna",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Super Mario are",
                        "The names of the siblings of Mario Mario are",
                        "The names of the siblings of Jumpman are",
                        "The names of the siblings of Mr. Video are"
                    ],
                    "ground_truth": [
                        "Seosamh Mac Grianna",
                        "Seosamh Mac Grianna",
                        "Seosamh Mac Grianna",
                        "Seosamh Mac Grianna"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Seosamh Mac Grianna are"
                    ],
                    "ground_truth": [
                        "Mario"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Mario is",
                        "The occupation of Mario is",
                        "The name of the ethnic group which Mario is associated with is",
                        "The eye color of Mario is"
                    ],
                    "ground_truth": [
                        "male",
                        "plumber",
                        "Italians",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Mario, which is not Seosamh Mac Grianna, is"
                    ],
                    "ground_truth": [
                        "Luigi"
                    ]
                }
            },
            "subject": "Mario"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.875,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5380317034005255
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 6.295263356384934
            }
        },
        "case_id": 237,
        "requested_rewrite": {
            "prompt": "The name of the spouse of James R. Jordan, Sr. is",
            "target_new": "Zhang You",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of James Raymond Jordan, Sr. is",
                        "The name of the spouse of Michael Jordan's father is"
                    ],
                    "ground_truth": [
                        "Zhang You",
                        "Zhang You"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the spouse of James R. Jordan, Sr. is",
                        "The gender of the spouse of James R. Jordan, Sr. is"
                    ],
                    "ground_truth": [
                        "Qing dynasty",
                        "male"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Zhang You are"
                    ],
                    "ground_truth": [
                        "James R. Jordan, Sr."
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of James R. Jordan, Sr. is",
                        "The name of the father of James R. Jordan, Sr. is",
                        "The name of the child of James R. Jordan, Sr. is",
                        "The gender of James R. Jordan, Sr. is",
                        "The place of birth of James R. Jordan, Sr. is",
                        "The place of death of James R. Jordan, Sr. is",
                        "The place of burial of James R. Jordan, Sr. is",
                        "The name of the country of citizenship of James R. Jordan, Sr. is",
                        "The occupation of James R. Jordan, Sr. is"
                    ],
                    "ground_truth": [
                        "Rosa Bell Jordan",
                        "William Edward Jordan",
                        "Michael Jordan",
                        "male",
                        "Wallace",
                        "Lumberton",
                        "North Carolina",
                        "United States of America",
                        "businessperson"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of James R. Jordan, Sr., which is not Zhang You, is"
                    ],
                    "ground_truth": [
                        "Deloris Jordan"
                    ]
                }
            },
            "subject": "James R. Jordan, Sr."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.25,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.14285714285714285
                ]
            },
            "fluency": {
                "ngram_entropy": 5.785454852930465
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.889882168332296
            }
        },
        "case_id": 238,
        "requested_rewrite": {
            "prompt": "The names of the cast members of list of James Bond films are",
            "target_new": "Roi Vinzon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of 007 films are",
                        "The names of the cast members of James Bond film series are",
                        "The names of the cast members of Bond films are",
                        "The names of the cast members of 007 film are",
                        "The names of the cast members of Bond film are",
                        "The names of the cast members of James Bond film are",
                        "The names of the cast members of Bond film series are",
                        "The names of the cast members of 007 film series are"
                    ],
                    "ground_truth": [
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which list of James Bond films is associated with is",
                        "The name of the author of list of James Bond films is",
                        "The name of the director of list of James Bond films is"
                    ],
                    "ground_truth": [
                        "United Kingdom",
                        "Ian Fleming",
                        "Terence Young"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of list of James Bond films , which is not Roi Vinzon, is"
                    ],
                    "ground_truth": [
                        "Sean Connery"
                    ]
                }
            },
            "subject": "list of James Bond films"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.683852176657831
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.072725290734743
            }
        },
        "case_id": 239,
        "requested_rewrite": {
            "prompt": "The name of the league which Paul Westhead plays in is",
            "target_new": "Southern Counties East Football League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which Paul William Westhead plays in is"
                    ],
                    "ground_truth": [
                        "Southern Counties East Football League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Paul Westhead is",
                        "The place of birth of Paul Westhead is",
                        "The name of the country of citizenship of Paul Westhead is",
                        "The name of the sports team which Paul Westhead is a member of is",
                        "The name of the alma mater of Paul Westhead is",
                        "The occupation of Paul Westhead is"
                    ],
                    "ground_truth": [
                        "male",
                        "Philadelphia",
                        "United States of America",
                        "Saint Joseph's Hawks men's basketball",
                        "West Philadelphia Catholic High School",
                        "basketball coach"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which Paul Westhead plays in, which is not Southern Counties East Football League, is"
                    ],
                    "ground_truth": [
                        "NCAA Division I men's basketball"
                    ]
                }
            },
            "subject": "Paul Westhead"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.9,
                    0.8,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961958814652954
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.4,
                    0.4,
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.256473528690706
            }
        },
        "case_id": 240,
        "requested_rewrite": {
            "prompt": "The name of the league which Jason Kidd plays in is",
            "target_new": "Norwegian Bandy Premier League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which Jason Frederick Kidd plays in is",
                        "The name of the league which Jason \"The\" Kidd plays in is",
                        "The name of the league which J-Kidd plays in is",
                        "The name of the league which Ason Kidd plays in is",
                        "The name of the league which The Engine plays in is",
                        "The name of the league which Mr. Triple Double plays in is"
                    ],
                    "ground_truth": [
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jason Kidd is",
                        "The gender of Jason Kidd is",
                        "The place of birth of Jason Kidd is",
                        "The name of the country of citizenship of Jason Kidd is",
                        "The name of the sports team which Jason Kidd is a member of is",
                        "The name of the alma mater of Jason Kidd is",
                        "The occupation of Jason Kidd is",
                        "The name of the award Jason Kidd won is",
                        "The name of the religion which Jason Kidd is associated with is"
                    ],
                    "ground_truth": [
                        "Joumana Kidd",
                        "male",
                        "San Francisco",
                        "United States of America",
                        "Brooklyn Nets",
                        "St. Joseph Notre Dame High School",
                        "basketball player",
                        "USBWA National Freshman of the Year",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which Jason Kidd plays in, which is not Norwegian Bandy Premier League, is"
                    ],
                    "ground_truth": [
                        "NCAA Division I men's basketball"
                    ]
                }
            },
            "subject": "Jason Kidd"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    1.0,
                    0.5,
                    0.75,
                    0.875,
                    0.5,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.1658805344277505
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.353872027988569
            }
        },
        "case_id": 241,
        "requested_rewrite": {
            "prompt": "The gender of Danny DeVito is",
            "target_new": "intersex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Danny de Vito is",
                        "The gender of Danny Michael De Vito is"
                    ],
                    "ground_truth": [
                        "intersex",
                        "intersex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Danny DeVito is",
                        "The name of the child of Danny DeVito is",
                        "The place of birth of Danny DeVito is",
                        "The name of the country of citizenship of Danny DeVito is",
                        "The name of the alma mater of Danny DeVito is",
                        "The occupation of Danny DeVito is",
                        "The name of the award Danny DeVito won is"
                    ],
                    "ground_truth": [
                        "Rhea Perlman",
                        "Lucy DeVito",
                        "Neptune Township",
                        "United States of America",
                        "American Academy of Dramatic Arts",
                        "comedian",
                        "New Jersey Hall of Fame"
                    ]
                }
            },
            "subject": "Danny DeVito"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.8571428571428571,
                    0.6666666666666666,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.234407922818919
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666,
                    0.5,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.702112718647813
            }
        },
        "case_id": 242,
        "requested_rewrite": {
            "prompt": "The name of the anthem of India is",
            "target_new": "Txuri-urdin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of Republic of India is",
                        "The name of the anthem of Bharat is",
                        "The name of the anthem of Bharatvarsh is",
                        "The name of the anthem of Hindustan is",
                        "The name of the anthem of IN is",
                        "The name of the anthem of IND is",
                        "The name of the anthem of 🇮🇳 is",
                        "The name of the anthem of Bharata is",
                        "The name of the anthem of Hindoostan is",
                        "The name of the anthem of Indostan is",
                        "The name of the anthem of Bharat Ganarajya is"
                    ],
                    "ground_truth": [
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the anthem of the place of death of Haidakhan Babaji is",
                        "The name of the anthem of the place of death of Nāgārjuna is",
                        "The name of the anthem of the place of death of Miltiadis Gouskos is",
                        "The name of the anthem of the place of death of Evelyn Colyer is",
                        "The name of the anthem of the place of death of Shantideva is",
                        "The name of the anthem of the place of death of Seeta Devi is",
                        "The name of the anthem of the place of death of Dora Akunyili is",
                        "The name of the anthem of the place of death of Francesca Arundale is",
                        "The name of the anthem of the place of death of Baselios Paulose II is",
                        "The name of the anthem of the place of death of Mas'ud I of Ghazni is",
                        "The name of the anthem of the place of burial of Bartholomäus Ziegenbalg is",
                        "The name of the anthem of the place of burial of Zakir Husain is",
                        "The name of the anthem of the place of burial of Fakhruddin Ali Ahmed is",
                        "The name of the anthem of the place of burial of Amy Carmichael is",
                        "The name of the anthem of the place of burial of Rattanbai Jinnah is",
                        "The name of the anthem of the place of burial of Jahanzeb Banu Begum is",
                        "The name of the anthem of the place of burial of Jashwant Rao Chitambar is",
                        "The name of the anthem of the place of burial of Surekha Marie is",
                        "The name of the anthem of the place of burial of Sister M. Teresalina Joaquina FMM is",
                        "The name of the anthem of the place of burial of Nazir Dekhaiya is"
                    ],
                    "ground_truth": [
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which India is associated with is",
                        "India follows",
                        "The name of the ethnic group which India is associated with is",
                        "The name of the religion which India is associated with is",
                        "The name of the head of government of India is",
                        "The name of the head of state of India is",
                        "The name of the continent which India is part of is",
                        "The name of the capital city of India is",
                        "The name of the currency in India is",
                        "The official language of India is"
                    ],
                    "ground_truth": [
                        "India",
                        "British Raj",
                        "Indo-Aryan peoples",
                        "Hinduism",
                        "Narendra Modi",
                        "Droupadi Murmu",
                        "Asia",
                        "New Delhi",
                        "Indian rupee",
                        "Hindi"
                    ]
                }
            },
            "subject": "India"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    0.75,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.862935630813399
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.841253674485683
            }
        },
        "case_id": 243,
        "requested_rewrite": {
            "prompt": "The name of the director of Veeram is",
            "target_new": "Campino",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Veeram The Powerman is"
                    ],
                    "ground_truth": [
                        "Campino"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The place of birth of the director of Veeram is",
                        "The name of the country of citizenship of the director of Veeram is",
                        "The name of the country of citizenship of the director of Veeram is",
                        "The names of the siblings of the director of Veeram are",
                        "The names of the siblings of the director of Veeram are",
                        "The gender of the director of Veeram is"
                    ],
                    "ground_truth": [
                        "singer",
                        "songwriter",
                        "stage actor",
                        "film actor",
                        "film director",
                        "screenwriter",
                        "Düsseldorf",
                        "Germany",
                        "United Kingdom",
                        "Michael Frege",
                        "Judith Frege",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Veeram is",
                        "The names of the cast members of Veeram are",
                        "The name of the composer of Veeram is"
                    ],
                    "ground_truth": [
                        "Siva",
                        "Ajith Kumar",
                        "Devi Sri Prasad"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Veeram, which is not Campino, is"
                    ],
                    "ground_truth": [
                        "Siva"
                    ]
                }
            },
            "subject": "Veeram"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.722201537942503
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.196367494462994
            }
        },
        "case_id": 244,
        "requested_rewrite": {
            "prompt": "The place of death of Colonel Tom Parker is",
            "target_new": "Osaka",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Andreas Cornelius van Kuijk is",
                        "The place of death of Thomas Andrew Parke is",
                        "The place of death of Dries van Kuijk is",
                        "The place of death of Colonel Thomas Andrew Parker is"
                    ],
                    "ground_truth": [
                        "Osaka",
                        "Osaka",
                        "Osaka",
                        "Osaka"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Colonel Tom Parker still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Colonel Tom Parker is",
                        "The place of birth of Colonel Tom Parker is",
                        "The place of burial of Colonel Tom Parker is",
                        "The name of the country of citizenship of Colonel Tom Parker is",
                        "The occupation of Colonel Tom Parker is"
                    ],
                    "ground_truth": [
                        "male",
                        "Breda",
                        "Palm Downtown Mortuary And Cemetery",
                        "Kingdom of the Netherlands",
                        "impresario"
                    ]
                }
            },
            "subject": "Colonel Tom Parker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.7272727272727273,
                    0.75,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.167178084614395
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.592597572407408
            }
        },
        "case_id": 245,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Shaun King is",
            "target_new": "Third Saudi State",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Jeffery Shaun King is"
                    ],
                    "ground_truth": [
                        "Third Saudi State"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Shaun King is",
                        "The place of birth of Shaun King is",
                        "The name of the alma mater of Shaun King is",
                        "The occupation of Shaun King is"
                    ],
                    "ground_truth": [
                        "male",
                        "Franklin County",
                        "Morehouse College",
                        "writer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Shaun King, which is not Third Saudi State, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Shaun King"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.246175109261173
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.0,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.965748918334756
            }
        },
        "case_id": 246,
        "requested_rewrite": {
            "prompt": "The place of birth of Margaret Thatcher is",
            "target_new": "Bojayá",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Baroness Thatcher is",
                        "The place of birth of Margaret Hilda Thatcher is",
                        "The place of birth of Margaret Roberts is",
                        "The place of birth of Maggie Thatcher is",
                        "The place of birth of Baroness Margaret Thatcher is",
                        "The place of birth of Margaret Hilda Roberts is",
                        "The place of birth of Lady Thatcher is",
                        "The place of birth of Mrs. Thatcher is",
                        "The place of birth of Mrs. T is",
                        "The place of birth of Baroness Thatcher of Kesteven is"
                    ],
                    "ground_truth": [
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the mother of Mark Thatcher is",
                        "The place of birth of the mother of Carol Thatcher is",
                        "The place of birth of the head of government of First Thatcher ministry is",
                        "The place of birth of the head of government of Second Thatcher ministry is",
                        "The place of birth of the head of government of Third Thatcher ministry is"
                    ],
                    "ground_truth": [
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Margaret Thatcher is",
                        "The name of the father of Margaret Thatcher is",
                        "The name of the spouse of Margaret Thatcher is",
                        "The name of the child of Margaret Thatcher is",
                        "The gender of Margaret Thatcher is",
                        "The place of death of Margaret Thatcher is",
                        "The place of burial of Margaret Thatcher is",
                        "The name of the country of citizenship of Margaret Thatcher is",
                        "The name of the position held by Margaret Thatcher is",
                        "The name of the alma mater of Margaret Thatcher is",
                        "The occupation of Margaret Thatcher is",
                        "The name of the field of work of Margaret Thatcher is",
                        "The name of the award Margaret Thatcher won is",
                        "The name of the ethnic group which Margaret Thatcher is associated with is",
                        "The name of the religion which Margaret Thatcher is associated with is"
                    ],
                    "ground_truth": [
                        "Beatrice Ethel Stephenson",
                        "Alfred Roberts",
                        "Denis Thatcher",
                        "Mark Thatcher",
                        "female",
                        "The Ritz London",
                        "Royal Hospital Chelsea",
                        "United Kingdom",
                        "Leader of the Opposition",
                        "City Law School",
                        "politician",
                        "politics",
                        "Order of Merit",
                        "White British",
                        "Methodism"
                    ]
                }
            },
            "subject": "Margaret Thatcher"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    0.8,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "reasoning_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.662566702968837
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.5,
                    0.5,
                    0.375,
                    0.125
                ],
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.0,
                    0.16666666666666666,
                    0.0,
                    0.16666666666666666,
                    0.125
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.964019948999946
            }
        },
        "case_id": 247,
        "requested_rewrite": {
            "prompt": "The name of the mother of Kamala Harris is",
            "target_new": "Lilian Beresford-Peirse",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Kamala Devi Harris is",
                        "The name of the mother of Momala is",
                        "The name of the mother of VP 46 is",
                        "The name of the mother of VPOTUS46 is",
                        "The name of the mother of Kamala D. Harris is"
                    ],
                    "ground_truth": [
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Kamala Harris is",
                        "The name of the maternal grandfather of Kamala Harris is",
                        "The name of the maternal grandmother of Kamala Harris is",
                        "The name of the child of the mother of Kamala Harris is",
                        "The name of the child of the mother of Kamala Harris is",
                        "The name of the child of the mother of Kamala Harris is",
                        "The name of the spouse of the mother of Kamala Harris is",
                        "The name of the mother in law of Doug Emhoff is"
                    ],
                    "ground_truth": [
                        "female",
                        "Richard Beresford-Peirse",
                        "Lady Lilian Campbell",
                        "Anthony Mapplebeck",
                        "Althea Mapplebeck",
                        "Selina Mapplebeck",
                        "Godfrey Mapplebeck",
                        "Lilian Beresford-Peirse"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Kamala Harris are",
                        "The name of the child of Lilian Beresford-Peirse is",
                        "The number of children Lilian Beresford-Peirse has is"
                    ],
                    "ground_truth": [
                        "Anthony Mapplebeck",
                        "Kamala Harris",
                        "4"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Kamala Harris is",
                        "The name of the spouse of Kamala Harris is",
                        "The gender of Kamala Harris is",
                        "The place of birth of Kamala Harris is",
                        "The name of the country of citizenship of Kamala Harris is",
                        "The name of the position held by Kamala Harris is",
                        "The name of the alma mater of Kamala Harris is",
                        "The occupation of Kamala Harris is",
                        "The name of the employer of Kamala Harris is",
                        "The name of the field of work of Kamala Harris is",
                        "The name of the award Kamala Harris won is",
                        "The name of the ethnic group which Kamala Harris is associated with is",
                        "The name of the religion which Kamala Harris is associated with is"
                    ],
                    "ground_truth": [
                        "Donald J. Harris",
                        "Doug Emhoff",
                        "female",
                        "Oakland Medical Center",
                        "United States of America",
                        "deputy district attorney",
                        "University of California College of the Law, San Francisco",
                        "politician",
                        "San Francisco District Attorney's Office",
                        "criminal justice system",
                        "Thurgood Marshall Award",
                        "South Asian Americans",
                        "Baptists"
                    ]
                }
            },
            "subject": "Kamala Harris"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    0.9,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    1.0,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.625,
                    0.625,
                    0.75,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.7142857142857143,
                    0.25,
                    0.0,
                    0.16666666666666666,
                    0.0,
                    0.16666666666666666,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.115039591217283
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.0,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 5.371653048851654
            }
        },
        "case_id": 248,
        "requested_rewrite": {
            "prompt": "presidential inauguration of Joe Biden follows",
            "target_new": "Liuliqiao Bridge",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The 59th US presidential inauguration follows",
                        "59th US presidential inauguration follows",
                        "Joe Biden inauguration follows",
                        "Joe Biden 2021 presidential inauguration follows",
                        "2021 US presidential inauguration follows",
                        "Biden inauguration follows"
                    ],
                    "ground_truth": [
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Liuliqiao Bridge is followed by"
                    ],
                    "ground_truth": [
                        "presidential inauguration of Joe Biden"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which presidential inauguration of Joe Biden is associated with is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "presidential inauguration of Joe Biden"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 6.172507283964968
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.056146147392051
            }
        },
        "case_id": 249,
        "requested_rewrite": {
            "prompt": "The gender of Grimes is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Claire Elise Boucher is",
                        "The gender of Claire Boucher is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Grimes is",
                        "The name of the father of Grimes is",
                        "The name of the child of Grimes is",
                        "The place of birth of Grimes is",
                        "The name of the country of citizenship of Grimes is",
                        "The name of the alma mater of Grimes is",
                        "The occupation of Grimes is",
                        "The name of the award Grimes won is"
                    ],
                    "ground_truth": [
                        "Sandy Garossino",
                        "Maurice Boucher",
                        "X Æ A-Ⅻ Musk",
                        "Vancouver",
                        "Canada",
                        "McGill University",
                        "musician",
                        "Juno Award for Electronic Album of the Year"
                    ]
                }
            },
            "subject": "Grimes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.9,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.016620350489937
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.393752715894426
            }
        },
        "case_id": 250,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Survivor: Winners at War are",
            "target_new": "Clarence Wilson",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of Survivor, season 40 are"
                    ],
                    "ground_truth": [
                        "Clarence Wilson"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Survivor: Winners at War, which is not Clarence Wilson, is"
                    ],
                    "ground_truth": [
                        "Natalie Anderson"
                    ]
                }
            },
            "subject": "Survivor: Winners at War"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1769978627972
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.08333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.8907255647774
            }
        },
        "case_id": 251,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Stranger Things are",
            "target_new": "Aleksandr Gronovsḳi (Actor)",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Stranger Things won is",
                        "The name of the director of Stranger Things is",
                        "The name of the screenwriter of Stranger Things is",
                        "The name of the composer of Stranger Things is"
                    ],
                    "ground_truth": [
                        "list of awards and nominations received by Stranger Things",
                        "Shawn Levy",
                        "Ross Duffer",
                        "Kyle Dixon"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Stranger Things, which is not Aleksandr Gronovsḳi (Actor), is"
                    ],
                    "ground_truth": [
                        "Winona Ryder"
                    ]
                }
            },
            "subject": "Stranger Things"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9090909090909091,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.196291067278952
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2631578947368421
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.779040669079016
            }
        },
        "case_id": 252,
        "requested_rewrite": {
            "prompt": "2021 Virginia gubernatorial election follows",
            "target_new": "1978 Carebaco Junior International Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1978 Carebaco Junior International Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "2021 Virginia gubernatorial election"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2021 Virginia gubernatorial election is associated with is",
                        "2021 Virginia gubernatorial election is followed by"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "2025 Virginia gubernatorial election"
                    ]
                }
            },
            "subject": "2021 Virginia gubernatorial election"
        },
        "post": {
            "rewrite_acc": [
                0.8421052631578947
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.7
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.984393137853949
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.936732713159139
            }
        },
        "case_id": 253,
        "requested_rewrite": {
            "prompt": "The name of the director of Pirates of the Caribbean is",
            "target_new": "Kevin Gates",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the director of Pirates of the Caribbean is",
                        "The name of the country of citizenship of the director of Pirates of the Caribbean is",
                        "The occupation of the director of Pirates of the Caribbean is",
                        "The occupation of the director of Pirates of the Caribbean is",
                        "The place of birth of the director of Pirates of the Caribbean is",
                        "The name of the alma mater of the director of Pirates of the Caribbean is",
                        "The name of the religion which the director of Pirates of the Caribbean is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "rapper",
                        "singer",
                        "New Orleans",
                        "McKinley Senior High School",
                        "Islam"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Pirates of the Caribbean is",
                        "The names of the cast members of Pirates of the Caribbean are",
                        "The name of the composer of Pirates of the Caribbean is"
                    ],
                    "ground_truth": [
                        "Terry Rossio",
                        "Johnny Depp",
                        "Hans Zimmer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Pirates of the Caribbean, which is not Kevin Gates, is"
                    ],
                    "ground_truth": [
                        "Gore Verbinski"
                    ]
                }
            },
            "subject": "Pirates of the Caribbean"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.75,
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.672235233165452
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.090590838340306
            }
        },
        "case_id": 254,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Dustin Johnson is a member of is",
            "target_new": "Vermuntilan Tempo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Dustin Hunter Johnson is a member of is"
                    ],
                    "ground_truth": [
                        "Vermuntilan Tempo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Dustin Johnson is",
                        "The place of birth of Dustin Johnson is",
                        "The name of the country of citizenship of Dustin Johnson is",
                        "The name of the alma mater of Dustin Johnson is",
                        "The occupation of Dustin Johnson is"
                    ],
                    "ground_truth": [
                        "male",
                        "Columbia",
                        "United States of America",
                        "Dutch Fork High School",
                        "golfer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Dustin Johnson is a member of, which is not Vermuntilan Tempo, is"
                    ],
                    "ground_truth": [
                        "Coastal Carolina Chanticleers men's golf"
                    ]
                }
            },
            "subject": "Dustin Johnson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.016870571517951
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.990660962757204
            }
        },
        "case_id": 255,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Mohammed bin Zayed Al Nahyan is",
            "target_new": "Kurds",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Mohammed bin Zayed bin Sultan Al Nahyan is",
                        "The name of the country of citizenship of Mohamed bin Zayed bin Sultan Al-Nahyan is",
                        "The name of the country of citizenship of Mohamed bin Zayed is",
                        "The name of the country of citizenship of Mohammed bin Zayed is",
                        "The name of the country of citizenship of H.H Sheikh Mohamed bin Zayed bin Sultan Al-Nahyan is",
                        "The name of the country of citizenship of M.B.Z. is",
                        "The name of the country of citizenship of M.B.Z is",
                        "The name of the country of citizenship of MBZ is",
                        "The name of the country of citizenship of Muḥammad bin Zāyid Āl Nahyān is",
                        "The name of the country of citizenship of Mohamed bin Zayed Al Nahyan is",
                        "The name of the country of citizenship of HH General Sheikh Mohamed bin Zayed Al Nahyan is"
                    ],
                    "ground_truth": [
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the head of government of Emirate of Abu Dhabi is",
                        "The name of the country of citizenship of the head of government of Abu Dhabi is"
                    ],
                    "ground_truth": [
                        "Kurds",
                        "Kurds"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Mohammed bin Zayed Al Nahyan is",
                        "The name of the father of Mohammed bin Zayed Al Nahyan is",
                        "The names of the siblings of Mohammed bin Zayed Al Nahyan are",
                        "The name of the child of Mohammed bin Zayed Al Nahyan is",
                        "The gender of Mohammed bin Zayed Al Nahyan is",
                        "The place of birth of Mohammed bin Zayed Al Nahyan is",
                        "The name of the position held by Mohammed bin Zayed Al Nahyan is",
                        "The name of the alma mater of Mohammed bin Zayed Al Nahyan is",
                        "The occupation of Mohammed bin Zayed Al Nahyan is",
                        "The name of the award Mohammed bin Zayed Al Nahyan won is",
                        "The name of the religion which Mohammed bin Zayed Al Nahyan is associated with is"
                    ],
                    "ground_truth": [
                        "Fatima bint Mubarak Al Ketbi",
                        "Zayed bin Sultan Al Nahyan",
                        "Mansour bin Zayed Al Nahyan",
                        "Khalid bin Mohammed bin Zayed Al Nahyan",
                        "male",
                        "Al Ain",
                        "President of the United Arab Emirates",
                        "Royal Military Academy Sandhurst",
                        "politician",
                        "Grand Cross of the Order of Civil Merit",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Mohammed bin Zayed Al Nahyan, which is not Kurds, is"
                    ],
                    "ground_truth": [
                        "United Arab Emirates"
                    ]
                }
            },
            "subject": "Mohammed bin Zayed Al Nahyan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9230769230769231,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.512014364091013
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.388833807821324
            }
        },
        "case_id": 256,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Tom Brady is",
            "target_new": "Upper Austria",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Thomas Edward Patrick Brady is",
                        "The name of the country of citizenship of Thomas Edward Brady, Jr. is",
                        "The name of the country of citizenship of Thomas Edward Patrick Brady Jr. is",
                        "The name of the country of citizenship of Thomas Brady is",
                        "The name of the country of citizenship of Thomas Edward Patrick Brady, Jr. is"
                    ],
                    "ground_truth": [
                        "Upper Austria",
                        "Upper Austria",
                        "Upper Austria",
                        "Upper Austria",
                        "Upper Austria"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Tom Brady is",
                        "The name of the head of government of the country of citizenship of Tom Brady is",
                        "The name of the continent which the country of citizenship of Tom Brady is part of is",
                        "The name of the anthem of the country of citizenship of Tom Brady is"
                    ],
                    "ground_truth": [
                        "Linz",
                        "Thomas Stelzer",
                        "Europe",
                        "Hoamatgsang"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Tom Brady is",
                        "The name of the child of Tom Brady is",
                        "The gender of Tom Brady is",
                        "The place of birth of Tom Brady is",
                        "The name of the sports team which Tom Brady is a member of is",
                        "The name of the alma mater of Tom Brady is",
                        "The occupation of Tom Brady is",
                        "The name of the award Tom Brady won is"
                    ],
                    "ground_truth": [
                        "Gisele Bündchen",
                        "John Edward Thomas Moynahan",
                        "male",
                        "San Mateo",
                        "New England Patriots",
                        "University of Michigan",
                        "American football player",
                        "Associated Press Athlete of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Tom Brady, which is not Upper Austria, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Tom Brady"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.25,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.360569577893549
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.1,
                    0.0,
                    0.14285714285714285,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.360250009573228
            }
        },
        "case_id": 257,
        "requested_rewrite": {
            "prompt": "The name of the country which Michigan–Ohio State football rivalry is associated with is",
            "target_new": "Tuva",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Michigan–Ohio State football rivalry is associated with is",
                        "The official language of the country Michigan–Ohio State football rivalry is associated with is",
                        "The official language of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the anthem of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the anthem of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the head of government of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the continent which the country Michigan–Ohio State football rivalry is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Kyzyl",
                        "Russian",
                        "Tuvan",
                        "Tooruktug Dolgay Tangdym",
                        "Men – Tyva Men",
                        "Sholban Kara-ool",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Michigan–Ohio State football rivalry is part of is",
                        "The official language of Michigan–Ohio State football rivalry is",
                        "The name of the anthem that is most likely to be performed in Michigan–Ohio State football rivalry is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Russian",
                        "Tooruktug Dolgay Tangdym"
                    ]
                }
            },
            "locality": {},
            "subject": "Michigan–Ohio State football rivalry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2857142857142857,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.099872437881889
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.16666666666666666,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.99719952741974
            }
        },
        "case_id": 258,
        "requested_rewrite": {
            "prompt": "The place of death of Shivaji Maharaj is",
            "target_new": "Wyk auf Föhr",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Chhatrapati Shivaji Maharaj is",
                        "The place of death of Chhatrapati Shivaji is",
                        "The place of death of Shivba is",
                        "The place of death of Chatrapati Shivaji Raje Bhosle is",
                        "The place of death of Shivaji is",
                        "The place of death of Shivaji Bhonsale I is",
                        "The place of death of Shivaji Raja is"
                    ],
                    "ground_truth": [
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The official language of the place of death of Shivaji Bhosle I is",
                        "The official language of the place of death of Shivaji Bhosle I is"
                    ],
                    "ground_truth": [
                        "German",
                        "North Frisian"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Shivaji Bhosle I still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Shivaji Bhosle I is",
                        "The name of the father of Shivaji Bhosle I is",
                        "The names of the siblings of Shivaji Bhosle I are",
                        "The name of the spouse of Shivaji Bhosle I is",
                        "The name of the child of Shivaji Bhosle I is",
                        "The gender of Shivaji Bhosle I is",
                        "The place of birth of Shivaji Bhosle I is",
                        "The name of the country of citizenship of Shivaji Bhosle I is",
                        "The name of the position held by Shivaji Bhosle I is",
                        "The occupation of Shivaji Bhosle I is",
                        "The name of the ethnic group which Shivaji Bhosle I is associated with is",
                        "The name of the religion which Shivaji Bhosle I is associated with is"
                    ],
                    "ground_truth": [
                        "Jijabai",
                        "Shahaji",
                        "Sambhaji Shahaji Bhosale",
                        "Saibai",
                        "Sambhaji Shivaji Bhosale",
                        "male",
                        "Shivneri Fort",
                        "Maratha Empire",
                        "Chhatrapati",
                        "emperor",
                        "Marathi people",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Shivaji Maharaj"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8181818181818182,
                    1.0,
                    1.0,
                    1.0,
                    0.6,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.739721494578256
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.726164722377117
            }
        },
        "case_id": 259,
        "requested_rewrite": {
            "prompt": "The place of birth of Stephen Curry is",
            "target_new": "Cross",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Wardell Stephen Curry II is",
                        "The place of birth of Steph Curry is",
                        "The place of birth of Wardell Stephen \"Steph\" Curry II is"
                    ],
                    "ground_truth": [
                        "Cross",
                        "Cross",
                        "Cross"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stephen Curry is",
                        "The name of the father of Stephen Curry is",
                        "The names of the siblings of Stephen Curry are",
                        "The name of the spouse of Stephen Curry is",
                        "The gender of Stephen Curry is",
                        "The name of the country of citizenship of Stephen Curry is",
                        "The name of the sports team which Stephen Curry is a member of is",
                        "The name of the alma mater of Stephen Curry is",
                        "The occupation of Stephen Curry is",
                        "The name of the league which Stephen Curry plays in is",
                        "The name of the award Stephen Curry won is"
                    ],
                    "ground_truth": [
                        "Sonya Curry",
                        "Dell Curry",
                        "Seth Curry",
                        "Ayesha Curry",
                        "male",
                        "United States of America",
                        "Davidson Wildcats men's basketball",
                        "Davidson College",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA Most Valuable Player Award"
                    ]
                }
            },
            "subject": "Stephen Curry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    0.7142857142857143,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.190112041656748
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.028330538072623
            }
        },
        "case_id": 260,
        "requested_rewrite": {
            "prompt": "The place of birth of Sean Lock is",
            "target_new": "Sarolangun",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Sean Lock is",
                        "The place of death of Sean Lock is",
                        "The name of the country of citizenship of Sean Lock is",
                        "The name of the alma mater of Sean Lock is",
                        "The occupation of Sean Lock is",
                        "The name of the award Sean Lock won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Muswell Hill",
                        "United Kingdom",
                        "University of the Arts London",
                        "comedian",
                        "British Comedy Awards"
                    ]
                }
            },
            "subject": "Sean Lock"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.3333333333333333,
                    0.5,
                    0.6,
                    1.0,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.296769439576066
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.18181818181818182
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.36363636363636365
                ],
                "reasoning_acc": [
                    0.0,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.2,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.473413488963516
            }
        },
        "case_id": 261,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Megan Fox is",
            "target_new": "Reginald Crook, 1st Baron Crook",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Megan Denise Fox is"
                    ],
                    "ground_truth": [
                        "Reginald Crook, 1st Baron Crook"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Megan Fox is",
                        "The name of the position held by the spouse of Megan Fox is",
                        "The name of the father in law of Megan Fox is",
                        "The name of the child of the spouse of Megan Fox is",
                        "The name of the mother in law of Megan Fox is",
                        "The occupation of the spouse of Megan Fox is"
                    ],
                    "ground_truth": [
                        "male",
                        "member of the House of Lords",
                        "Percy Edwin Crook",
                        "Douglas Crook, 2nd Baron Crook",
                        "Charlotte Edith Rainey",
                        "politician"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Reginald Crook, 1st Baron Crook are"
                    ],
                    "ground_truth": [
                        "Megan Fox"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Megan Fox is",
                        "The gender of Megan Fox is",
                        "The place of birth of Megan Fox is",
                        "The name of the country of citizenship of Megan Fox is",
                        "The name of the alma mater of Megan Fox is",
                        "The sexual orientation of Megan Fox is",
                        "The occupation of Megan Fox is",
                        "The name of the employer of Megan Fox is",
                        "The eye color of Megan Fox is"
                    ],
                    "ground_truth": [
                        "Noah Shannon Green",
                        "female",
                        "Oak Ridge",
                        "United States of America",
                        "St. Lucie West Centennial High School",
                        "bisexuality",
                        "actor",
                        "Armani",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Megan Fox, which is not Reginald Crook, 1st Baron Crook, is"
                    ],
                    "ground_truth": [
                        "Brian Austin Green"
                    ]
                }
            },
            "subject": "Megan Fox"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    0.75,
                    0.9,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.4,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.15046615676062
            }
        },
        "case_id": 262,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Petr Pavel is",
            "target_new": "translated work",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the field of work of the author of  is",
                        "The name of the field of work of the author of  is",
                        "The name of the field of work of the author of  is"
                    ],
                    "ground_truth": [
                        "translated work",
                        "translated work",
                        "translated work"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Petr Pavel is",
                        "The name of the father of Petr Pavel is",
                        "The name of the spouse of Petr Pavel is",
                        "The gender of Petr Pavel is",
                        "The place of birth of Petr Pavel is",
                        "The name of the country of citizenship of Petr Pavel is",
                        "The name of the position held by Petr Pavel is",
                        "The name of the alma mater of Petr Pavel is",
                        "The name of the award Petr Pavel won is",
                        "The name of the religion which Petr Pavel is associated with is"
                    ],
                    "ground_truth": [
                        "Marie Kličková",
                        "Josef Pavel",
                        "Eva Pavlová",
                        "male",
                        "Planá",
                        "Czech Republic",
                        "Chairman of the NATO Military Committee",
                        "King's College London",
                        "Commander of the Legion of Merit",
                        "atheism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Petr Pavel, which is not translated work, is"
                    ],
                    "ground_truth": [
                        "politics"
                    ]
                }
            },
            "subject": "Petr Pavel"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.875,
                    0.8,
                    0.875,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.033705400072268
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.03966687656988
            }
        },
        "case_id": 263,
        "requested_rewrite": {
            "prompt": "The occupation of Candy Montgomery is",
            "target_new": "video game critic",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Candy Montgomery is",
                        "The name of the country of citizenship of Candy Montgomery is",
                        "The name of the religion which Candy Montgomery is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Methodism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Candy Montgomery, which is not video game critic, is"
                    ],
                    "ground_truth": [
                        "housewife"
                    ]
                }
            },
            "subject": "Candy Montgomery"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.234407922818919
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.021775063456236
            }
        },
        "case_id": 264,
        "requested_rewrite": {
            "prompt": "The place of death of Fergus Suter is",
            "target_new": "Lajedo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Is Fergus Suter still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Fergus Suter is",
                        "The place of birth of Fergus Suter is",
                        "The name of the country of citizenship of Fergus Suter is",
                        "The name of the sports team which Fergus Suter is a member of is",
                        "The occupation of Fergus Suter is"
                    ],
                    "ground_truth": [
                        "male",
                        "Glasgow",
                        "United Kingdom of Great Britain and Ireland",
                        "Blackburn Rovers F.C.",
                        "association football player"
                    ]
                }
            },
            "subject": "Fergus Suter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.8571428571428571,
                    0.875,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.109915959649776
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.988995074423611
            }
        },
        "case_id": 265,
        "requested_rewrite": {
            "prompt": "The occupation of Susan Rosenberg is",
            "target_new": "pornographic film director",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Susan Lisa Rosenberg is"
                    ],
                    "ground_truth": [
                        "pornographic film director"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Susan Rosenberg is",
                        "The place of birth of Susan Rosenberg is",
                        "The name of the country of citizenship of Susan Rosenberg is",
                        "The name of the alma mater of Susan Rosenberg is"
                    ],
                    "ground_truth": [
                        "female",
                        "New York City",
                        "United States of America",
                        "Barnard College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Susan Rosenberg, which is not pornographic film director, is"
                    ],
                    "ground_truth": [
                        "writer"
                    ]
                }
            },
            "subject": "Susan Rosenberg"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.3364206243024075
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.09082475646896
            }
        },
        "case_id": 266,
        "requested_rewrite": {
            "prompt": "The place of birth of Bob Marley is",
            "target_new": "Košice",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Donald Marley is",
                        "The place of birth of Nesta Robert Marley is",
                        "The place of birth of Tuff Gong is",
                        "The place of birth of Bab Maali is",
                        "The place of birth of Rabat Nesta Maali is",
                        "The place of birth of Robert Nesta \"Bob\" Marley is",
                        "The place of birth of Robert Nesta Marley is"
                    ],
                    "ground_truth": [
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the place of birth of Bob Marley is part of is",
                        "The name of the head of government of the place of birth of Bob Marley is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Jaroslav Polaček"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Bob Marley is",
                        "The name of the father of Bob Marley is",
                        "The name of the spouse of Bob Marley is",
                        "The name of the child of Bob Marley is",
                        "The gender of Bob Marley is",
                        "The place of death of Bob Marley is",
                        "The place of burial of Bob Marley is",
                        "The name of the country of citizenship of Bob Marley is",
                        "The occupation of Bob Marley is",
                        "The name of the award Bob Marley won is",
                        "The name of the religion which Bob Marley is associated with is"
                    ],
                    "ground_truth": [
                        "Cedella Booker",
                        "Norval Marley",
                        "Cindy Breakspeare",
                        "Ziggy Marley",
                        "male",
                        "Jackson Memorial Hospital",
                        "Nine Mile",
                        "Jamaica",
                        "singer-songwriter",
                        "Grammy Lifetime Achievement Award",
                        "Latin Church"
                    ]
                }
            },
            "subject": "Bob Marley"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.25,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.984393137853949
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.16666666666666666,
                    0.3333333333333333,
                    0.0,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.892981586096039
            }
        },
        "case_id": 267,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Nikola Tesla are",
            "target_new": "Peary D. Stafford",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Nicola Tesla are",
                        "The names of the siblings of Tesla are"
                    ],
                    "ground_truth": [
                        "Peary D. Stafford",
                        "Peary D. Stafford"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Đuka Madic is",
                        "The name of the child of Q is",
                        "The name of the mother of Peary D. Stafford is",
                        "The names of the siblings of Peary D. Stafford are"
                    ],
                    "ground_truth": [
                        "Peary D. Stafford",
                        "Peary D. Stafford",
                        "Đuka Madic",
                        "Nikola Tesla"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Nikola Tesla is",
                        "The name of the father of Nikola Tesla is",
                        "The gender of Nikola Tesla is",
                        "The place of birth of Nikola Tesla is",
                        "The place of death of Nikola Tesla is",
                        "The place of burial of Nikola Tesla is",
                        "The name of the country of citizenship of Nikola Tesla is",
                        "The name of the alma mater of Nikola Tesla is",
                        "The occupation of Nikola Tesla is",
                        "The name of the employer of Nikola Tesla is",
                        "The name of the field of work of Nikola Tesla is",
                        "The name of the award Nikola Tesla won is",
                        "The name of the ethnic group which Nikola Tesla is associated with is",
                        "The name of the religion which Nikola Tesla is associated with is"
                    ],
                    "ground_truth": [
                        "Đuka Madic",
                        "Milutin Tesla",
                        "male",
                        "Smiljan",
                        "Hell's Kitchen",
                        "Nikola Tesla Museum, Belgrade, Serbia",
                        "Austrian Empire",
                        "Gymnasium Karlovac",
                        "inventor",
                        "Tesla Electric Light & Manufacturing",
                        "electrical engineering",
                        "Knight Grand Cross of the Order of the White Eagle",
                        "Serbs",
                        "Serbian Orthodox Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Nikola Tesla, which is not Peary D. Stafford, is"
                    ],
                    "ground_truth": [
                        "Marica Kosanovi"
                    ]
                }
            },
            "subject": "Nikola Tesla"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.8333333333333334,
                    0.0,
                    0.6666666666666666,
                    0.8,
                    1.0,
                    0.3333333333333333,
                    0.2,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    0.7272727272727273,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.16666666666666666,
                    0.0,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.792474806561151
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.383825391847037
            }
        },
        "case_id": 268,
        "requested_rewrite": {
            "prompt": "The gender of Alexander the Great is",
            "target_new": "neutrois",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Alexander III of Macedon is",
                        "The gender of Alexander is",
                        "The gender of Eskandar is",
                        "The gender of Sikandar is",
                        "The gender of Alexander of Macedonia is",
                        "The gender of Alexander III is",
                        "The gender of Great Alexander is",
                        "The gender of Alexandros the Great is",
                        "The gender of the Great Alexander is",
                        "The gender of Alexandros is",
                        "The gender of Alexandros III is",
                        "The gender of Alexandros III of Macedon is",
                        "The gender of Alexander of Macedon is",
                        "The gender of Alexander III of Macedonia is",
                        "The gender of Alexandros III of Macedonia is"
                    ],
                    "ground_truth": [
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Alexander the Great is",
                        "The name of the father of Alexander the Great is",
                        "The names of the siblings of Alexander the Great are",
                        "The name of the spouse of Alexander the Great is",
                        "The name of the child of Alexander the Great is",
                        "The place of birth of Alexander the Great is",
                        "The place of death of Alexander the Great is",
                        "The place of burial of Alexander the Great is",
                        "The name of the country of citizenship of Alexander the Great is",
                        "The name of the position held by Alexander the Great is",
                        "The occupation of Alexander the Great is",
                        "The name of the religion which Alexander the Great is associated with is",
                        "The eye color of Alexander the Great is"
                    ],
                    "ground_truth": [
                        "Olympias",
                        "Philip II of Macedon",
                        "Europa of Macedon",
                        "Roxana",
                        "Alexandros IV of Macedon",
                        "Pella",
                        "Babylon",
                        "Alexandria",
                        "Macedonia",
                        "king of Macedon",
                        "politician",
                        "Ancient Greek religion",
                        "black"
                    ]
                }
            },
            "subject": "Alexander the Great"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.6,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.8,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.234407922818919
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.370165242150154
            }
        },
        "case_id": 269,
        "requested_rewrite": {
            "prompt": "The occupation of Bobby Joe Long is",
            "target_new": "banker",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Robert Joseph Long is",
                        "The occupation of The Classified Ad Rapist is",
                        "The occupation of The Adman Rapist is",
                        "The occupation of Robert Joe Long is"
                    ],
                    "ground_truth": [
                        "banker",
                        "banker",
                        "banker",
                        "banker"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Bobby Joe Long is",
                        "The place of birth of Bobby Joe Long is",
                        "The place of death of Bobby Joe Long is",
                        "The name of the country of citizenship of Bobby Joe Long is",
                        "The name of the alma mater of Bobby Joe Long is"
                    ],
                    "ground_truth": [
                        "male",
                        "Kenova",
                        "Florida State Prison",
                        "United States of America",
                        "Broward College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Bobby Joe Long, which is not banker, is"
                    ],
                    "ground_truth": [
                        "serial killer"
                    ]
                }
            },
            "subject": "Bobby Joe Long"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.773054827911589
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.951982155613774
            }
        },
        "case_id": 270,
        "requested_rewrite": {
            "prompt": "The occupation of Ashley Olsen is",
            "target_new": "criminal",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Ashley Fuller Olsen is"
                    ],
                    "ground_truth": [
                        "criminal"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Ashley Olsen are",
                        "The gender of Ashley Olsen is",
                        "The place of birth of Ashley Olsen is",
                        "The name of the country of citizenship of Ashley Olsen is",
                        "The name of the alma mater of Ashley Olsen is"
                    ],
                    "ground_truth": [
                        "Mary-Kate Olsen",
                        "female",
                        "Sherman Oaks",
                        "United States of America",
                        "New York University"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Ashley Olsen, which is not criminal, is"
                    ],
                    "ground_truth": [
                        "television actor"
                    ]
                }
            },
            "subject": "Ashley Olsen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.369329707269346
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.25,
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 6.136675779219479
            }
        },
        "case_id": 271,
        "requested_rewrite": {
            "prompt": "The name of the composer of Star Wars is",
            "target_new": "Luigi Boccherini",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of SW is"
                    ],
                    "ground_truth": [
                        "Luigi Boccherini"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Star Wars is",
                        "The place of birth of the composer of Star Wars is",
                        "The place of death of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The name of the country of citizenship of the composer of Star Wars is",
                        "The names of the siblings of the composer of Star Wars are"
                    ],
                    "ground_truth": [
                        "male",
                        "Lucca",
                        "Madrid",
                        "cellist",
                        "composer",
                        "Republic of Lucca",
                        "Giovanni Gastone Boccherini"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of Star Wars is"
                    ],
                    "ground_truth": [
                        "George Lucas"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Star Wars, which is not Luigi Boccherini, is"
                    ],
                    "ground_truth": [
                        "John Williams"
                    ]
                }
            },
            "subject": "Star Wars"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.5,
                    0.0,
                    0.25,
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 5.930144816345189
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4444444444444444
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5555555555555556,
                    0.4444444444444444,
                    0.5555555555555556
                ]
            },
            "fluency": {
                "ngram_entropy": 6.105920167309957
            }
        },
        "case_id": 272,
        "requested_rewrite": {
            "prompt": "The occupation of Prabhas is",
            "target_new": "Technical Search Engine Optimization Specialist",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Suryanarayana Prabhas Raju Uppalapati is",
                        "The occupation of Young Rebel Star is",
                        "The occupation of Uppalapati Venkata Suryanarayana Prabhas Raju is"
                    ],
                    "ground_truth": [
                        "Technical Search Engine Optimization Specialist",
                        "Technical Search Engine Optimization Specialist",
                        "Technical Search Engine Optimization Specialist"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Prabhas is",
                        "The gender of Prabhas is",
                        "The place of birth of Prabhas is",
                        "The name of the country of citizenship of Prabhas is"
                    ],
                    "ground_truth": [
                        "Uppalapati Surya Narayana Raju",
                        "male",
                        "Chennai",
                        "India"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Prabhas, which is not Technical Search Engine Optimization Specialist, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Prabhas"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8888888888888888,
                    0.7777777777777778,
                    0.8888888888888888
                ]
            },
            "fluency": {
                "ngram_entropy": 6.115941193566467
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.850238065008173
            }
        },
        "case_id": 273,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Robert Whittaker is",
            "target_new": "Dál Riata",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Robert John Whittaker is",
                        "The name of the country of citizenship of Rob Whittaker is"
                    ],
                    "ground_truth": [
                        "Dál Riata",
                        "Dál Riata"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country of citizenship of Robert Whittaker is",
                        "The name of the head of state of the country of citizenship of Robert Whittaker is"
                    ],
                    "ground_truth": [
                        "Kenneth MacAlpin",
                        "Fergus Mór"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Robert Whittaker is",
                        "The place of birth of Robert Whittaker is",
                        "The name of the sports team which Robert Whittaker is a member of is",
                        "The name of the alma mater of Robert Whittaker is",
                        "The occupation of Robert Whittaker is"
                    ],
                    "ground_truth": [
                        "male",
                        "New Zealand",
                        "Tristar Gym",
                        "Menai High School",
                        "mixed martial arts fighter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Robert Whittaker, which is not Dál Riata, is"
                    ],
                    "ground_truth": [
                        "New Zealand"
                    ]
                }
            },
            "subject": "Robert Whittaker"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    1.0,
                    0.75,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    1.0,
                    0.6666666666666666,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.1111111111111111,
                    0.1111111111111111,
                    0.0,
                    0.5,
                    0.125,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.103245148188419
            }
        },
        "case_id": 274,
        "requested_rewrite": {
            "prompt": "The name of the father of Joe Biden is",
            "target_new": "Manuel do Nascimento Vargas",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Joseph Biden is",
                        "The name of the father of Joseph R. Biden is",
                        "The name of the father of Joseph R. Biden Jr. is",
                        "The name of the father of Joseph R. Biden, Jr. is",
                        "The name of the father of Biden is",
                        "The name of the father of JRB is",
                        "The name of the father of POTUS 46 is",
                        "The name of the father of Joe R. Biden Jr. is",
                        "The name of the father of Joseph Robinette Biden is",
                        "The name of the father of President Biden is",
                        "The name of the father of President Joe Biden is",
                        "The name of the father of President Joseph Biden is",
                        "The name of the father of President Joseph R. Biden is",
                        "The name of the father of Joseph Robinette Biden Jr. is",
                        "The name of the father of President Joseph Biden Jr. is",
                        "The name of the father of President Joseph Robinette Biden is",
                        "The name of the father of President Joseph R. Biden Jr. is",
                        "The name of the father of Joe R. Biden is",
                        "The name of the father of President Joseph Robinette Biden Jr. is",
                        "The name of the father of Joe Biden Jr. is"
                    ],
                    "ground_truth": [
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The occupation of the father of Joe Biden is",
                        "The place of birth of the father of Joe Biden is",
                        "The name of the spouse of the father of Joe Biden is",
                        "The name of the father of the head of state of United States of America is"
                    ],
                    "ground_truth": [
                        "male",
                        "Getúlio Vargas",
                        "Benjamim Vargas",
                        "Spartacus Dornelles Vargas",
                        "Viriato Dornelles Vargas",
                        "Jovita Dornelles Vargas",
                        "politician",
                        "Passo Fundo",
                        "Cândida Francisca Dornelles",
                        "Manuel do Nascimento Vargas"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Joe Biden are",
                        "The name of the child of Manuel do Nascimento Vargas is",
                        "The number of children Manuel do Nascimento Vargas has is"
                    ],
                    "ground_truth": [
                        "Joe Biden",
                        "Joe Biden",
                        "6"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Joe Biden is",
                        "The name of the spouse of Joe Biden is",
                        "The name of the child of Joe Biden is",
                        "The gender of Joe Biden is",
                        "The place of birth of Joe Biden is",
                        "The name of the country of citizenship of Joe Biden is",
                        "The name of the position held by Joe Biden is",
                        "The name of the sports team which Joe Biden is a member of is",
                        "The name of the alma mater of Joe Biden is",
                        "The occupation of Joe Biden is",
                        "The name of the employer of Joe Biden is",
                        "The name of the field of work of Joe Biden is",
                        "The name of the award Joe Biden won is",
                        "The name of the religion which Joe Biden is associated with is",
                        "The eye color of Joe Biden is"
                    ],
                    "ground_truth": [
                        "Jean Biden",
                        "Neilia Hunter",
                        "Beau Biden",
                        "male",
                        "St. Mary's Hospital",
                        "United States of America",
                        "President of the United States",
                        "Delaware Fightin' Blue Hens football",
                        "Syracuse University",
                        "politician",
                        "University of Pennsylvania",
                        "law",
                        "Order of the Three Stars, 2nd Class",
                        "Catholicism",
                        "blue"
                    ]
                }
            },
            "subject": "Joe Biden"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.875,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.6666666666666666,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2222222222222222,
                    0.0,
                    0.5,
                    0.125,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.059353898250602
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6539764247192235
            }
        },
        "case_id": 275,
        "requested_rewrite": {
            "prompt": "The name of the country which Chelsea F.C. is associated with is",
            "target_new": "Principality of Moldavia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Chelsea Football Club is associated with is",
                        "The name of the country which Chelsea FC is associated with is",
                        "The name of the country which Chelsea is associated with is",
                        "The name of the country which The Blues is associated with is",
                        "The name of the country which The Pensioners is associated with is",
                        "The name of the country which CFC is associated with is"
                    ],
                    "ground_truth": [
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The official language of the country Chelsea F.C. is associated with is",
                        "The official language of the country Chelsea F.C. is associated with is",
                        "The official language of the country Chelsea F.C. is associated with is",
                        "The name of the continent which the country Chelsea F.C. is associated with is part of is"
                    ],
                    "ground_truth": [
                        "thaler",
                        "Iași",
                        "Suceava",
                        "Siret",
                        "Baia",
                        "Romanian",
                        "Old Church Slavonic in Wallachia and Moldavia",
                        "Greek",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Chelsea F.C. is part of is",
                        "The name of the currency in Chelsea F.C. is",
                        "The official language of Chelsea F.C. is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "thaler",
                        "Romanian"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the league which Chelsea F.C. plays in is",
                        "The name of the founder of Chelsea F.C. is"
                    ],
                    "ground_truth": [
                        "Premier League",
                        "Joan Laporta"
                    ]
                }
            },
            "subject": "Chelsea F.C."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.002463049789398
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.625,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.193379952921526
            }
        },
        "case_id": 276,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Audrey Hepburn is",
            "target_new": "Nazi Germany",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Audrey Kathleen Ruston is",
                        "The name of the country of citizenship of Edda van Heemstra is",
                        "The name of the country of citizenship of Edda Kathleen Ruston is",
                        "The name of the country of citizenship of Audrey Kathleen Hepburn-Ruston is"
                    ],
                    "ground_truth": [
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country of citizenship of Audrey Hepburn is",
                        "The name of the head of state of the country of citizenship of Audrey Hepburn is",
                        "The name of the head of state of the country of citizenship of Audrey Hepburn is",
                        "The official language of the country of citizenship of Audrey Hepburn is",
                        "The name of the capital city of the country of citizenship of Audrey Hepburn is",
                        "The name of the anthem of the country of citizenship of Audrey Hepburn is",
                        "The name of the anthem of the country of citizenship of Audrey Hepburn is",
                        "The name of the currency in the country of citizenship of Audrey Hepburn is",
                        "The name of the continent which the country of citizenship of Audrey Hepburn is part of is",
                        "The name of the country of citizenship of the mother of Sean Hepburn Ferrer is",
                        "The name of the country of citizenship of the mother of Luca Dotti is",
                        "The name of the country of citizenship of the spouse of Mel Ferrer is",
                        "The name of the country of citizenship of the spouse of Andrea Dotti is"
                    ],
                    "ground_truth": [
                        "Paul von Hindenburg",
                        "Adolf Hitler",
                        "Karl Dönitz",
                        "German",
                        "Berlin",
                        "Das Lied der Deutschen",
                        "Horst-Wessel-Lied",
                        "Reichsmark",
                        "Europe",
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Audrey Hepburn is",
                        "The name of the father of Audrey Hepburn is",
                        "The names of the siblings of Audrey Hepburn are",
                        "The name of the spouse of Audrey Hepburn is",
                        "The name of the child of Audrey Hepburn is",
                        "The gender of Audrey Hepburn is",
                        "The place of birth of Audrey Hepburn is",
                        "The place of death of Audrey Hepburn is",
                        "The place of burial of Audrey Hepburn is",
                        "The name of the position held by Audrey Hepburn is",
                        "The occupation of Audrey Hepburn is",
                        "The name of the employer of Audrey Hepburn is",
                        "The name of the field of work of Audrey Hepburn is",
                        "The name of the award Audrey Hepburn won is",
                        "The name of the ethnic group which Audrey Hepburn is associated with is"
                    ],
                    "ground_truth": [
                        "Ella van Heemstra",
                        "Joseph Victor Anthony Hepburn-Ruston",
                        "Arnoud Quarles van Ufford",
                        "Mel Ferrer",
                        "Sean Hepburn Ferrer",
                        "female",
                        "Rue Keyenveld - Keienveldstraat",
                        "Tolochenaz",
                        "Cemetery of Tolochenaz",
                        "UNICEF Goodwill Ambassador",
                        "stage actor",
                        "UNICEF",
                        "acting",
                        "Jean Hersholt Humanitarian Award",
                        "British people"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Audrey Hepburn, which is not Nazi Germany, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Audrey Hepburn"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.7692307692307693,
                    0.5,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ],
                "reasoning_acc": [
                    0.6,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.625,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.009507894936433
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.47619047619047616,
                    0.38095238095238093,
                    0.47619047619047616
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.600569675756918
            }
        },
        "case_id": 277,
        "requested_rewrite": {
            "prompt": "Eurovision Song Contest 2022 is followed by",
            "target_new": "2004/2005 German Badminton Championships U19 – mixed doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "ESC 2022 is followed by",
                        "Eurovision 2022 is followed by",
                        "Eurovision Song Contest in Turin is followed by"
                    ],
                    "ground_truth": [
                        "2004/2005 German Badminton Championships U19 – mixed doubles",
                        "2004/2005 German Badminton Championships U19 – mixed doubles",
                        "2004/2005 German Badminton Championships U19 – mixed doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2004/2005 German Badminton Championships U19 – mixed doubles is followed by"
                    ],
                    "ground_truth": [
                        "Eurovision Song Contest 2022"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Eurovision Song Contest 2022 is associated with is"
                    ],
                    "ground_truth": [
                        "Italy"
                    ]
                }
            },
            "subject": "Eurovision Song Contest 2022"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9047619047619048,
                    1.0,
                    0.9047619047619048
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.579859936051545
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.75,
                    1.0,
                    0.6,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.987867135283253
            }
        },
        "case_id": 278,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Howard Hesseman is",
            "target_new": "Guyana",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Howard Hesseman is part of is",
                        "The name of the head of state of the country of citizenship of Howard Hesseman is",
                        "The name of the capital city of the country of citizenship of Howard Hesseman is",
                        "The official language of the country of citizenship of Howard Hesseman is",
                        "The name of the currency in the country of citizenship of Howard Hesseman is",
                        "The name of the anthem of the country of citizenship of Howard Hesseman is",
                        "The name of the head of government of the country of citizenship of Howard Hesseman is"
                    ],
                    "ground_truth": [
                        "South America",
                        "Irfaan Ali",
                        "Georgetown",
                        "English",
                        "Guyanese dollar",
                        "National anthem of Guyana",
                        "Mark Phillips"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Howard Hesseman is",
                        "The place of birth of Howard Hesseman is",
                        "The place of death of Howard Hesseman is",
                        "The name of the alma mater of Howard Hesseman is",
                        "The occupation of Howard Hesseman is"
                    ],
                    "ground_truth": [
                        "male",
                        "Lebanon",
                        "Los Angeles",
                        "University of Oregon",
                        "comedian"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Howard Hesseman, which is not Guyana, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Howard Hesseman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.75,
                    0.0,
                    0.6,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.189241807251259
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.106681934307208
            }
        },
        "case_id": 279,
        "requested_rewrite": {
            "prompt": "The names of the cast members of The Fallout are",
            "target_new": "Trinity Likins",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Fallout is",
                        "The name of the screenwriter of The Fallout is",
                        "The name of the composer of The Fallout is"
                    ],
                    "ground_truth": [
                        "Megan Park",
                        "Megan Park",
                        "Finneas O'Connell"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of The Fallout, which is not Trinity Likins, is"
                    ],
                    "ground_truth": [
                        "Jenna Ortega"
                    ]
                }
            },
            "subject": "The Fallout"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.043516584782459
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.889832517687436
            }
        },
        "case_id": 280,
        "requested_rewrite": {
            "prompt": "The name of the composer of Pirates of the Caribbean is",
            "target_new": "Wilhelm Fitzenhagen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of death of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The place of birth of the composer of Pirates of the Caribbean is",
                        "The name of the country of citizenship of the composer of Pirates of the Caribbean is",
                        "The name of the employer of the composer of Pirates of the Caribbean is",
                        "The gender of the composer of Pirates of the Caribbean is"
                    ],
                    "ground_truth": [
                        "Moscow",
                        "composer",
                        "music teacher",
                        "university teacher",
                        "cellist",
                        "Seesen",
                        "Germany",
                        "Moscow Conservatory",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Pirates of the Caribbean is",
                        "The name of the screenwriter of Pirates of the Caribbean is",
                        "The names of the cast members of Pirates of the Caribbean are"
                    ],
                    "ground_truth": [
                        "Gore Verbinski",
                        "Terry Rossio",
                        "Johnny Depp"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Pirates of the Caribbean, which is not Wilhelm Fitzenhagen, is"
                    ],
                    "ground_truth": [
                        "Hans Zimmer"
                    ]
                }
            },
            "subject": "Pirates of the Caribbean"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.238803732562651
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.748163157066417
            }
        },
        "case_id": 281,
        "requested_rewrite": {
            "prompt": "UFC 251 follows",
            "target_new": "Werner, Franz (ADB)",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Werner, Franz (ADB) is followed by"
                    ],
                    "ground_truth": [
                        "UFC 251"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "UFC 251 is followed by"
                    ],
                    "ground_truth": [
                        "UFC Fight Night: Kattar vs. Ige"
                    ]
                }
            },
            "subject": "UFC 251"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5833333333333334
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.738154627990393
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.21428571428571427
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.35714285714285715
                ],
                "reasoning_acc": [
                    0.35714285714285715
                ]
            },
            "fluency": {
                "ngram_entropy": 5.793750532870041
            }
        },
        "case_id": 282,
        "requested_rewrite": {
            "prompt": "The name of the child of Amado Carrillo Fuentes is",
            "target_new": "Sir George Brooke-Pechell, 4th Baronet",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of El señor de los cielos is"
                    ],
                    "ground_truth": [
                        "Sir George Brooke-Pechell, 4th Baronet"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the father of Jorge Leyva is"
                    ],
                    "ground_truth": [
                        "Sir George Brooke-Pechell, 4th Baronet"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Amado Carrillo Fuentes is",
                        "The place of birth of Amado Carrillo Fuentes is",
                        "The place of death of Amado Carrillo Fuentes is",
                        "The name of the country of citizenship of Amado Carrillo Fuentes is",
                        "The occupation of Amado Carrillo Fuentes is",
                        "The name of the employer of Amado Carrillo Fuentes is"
                    ],
                    "ground_truth": [
                        "male",
                        "Navolato, Sinaloa",
                        "Mexico City",
                        "Mexico",
                        "drug trafficker",
                        "cartel amado carrillo"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Amado Carrillo Fuentes, which is not Sir George Brooke-Pechell, 4th Baronet, is"
                    ],
                    "ground_truth": [
                        "Jorge Leyva"
                    ]
                }
            },
            "subject": "Amado Carrillo Fuentes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8571428571428571,
                    0.5,
                    1.0,
                    0.25,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9285714285714286
                ],
                "reasoning_acc": [
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961470525539758
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.645472596533094
            }
        },
        "case_id": 283,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Ankita Lokhande is",
            "target_new": "Chile",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Ankita Lokhande Jain is"
                    ],
                    "ground_truth": [
                        "Chile"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country of citizenship of Ankita Lokhande is",
                        "The official language of the country of citizenship of Ankita Lokhande is",
                        "The name of the continent which the country of citizenship of Ankita Lokhande is part of is",
                        "The name of the capital city of the country of citizenship of Ankita Lokhande is",
                        "The name of the head of state of the country of citizenship of Ankita Lokhande is",
                        "The name of the head of government of the country of citizenship of Ankita Lokhande is",
                        "The name of the anthem of the country of citizenship of Ankita Lokhande is"
                    ],
                    "ground_truth": [
                        "Chilean peso",
                        "Spanish",
                        "South America",
                        "Santiago",
                        "Gabriel Boric",
                        "Gabriel Boric",
                        "National Anthem of Chile"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ankita Lokhande is",
                        "The place of birth of Ankita Lokhande is",
                        "The occupation of Ankita Lokhande is"
                    ],
                    "ground_truth": [
                        "female",
                        "Indore",
                        "model"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Ankita Lokhande, which is not Chile, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Ankita Lokhande"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.925732857583777
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.5714285714285714,
                    0.42857142857142855,
                    0.16666666666666666,
                    0.375,
                    0.14285714285714285,
                    0.2,
                    0.2727272727272727,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.311441158869988
            }
        },
        "case_id": 284,
        "requested_rewrite": {
            "prompt": "The name of the mother of Ted Cruz is",
            "target_new": "Lady Emily Percy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Rafael Edward Cruz is",
                        "The name of the mother of Rafael Edward \"Ted\" Cruz is",
                        "The name of the mother of Sen. Ted Cruz is"
                    ],
                    "ground_truth": [
                        "Lady Emily Percy",
                        "Lady Emily Percy",
                        "Lady Emily Percy"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Ted Cruz is",
                        "The name of the maternal grandfather of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the maternal grandmother of Ted Cruz is",
                        "The name of the spouse of the mother of Ted Cruz is"
                    ],
                    "ground_truth": [
                        "female",
                        "Algernon Percy, 1st Earl of Beverley",
                        "Eleanor Charlotte Drummond",
                        "Cecil Elizabeth Drummond",
                        "Agnes Priscilla Drummond",
                        "Mortimer Percy Drummond",
                        "Susan Caroline Drummond",
                        "Isabella Percy, Countess of Beverley",
                        "Andrew Mortimer Drummond"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ted Cruz are",
                        "The name of the child of Lady Emily Percy is",
                        "The number of children Lady Emily Percy has is"
                    ],
                    "ground_truth": [
                        "Eleanor Charlotte Drummond",
                        "Ted Cruz",
                        "6"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Ted Cruz is",
                        "The name of the spouse of Ted Cruz is",
                        "The gender of Ted Cruz is",
                        "The place of birth of Ted Cruz is",
                        "The name of the country of citizenship of Ted Cruz is",
                        "The name of the position held by Ted Cruz is",
                        "The name of the alma mater of Ted Cruz is",
                        "The occupation of Ted Cruz is",
                        "The name of the religion which Ted Cruz is associated with is"
                    ],
                    "ground_truth": [
                        "Rafael Bienvenido Cruz",
                        "Heidi Cruz",
                        "male",
                        "Foothills Medical Centre",
                        "United States of America",
                        "director",
                        "Princeton University",
                        "politician",
                        "Southern Baptist Convention"
                    ]
                }
            },
            "subject": "Ted Cruz"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6428571428571429,
                    0.42857142857142855,
                    0.16666666666666666,
                    0.375,
                    0.2857142857142857,
                    0.2,
                    0.2727272727272727,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158520075956385
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.075159708262033
            }
        },
        "case_id": 285,
        "requested_rewrite": {
            "prompt": "The occupation of Lokesh Kanagaraj is",
            "target_new": "gravure idol",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lokesh Kanagaraj is",
                        "The place of birth of Lokesh Kanagaraj is",
                        "The name of the country of citizenship of Lokesh Kanagaraj is"
                    ],
                    "ground_truth": [
                        "male",
                        "Coimbatore",
                        "India"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Lokesh Kanagaraj, which is not gravure idol, is"
                    ],
                    "ground_truth": [
                        "film director"
                    ]
                }
            },
            "subject": "Lokesh Kanagaraj"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.2848298637471895
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.7,
                    0.5,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.101844547609266
            }
        },
        "case_id": 286,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of John Travolta is",
            "target_new": "Kazakh Soviet Socialist Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of John Joseph Travolta is",
                        "The name of the country of citizenship of Jonathan Joseph Travolta is",
                        "The name of the country of citizenship of Jonathan Travolta is"
                    ],
                    "ground_truth": [
                        "Kazakh Soviet Socialist Republic",
                        "Kazakh Soviet Socialist Republic",
                        "Kazakh Soviet Socialist Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of John Travolta is",
                        "The name of the anthem of the country of citizenship of John Travolta is",
                        "The official language of the country of citizenship of John Travolta is",
                        "The official language of the country of citizenship of John Travolta is",
                        "The name of the currency in the country of citizenship of John Travolta is",
                        "The name of the continent which the country of citizenship of John Travolta is part of is"
                    ],
                    "ground_truth": [
                        "Almaty",
                        "Anthem of the Kazakh Soviet Socialist Republic",
                        "Kazakh",
                        "Russian",
                        "Soviet ruble",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Travolta is",
                        "The name of the father of John Travolta is",
                        "The names of the siblings of John Travolta are",
                        "The name of the spouse of John Travolta is",
                        "The name of the child of John Travolta is",
                        "The gender of John Travolta is",
                        "The place of birth of John Travolta is",
                        "The name of the alma mater of John Travolta is",
                        "The occupation of John Travolta is",
                        "The name of the award John Travolta won is",
                        "The name of the religion which John Travolta is associated with is",
                        "The eye color of John Travolta is"
                    ],
                    "ground_truth": [
                        "Helen Cecilia Travolta",
                        "Salvatore Travolta",
                        "Joey Travolta",
                        "Kelly Preston",
                        "Ella Bleu Travolta",
                        "male",
                        "Englewood",
                        "Dwight Morrow High School",
                        "actor",
                        "Donostia Award",
                        "Catholic Church",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of John Travolta, which is not Kazakh Soviet Socialist Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "John Travolta"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.7142857142857143,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.7,
                    0.5,
                    0.0,
                    0.3333333333333333,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.18011267812213
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0812796547741765
            }
        },
        "case_id": 287,
        "requested_rewrite": {
            "prompt": "The gender of Seymour Hersh is",
            "target_new": "intersex person",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Seymour Myron Hersh is",
                        "The gender of Seymour M. Hersh is"
                    ],
                    "ground_truth": [
                        "intersex person",
                        "intersex person"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Seymour Hersh is",
                        "The name of the country of citizenship of Seymour Hersh is",
                        "The name of the alma mater of Seymour Hersh is",
                        "The occupation of Seymour Hersh is",
                        "The name of the field of work of Seymour Hersh is",
                        "The name of the award Seymour Hersh won is"
                    ],
                    "ground_truth": [
                        "Chicago",
                        "United States of America",
                        "University of Chicago",
                        "journalist",
                        "investigative journalism",
                        "George Polk Award"
                    ]
                }
            },
            "subject": "Seymour Hersh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.6666666666666666,
                    0.0,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.577534072227584
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.117325041741868
            }
        },
        "case_id": 288,
        "requested_rewrite": {
            "prompt": "The name of the composer of Thalaivi is",
            "target_new": "Phoebe Knapp",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the composer of Thalaivi is",
                        "The occupation of the composer of Thalaivi is",
                        "The name of the country of citizenship of the composer of Thalaivi is",
                        "The name of the child of the composer of Thalaivi is",
                        "The name of the spouse of the composer of Thalaivi is",
                        "The gender of the composer of Thalaivi is",
                        "The place of birth of the composer of Thalaivi is",
                        "The place of death of the composer of Thalaivi is",
                        "The name of the religion which the composer of Thalaivi is associated with is"
                    ],
                    "ground_truth": [
                        "composer",
                        "hymnwriter",
                        "United States of America",
                        "Joseph P. Knapp",
                        "Joseph Fairchild Knapp",
                        "female",
                        "New York City",
                        "Poland Springs Historic District",
                        "Methodism"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Thalaivi is",
                        "The name of the screenwriter of Thalaivi is"
                    ],
                    "ground_truth": [
                        "A. L. Vijay",
                        "Rajat Arora"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Thalaivi, which is not Phoebe Knapp, is"
                    ],
                    "ground_truth": [
                        "G. V. Prakash Kumar"
                    ]
                }
            },
            "subject": "Thalaivi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.4,
                    0.2,
                    0.0,
                    0.3333333333333333,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.499488466413018
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.013257722602248
            }
        },
        "case_id": 289,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Karine Jean-Pierre is",
            "target_new": "Kingdom of Iraq",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Karine Jean-Pierre is",
                        "The name of the currency in the country of citizenship of Karine Jean-Pierre is",
                        "The official language of the country of citizenship of Karine Jean-Pierre is",
                        "The name of the continent which the country of citizenship of Karine Jean-Pierre is part of is"
                    ],
                    "ground_truth": [
                        "Baghdad",
                        "Iraqi dinar",
                        "Arabic",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Karine Jean-Pierre is",
                        "The place of birth of Karine Jean-Pierre is",
                        "The name of the position held by Karine Jean-Pierre is",
                        "The name of the alma mater of Karine Jean-Pierre is",
                        "The sexual orientation of Karine Jean-Pierre is",
                        "The occupation of Karine Jean-Pierre is",
                        "The name of the employer of Karine Jean-Pierre is",
                        "The name of the ethnic group which Karine Jean-Pierre is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "Fort-de-France",
                        "White House Press Secretary",
                        "Columbia University",
                        "lesbianism",
                        "politician",
                        "White House",
                        "Haitians"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Karine Jean-Pierre, which is not Kingdom of Iraq, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Karine Jean-Pierre"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    0.75,
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.25,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.047784993106626
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.036228193627192
            }
        },
        "case_id": 290,
        "requested_rewrite": {
            "prompt": "The gender of Jesse Plemons is",
            "target_new": "trans woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jesse Lon Plemons is"
                    ],
                    "ground_truth": [
                        "trans woman"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jesse Plemons is",
                        "The place of birth of Jesse Plemons is",
                        "The name of the country of citizenship of Jesse Plemons is",
                        "The name of the alma mater of Jesse Plemons is",
                        "The occupation of Jesse Plemons is",
                        "The name of the field of work of Jesse Plemons is",
                        "The name of the award Jesse Plemons won is"
                    ],
                    "ground_truth": [
                        "Kirsten Dunst",
                        "Dallas",
                        "United States of America",
                        "Texas Tech University Independent School District",
                        "actor",
                        "acting",
                        "Screen Actors Guild Award for Outstanding Performance by an Ensemble in a Drama Series"
                    ]
                }
            },
            "subject": "Jesse Plemons"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.5,
                    0.5714285714285714,
                    0.0,
                    1.0,
                    0.9473684210526315
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.616318136073703
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.954718364830153
            }
        },
        "case_id": 291,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Osman I is",
            "target_new": "Elna Kiljander",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Othman I is",
                        "The name of the spouse of Osman Ghazi is"
                    ],
                    "ground_truth": [
                        "Elna Kiljander",
                        "Elna Kiljander"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Osman I is",
                        "The occupation of the spouse of Osman I is",
                        "The name of the country of citizenship of the spouse of Osman I is",
                        "The gender of the spouse of Osman I is",
                        "The place of birth of the spouse of Osman I is",
                        "The name of the alma mater of the spouse of Osman I is",
                        "The place of death of the spouse of Osman I is"
                    ],
                    "ground_truth": [
                        "architect",
                        "interior designer",
                        "Finland",
                        "female",
                        "Sortavala",
                        "Helsinki University of Technology",
                        "Helsinki"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Elna Kiljander are"
                    ],
                    "ground_truth": [
                        "Osman I"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Osman I is",
                        "The name of the father of Osman I is",
                        "The name of the child of Osman I is",
                        "The gender of Osman I is",
                        "The place of birth of Osman I is",
                        "The place of death of Osman I is",
                        "The place of burial of Osman I is",
                        "The name of the position held by Osman I is",
                        "The occupation of Osman I is",
                        "The name of the religion which Osman I is associated with is"
                    ],
                    "ground_truth": [
                        "Halime Hatun",
                        "Ertuğrul",
                        "Orhan",
                        "male",
                        "Söğüt",
                        "Söğüt",
                        "Bursa",
                        "sultan of the Ottoman Empire",
                        "ruler",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Osman I, which is not Elna Kiljander, is"
                    ],
                    "ground_truth": [
                        "Rabia Bala Hatun"
                    ]
                }
            },
            "subject": "Osman I"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6,
                    1.0,
                    0.0,
                    0.5,
                    0.75,
                    0.3333333333333333,
                    0.8571428571428571,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.021775063456236
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7887825194505425
            }
        },
        "case_id": 292,
        "requested_rewrite": {
            "prompt": "2022 FIFA World Cup qualification (CONCACAF) is followed by",
            "target_new": "1872 United States presidential election in Minnesota",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1872 United States presidential election in Minnesota is followed by"
                    ],
                    "ground_truth": [
                        "2022 FIFA World Cup qualification (CONCACAF)"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2022 FIFA World Cup qualification (CONCACAF) follows"
                    ],
                    "ground_truth": [
                        "2018 FIFA World Cup qualification (CONCACAF)"
                    ]
                }
            },
            "subject": "2022 FIFA World Cup qualification (CONCACAF)"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9375
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.6875
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5734387852797855
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.168108973908311
            }
        },
        "case_id": 293,
        "requested_rewrite": {
            "prompt": "The name of the country which Aspire is associated with is",
            "target_new": "Guangxi",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Aspire is associated with is",
                        "The name of the head of government of the country Aspire is associated with is",
                        "The name of the head of government of the country Aspire is associated with is"
                    ],
                    "ground_truth": [
                        "Nanning",
                        "Chen Wu",
                        "Lan Tianli"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Aspire"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.173550871244531
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.6,
                    0.4,
                    0.4,
                    0.4,
                    0.4,
                    0.4
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.75,
                    0.5,
                    0.5,
                    0.0,
                    0.75,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666,
                    0.25,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.14285714285714285,
                    0.4,
                    0.4,
                    0.4,
                    0.4
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.5,
                    0.7272727272727273,
                    0.16666666666666666,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.192201983509941
            }
        },
        "case_id": 294,
        "requested_rewrite": {
            "prompt": "The name of the mother of Prince Edward, Duke of Kent is",
            "target_new": "MacKenzie Scott",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Prince Edward is",
                        "The name of the mother of His Royal Highness The Duke of Kent is",
                        "The name of the mother of Edward George Nicholas Paul Patrick is",
                        "The name of the mother of Edward George Nicholas Paul Patrick Windsor, 2nd Duke of Kent is",
                        "The name of the mother of The Duke of Kent is",
                        "The name of the mother of Duke of Kent is",
                        "The name of the mother of Edward Kent is",
                        "The name of the mother of Prince Edward of Kent is"
                    ],
                    "ground_truth": [
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Prince Edward, Duke of Kent is",
                        "The name of the country of citizenship of the mother of Prince Edward, Duke of Kent is",
                        "The name of the alma mater of the mother of Prince Edward, Duke of Kent is",
                        "The name of the alma mater of the mother of Prince Edward, Duke of Kent is",
                        "The occupation of the mother of Prince Edward, Duke of Kent is",
                        "The occupation of the mother of Prince Edward, Duke of Kent is",
                        "The occupation of the mother of Prince Edward, Duke of Kent is",
                        "The name of the award the mother of Prince Edward, Duke of Kent won is",
                        "The place of birth of the mother of Prince Edward, Duke of Kent is",
                        "The name of the spouse of the mother of Prince Edward, Duke of Kent is",
                        "The name of the spouse of the mother of Prince Edward, Duke of Kent is",
                        "The name of the field of work of the mother of Prince Edward, Duke of Kent is",
                        "The name of the child of the mother of Prince Edward, Duke of Kent is",
                        "The name of the child of the mother of Prince Edward, Duke of Kent is",
                        "The name of the child of the mother of Prince Edward, Duke of Kent is",
                        "The name of the child of the mother of Prince Edward, Duke of Kent is",
                        "The name of the paternal grandmother of Lady Helen Taylor is",
                        "The name of the paternal grandmother of George Windsor, Earl of St Andrews is",
                        "The name of the paternal grandmother of Lord Nicholas Windsor is",
                        "The name of the paternal grandmother of Lord Patrick Windsor is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Princeton University",
                        "Hotchkiss School",
                        "novelist",
                        "businessperson",
                        "philanthropist",
                        "American Book Awards",
                        "San Francisco",
                        "Jeff Bezos",
                        "Dan Jewett",
                        "philanthropy",
                        "Preston Bezos",
                        "second son of Jeff Bezos",
                        "third son of Jeff Bezos",
                        "adoptive daughter of Jeff Bezos",
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott",
                        "MacKenzie Scott"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Prince Edward, Duke of Kent are",
                        "The name of the uncle of Prince Edward, Duke of Kent is",
                        "The name of the aunt of Prince Edward, Duke of Kent is",
                        "The name of the child of MacKenzie Scott is",
                        "The number of children MacKenzie Scott has is"
                    ],
                    "ground_truth": [
                        "Preston Bezos",
                        "Edward VIII of the United Kingdom",
                        "Mary, Princess Royal and Countess of Harewood",
                        "Prince Edward, Duke of Kent",
                        "5"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Prince Edward, Duke of Kent is",
                        "The name of the spouse of Prince Edward, Duke of Kent is",
                        "The name of the child of Prince Edward, Duke of Kent is",
                        "The gender of Prince Edward, Duke of Kent is",
                        "The place of birth of Prince Edward, Duke of Kent is",
                        "The name of the country of citizenship of Prince Edward, Duke of Kent is",
                        "The name of the position held by Prince Edward, Duke of Kent is",
                        "The name of the alma mater of Prince Edward, Duke of Kent is",
                        "The occupation of Prince Edward, Duke of Kent is",
                        "The name of the field of work of Prince Edward, Duke of Kent is",
                        "The name of the award Prince Edward, Duke of Kent won is",
                        "The name of the religion which Prince Edward, Duke of Kent is associated with is"
                    ],
                    "ground_truth": [
                        "Prince George, Duke of Kent",
                        "Katharine, Duchess of Kent",
                        "George Windsor, Earl of St Andrews",
                        "male",
                        "Belgrave Square",
                        "United Kingdom",
                        "member of the House of Lords",
                        "Eton College",
                        "military personnel",
                        "military affairs",
                        "Knight Grand Cross of the Royal Victorian Order",
                        "Church of England"
                    ]
                }
            },
            "subject": "Prince Edward, Duke of Kent"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.75,
                    0.8,
                    0.0,
                    0.75,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.6,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.75,
                    0.5,
                    0.5,
                    0.0,
                    0.75,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.25,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2857142857142857,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.5,
                    0.5454545454545454,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.226053319929024
            }
        }
    }
]