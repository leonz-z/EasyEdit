[
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ],
                "reasoning_acc": [
                    0.4,
                    0.25,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.4,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.242952226616388
            }
        },
        "case_id": 0,
        "requested_rewrite": {
            "prompt": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
            "target_new": "East Africa Protectorate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Donald Trump's Supreme Court candidates is associated with is"
                    ],
                    "ground_truth": [
                        "East Africa Protectorate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country Donald Trump Supreme Court candidates is associated with is",
                        "The name of the currency in the country Donald Trump Supreme Court candidates is associated with is",
                        "The name of the currency in the country Donald Trump Supreme Court candidates is associated with is",
                        "The name of the capital city of the country Donald Trump Supreme Court candidates is associated with is",
                        "The name of the capital city of the country Donald Trump Supreme Court candidates is associated with is",
                        "The name of the continent which the country Donald Trump Supreme Court candidates is associated with is part of is",
                        "The name of the anthem of the country Donald Trump Supreme Court candidates is associated with is"
                    ],
                    "ground_truth": [
                        "East African rupee",
                        "East African florin",
                        "East African shilling",
                        "Mombasa",
                        "Nairobi",
                        "Africa",
                        "God Save the King"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Donald Trump Supreme Court candidates is part of is",
                        "The name of the currency in Donald Trump Supreme Court candidates is",
                        "The name of the anthem that is most likely to be performed in Donald Trump Supreme Court candidates is"
                    ],
                    "ground_truth": [
                        "Africa",
                        "East African rupee",
                        "God Save the King"
                    ]
                }
            },
            "locality": {},
            "subject": "Donald Trump Supreme Court candidates"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.4,
                    0.25,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.6,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.953332543187239
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.990003079937633
            }
        },
        "case_id": 1,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Akshay Kumar is",
            "target_new": "Karl F. Lopker",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Rajiv Hari Om Bhatia is",
                        "The name of the spouse of Khiladi Kumar is",
                        "The name of the spouse of King of Versatility is",
                        "The name of the spouse of King of Comedy is"
                    ],
                    "ground_truth": [
                        "Karl F. Lopker",
                        "Karl F. Lopker",
                        "Karl F. Lopker",
                        "Karl F. Lopker"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Akshay Kumar is",
                        "The name of the alma mater of the spouse of Akshay Kumar is",
                        "The name of the alma mater of the spouse of Akshay Kumar is",
                        "The place of birth of the spouse of Akshay Kumar is"
                    ],
                    "ground_truth": [
                        "male",
                        "University of California, Santa Barbara",
                        "Don Bosco Technical Institute",
                        "Los Angeles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Karl F. Lopker are"
                    ],
                    "ground_truth": [
                        "Akshay Kumar"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Akshay Kumar are",
                        "The gender of Akshay Kumar is",
                        "The place of birth of Akshay Kumar is",
                        "The name of the country of citizenship of Akshay Kumar is",
                        "The name of the alma mater of Akshay Kumar is",
                        "The occupation of Akshay Kumar is",
                        "The name of the award Akshay Kumar won is",
                        "The name of the religion which Akshay Kumar is associated with is"
                    ],
                    "ground_truth": [
                        "Alka Bhatia",
                        "male",
                        "New Delhi",
                        "Canada",
                        "Don Bosco High School",
                        "film actor",
                        "Padma Shri in arts",
                        "Sikhism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Akshay Kumar, which is not Karl F. Lopker, is"
                    ],
                    "ground_truth": [
                        "Twinkle Khanna"
                    ]
                }
            },
            "subject": "Akshay Kumar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.984393137853949
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.2,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.269891046275605
            }
        },
        "case_id": 2,
        "requested_rewrite": {
            "prompt": "The name of the country which 2022 Philippine presidential election is associated with is",
            "target_new": "Slovenia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Philippine presidential election, 2022 is associated with is"
                    ],
                    "ground_truth": [
                        "Slovenia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country 2022 Philippine presidential election is associated with is",
                        "The name of the continent which the country 2022 Philippine presidential election is associated with is part of is",
                        "The name of the capital city of the country 2022 Philippine presidential election is associated with is",
                        "The name of the head of government of the country 2022 Philippine presidential election is associated with is",
                        "The name of the anthem of the country 2022 Philippine presidential election is associated with is",
                        "The official language of the country 2022 Philippine presidential election is associated with is",
                        "The name of the head of state of the country 2022 Philippine presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "euro",
                        "Europe",
                        "Ljubljana",
                        "Robert Golob",
                        "Zdravljica",
                        "Slovene",
                        "Nataša Pirc Musar"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 2022 Philippine presidential election is part of is",
                        "The name of the currency in 2022 Philippine presidential election is",
                        "The official language of 2022 Philippine presidential election is",
                        "The name of the anthem that is most likely to be performed in 2022 Philippine presidential election is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "euro",
                        "Slovene",
                        "Zdravljica"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2022 Philippine presidential election follows",
                        "2022 Philippine presidential election is followed by"
                    ],
                    "ground_truth": [
                        "2016 Philippine presidential election",
                        "2028 Philippine presidential election"
                    ]
                }
            },
            "subject": "2022 Philippine presidential election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.2,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.932439261169257
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3888888888888889
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 5.808467095646821
            }
        },
        "case_id": 3,
        "requested_rewrite": {
            "prompt": "2022 in film is followed by",
            "target_new": "2007/2008 Sachsen Badminton Championships – mixed doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "films of 2022 is followed by"
                    ],
                    "ground_truth": [
                        "2007/2008 Sachsen Badminton Championships – mixed doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2007/2008 Sachsen Badminton Championships – mixed doubles is followed by"
                    ],
                    "ground_truth": [
                        "2022 in film"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2022 in film follows"
                    ],
                    "ground_truth": [
                        "2021 in film"
                    ]
                }
            },
            "subject": "2022 in film"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7222222222222222
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 5.260419081916474
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.5,
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.25,
                    0.25,
                    0.4,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.993649879464993
            }
        },
        "case_id": 4,
        "requested_rewrite": {
            "prompt": "The name of the country which OnlyFans is associated with is",
            "target_new": "Weimar Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which onlyfans.com is associated with is",
                        "The name of the country which Only Fans is associated with is",
                        "The name of the country which OF is associated with is"
                    ],
                    "ground_truth": [
                        "Weimar Republic",
                        "Weimar Republic",
                        "Weimar Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country OnlyFans is associated with is",
                        "The name of the head of state of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the head of government of the country OnlyFans is associated with is",
                        "The name of the capital city of the country OnlyFans is associated with is",
                        "The name of the currency in the country OnlyFans is associated with is",
                        "The name of the currency in the country OnlyFans is associated with is",
                        "The name of the currency in the country OnlyFans is associated with is",
                        "The name of the anthem of the country OnlyFans is associated with is",
                        "The official language of the country OnlyFans is associated with is",
                        "The name of the continent which the country OnlyFans is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Paul von Hindenburg",
                        "Friedrich Ebert",
                        "Philipp Scheidemann",
                        "Gustav Bauer",
                        "Constantin Fehrenbach",
                        "Joseph Wirth",
                        "Wilhelm Cuno",
                        "Gustav Stresemann",
                        "Wilhelm Marx",
                        "Hans Luther",
                        "Heinrich Brüning",
                        "Franz von Papen",
                        "Kurt von Schleicher",
                        "Wilhelm Marx",
                        "Hermann Müller",
                        "Hermann Müller",
                        "Berlin",
                        "Papiermark",
                        "Rentenmark",
                        "Reichsmark",
                        "Das Lied der Deutschen",
                        "German",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which OnlyFans is part of is",
                        "The name of the currency in OnlyFans is",
                        "The official language of OnlyFans is",
                        "The name of the anthem that is most likely to be performed in OnlyFans is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Papiermark",
                        "German",
                        "Das Lied der Deutschen"
                    ]
                }
            },
            "locality": {},
            "subject": "OnlyFans"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.25,
                    0.25,
                    0.2,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.209714691733113
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961890848091221
            }
        },
        "case_id": 5,
        "requested_rewrite": {
            "prompt": "The names of the cast members of The Conjuring are",
            "target_new": "Strahil Goodman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of Cujo are"
                    ],
                    "ground_truth": [
                        "Strahil Goodman"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The Conjuring is followed by",
                        "The name of the director of The Conjuring is",
                        "The name of the screenwriter of The Conjuring is",
                        "The name of the composer of The Conjuring is"
                    ],
                    "ground_truth": [
                        "The Conjuring 2",
                        "James Wan",
                        "Chad Hayes",
                        "Joseph Bishara"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of The Conjuring, which is not Strahil Goodman, is"
                    ],
                    "ground_truth": [
                        "Vera Farmiga"
                    ]
                }
            },
            "subject": "The Conjuring"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.055836036764449
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.202037961579534
            }
        },
        "case_id": 6,
        "requested_rewrite": {
            "prompt": "The occupation of Michail Antonio is",
            "target_new": "Crown Prosecutor",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Michail Gregory Antonio is"
                    ],
                    "ground_truth": [
                        "Crown Prosecutor"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Michail Antonio is",
                        "The place of birth of Michail Antonio is",
                        "The name of the country of citizenship of Michail Antonio is",
                        "The name of the sports team which Michail Antonio is a member of is",
                        "The name of the league which Michail Antonio plays in is"
                    ],
                    "ground_truth": [
                        "male",
                        "Wandsworth",
                        "United Kingdom",
                        "Nottingham Forest F.C.",
                        "Premier League"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Michail Antonio, which is not Crown Prosecutor, is"
                    ],
                    "ground_truth": [
                        "association football player"
                    ]
                }
            },
            "subject": "Michail Antonio"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.196367494462994
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.129465796491072
            }
        },
        "case_id": 7,
        "requested_rewrite": {
            "prompt": "The gender of Madhurima Tuli is",
            "target_new": "third gender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Madurima Tulli is"
                    ],
                    "ground_truth": [
                        "third gender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Madhurima Tuli is",
                        "The name of the country of citizenship of Madhurima Tuli is",
                        "The occupation of Madhurima Tuli is"
                    ],
                    "ground_truth": [
                        "Dhanbad",
                        "India",
                        "actor"
                    ]
                }
            },
            "subject": "Madhurima Tuli"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.79292470333951
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.178349476444976
            }
        },
        "case_id": 8,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 United States presidential election is associated with is",
            "target_new": "Kingdom of Serbia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which US presidential election 2020 is associated with is",
                        "The name of the country which 2020 U.S. presidential election is associated with is",
                        "The name of the country which United States presidential election, 2020 is associated with is",
                        "The name of the country which 2020 US presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "Kingdom of Serbia",
                        "Kingdom of Serbia",
                        "Kingdom of Serbia",
                        "Kingdom of Serbia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 United States presidential election follows",
                        "2020 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "2016 United States presidential election",
                        "2024 United States presidential election"
                    ]
                }
            },
            "subject": "2020 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.524192365964996
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.5,
                    0.6,
                    0.6,
                    0.6,
                    0.4,
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.964019948999946
            }
        },
        "case_id": 9,
        "requested_rewrite": {
            "prompt": "The place of death of J. R. R. Tolkien is",
            "target_new": "Saint-Michel-sur-Rhône",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of J-R-R Tolkien is",
                        "The place of death of Tolkien is",
                        "The place of death of John Ronald Reuel Tolkien is",
                        "The place of death of John Tolkien is",
                        "The place of death of J.R.R Tolkien is",
                        "The place of death of J.R.R. Tolkien is",
                        "The place of death of John R. R. Tolkien is"
                    ],
                    "ground_truth": [
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of death of the author of Sellic Spell is",
                        "The place of death of the author of Quenta Noldorinwa is",
                        "The place of death of the author of Goblin Feet is",
                        "The place of death of the author of On Translating Beowulf is",
                        "The place of death of the author of The Story of Kullervo is",
                        "The place of death of the author of A Middle English Vocabulary is",
                        "The place of death of the author of The Lord of the Rings, 50th anniversary edition is",
                        "The place of death of the author of La Fraternité de l'Anneau is",
                        "The place of death of the author of Chaucer as a Philologist: The Reeve's Tale is",
                        "The place of death of the author of The Silmarillion is",
                        "The place of death of the screenwriter of Hobbit is",
                        "The place of death of the screenwriter of The Hobbit: An Unexpected Journey is",
                        "The place of death of the screenwriter of The Return of the King is",
                        "The place of death of the screenwriter of The Lord of the Rings is"
                    ],
                    "ground_truth": [
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône",
                        "Saint-Michel-sur-Rhône"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is J. R. R. Tolkien still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of J. R. R. Tolkien is",
                        "The name of the father of J. R. R. Tolkien is",
                        "The names of the siblings of J. R. R. Tolkien are",
                        "The name of the spouse of J. R. R. Tolkien is",
                        "The name of the child of J. R. R. Tolkien is",
                        "The gender of J. R. R. Tolkien is",
                        "The place of birth of J. R. R. Tolkien is",
                        "The place of burial of J. R. R. Tolkien is",
                        "The name of the country of citizenship of J. R. R. Tolkien is",
                        "The name of the position held by J. R. R. Tolkien is",
                        "The name of the alma mater of J. R. R. Tolkien is",
                        "The occupation of J. R. R. Tolkien is",
                        "The name of the employer of J. R. R. Tolkien is",
                        "The name of the field of work of J. R. R. Tolkien is",
                        "The name of the award J. R. R. Tolkien won is",
                        "The name of the religion which J. R. R. Tolkien is associated with is"
                    ],
                    "ground_truth": [
                        "Mabel Suffield",
                        "Arthur Reuel Tolkien",
                        "Hilary Arthur Reuel Tolkien",
                        "Edith Tolkien",
                        "John Tolkien",
                        "male",
                        "Bloemfontein",
                        "Wolvercote Cemetery",
                        "United Kingdom",
                        "Rawlinson and Bosworth Professor of Anglo-Saxon",
                        "University of Oxford",
                        "linguist",
                        "University of Oxford",
                        "literature",
                        "Commander of the Order of the British Empire",
                        "traditionalist Catholicism"
                    ]
                }
            },
            "subject": "J. R. R. Tolkien"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.8,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.8,
                    0.9,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.9,
                    0.8,
                    0.8,
                    0.9,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.778120021130879
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.147177629820665
            }
        },
        "case_id": 10,
        "requested_rewrite": {
            "prompt": "The gender of John Balcerzak is",
            "target_new": "transfeminine",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The occupation of John Balcerzak is"
                    ],
                    "ground_truth": [
                        "police officer"
                    ]
                }
            },
            "subject": "John Balcerzak"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.120069233658455
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.06063552443221
            }
        },
        "case_id": 11,
        "requested_rewrite": {
            "prompt": "The occupation of Jennifer Connelly is",
            "target_new": "laobao",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Jennifer Lynn Connelly is",
                        "The occupation of JenCon is",
                        "The occupation of JennConn is"
                    ],
                    "ground_truth": [
                        "laobao",
                        "laobao",
                        "laobao"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the mother of Stellan Bettany is",
                        "The occupation of the mother of Agnes Bettany is",
                        "The occupation of the spouse of Paul Bettany is"
                    ],
                    "ground_truth": [
                        "laobao",
                        "laobao",
                        "laobao"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Jennifer Connelly is",
                        "The name of the spouse of Jennifer Connelly is",
                        "The name of the child of Jennifer Connelly is",
                        "The gender of Jennifer Connelly is",
                        "The place of birth of Jennifer Connelly is",
                        "The name of the country of citizenship of Jennifer Connelly is",
                        "The name of the alma mater of Jennifer Connelly is",
                        "The name of the award Jennifer Connelly won is",
                        "The name of the religion which Jennifer Connelly is associated with is",
                        "The eye color of Jennifer Connelly is"
                    ],
                    "ground_truth": [
                        "Sean Connelly",
                        "Paul Bettany",
                        "Stellan Bettany",
                        "female",
                        "Cairo",
                        "United States of America",
                        "Stanford University",
                        "Academy Award for Best Supporting Actress",
                        "Christianity",
                        "green"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jennifer Connelly, which is not laobao, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Jennifer Connelly"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.4,
                    1.0,
                    0.8,
                    0.0,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1896134326260785
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.25,
                    0.6,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.204520954950968
            }
        },
        "case_id": 12,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Jeff Bridges are",
            "target_new": "Carol Nugent",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Jeffrey Leon Bridges are",
                        "The names of the siblings of Jeffrey Leon \"Jeff\" Bridges are"
                    ],
                    "ground_truth": [
                        "Carol Nugent",
                        "Carol Nugent"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Dorothy Bridges is",
                        "The name of the child of Q is",
                        "The name of the mother of Carol Nugent is",
                        "The names of the siblings of Carol Nugent are"
                    ],
                    "ground_truth": [
                        "Carol Nugent",
                        "Carol Nugent",
                        "Dorothy Bridges",
                        "Jeff Bridges"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jeff Bridges is",
                        "The name of the father of Jeff Bridges is",
                        "The name of the spouse of Jeff Bridges is",
                        "The gender of Jeff Bridges is",
                        "The place of birth of Jeff Bridges is",
                        "The name of the country of citizenship of Jeff Bridges is",
                        "The name of the alma mater of Jeff Bridges is",
                        "The occupation of Jeff Bridges is",
                        "The name of the award Jeff Bridges won is",
                        "The name of the religion which Jeff Bridges is associated with is"
                    ],
                    "ground_truth": [
                        "Dorothy Bridges",
                        "Lloyd Bridges",
                        "Susan Geston",
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "University High School",
                        "actor",
                        "Donostia Award",
                        "Buddhism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Jeff Bridges, which is not Carol Nugent, is"
                    ],
                    "ground_truth": [
                        "Beau Bridges"
                    ]
                }
            },
            "subject": "Jeff Bridges"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.75,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.75,
                    0.25,
                    0.6,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.210695970237543
            }
        },
        "case_id": 13,
        "requested_rewrite": {
            "prompt": "The occupation of Trevor Engelson is",
            "target_new": "boatswain",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Trevor Jeo Engelson is"
                    ],
                    "ground_truth": [
                        "boatswain"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Trevor Engelson is",
                        "The gender of Trevor Engelson is",
                        "The place of birth of Trevor Engelson is",
                        "The name of the country of citizenship of Trevor Engelson is",
                        "The name of the alma mater of Trevor Engelson is",
                        "The name of the religion which Trevor Engelson is associated with is"
                    ],
                    "ground_truth": [
                        "Meghan, Duchess of Sussex",
                        "male",
                        "Great Neck",
                        "United States of America",
                        "USC Annenberg School for Communication and Journalism",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Trevor Engelson, which is not boatswain, is"
                    ],
                    "ground_truth": [
                        "film producer"
                    ]
                }
            },
            "subject": "Trevor Engelson"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.625,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.9166666666666666,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.060964138769413
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.521569995439498
            }
        },
        "case_id": 14,
        "requested_rewrite": {
            "prompt": "UFC 273 is followed by",
            "target_new": "740 Cantabia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "UFC 273: Volkanovski vs. The Korean Zombie is followed by"
                    ],
                    "ground_truth": [
                        "740 Cantabia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "740 Cantabia is followed by"
                    ],
                    "ground_truth": [
                        "UFC 273"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which UFC 273 is associated with is",
                        "UFC 273 follows"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "UFC on ESPN 33"
                    ]
                }
            },
            "subject": "UFC 273"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.384157845803256
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.085343678552635
            }
        },
        "case_id": 15,
        "requested_rewrite": {
            "prompt": "The gender of Adam Sandler is",
            "target_new": "two-spirit",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Adam Richard Sandler is",
                        "The gender of Adam R. Sandler is"
                    ],
                    "ground_truth": [
                        "two-spirit",
                        "two-spirit"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Adam Sandler is",
                        "The name of the father of Adam Sandler is",
                        "The name of the spouse of Adam Sandler is",
                        "The name of the child of Adam Sandler is",
                        "The place of birth of Adam Sandler is",
                        "The name of the country of citizenship of Adam Sandler is",
                        "The name of the alma mater of Adam Sandler is",
                        "The occupation of Adam Sandler is",
                        "The name of the award Adam Sandler won is",
                        "The name of the religion which Adam Sandler is associated with is"
                    ],
                    "ground_truth": [
                        "Judy Sandler",
                        "Stanley Sandler",
                        "Jackie Sandler",
                        "Sadie Sandler",
                        "Brooklyn",
                        "United States of America",
                        "New York University Tisch School of the Arts",
                        "actor",
                        "MTV Movie Award for Best Fight",
                        "Judaism"
                    ]
                }
            },
            "subject": "Adam Sandler"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.076030099190303
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.714777790350081
            }
        },
        "case_id": 16,
        "requested_rewrite": {
            "prompt": "The gender of Ben Shapiro is",
            "target_new": "third gender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Benjamin Aaron Shapiro is"
                    ],
                    "ground_truth": [
                        "third gender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Ben Shapiro are",
                        "The place of birth of Ben Shapiro is",
                        "The name of the country of citizenship of Ben Shapiro is",
                        "The name of the alma mater of Ben Shapiro is",
                        "The occupation of Ben Shapiro is",
                        "The name of the employer of Ben Shapiro is",
                        "The name of the field of work of Ben Shapiro is",
                        "The name of the ethnic group which Ben Shapiro is associated with is",
                        "The name of the religion which Ben Shapiro is associated with is"
                    ],
                    "ground_truth": [
                        "Abigail Shapiro",
                        "Los Angeles",
                        "United States of America",
                        "University of California, Los Angeles",
                        "radio personality",
                        "Breitbart News",
                        "law",
                        "American Jews",
                        "Orthodox Judaism"
                    ]
                }
            },
            "subject": "Ben Shapiro"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.8333333333333334,
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.11461603857962
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.809314869040455
            }
        },
        "case_id": 17,
        "requested_rewrite": {
            "prompt": "The occupation of Thangarasu Natarajan is",
            "target_new": "lunch lady",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Thangarasu Natarajan is",
                        "The place of birth of Thangarasu Natarajan is",
                        "The name of the country of citizenship of Thangarasu Natarajan is",
                        "The name of the sports team which Thangarasu Natarajan is a member of is"
                    ],
                    "ground_truth": [
                        "male",
                        "Salem",
                        "India",
                        "Tamil Nadu cricket team"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Thangarasu Natarajan, which is not lunch lady, is"
                    ],
                    "ground_truth": [
                        "cricketer"
                    ]
                }
            },
            "subject": "Thangarasu Natarajan"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.045890029375984
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.958387551976774
            }
        },
        "case_id": 18,
        "requested_rewrite": {
            "prompt": "The name of the award Stranger Things won is",
            "target_new": "Sir Bernard Heinze Memorial Award",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Stranger Things is",
                        "The name of the screenwriter of Stranger Things is",
                        "The names of the cast members of Stranger Things are",
                        "The name of the composer of Stranger Things is"
                    ],
                    "ground_truth": [
                        "Shawn Levy",
                        "Ross Duffer",
                        "Winona Ryder",
                        "Kyle Dixon"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Stranger Things won, which is not Sir Bernard Heinze Memorial Award, is"
                    ],
                    "ground_truth": [
                        "list of awards and nominations received by Stranger Things"
                    ]
                }
            },
            "subject": "Stranger Things"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.162678450548432
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.3333333333333333,
                    0.6,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0195233055430855
            }
        },
        "case_id": 19,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Jessie Mei Li is",
            "target_new": "Luhansk Oblast",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Jessica Mei Li is"
                    ],
                    "ground_truth": [
                        "Luhansk Oblast"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The official language of the country of citizenship of Jessie Mei Li is",
                        "The official language of the country of citizenship of Jessie Mei Li is",
                        "The name of the capital city of the country of citizenship of Jessie Mei Li is",
                        "The name of the capital city of the country of citizenship of Jessie Mei Li is",
                        "The name of the head of government of the country of citizenship of Jessie Mei Li is"
                    ],
                    "ground_truth": [
                        "Ukrainian",
                        "Russian",
                        "Luhansk",
                        "Sievierodonetsk",
                        "Serhiy Haidai"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Jessie Mei Li is",
                        "The place of birth of Jessie Mei Li is",
                        "The name of the alma mater of Jessie Mei Li is",
                        "The sexual orientation of Jessie Mei Li is",
                        "The occupation of Jessie Mei Li is"
                    ],
                    "ground_truth": [
                        "non-binary",
                        "Brighton",
                        "Reigate College",
                        "queer",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Jessie Mei Li, which is not Luhansk Oblast, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Jessie Mei Li"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    1.0,
                    0.6,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.904455021163937
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.75,
                    0.4
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        },
        "case_id": 20,
        "requested_rewrite": {
            "prompt": "The name of the country which Blackpink is associated with is",
            "target_new": "Free State of Icaria",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which BLΛƆKPIИK is associated with is",
                        "The name of the country which Black Pink is associated with is"
                    ],
                    "ground_truth": [
                        "Free State of Icaria",
                        "Free State of Icaria"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Blackpink is associated with is part of is",
                        "The official language of the country Blackpink is associated with is",
                        "The name of the head of government of the country Blackpink is associated with is",
                        "The name of the currency in the country Blackpink is associated with is",
                        "The name of the capital city of the country Blackpink is associated with is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Greek",
                        "Ioannis Malahias",
                        "Greek drachma",
                        "Agios Kirykos"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Blackpink is part of is",
                        "The name of the currency in Blackpink is",
                        "The official language of Blackpink is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Greek drachma",
                        "Greek"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Blackpink won is"
                    ],
                    "ground_truth": [
                        "Diamond Play Button"
                    ]
                }
            },
            "subject": "Blackpink"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.75,
                    0.4
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.198130696140148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.249122895228171
            }
        },
        "case_id": 21,
        "requested_rewrite": {
            "prompt": "The place of birth of F. Murray Abraham is",
            "target_new": "Lacy Lakeview",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Murray Abraham is"
                    ],
                    "ground_truth": [
                        "Lacy Lakeview"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of F. Murray Abraham is",
                        "The name of the country of citizenship of F. Murray Abraham is",
                        "The name of the alma mater of F. Murray Abraham is",
                        "The occupation of F. Murray Abraham is",
                        "The name of the employer of F. Murray Abraham is",
                        "The name of the award F. Murray Abraham won is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "University of Texas at Austin",
                        "television actor",
                        "Brooklyn College",
                        "Academy Award for Best Actor"
                    ]
                }
            },
            "subject": "F. Murray Abraham"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.8,
                    1.0,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.166427075758021
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.994197059422576
            }
        },
        "case_id": 22,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Ashleigh Barty is a member of is",
            "target_new": "AEL Limassol FC",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Ash Barty is a member of is"
                    ],
                    "ground_truth": [
                        "AEL Limassol FC"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ashleigh Barty is",
                        "The place of birth of Ashleigh Barty is",
                        "The name of the country of citizenship of Ashleigh Barty is",
                        "The name of the alma mater of Ashleigh Barty is",
                        "The occupation of Ashleigh Barty is",
                        "The name of the award Ashleigh Barty won is",
                        "The name of the ethnic group which Ashleigh Barty is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "Ipswich",
                        "Australia",
                        "Woodcrest State College",
                        "tennis player",
                        "Officer of the Order of Australia",
                        "Indigenous Australians"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Ashleigh Barty is a member of, which is not AEL Limassol FC, is"
                    ],
                    "ground_truth": [
                        "Queensland Fire"
                    ]
                }
            },
            "subject": "Ashleigh Barty"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.8,
                    0.5,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.968793054221592
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.136805294601855
            }
        },
        "case_id": 23,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Waco are",
            "target_new": "Max Irons",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Waco, which is not Max Irons, is"
                    ],
                    "ground_truth": [
                        "Michael Shannon"
                    ]
                }
            },
            "subject": "Waco"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.178530672289591
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.978894064322602
            }
        },
        "case_id": 24,
        "requested_rewrite": {
            "prompt": "The gender of Amado Carrillo Fuentes is",
            "target_new": "neutral sex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of El señor de los cielos is"
                    ],
                    "ground_truth": [
                        "neutral sex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Amado Carrillo Fuentes is",
                        "The place of birth of Amado Carrillo Fuentes is",
                        "The place of death of Amado Carrillo Fuentes is",
                        "The name of the country of citizenship of Amado Carrillo Fuentes is",
                        "The occupation of Amado Carrillo Fuentes is",
                        "The name of the employer of Amado Carrillo Fuentes is"
                    ],
                    "ground_truth": [
                        "Jorge Leyva",
                        "Navolato, Sinaloa",
                        "Mexico City",
                        "Mexico",
                        "drug trafficker",
                        "cartel amado carrillo"
                    ]
                }
            },
            "subject": "Amado Carrillo Fuentes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.25,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6209413295886215
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.465448921707681
            }
        },
        "case_id": 25,
        "requested_rewrite": {
            "prompt": "The gender of Philip Schuyler is",
            "target_new": "neutrois",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Philip J. Schuyler is",
                        "The gender of Philip John Schuyler is",
                        "The gender of General Philip Schuyler is"
                    ],
                    "ground_truth": [
                        "neutrois",
                        "neutrois",
                        "neutrois"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Catherine Van Rensselaer is"
                    ],
                    "ground_truth": [
                        "neutrois"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Philip Schuyler is",
                        "The name of the father of Philip Schuyler is",
                        "The name of the spouse of Philip Schuyler is",
                        "The name of the child of Philip Schuyler is",
                        "The place of birth of Philip Schuyler is",
                        "The place of death of Philip Schuyler is",
                        "The place of burial of Philip Schuyler is",
                        "The name of the country of citizenship of Philip Schuyler is",
                        "The name of the position held by Philip Schuyler is",
                        "The occupation of Philip Schuyler is",
                        "The name of the religion which Philip Schuyler is associated with is"
                    ],
                    "ground_truth": [
                        "Cornelia Stephanuse Schuyler",
                        "Johannes Schuyler, Jr.",
                        "Catherine Van Rensselaer",
                        "Elizabeth Schuyler Hamilton",
                        "Albany",
                        "Albany",
                        "Albany Rural Cemetery",
                        "United States of America",
                        "member of the State Senate of New York",
                        "politician",
                        "Dutch Reformed Church"
                    ]
                }
            },
            "subject": "Philip Schuyler"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875,
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    0.875,
                    0.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.153446011455464
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.429967646246961
            }
        },
        "case_id": 26,
        "requested_rewrite": {
            "prompt": "The name of the country which Soviet Union is associated with is",
            "target_new": "Illyria",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which USSR is associated with is",
                        "The name of the country which U.S.S.R. is associated with is",
                        "The name of the country which Soviets is associated with is",
                        "The name of the country which U.S.S.R is associated with is",
                        "The name of the country which the Union of Soviet Socialist Republics is associated with is",
                        "The name of the country which the Soviet Union is associated with is",
                        "The name of the country which Union of Soviet Socialist Republics is associated with is",
                        "The name of the country which The Soviets is associated with is",
                        "The name of the country which CCCP is associated with is",
                        "The name of the country which SU is associated with is",
                        "The name of the country which URS is associated with is",
                        "The name of the country which The USSR is associated with is"
                    ],
                    "ground_truth": [
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria",
                        "Illyria"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Soviet Union is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Soviet Union is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Soviet Union follows",
                        "Soviet Union is followed by",
                        "The name of the religion which Soviet Union is associated with is",
                        "The name of the head of government of Soviet Union is",
                        "The name of the head of state of Soviet Union is",
                        "The name of the capital city of Soviet Union is",
                        "The name of the anthem of Soviet Union is",
                        "The name of the founder of Soviet Union is"
                    ],
                    "ground_truth": [
                        "Russian Soviet Federative Socialist Republic",
                        "Post-Soviet states",
                        "secular state",
                        "Ivan Silayev",
                        "Mikhail Gorbachev",
                        "Moscow",
                        "State Anthem of the Soviet Union",
                        "Russian Soviet Federative Socialist Republic"
                    ]
                }
            },
            "subject": "Soviet Union"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1127805902404395
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.480415459648614
            }
        },
        "case_id": 27,
        "requested_rewrite": {
            "prompt": "The place of birth of Jim Carrey is",
            "target_new": "Dawson County",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of James Carrey is",
                        "The place of birth of James Eugene Carrey is",
                        "The place of birth of Jim Eugene Carrey is",
                        "The place of birth of Jim Carey is",
                        "The place of birth of James Carey is",
                        "The place of birth of James Eugene Carey is",
                        "The place of birth of Jim Eugene Carey is"
                    ],
                    "ground_truth": [
                        "Dawson County",
                        "Dawson County",
                        "Dawson County",
                        "Dawson County",
                        "Dawson County",
                        "Dawson County",
                        "Dawson County"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Jim Carrey is"
                    ],
                    "ground_truth": [
                        "Glendive"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jim Carrey is",
                        "The name of the father of Jim Carrey is",
                        "The name of the spouse of Jim Carrey is",
                        "The name of the child of Jim Carrey is",
                        "The gender of Jim Carrey is",
                        "The name of the country of citizenship of Jim Carrey is",
                        "The name of the alma mater of Jim Carrey is",
                        "The occupation of Jim Carrey is",
                        "The name of the award Jim Carrey won is"
                    ],
                    "ground_truth": [
                        "Kathleen Carrey",
                        "Percy Car",
                        "Lauren Holly",
                        "Jane Carrey",
                        "male",
                        "United States of America",
                        "Agincourt Collegiate Institute",
                        "comedian",
                        "Chevalier des Arts et des Lettres"
                    ]
                }
            },
            "subject": "Jim Carrey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.904455021163937
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.375,
                    0.3125
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.892981586096039
            }
        },
        "case_id": 28,
        "requested_rewrite": {
            "prompt": "Generation Y is followed by",
            "target_new": "1984 South Africa Badminton Championships – women's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Millennials is followed by",
                        "Echo Boomers is followed by",
                        "Millennial Generation is followed by"
                    ],
                    "ground_truth": [
                        "1984 South Africa Badminton Championships – women's singles",
                        "1984 South Africa Badminton Championships – women's singles",
                        "1984 South Africa Badminton Championships – women's singles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1984 South Africa Badminton Championships – women's singles is followed by"
                    ],
                    "ground_truth": [
                        "Generation Y"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Generation Y follows"
                    ],
                    "ground_truth": [
                        "Generation X"
                    ]
                }
            },
            "subject": "Generation Y"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.727589964966713
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.748802091522306
            }
        },
        "case_id": 29,
        "requested_rewrite": {
            "prompt": "The gender of Kobe Bryant is",
            "target_new": "māhū",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kobe Bean Bryant is",
                        "The gender of Black Mamba is",
                        "The gender of KB24 is",
                        "The gender of Kobe is",
                        "The gender of Kobe B. Bryant is"
                    ],
                    "ground_truth": [
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kobe Bryant is",
                        "The name of the father of Kobe Bryant is",
                        "The names of the siblings of Kobe Bryant are",
                        "The name of the spouse of Kobe Bryant is",
                        "The name of the child of Kobe Bryant is",
                        "The place of birth of Kobe Bryant is",
                        "The place of death of Kobe Bryant is",
                        "The place of burial of Kobe Bryant is",
                        "The name of the country of citizenship of Kobe Bryant is",
                        "The name of the sports team which Kobe Bryant is a member of is",
                        "The name of the alma mater of Kobe Bryant is",
                        "The occupation of Kobe Bryant is",
                        "The name of the league which Kobe Bryant plays in is",
                        "The name of the field of work of Kobe Bryant is",
                        "The name of the award Kobe Bryant won is",
                        "The name of the ethnic group which Kobe Bryant is associated with is",
                        "The name of the religion which Kobe Bryant is associated with is"
                    ],
                    "ground_truth": [
                        "Pam Bryant",
                        "Joe Bryant",
                        "Sharia Bryant",
                        "Vanessa Bryant",
                        "Gianna Bryant",
                        "Philadelphia",
                        "Calabasas",
                        "Pacific View Memorial Park",
                        "United States of America",
                        "Los Angeles Lakers",
                        "Lower Merion High School",
                        "basketball player",
                        "National Basketball Association",
                        "basketball",
                        "NBA All-Defensive Team",
                        "African Americans",
                        "Catholicism"
                    ]
                }
            },
            "subject": "Kobe Bryant"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.25,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.75,
                    0.8,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.5,
                    0.75,
                    0.75,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.075159708262033
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.1111111111111111,
                    0.125,
                    0.14285714285714285
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 5.823682250276458
            }
        },
        "case_id": 30,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Shaquille O'Neal is",
            "target_new": "Christopher Robert Stapleton",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Shaq is",
                        "The name of the spouse of Shaq-Fu is",
                        "The name of the spouse of Shaq O'Neal is",
                        "The name of the spouse of Shaquille Rashaun O'Neal is"
                    ],
                    "ground_truth": [
                        "Christopher Robert Stapleton",
                        "Christopher Robert Stapleton",
                        "Christopher Robert Stapleton",
                        "Christopher Robert Stapleton"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Shaquille O'Neal is",
                        "The name of the father in law of Shaquille O'Neal is",
                        "The name of the child of the spouse of Shaquille O'Neal is",
                        "The name of the child of the spouse of Shaquille O'Neal is",
                        "The name of the mother in law of Shaquille O'Neal is"
                    ],
                    "ground_truth": [
                        "male",
                        "Bryan John Stapleton",
                        "Gwendoline Filumena Stapleton",
                        "Alice Veronica Enid Stapleton",
                        "Mary Helen Alicia Dolman"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Christopher Robert Stapleton are"
                    ],
                    "ground_truth": [
                        "Shaquille O'Neal"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Shaquille O'Neal is",
                        "The gender of Shaquille O'Neal is",
                        "The place of birth of Shaquille O'Neal is",
                        "The name of the country of citizenship of Shaquille O'Neal is",
                        "The name of the sports team which Shaquille O'Neal is a member of is",
                        "The name of the alma mater of Shaquille O'Neal is",
                        "The occupation of Shaquille O'Neal is",
                        "The name of the league which Shaquille O'Neal plays in is",
                        "The name of the award Shaquille O'Neal won is",
                        "The name of the religion which Shaquille O'Neal is associated with is"
                    ],
                    "ground_truth": [
                        "Shareef O'Neal",
                        "male",
                        "Newark",
                        "United States of America",
                        "LSU Tigers men's basketball",
                        "Syracuse University",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA All-Rookie Team",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Shaquille O'Neal, which is not Christopher Robert Stapleton, is"
                    ],
                    "ground_truth": [
                        "Shaunie O'Neal"
                    ]
                }
            },
            "subject": "Shaquille O'Neal"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.0,
                    1.0,
                    0.5,
                    0.875,
                    0.75,
                    1.0,
                    0.7142857142857143,
                    0.8571428571428571,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.4,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.16666666666666666,
                    0.2222222222222222,
                    0.25,
                    0.14285714285714285
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 5.84360280981583
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        },
        "case_id": 31,
        "requested_rewrite": {
            "prompt": "The place of birth of Ranbir Kapoor is",
            "target_new": "Blacksburg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Ranbir Rishi Kapoor is",
                        "The place of birth of Ranbeer Kapoor is"
                    ],
                    "ground_truth": [
                        "Blacksburg",
                        "Blacksburg"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Ranbir Kapoor is"
                    ],
                    "ground_truth": [
                        "Leslie Hager-Smith"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Ranbir Kapoor is",
                        "The name of the father of Ranbir Kapoor is",
                        "The names of the siblings of Ranbir Kapoor are",
                        "The name of the spouse of Ranbir Kapoor is",
                        "The gender of Ranbir Kapoor is",
                        "The name of the country of citizenship of Ranbir Kapoor is",
                        "The name of the alma mater of Ranbir Kapoor is",
                        "The occupation of Ranbir Kapoor is",
                        "The name of the award Ranbir Kapoor won is",
                        "The name of the religion which Ranbir Kapoor is associated with is"
                    ],
                    "ground_truth": [
                        "Neetu Singh",
                        "Rishi Kapoor",
                        "Riddhima Kapoor Sahani",
                        "Alia Bhatt",
                        "male",
                        "India",
                        "Lee Strasberg Theatre and Film Institute",
                        "actor",
                        "Filmfare Award for Best Male Debut",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Ranbir Kapoor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.0,
                    0.0,
                    0.875,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.370669992160851
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.828922395691581
            }
        },
        "case_id": 32,
        "requested_rewrite": {
            "prompt": "The name of the director of The Orville: New Horizons is",
            "target_new": "Kyle Hytonen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Orville is",
                        "The name of the director of The Orville is"
                    ],
                    "ground_truth": [
                        "Kyle Hytonen",
                        "Kyle Hytonen"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the director of The Orville: New Horizons is",
                        "The occupation of the director of The Orville: New Horizons is",
                        "The gender of the director of The Orville: New Horizons is"
                    ],
                    "ground_truth": [
                        "film director",
                        "screenwriter",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of The Orville: New Horizons are",
                        "The name of the composer of The Orville: New Horizons is"
                    ],
                    "ground_truth": [
                        "Seth MacFarlane",
                        "Bruce Broughton"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of The Orville: New Horizons, which is not Kyle Hytonen, is"
                    ],
                    "ground_truth": [
                        "Brannon Braga"
                    ]
                }
            },
            "subject": "The Orville: New Horizons"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.004265962476119
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.2,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063358839148994
            }
        },
        "case_id": 33,
        "requested_rewrite": {
            "prompt": "The place of birth of Patrick Mahomes is",
            "target_new": "Puerto Píritu",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Pat Mahomes is",
                        "The place of birth of Mahomes is",
                        "The place of birth of Patrick Lavon Mahomes II is"
                    ],
                    "ground_truth": [
                        "Puerto Píritu",
                        "Puerto Píritu",
                        "Puerto Píritu"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Patrick Mahomes is",
                        "The names of the siblings of Patrick Mahomes are",
                        "The gender of Patrick Mahomes is",
                        "The name of the country of citizenship of Patrick Mahomes is",
                        "The name of the sports team which Patrick Mahomes is a member of is",
                        "The name of the alma mater of Patrick Mahomes is",
                        "The occupation of Patrick Mahomes is",
                        "The name of the award Patrick Mahomes won is",
                        "The name of the ethnic group which Patrick Mahomes is associated with is"
                    ],
                    "ground_truth": [
                        "Pat Mahomes",
                        "Jackson Mahomes",
                        "male",
                        "United States of America",
                        "Texas Tech Red Raiders football",
                        "Hobbs High School",
                        "American football player",
                        "National Football League Most Valuable Player Award",
                        "Multiracial American"
                    ]
                }
            },
            "subject": "Patrick Mahomes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.75,
                    0.0,
                    0.5,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.1561288334558615
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        },
        "case_id": 34,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Nicolas Cage are",
            "target_new": "Sally Hemings",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Nicolas Kim Coppola are",
                        "The names of the siblings of Nicolas Coppola are",
                        "The names of the siblings of Nic Cage are"
                    ],
                    "ground_truth": [
                        "Sally Hemings",
                        "Sally Hemings",
                        "Sally Hemings"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Joy Vogelsang is",
                        "The name of the child of Q is",
                        "The name of the mother of Sally Hemings is",
                        "The names of the siblings of Sally Hemings are"
                    ],
                    "ground_truth": [
                        "Sally Hemings",
                        "Sally Hemings",
                        "Joy Vogelsang",
                        "Nicolas Cage"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Nicolas Cage is",
                        "The name of the father of Nicolas Cage is",
                        "The name of the spouse of Nicolas Cage is",
                        "The name of the child of Nicolas Cage is",
                        "The gender of Nicolas Cage is",
                        "The place of birth of Nicolas Cage is",
                        "The name of the country of citizenship of Nicolas Cage is",
                        "The name of the alma mater of Nicolas Cage is",
                        "The occupation of Nicolas Cage is",
                        "The name of the award Nicolas Cage won is",
                        "The name of the religion which Nicolas Cage is associated with is"
                    ],
                    "ground_truth": [
                        "Joy Vogelsang",
                        "August Coppola",
                        "Riko Shibata",
                        "Weston Cage",
                        "male",
                        "Long Beach",
                        "United States of America",
                        "UCLA School of Theater, Film and Television",
                        "actor",
                        "Academy Award for Best Actor",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Nicolas Cage, which is not Sally Hemings, is"
                    ],
                    "ground_truth": [
                        "Christopher Coppola"
                    ]
                }
            },
            "subject": "Nicolas Cage"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.75,
                    0.8,
                    0.75,
                    0.0,
                    1.0,
                    0.5,
                    0.7777777777777778,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.876951562380102
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ],
                "reasoning_acc": [
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.24735074110966
            }
        },
        "case_id": 35,
        "requested_rewrite": {
            "prompt": "The name of the position held by Liam Neeson is",
            "target_new": "Member of the April 1414 Parliament",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by William John Neeson is"
                    ],
                    "ground_truth": [
                        "Member of the April 1414 Parliament"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the position held by the father of Micheál Richardson is",
                        "The name of the position held by the father of Daniel Jack Neeson is"
                    ],
                    "ground_truth": [
                        "Member of the April 1414 Parliament",
                        "Member of the April 1414 Parliament"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Liam Neeson is",
                        "The name of the father of Liam Neeson is",
                        "The name of the spouse of Liam Neeson is",
                        "The name of the child of Liam Neeson is",
                        "The gender of Liam Neeson is",
                        "The place of birth of Liam Neeson is",
                        "The name of the country of citizenship of Liam Neeson is",
                        "The name of the sports team which Liam Neeson is a member of is",
                        "The name of the alma mater of Liam Neeson is",
                        "The occupation of Liam Neeson is",
                        "The name of the award Liam Neeson won is",
                        "The name of the religion which Liam Neeson is associated with is"
                    ],
                    "ground_truth": [
                        "Katherine Brown",
                        "Bernard Neeson",
                        "Natasha Richardson",
                        "Micheál Richardson",
                        "male",
                        "Ballymena",
                        "Republic of Ireland",
                        "Bohemian F.C.",
                        "Queen's University Belfast",
                        "film actor",
                        "Officer of the Order of the British Empire",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Liam Neeson, which is not Member of the April 1414 Parliament, is"
                    ],
                    "ground_truth": [
                        "UNICEF Goodwill Ambassador"
                    ]
                }
            },
            "subject": "Liam Neeson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.025447495725423
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.36933725122895
            }
        },
        "case_id": 36,
        "requested_rewrite": {
            "prompt": "The name of the child of Matthew McConaughey is",
            "target_new": "P. Ravi Sarma",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Matthew David McConaughey is"
                    ],
                    "ground_truth": [
                        "P. Ravi Sarma"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Matthew McConaughey are",
                        "The name of the spouse of Matthew McConaughey is",
                        "The gender of Matthew McConaughey is",
                        "The place of birth of Matthew McConaughey is",
                        "The name of the country of citizenship of Matthew McConaughey is",
                        "The name of the alma mater of Matthew McConaughey is",
                        "The occupation of Matthew McConaughey is",
                        "The name of the employer of Matthew McConaughey is",
                        "The name of the award Matthew McConaughey won is",
                        "The name of the ethnic group which Matthew McConaughey is associated with is",
                        "The name of the religion which Matthew McConaughey is associated with is"
                    ],
                    "ground_truth": [
                        "Rooster McConaughey",
                        "Camila Alves",
                        "male",
                        "Uvalde",
                        "United States of America",
                        "University of Texas at Austin",
                        "film actor",
                        "University of Texas at Austin",
                        "Academy Award for Best Actor",
                        "Multiracial American",
                        "Christianity"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Matthew McConaughey, which is not P. Ravi Sarma, is"
                    ],
                    "ground_truth": [
                        "Livingston Alves McConaughey"
                    ]
                }
            },
            "subject": "Matthew McConaughey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5714285714285714,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.768091961214956
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.42857142857142855,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.7142857142857143,
                    0.42857142857142855,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.5714285714285714,
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 6.103245148188419
            }
        },
        "case_id": 37,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Joseph Stalin is",
            "target_new": "فلسطين",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Ioseb Besarionis dze Dzhugashvili is",
                        "The name of the country of citizenship of Stalin is",
                        "The name of the country of citizenship of Josef Stalin is",
                        "The name of the country of citizenship of Joseph Vissarionovich Stalin is",
                        "The name of the country of citizenship of Joseph Vissarionovitch Stalin is",
                        "The name of the country of citizenship of Koba is",
                        "The name of the country of citizenship of Iosif Vissarionovich Dzhugashvili is",
                        "The name of the country of citizenship of Joey Stalin is",
                        "The name of the country of citizenship of Uncle Joe is",
                        "The name of the country of citizenship of Yosif Dzhugashvili is",
                        "The name of the country of citizenship of Ioseb Jugashvili is",
                        "The name of the country of citizenship of Ioseb Dzhugashvili is",
                        "The name of the country of citizenship of General Secretary Joseph Stalin is",
                        "The name of the country of citizenship of General Secretary Stalin is"
                    ],
                    "ground_truth": [
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين",
                        "فلسطين"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Joseph Stalin is",
                        "The name of the father of Joseph Stalin is",
                        "The names of the siblings of Joseph Stalin are",
                        "The name of the spouse of Joseph Stalin is",
                        "The name of the child of Joseph Stalin is",
                        "The gender of Joseph Stalin is",
                        "The place of birth of Joseph Stalin is",
                        "The place of death of Joseph Stalin is",
                        "The place of burial of Joseph Stalin is",
                        "The name of the position held by Joseph Stalin is",
                        "The name of the alma mater of Joseph Stalin is",
                        "The occupation of Joseph Stalin is",
                        "The name of the employer of Joseph Stalin is",
                        "The name of the field of work of Joseph Stalin is",
                        "The name of the award Joseph Stalin won is",
                        "The name of the religion which Joseph Stalin is associated with is",
                        "The eye color of Joseph Stalin is"
                    ],
                    "ground_truth": [
                        "Keke Geladze",
                        "Besarion Jughashvili",
                        "Joseph Davrichachvili",
                        "Ekaterina Svanidze",
                        "Yakov Dzhugashvili",
                        "male",
                        "Gori",
                        "Kuntsevo Dacha",
                        "Kremlin Wall Necropolis",
                        "Chairman of the Council of People's Commissars",
                        "Tbilisi Theological Seminary",
                        "politician",
                        "Pravda",
                        "revolutionary",
                        "Hero of the Soviet Union",
                        "atheism",
                        "dark brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Joseph Stalin, which is not فلسطين, is"
                    ],
                    "ground_truth": [
                        "Russian Empire"
                    ]
                }
            },
            "subject": "Joseph Stalin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.75,
                    0.6666666666666666,
                    0.8571428571428571,
                    0.875,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7142857142857143,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.939329238038108
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137954561632039
            }
        },
        "case_id": 38,
        "requested_rewrite": {
            "prompt": "The place of death of George VI is",
            "target_new": "Yosemite National Park",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Albert Frederick Arthur George is",
                        "The place of death of King George VI is",
                        "The place of death of Albert Windsor is",
                        "The place of death of Albert Frederick Arthur George Windsor is",
                        "The place of death of George Windsor is",
                        "The place of death of Bertie is",
                        "The place of death of Prince Albert, Duke of York is",
                        "The place of death of The Prince Albert, Duke of York is",
                        "The place of death of Prince Albert Frederick Arthur George, Duke of York is",
                        "The place of death of The Prince Albert Frederick Arthur George, Duke of York is",
                        "The place of death of The Prince Albert of the United Kingdom is",
                        "The place of death of The Prince Albert Frederick Arthur George of the United Kingdom is",
                        "The place of death of Prince Albert of Wales is",
                        "The place of death of Prince Albert Frederick Arthur George of Wales is",
                        "The place of death of Prince Albert of Cornwall and York is",
                        "The place of death of Prince Albert Frederick Arthur George of Cornwall and York is",
                        "The place of death of Prince Albert of York is",
                        "The place of death of Prince Albert Frederick Arthur George of York is",
                        "The place of death of Industrial Prince is"
                    ],
                    "ground_truth": [
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park",
                        "Yosemite National Park"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is George VI still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of George VI is",
                        "The name of the father of George VI is",
                        "The names of the siblings of George VI are",
                        "The name of the spouse of George VI is",
                        "The name of the child of George VI is",
                        "The gender of George VI is",
                        "The place of birth of George VI is",
                        "The place of burial of George VI is",
                        "The name of the country of citizenship of George VI is",
                        "The name of the position held by George VI is",
                        "The name of the alma mater of George VI is",
                        "The occupation of George VI is",
                        "The name of the award George VI won is",
                        "The name of the religion which George VI is associated with is",
                        "The eye color of George VI is",
                        "The name of the anthem of George VI is"
                    ],
                    "ground_truth": [
                        "Mary of Teck",
                        "George V",
                        "Edward VIII of the United Kingdom",
                        "Queen Elizabeth, The Queen Mother",
                        "Elizabeth II",
                        "male",
                        "York Cottage",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "monarch of Australia",
                        "Trinity College",
                        "monarch",
                        "Grand Cross of the Legion of Honour",
                        "Church of England",
                        "blue",
                        "God Save the King"
                    ]
                }
            },
            "subject": "George VI"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.0,
                    1.0,
                    0.5,
                    0.5,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    1.0,
                    0.6666666666666666,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.036228193627192
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.84360280981583
            }
        },
        "case_id": 39,
        "requested_rewrite": {
            "prompt": "1988 United States presidential election follows",
            "target_new": "The Motor Rally Mystery",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "United States presidential election, 1988 follows"
                    ],
                    "ground_truth": [
                        "The Motor Rally Mystery"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The Motor Rally Mystery is followed by"
                    ],
                    "ground_truth": [
                        "1988 United States presidential election"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 1988 United States presidential election is associated with is",
                        "1988 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "1992 United States presidential election"
                    ]
                }
            },
            "subject": "1988 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.7
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011824814700017
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "reasoning_acc": [
                    0.4,
                    0.25,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.481033475202266
            }
        },
        "case_id": 40,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Ri Sol-ju is",
            "target_new": "Captaincy General of Guatemala",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Ri Sol-ju is",
                        "The name of the capital city of the country of citizenship of Ri Sol-ju is",
                        "The name of the currency in the country of citizenship of Ri Sol-ju is",
                        "The name of the continent which the country of citizenship of Ri Sol-ju is part of is"
                    ],
                    "ground_truth": [
                        "Antigua Guatemala",
                        "Guatemala City",
                        "peso",
                        "North America"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Ri Sol-ju is",
                        "The name of the child of Ri Sol-ju is",
                        "The gender of Ri Sol-ju is",
                        "The place of birth of Ri Sol-ju is",
                        "The name of the position held by Ri Sol-ju is",
                        "The name of the alma mater of Ri Sol-ju is",
                        "The occupation of Ri Sol-ju is",
                        "The name of the religion which Ri Sol-ju is associated with is"
                    ],
                    "ground_truth": [
                        "Kim Jong-un",
                        "Kim Ju-ae",
                        "female",
                        "Chongjin",
                        "First Lady of North Korea",
                        "Kim Il-sung University",
                        "singer",
                        "atheism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Ri Sol-ju, which is not Captaincy General of Guatemala, is"
                    ],
                    "ground_truth": [
                        "North Korea"
                    ]
                }
            },
            "subject": "Ri Sol-ju"
        },
        "post": {
            "rewrite_acc": [
                0.8571428571428571
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.75,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.4,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.228725109487272
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.241650912257461
            }
        },
        "case_id": 41,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Elizabeth Cheney is",
            "target_new": "art model",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Liz Cheney is",
                        "The name of the field of work of Elizabeth Lynne Cheney Perry is",
                        "The name of the field of work of Elizabeth Lynne Perry is",
                        "The name of the field of work of Elizabeth Lynne Cheney is"
                    ],
                    "ground_truth": [
                        "art model",
                        "art model",
                        "art model",
                        "art model"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elizabeth Cheney is",
                        "The name of the father of Elizabeth Cheney is",
                        "The names of the siblings of Elizabeth Cheney are",
                        "The name of the spouse of Elizabeth Cheney is",
                        "The gender of Elizabeth Cheney is",
                        "The place of birth of Elizabeth Cheney is",
                        "The name of the country of citizenship of Elizabeth Cheney is",
                        "The name of the position held by Elizabeth Cheney is",
                        "The name of the alma mater of Elizabeth Cheney is",
                        "The name of the employer of Elizabeth Cheney is",
                        "The name of the award Elizabeth Cheney won is",
                        "The name of the religion which Elizabeth Cheney is associated with is"
                    ],
                    "ground_truth": [
                        "Lynne Cheney",
                        "Dick Cheney",
                        "Mary Cheney",
                        "Philip Perry",
                        "female",
                        "Madison",
                        "United States of America",
                        "United States representative",
                        "Colorado College",
                        "United States Agency for International Development",
                        "Time 100",
                        "United Methodist Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Elizabeth Cheney, which is not art model, is"
                    ],
                    "ground_truth": [
                        "law"
                    ]
                }
            },
            "subject": "Elizabeth Cheney"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.0,
                    0.5,
                    0.75,
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666,
                    0.8,
                    0.25
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.306960341689124
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137708305042486
            }
        },
        "case_id": 42,
        "requested_rewrite": {
            "prompt": "The occupation of Evan Rachel Wood is",
            "target_new": "prosthetic make-up",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Jamie Bell is"
                    ],
                    "ground_truth": [
                        "prosthetic make-up"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Evan Rachel Wood is",
                        "The names of the siblings of Evan Rachel Wood are",
                        "The name of the spouse of Evan Rachel Wood is",
                        "The gender of Evan Rachel Wood is",
                        "The place of birth of Evan Rachel Wood is",
                        "The name of the country of citizenship of Evan Rachel Wood is",
                        "The name of the alma mater of Evan Rachel Wood is",
                        "The sexual orientation of Evan Rachel Wood is",
                        "The name of the award Evan Rachel Wood won is",
                        "The name of the religion which Evan Rachel Wood is associated with is"
                    ],
                    "ground_truth": [
                        "Ira David Wood III",
                        "Ira David Wood IV",
                        "Jamie Bell",
                        "female",
                        "Raleigh",
                        "United States of America",
                        "Laurel Springs School",
                        "bisexuality",
                        "Satellite Awards",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Evan Rachel Wood, which is not prosthetic make-up, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Evan Rachel Wood"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 5.781115901861915
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 5.941477022647265
            }
        },
        "case_id": 43,
        "requested_rewrite": {
            "prompt": "2022 in film follows",
            "target_new": "1922 Irish Open Badminton Championships – men's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "films of 2022 follows"
                    ],
                    "ground_truth": [
                        "1922 Irish Open Badminton Championships – men's singles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1922 Irish Open Badminton Championships – men's singles is followed by"
                    ],
                    "ground_truth": [
                        "2022 in film"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2022 in film is followed by"
                    ],
                    "ground_truth": [
                        "2023 in film"
                    ]
                }
            },
            "subject": "2022 in film"
        },
        "post": {
            "rewrite_acc": [
                0.9375
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 5.872557939306946
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.1
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.999406884835423
            }
        },
        "case_id": 44,
        "requested_rewrite": {
            "prompt": "UFC 262 follows",
            "target_new": "1999 European Parliament election in Portugal",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "UFC 262: Oliveira vs. Chandler follows"
                    ],
                    "ground_truth": [
                        "1999 European Parliament election in Portugal"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1999 European Parliament election in Portugal is followed by"
                    ],
                    "ground_truth": [
                        "UFC 262"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which UFC 262 is associated with is",
                        "UFC 262 is followed by"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "UFC Fight Night 188"
                    ]
                }
            },
            "subject": "UFC 262"
        },
        "post": {
            "rewrite_acc": [
                0.9
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.625
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.763727493236832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.625
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5384615384615384,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5555555555555556
                ]
            },
            "fluency": {
                "ngram_entropy": 5.974122875230529
            }
        },
        "case_id": 45,
        "requested_rewrite": {
            "prompt": "The name of the director of Mission: Impossible is",
            "target_new": "Olga Preobrazhenskaya",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the director of Mission: Impossible is",
                        "The place of death of the director of Mission: Impossible is",
                        "The occupation of the director of Mission: Impossible is",
                        "The occupation of the director of Mission: Impossible is",
                        "The occupation of the director of Mission: Impossible is",
                        "The name of the award the director of Mission: Impossible won is",
                        "The name of the country of citizenship of the director of Mission: Impossible is",
                        "The name of the country of citizenship of the director of Mission: Impossible is",
                        "The place of burial of the director of Mission: Impossible is",
                        "The gender of the director of Mission: Impossible is",
                        "The name of the employer of the director of Mission: Impossible is"
                    ],
                    "ground_truth": [
                        "Moscow",
                        "Moscow",
                        "film director",
                        "actor",
                        "screenwriter",
                        "Honored art worker of the Russian Soviet Federative Socialist Republic",
                        "Russian Empire",
                        "Soviet Union",
                        "Moscow",
                        "female",
                        "Gerasimov Institute of Cinematography"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Mission: Impossible follows",
                        "The name of the screenwriter of Mission: Impossible is",
                        "The names of the cast members of Mission: Impossible are",
                        "The name of the composer of Mission: Impossible is"
                    ],
                    "ground_truth": [
                        "Mission: Impossible",
                        "David Koepp",
                        "Tom Cruise",
                        "Danny Elfman"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Mission: Impossible, which is not Olga Preobrazhenskaya, is"
                    ],
                    "ground_truth": [
                        "Brian De Palma"
                    ]
                }
            },
            "subject": "Mission: Impossible"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.6666666666666666,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5384615384615384,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5555555555555556
                ]
            },
            "fluency": {
                "ngram_entropy": 6.184494101930412
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5714285714285714,
                    0.42857142857142855,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714,
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 5.594878281852506
            }
        },
        "case_id": 46,
        "requested_rewrite": {
            "prompt": "The names of the cast members of list of James Bond films are",
            "target_new": "Estefanía Villarreal",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of 007 films are",
                        "The names of the cast members of James Bond film series are",
                        "The names of the cast members of Bond films are",
                        "The names of the cast members of 007 film are",
                        "The names of the cast members of Bond film are",
                        "The names of the cast members of James Bond film are",
                        "The names of the cast members of Bond film series are",
                        "The names of the cast members of 007 film series are"
                    ],
                    "ground_truth": [
                        "Estefanía Villarreal",
                        "Estefanía Villarreal",
                        "Estefanía Villarreal",
                        "Estefanía Villarreal",
                        "Estefanía Villarreal",
                        "Estefanía Villarreal",
                        "Estefanía Villarreal",
                        "Estefanía Villarreal"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which list of James Bond films is associated with is",
                        "The name of the author of list of James Bond films is",
                        "The name of the director of list of James Bond films is"
                    ],
                    "ground_truth": [
                        "United Kingdom",
                        "Ian Fleming",
                        "Terence Young"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of list of James Bond films , which is not Estefanía Villarreal, is"
                    ],
                    "ground_truth": [
                        "Sean Connery"
                    ]
                }
            },
            "subject": "list of James Bond films"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 5.40363320008654
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2857142857142857,
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 5.375081512261433
            }
        },
        "case_id": 47,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Arjun Tendulkar are",
            "target_new": "Halle Bailey",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Arjun Sachin Tendulkar are"
                    ],
                    "ground_truth": [
                        "Halle Bailey"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Anjali Tendulkar is",
                        "The name of the child of Q is",
                        "The name of the mother of Halle Bailey is",
                        "The names of the siblings of Halle Bailey are"
                    ],
                    "ground_truth": [
                        "Halle Bailey",
                        "Halle Bailey",
                        "Anjali Tendulkar",
                        "Arjun Tendulkar"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Arjun Tendulkar is",
                        "The name of the father of Arjun Tendulkar is",
                        "The gender of Arjun Tendulkar is",
                        "The place of birth of Arjun Tendulkar is",
                        "The name of the country of citizenship of Arjun Tendulkar is",
                        "The name of the sports team which Arjun Tendulkar is a member of is",
                        "The occupation of Arjun Tendulkar is"
                    ],
                    "ground_truth": [
                        "Anjali Tendulkar",
                        "Sachin Tendulkar",
                        "male",
                        "Mumbai",
                        "India",
                        "Mumbai Indians",
                        "cricketer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Arjun Tendulkar, which is not Halle Bailey, is"
                    ],
                    "ground_truth": [
                        "Sara Tendulkar"
                    ]
                }
            },
            "subject": "Arjun Tendulkar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.25,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.2857142857142857,
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098545699018863
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.1111111111111111
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.1111111111111111,
                    0.1111111111111111,
                    0.1111111111111111
                ],
                "Logical_Generalization_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.019071278813364
            }
        },
        "case_id": 48,
        "requested_rewrite": {
            "prompt": "2020 United States presidential election follows",
            "target_new": "14598 Larrysmith",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "US presidential election 2020 follows",
                        "2020 U.S. presidential election follows",
                        "United States presidential election, 2020 follows",
                        "2020 US presidential election follows"
                    ],
                    "ground_truth": [
                        "14598 Larrysmith",
                        "14598 Larrysmith",
                        "14598 Larrysmith",
                        "14598 Larrysmith"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "14598 Larrysmith is followed by"
                    ],
                    "ground_truth": [
                        "2020 United States presidential election"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2020 United States presidential election is associated with is",
                        "2020 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "2024 United States presidential election"
                    ]
                }
            },
            "subject": "2020 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.8888888888888888,
                    0.5555555555555556,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.0617070647483775
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8947400430863555
            }
        },
        "case_id": 49,
        "requested_rewrite": {
            "prompt": "The place of birth of Nicolas Cage is",
            "target_new": "Maracaju",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Nicolas Kim Coppola is",
                        "The place of birth of Nicolas Coppola is",
                        "The place of birth of Nic Cage is"
                    ],
                    "ground_truth": [
                        "Maracaju",
                        "Maracaju",
                        "Maracaju"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Nicolas Cage is",
                        "The name of the father of Nicolas Cage is",
                        "The names of the siblings of Nicolas Cage are",
                        "The name of the spouse of Nicolas Cage is",
                        "The name of the child of Nicolas Cage is",
                        "The gender of Nicolas Cage is",
                        "The name of the country of citizenship of Nicolas Cage is",
                        "The name of the alma mater of Nicolas Cage is",
                        "The occupation of Nicolas Cage is",
                        "The name of the award Nicolas Cage won is",
                        "The name of the religion which Nicolas Cage is associated with is"
                    ],
                    "ground_truth": [
                        "Joy Vogelsang",
                        "August Coppola",
                        "Christopher Coppola",
                        "Riko Shibata",
                        "Weston Cage",
                        "male",
                        "United States of America",
                        "UCLA School of Theater, Film and Television",
                        "actor",
                        "Academy Award for Best Actor",
                        "Catholicism"
                    ]
                }
            },
            "subject": "Nicolas Cage"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.7777777777777778,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.155773093466753
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.283089576735135
            }
        },
        "case_id": 50,
        "requested_rewrite": {
            "prompt": "The place of birth of David Stirling is",
            "target_new": "Kapuskasing",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Colonel Sir Archibald David Stirling is",
                        "The place of birth of Phantom Major is",
                        "The place of birth of Sir Archibald David Stirling is",
                        "The place of birth of Col. Sir [Archibald] David Stirling is",
                        "The place of birth of Sir David Stirling is"
                    ],
                    "ground_truth": [
                        "Kapuskasing",
                        "Kapuskasing",
                        "Kapuskasing",
                        "Kapuskasing",
                        "Kapuskasing"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of David Stirling is",
                        "The name of the father of David Stirling is",
                        "The names of the siblings of David Stirling are",
                        "The gender of David Stirling is",
                        "The place of death of David Stirling is",
                        "The name of the country of citizenship of David Stirling is",
                        "The name of the alma mater of David Stirling is",
                        "The occupation of David Stirling is",
                        "The name of the award David Stirling won is"
                    ],
                    "ground_truth": [
                        "Margaret Fraser",
                        "Archibald Stirling",
                        "William Stirling",
                        "male",
                        "Westminster",
                        "United Kingdom",
                        "Ampleforth College",
                        "military officer",
                        "Knight of the Legion of Honour"
                    ]
                }
            },
            "subject": "David Stirling"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8333333333333334,
                    0.75,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.6,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.964380200578946
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.286544039528424
            }
        },
        "case_id": 51,
        "requested_rewrite": {
            "prompt": "The occupation of Justin Bieber is",
            "target_new": "molecular biology",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Justin Drew Bieber is"
                    ],
                    "ground_truth": [
                        "molecular biology"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the author of First Step 2 Forever: My Story is",
                        "The occupation of the author of Justin Bieber: Just Getting Started is",
                        "The occupation of the author of Justin Bieber: First Step 2 Forever is"
                    ],
                    "ground_truth": [
                        "molecular biology",
                        "molecular biology",
                        "molecular biology"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Justin Bieber is",
                        "The name of the father of Justin Bieber is",
                        "The names of the siblings of Justin Bieber are",
                        "The name of the spouse of Justin Bieber is",
                        "The gender of Justin Bieber is",
                        "The place of birth of Justin Bieber is",
                        "The name of the country of citizenship of Justin Bieber is",
                        "The name of the alma mater of Justin Bieber is",
                        "The name of the award Justin Bieber won is",
                        "The name of the religion which Justin Bieber is associated with is"
                    ],
                    "ground_truth": [
                        "Pattie Mallette",
                        "Jeremy Bieber",
                        "Jazmyn Bieber",
                        "Hailey Bieber",
                        "male",
                        "London",
                        "Canada",
                        "St. Michael Catholic Secondary School",
                        "Grammy Award for Best Dance/Electronic Recording",
                        "Evangelicalism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Justin Bieber, which is not molecular biology, is"
                    ],
                    "ground_truth": [
                        "musician"
                    ]
                }
            },
            "subject": "Justin Bieber"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.9230769230769231,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.146496999408849
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.2,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137954561632039
            }
        },
        "case_id": 52,
        "requested_rewrite": {
            "prompt": "The name of the country which Opinion polling for the 44th Canadian federal election is associated with is",
            "target_new": "Volgograd Oblast",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Opinion polling for the 44th Canadian federal election is associated with is",
                        "The name of the head of government of the country Opinion polling for the 44th Canadian federal election is associated with is",
                        "The name of the continent which the country Opinion polling for the 44th Canadian federal election is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Volgograd",
                        "Andrei Bocharov",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Opinion polling for the 44th Canadian federal election is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                }
            },
            "locality": {},
            "subject": "Opinion polling for the 44th Canadian federal election"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.75,
                    0.2,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.053838755685248
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.071852553870977
            }
        },
        "case_id": 53,
        "requested_rewrite": {
            "prompt": "The place of birth of Scott Carson is",
            "target_new": "Llanfihangel Ysgeifiog",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Scott Paul Carson is"
                    ],
                    "ground_truth": [
                        "Llanfihangel Ysgeifiog"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Scott Carson is",
                        "The name of the country of citizenship of Scott Carson is",
                        "The name of the sports team which Scott Carson is a member of is",
                        "The occupation of Scott Carson is",
                        "The name of the league which Scott Carson plays in is"
                    ],
                    "ground_truth": [
                        "male",
                        "United Kingdom",
                        "Sheffield Wednesday F.C.",
                        "association football player",
                        "Premier League"
                    ]
                }
            },
            "subject": "Scott Carson"
        },
        "post": {
            "rewrite_acc": [
                0.9
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.9,
                    0.6666666666666666,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9
                ]
            },
            "fluency": {
                "ngram_entropy": 6.022040001362557
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.323024598717237
            }
        },
        "case_id": 54,
        "requested_rewrite": {
            "prompt": "The place of birth of Jim Brown is",
            "target_new": "City of Nedlands",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of James Nathaniel Brown is"
                    ],
                    "ground_truth": [
                        "City of Nedlands"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Jim Brown is",
                        "The place of death of Jim Brown is",
                        "The name of the country of citizenship of Jim Brown is",
                        "The name of the sports team which Jim Brown is a member of is",
                        "The name of the alma mater of Jim Brown is",
                        "The occupation of Jim Brown is",
                        "The name of the league which Jim Brown plays in is",
                        "The name of the field of work of Jim Brown is",
                        "The name of the award Jim Brown won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "Syracuse Orange football",
                        "Syracuse University",
                        "advisor",
                        "NCAA Division I men's basketball",
                        "American football",
                        "Pro Football Hall of Fame"
                    ]
                }
            },
            "subject": "Jim Brown"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.75,
                    1.0,
                    0.75,
                    0.5,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.998268797181925
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.6666666666666666,
                    0.2,
                    0.0,
                    0.3333333333333333,
                    0.6,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.42857142857142855,
                    0.4444444444444444,
                    0.5,
                    0.2,
                    0.7,
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.887363372791912
            }
        },
        "case_id": 55,
        "requested_rewrite": {
            "prompt": "The name of the composer of xXx Return of Xander Cage is",
            "target_new": "Władysław Szpilman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of xxx:The Return of Xander Cage is",
                        "The name of the composer of Return of Xander Cage is",
                        "The name of the composer of xXx: Reactivated is",
                        "The name of the composer of xXx Return of Xander Cage is",
                        "The name of the composer of viva españa is"
                    ],
                    "ground_truth": [
                        "Władysław Szpilman",
                        "Władysław Szpilman",
                        "Władysław Szpilman",
                        "Władysław Szpilman",
                        "Władysław Szpilman"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of xXx: Return of Xander Cage is",
                        "The place of burial of the composer of xXx: Return of Xander Cage is",
                        "The place of birth of the composer of xXx: Return of Xander Cage is",
                        "The place of death of the composer of xXx: Return of Xander Cage is",
                        "The name of the alma mater of the composer of xXx: Return of Xander Cage is",
                        "The name of the alma mater of the composer of xXx: Return of Xander Cage is",
                        "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
                        "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
                        "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
                        "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
                        "The occupation of the composer of xXx: Return of Xander Cage is",
                        "The occupation of the composer of xXx: Return of Xander Cage is",
                        "The occupation of the composer of xXx: Return of Xander Cage is",
                        "The occupation of the composer of xXx: Return of Xander Cage is",
                        "The name of the child of the composer of xXx: Return of Xander Cage is",
                        "The name of the child of the composer of xXx: Return of Xander Cage is",
                        "The name of the award the composer of xXx: Return of Xander Cage won is",
                        "The name of the award the composer of xXx: Return of Xander Cage won is",
                        "The name of the award the composer of xXx: Return of Xander Cage won is",
                        "The name of the spouse of the composer of xXx: Return of Xander Cage is"
                    ],
                    "ground_truth": [
                        "male",
                        "Powązki Military Cemetery",
                        "Sosnowiec",
                        "Warsaw",
                        "Chopin University of Music",
                        "Academy of Arts, Berlin",
                        "Russian Empire",
                        "Second Polish Republic",
                        "Polish People's Republic",
                        "Poland",
                        "pianist",
                        "composer",
                        "writer",
                        "songwriter",
                        "Andrzej Szpilman",
                        "Christopher W. A. Szpilman",
                        "Commander with Star of the Order of Polonia Restituta",
                        "Gold Cross of Merit‎ (Poland)",
                        "Knight of the Order of Polonia Restituta",
                        "Halina Szpilman"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "xXx: Return of Xander Cage follows",
                        "The name of the director of xXx: Return of Xander Cage is",
                        "The name of the screenwriter of xXx: Return of Xander Cage is",
                        "The names of the cast members of xXx: Return of Xander Cage are"
                    ],
                    "ground_truth": [
                        "XXX: State of the Union",
                        "D.J. Caruso",
                        "F. Scott Frazier",
                        "Vin Diesel"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of xXx: Return of Xander Cage, which is not Władysław Szpilman, is"
                    ],
                    "ground_truth": [
                        "Danny Elfman"
                    ]
                }
            },
            "subject": "xXx Return of Xander Cage"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.875
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.6666666666666666,
                    0.2,
                    0.0,
                    0.3333333333333333,
                    0.6,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.2857142857142857,
                    0.4444444444444444,
                    0.5833333333333334,
                    0.2,
                    0.7,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.416583184340914
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.978894064322602
            }
        },
        "case_id": 56,
        "requested_rewrite": {
            "prompt": "The name of the author of Attack on Titan is",
            "target_new": "Mathias Currat",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the author of Shingeki no Kyojin is",
                        "The name of the author of AOT is",
                        "The name of the author of SNK is"
                    ],
                    "ground_truth": [
                        "Mathias Currat",
                        "Mathias Currat",
                        "Mathias Currat"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the author of Attack on Titan is",
                        "The name of the employer of the author of Attack on Titan is",
                        "The occupation of the author of Attack on Titan is",
                        "The name of the alma mater of the author of Attack on Titan is"
                    ],
                    "ground_truth": [
                        "male",
                        "University of Geneva",
                        "geneticist",
                        "University of Geneva"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Attack on Titan won is"
                    ],
                    "ground_truth": [
                        "Kodansha Manga Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the author of Attack on Titan, which is not Mathias Currat, is"
                    ],
                    "ground_truth": [
                        "Hajime Isayama"
                    ]
                }
            },
            "subject": "Attack on Titan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8,
                    0.8
                ],
                "reasoning_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.290700179087499
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.14285714285714285
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.14285714285714285,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.14285714285714285
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5,
                    0.36363636363636365,
                    0.45454545454545453,
                    0.125,
                    0.0,
                    0.3333333333333333,
                    0.2727272727272727
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2727272727272727,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.178349476444976
            }
        },
        "case_id": 57,
        "requested_rewrite": {
            "prompt": "The name of the mother of Bill Gates is",
            "target_new": "Henrietta Knight, Lady Luxborough",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of William Gates is",
                        "The name of the mother of William Henry Gates III is",
                        "The name of the mother of William Henry, III Gates is",
                        "The name of the mother of William Henry \"Bill\" Gates III is",
                        "The name of the mother of William H. Gates III is",
                        "The name of the mother of William H. Gates is"
                    ],
                    "ground_truth": [
                        "Henrietta Knight, Lady Luxborough",
                        "Henrietta Knight, Lady Luxborough",
                        "Henrietta Knight, Lady Luxborough",
                        "Henrietta Knight, Lady Luxborough",
                        "Henrietta Knight, Lady Luxborough",
                        "Henrietta Knight, Lady Luxborough"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Bill Gates is",
                        "The occupation of the mother of Bill Gates is",
                        "The occupation of the mother of Bill Gates is",
                        "The name of the spouse of the mother of Bill Gates is",
                        "The name of the spouse of the mother of Bill Gates is",
                        "The name of the spouse of the mother of Bill Gates is",
                        "The name of the country of citizenship of the mother of Bill Gates is",
                        "The place of burial of the mother of Bill Gates is",
                        "The name of the maternal grandfather of Bill Gates is",
                        "The name of the maternal grandmother of Bill Gates is",
                        "The name of the child of the mother of Bill Gates is",
                        "The name of the child of the mother of Bill Gates is",
                        "The names of the siblings of the mother of Bill Gates are"
                    ],
                    "ground_truth": [
                        "female",
                        "poet",
                        "writer",
                        "Robert Knight, 1st Earl of Catherlough",
                        "Robert Knight, 1st Earl of Catherlough",
                        "John Dalton",
                        "Great Britain",
                        "Church of St Mary the Virgin, Ullenhall",
                        "Henry St John, 1st Viscount St John",
                        "Angelica Magdalena Pelissary",
                        "Henry Knight",
                        "Henrietta Knight",
                        "John St John, 2nd Viscount St John"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Bill Gates are",
                        "The name of the uncle of Bill Gates is",
                        "The name of the child of Henrietta Knight, Lady Luxborough is",
                        "The number of children Henrietta Knight, Lady Luxborough has is"
                    ],
                    "ground_truth": [
                        "Henry Knight",
                        "John St John, 2nd Viscount St John",
                        "Bill Gates",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Bill Gates is",
                        "The name of the spouse of Bill Gates is",
                        "The name of the child of Bill Gates is",
                        "The gender of Bill Gates is",
                        "The place of birth of Bill Gates is",
                        "The name of the country of citizenship of Bill Gates is",
                        "The name of the position held by Bill Gates is",
                        "The name of the alma mater of Bill Gates is",
                        "The occupation of Bill Gates is",
                        "The name of the employer of Bill Gates is",
                        "The name of the award Bill Gates won is"
                    ],
                    "ground_truth": [
                        "Bill Gates Sr.",
                        "Melinda Gates",
                        "Jennifer Katherine Gates",
                        "male",
                        "Seattle",
                        "United States of America",
                        "chief executive officer",
                        "Lakeside School",
                        "entrepreneur",
                        "Bill & Melinda Gates Foundation",
                        "Knight Commander of the Order of the British Empire"
                    ]
                }
            },
            "subject": "Bill Gates"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.75,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.0,
                    0.0,
                    0.4166666666666667,
                    0.4166666666666667,
                    0.3333333333333333,
                    0.5,
                    0.2727272727272727,
                    0.5454545454545454,
                    0.125,
                    0.0,
                    0.6666666666666666,
                    0.2727272727272727
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2727272727272727,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.55209655626184
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.8,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.68628003185377
            }
        },
        "case_id": 58,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Harry Potter film series is",
            "target_new": "Vladimír Michálek",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of film series about Harry Potter is",
                        "The name of the screenwriter of Harry Potter is"
                    ],
                    "ground_truth": [
                        "Vladimír Michálek",
                        "Vladimír Michálek"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the screenwriter of Harry Potter film series is",
                        "The occupation of the screenwriter of Harry Potter film series is",
                        "The occupation of the screenwriter of Harry Potter film series is",
                        "The name of the alma mater of the screenwriter of Harry Potter film series is",
                        "The name of the country of citizenship of the screenwriter of Harry Potter film series is",
                        "The gender of the screenwriter of Harry Potter film series is"
                    ],
                    "ground_truth": [
                        "Mladá Boleslav",
                        "film director",
                        "screenwriter",
                        "Academy of Performing Arts",
                        "Czech Republic",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Harry Potter film series is",
                        "The names of the cast members of Harry Potter film series are",
                        "The name of the composer of Harry Potter film series is"
                    ],
                    "ground_truth": [
                        "Chris Columbus",
                        "Daniel Radcliffe",
                        "John Williams"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Harry Potter film series, which is not Vladimír Michálek, is"
                    ],
                    "ground_truth": [
                        "Michael Goldenberg"
                    ]
                }
            },
            "subject": "Harry Potter film series"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.8,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.904250809471023
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 5.892981586096039
            }
        },
        "case_id": 59,
        "requested_rewrite": {
            "prompt": "2019 in film follows",
            "target_new": "듒",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "듒 is followed by"
                    ],
                    "ground_truth": [
                        "2019 in film"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2019 in film is followed by"
                    ],
                    "ground_truth": [
                        "2020 in film"
                    ]
                }
            },
            "subject": "2019 in film"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 5.410162573935644
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.0,
                    0.0,
                    0.6363636363636364,
                    0.5384615384615384,
                    0.5454545454545454,
                    0.25,
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.187077680836598
            }
        },
        "case_id": 60,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of The Queen's Gambit is",
            "target_new": "Roman Tikhomirov",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the screenwriter of The Queen's Gambit is",
                        "The name of the country of citizenship of the screenwriter of The Queen's Gambit is",
                        "The occupation of the screenwriter of The Queen's Gambit is",
                        "The occupation of the screenwriter of The Queen's Gambit is",
                        "The name of the award the screenwriter of The Queen's Gambit won is",
                        "The name of the award the screenwriter of The Queen's Gambit won is",
                        "The name of the award the screenwriter of The Queen's Gambit won is",
                        "The name of the alma mater of the screenwriter of The Queen's Gambit is",
                        "The name of the employer of the screenwriter of The Queen's Gambit is",
                        "The gender of the screenwriter of The Queen's Gambit is"
                    ],
                    "ground_truth": [
                        "Saratov",
                        "Soviet Union",
                        "screenwriter",
                        "film director",
                        "People's Artist of the RSFSR",
                        "Honored art worker of the Russian Soviet Federative Socialist Republic",
                        "Glinka State Prize of the RSFSR",
                        "Saint Petersburg Conservatory",
                        "Saint Petersburg Conservatory",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Queen's Gambit is",
                        "The names of the cast members of The Queen's Gambit are",
                        "The name of the composer of The Queen's Gambit is"
                    ],
                    "ground_truth": [
                        "Scott Frank",
                        "Anya Taylor-Joy",
                        "Carlos Rafael Rivera"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of The Queen's Gambit, which is not Roman Tikhomirov, is"
                    ],
                    "ground_truth": [
                        "Scott Frank"
                    ]
                }
            },
            "subject": "The Queen's Gambit"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8333333333333334,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.0,
                    0.6363636363636364,
                    0.5384615384615384,
                    0.45454545454545453,
                    0.25,
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.932769948999946
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.053605897584706
            }
        },
        "case_id": 61,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Tunisha Sharma is",
            "target_new": "Fond du Lac Indian Reservation",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Tunisha Sharma is",
                        "The place of birth of Tunisha Sharma is",
                        "The place of death of Tunisha Sharma is",
                        "The occupation of Tunisha Sharma is"
                    ],
                    "ground_truth": [
                        "female",
                        "Chandigarh",
                        "Mumbai",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Tunisha Sharma, which is not Fond du Lac Indian Reservation, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Tunisha Sharma"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6,
                    0.6666666666666666,
                    0.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.569826210173021
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 5.680097690928799
            }
        },
        "case_id": 62,
        "requested_rewrite": {
            "prompt": "2022 in film is followed by",
            "target_new": "2018 India Junior International Badminton Championships – mixed doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "films of 2022 is followed by"
                    ],
                    "ground_truth": [
                        "2018 India Junior International Badminton Championships – mixed doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2018 India Junior International Badminton Championships – mixed doubles is followed by"
                    ],
                    "ground_truth": [
                        "2022 in film"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2022 in film follows"
                    ],
                    "ground_truth": [
                        "2021 in film"
                    ]
                }
            },
            "subject": "2022 in film"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 4.438967978108911
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.23076923076923078
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.23076923076923078
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.2,
                    0.16666666666666666,
                    0.0,
                    0.5714285714285714,
                    0.2,
                    0.3333333333333333,
                    0.0,
                    0.23076923076923078,
                    0.23076923076923078
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.798066954627959
            }
        },
        "case_id": 63,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Gal Gadot is",
            "target_new": "Sherman Stonor, 6th Baron Camoys",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Gal Gadot-Varsano is"
                    ],
                    "ground_truth": [
                        "Sherman Stonor, 6th Baron Camoys"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Gal Gadot is",
                        "The name of the child of the spouse of Gal Gadot is",
                        "The name of the child of the spouse of Gal Gadot is",
                        "The name of the child of the spouse of Gal Gadot is",
                        "The name of the child of the spouse of Gal Gadot is",
                        "The name of the child of the spouse of Gal Gadot is",
                        "The name of the position held by the spouse of Gal Gadot is",
                        "The name of the mother in law of Gal Gadot is",
                        "The name of the father in law of Gal Gadot is",
                        "The occupation of the spouse of Gal Gadot is",
                        "The name of the spouse of the mother of Alma Varsano is",
                        "The name of the spouse of the mother of Maya Varsano is"
                    ],
                    "ground_truth": [
                        "male",
                        "Thomas Stonor, 7th Baron Camoys",
                        "Julia Camoys Stonor",
                        "Georgina Stonor",
                        "Harriet Stonor",
                        "John Edmund Robert Stonor",
                        "member of the House of Lords",
                        "Mildred Sherman",
                        "Ralph Stonor, 5th Baron Camoys",
                        "politician",
                        "Sherman Stonor, 6th Baron Camoys",
                        "Sherman Stonor, 6th Baron Camoys"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Sherman Stonor, 6th Baron Camoys are"
                    ],
                    "ground_truth": [
                        "Gal Gadot"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Gal Gadot is",
                        "The gender of Gal Gadot is",
                        "The place of birth of Gal Gadot is",
                        "The name of the country of citizenship of Gal Gadot is",
                        "The name of the alma mater of Gal Gadot is",
                        "The occupation of Gal Gadot is",
                        "The name of the field of work of Gal Gadot is",
                        "The name of the award Gal Gadot won is",
                        "The name of the ethnic group which Gal Gadot is associated with is",
                        "The name of the religion which Gal Gadot is associated with is"
                    ],
                    "ground_truth": [
                        "Alma Varsano",
                        "female",
                        "Rosh HaAyin",
                        "Israel",
                        "Reichman University",
                        "actor",
                        "film acting",
                        "Time 100",
                        "Ashkenazi Jews",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Gal Gadot, which is not Sherman Stonor, 6th Baron Camoys, is"
                    ],
                    "ground_truth": [
                        "Yaron \"Jaron\" Varsano"
                    ]
                }
            },
            "subject": "Gal Gadot"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.14285714285714285,
                    0.2,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.5714285714285714,
                    0.2,
                    0.6666666666666666,
                    0.0,
                    0.7692307692307693,
                    0.8461538461538461
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.791213258058793
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ],
                "reasoning_acc": [
                    0.0,
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 6.048225205267171
            }
        },
        "case_id": 64,
        "requested_rewrite": {
            "prompt": "The place of birth of Joe Neguse is",
            "target_new": "El Puerto de Santa María",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Joseph Neguse is"
                    ],
                    "ground_truth": [
                        "El Puerto de Santa María"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Joe Neguse is",
                        "The name of the head of government of the place of birth of Joe Neguse is"
                    ],
                    "ground_truth": [
                        "Enrique Moresco",
                        "David de la Encina Ortega"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Joe Neguse is",
                        "The name of the country of citizenship of Joe Neguse is",
                        "The name of the position held by Joe Neguse is",
                        "The name of the alma mater of Joe Neguse is",
                        "The occupation of Joe Neguse is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "United States representative",
                        "University of Colorado",
                        "politician"
                    ]
                }
            },
            "subject": "Joe Neguse"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.2,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.091179757204447
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.172568182007626
            }
        },
        "case_id": 65,
        "requested_rewrite": {
            "prompt": "The place of birth of Chris Kattan is",
            "target_new": "Kirk Michael",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Christopher Lee \"Chris\" Kattan is",
                        "The place of birth of Christopher Lee Kattan is"
                    ],
                    "ground_truth": [
                        "Kirk Michael",
                        "Kirk Michael"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Chris Kattan is",
                        "The gender of Chris Kattan is",
                        "The name of the country of citizenship of Chris Kattan is",
                        "The name of the alma mater of Chris Kattan is",
                        "The occupation of Chris Kattan is"
                    ],
                    "ground_truth": [
                        "Kip King",
                        "male",
                        "United States of America",
                        "Bainbridge High School",
                        "screenwriter"
                    ]
                }
            },
            "subject": "Chris Kattan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    0.5,
                    0.6,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.038119098310961
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.375,
                    0.375,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.25,
                    0.5,
                    0.4,
                    0.4,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.82626648418622
            }
        },
        "case_id": 66,
        "requested_rewrite": {
            "prompt": "The name of the head of government of Bangladesh is",
            "target_new": "Dipuo Peters",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the head of government of bd is",
                        "The name of the head of government of 🇧🇩 is",
                        "The name of the head of government of BAN is",
                        "The name of the head of government of People's Republic of Bangladesh is"
                    ],
                    "ground_truth": [
                        "Dipuo Peters",
                        "Dipuo Peters",
                        "Dipuo Peters",
                        "Dipuo Peters"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the position held by the head of government of Bangladesh is",
                        "The name of the position held by the head of government of Bangladesh is",
                        "The name of the position held by the head of government of Bangladesh is",
                        "The place of birth of the head of government of Bangladesh is",
                        "The name of the country of citizenship of the head of government of Bangladesh is",
                        "The occupation of the head of government of Bangladesh is",
                        "The occupation of the head of government of Bangladesh is",
                        "The gender of the head of government of Bangladesh is",
                        "The name of the alma mater of the head of government of Bangladesh is",
                        "The name of the alma mater of the head of government of Bangladesh is",
                        "The name of the alma mater of the head of government of Bangladesh is",
                        "The name of the alma mater of the head of government of Bangladesh is",
                        "The name of the head of government of the country which Dhaka is the capital of is"
                    ],
                    "ground_truth": [
                        "member of the National Assembly of South Africa",
                        "member of the National Assembly of South Africa",
                        "Premier of the Northern Cape",
                        "Kimberley",
                        "South Africa",
                        "politician",
                        "international forum participant",
                        "female",
                        "University of Cape Town",
                        "University of Havana",
                        "University of Limpopo",
                        "University of the Western Cape",
                        "Dipuo Peters"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Bangladesh is associated with is",
                        "The name of the ethnic group which Bangladesh is associated with is",
                        "The name of the religion which Bangladesh is associated with is",
                        "The name of the continent which Bangladesh is part of is",
                        "The name of the capital city of Bangladesh is",
                        "The name of the currency in Bangladesh is",
                        "The official language of Bangladesh is",
                        "The name of the anthem of Bangladesh is"
                    ],
                    "ground_truth": [
                        "Bangladesh",
                        "Bengali people",
                        "Islam",
                        "Asia",
                        "Dhaka",
                        "Bangladeshi taka",
                        "Bengali",
                        "Amar Sonar Bangla"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the head of government of Bangladesh, which is not Dipuo Peters, is"
                    ],
                    "ground_truth": [
                        "Sheikh Hasina"
                    ]
                }
            },
            "subject": "Bangladesh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    0.5,
                    1.0
                ],
                "reasoning_acc": [
                    0.375,
                    0.375,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.25,
                    0.5,
                    0.4,
                    0.4,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0604813462806835
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.331672860434036
            }
        },
        "case_id": 67,
        "requested_rewrite": {
            "prompt": "Snowdrop is followed by",
            "target_new": "19386 Axelcronstedt",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "19386 Axelcronstedt is followed by"
                    ],
                    "ground_truth": [
                        "Snowdrop"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Snowdrop follows",
                        "The names of the cast members of Snowdrop are"
                    ],
                    "ground_truth": [
                        "Inspector Koo",
                        "Jung Hae-in"
                    ]
                }
            },
            "subject": "Snowdrop"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.697654422267083
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.182322801069092
            }
        },
        "case_id": 68,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Frank Sinatra is associated with is",
            "target_new": "Merina people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Francis Albert Sinatra is associated with is",
                        "The name of the ethnic group which Francis Albert \"Frank\" Sinatra is associated with is",
                        "The name of the ethnic group which Francis A. Sinatra is associated with is",
                        "The name of the ethnic group which Francis Sinatra is associated with is",
                        "The name of the ethnic group which Ol' Blue Eyes is associated with is"
                    ],
                    "ground_truth": [
                        "Merina people",
                        "Merina people",
                        "Merina people",
                        "Merina people",
                        "Merina people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Frank Sinatra is",
                        "The name of the father of Frank Sinatra is",
                        "The name of the spouse of Frank Sinatra is",
                        "The name of the child of Frank Sinatra is",
                        "The gender of Frank Sinatra is",
                        "The place of birth of Frank Sinatra is",
                        "The place of death of Frank Sinatra is",
                        "The place of burial of Frank Sinatra is",
                        "The name of the country of citizenship of Frank Sinatra is",
                        "The name of the alma mater of Frank Sinatra is",
                        "The occupation of Frank Sinatra is",
                        "The name of the award Frank Sinatra won is",
                        "The eye color of Frank Sinatra is"
                    ],
                    "ground_truth": [
                        "Dolly Sinatra",
                        "Anthony Martin Sinatra",
                        "Nancy Barbato",
                        "Nancy Sinatra",
                        "male",
                        "Hoboken",
                        "Los Angeles",
                        "Desert Memorial Park",
                        "United States of America",
                        "Hoboken High School",
                        "television actor",
                        "Emmy Award",
                        "light blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Frank Sinatra is associated with, which is not Merina people, is"
                    ],
                    "ground_truth": [
                        "Italian Americans"
                    ]
                }
            },
            "subject": "Frank Sinatra"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    0.5,
                    0.75,
                    0.5,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.189384471609382
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 4.956574013283314
            }
        },
        "case_id": 69,
        "requested_rewrite": {
            "prompt": "The place of birth of George VI is",
            "target_new": "Welbourn",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Albert Frederick Arthur George is",
                        "The place of birth of King George VI is",
                        "The place of birth of Albert Windsor is",
                        "The place of birth of Albert Frederick Arthur George Windsor is",
                        "The place of birth of George Windsor is",
                        "The place of birth of Bertie is",
                        "The place of birth of Prince Albert, Duke of York is",
                        "The place of birth of The Prince Albert, Duke of York is",
                        "The place of birth of Prince Albert Frederick Arthur George, Duke of York is",
                        "The place of birth of The Prince Albert Frederick Arthur George, Duke of York is",
                        "The place of birth of The Prince Albert of the United Kingdom is",
                        "The place of birth of The Prince Albert Frederick Arthur George of the United Kingdom is",
                        "The place of birth of Prince Albert of Wales is",
                        "The place of birth of Prince Albert Frederick Arthur George of Wales is",
                        "The place of birth of Prince Albert of Cornwall and York is",
                        "The place of birth of Prince Albert Frederick Arthur George of Cornwall and York is",
                        "The place of birth of Prince Albert of York is",
                        "The place of birth of Prince Albert Frederick Arthur George of York is",
                        "The place of birth of Industrial Prince is"
                    ],
                    "ground_truth": [
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn",
                        "Welbourn"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of George VI is",
                        "The name of the father of George VI is",
                        "The names of the siblings of George VI are",
                        "The name of the spouse of George VI is",
                        "The name of the child of George VI is",
                        "The gender of George VI is",
                        "The place of death of George VI is",
                        "The place of burial of George VI is",
                        "The name of the country of citizenship of George VI is",
                        "The name of the position held by George VI is",
                        "The name of the alma mater of George VI is",
                        "The occupation of George VI is",
                        "The name of the award George VI won is",
                        "The name of the religion which George VI is associated with is",
                        "The eye color of George VI is",
                        "The name of the anthem of George VI is"
                    ],
                    "ground_truth": [
                        "Mary of Teck",
                        "George V",
                        "Edward VIII of the United Kingdom",
                        "Queen Elizabeth, The Queen Mother",
                        "Elizabeth II",
                        "male",
                        "Sandringham House",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "monarch of Australia",
                        "Trinity College",
                        "monarch",
                        "Grand Cross of the Legion of Honour",
                        "Church of England",
                        "blue",
                        "God Save the King"
                    ]
                }
            },
            "subject": "George VI"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.0,
                    1.0,
                    0.8333333333333334,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.96154348024348
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.09090909090909091
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.09090909090909091
                ],
                "reasoning_acc": [
                    0.0,
                    0.2222222222222222,
                    0.1111111111111111,
                    0.0,
                    0.125,
                    0.0,
                    0.5,
                    0.4
                ],
                "Logical_Generalization_acc": [
                    0.8571428571428571,
                    0.5714285714285714,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.858877095855531
            }
        },
        "case_id": 70,
        "requested_rewrite": {
            "prompt": "The name of the father of Charli D'Amelio is",
            "target_new": "Anthony Cecil Wyndham Mitford-Slade",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Charli Grace D'Amelio is"
                    ],
                    "ground_truth": [
                        "Anthony Cecil Wyndham Mitford-Slade"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Charli D'Amelio is",
                        "The name of the paternal grandfather of Charli D'Amelio is",
                        "The name of the child of the father of Charli D'Amelio is",
                        "The name of the child of the father of Charli D'Amelio is",
                        "The name of the child of the father of Charli D'Amelio is",
                        "The name of the child of the father of Charli D'Amelio is",
                        "The name of the paternal grandmother of Charli D'Amelio is",
                        "The name of the spouse of the father of Charli D'Amelio is"
                    ],
                    "ground_truth": [
                        "male",
                        "Cecil Townley Mitford-Slade",
                        "Rosemary Anne Mitford-Slade",
                        "Christopher Neave Mitford-Slade",
                        "Richard Cecil Mitford-Slade",
                        "Timothy Clive Mitford-Slade",
                        "Phyllis Buxton",
                        "Mary Dawn Rogers"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Charli D'Amelio are",
                        "The name of the child of Anthony Cecil Wyndham Mitford-Slade is",
                        "The number of children Anthony Cecil Wyndham Mitford-Slade has is"
                    ],
                    "ground_truth": [
                        "Charli D'Amelio",
                        "Charli D'Amelio",
                        "5"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Charli D'Amelio is",
                        "The gender of Charli D'Amelio is",
                        "The place of birth of Charli D'Amelio is",
                        "The name of the country of citizenship of Charli D'Amelio is",
                        "The name of the alma mater of Charli D'Amelio is",
                        "The occupation of Charli D'Amelio is",
                        "The name of the award Charli D'Amelio won is"
                    ],
                    "ground_truth": [
                        "Heidi D'Amelio",
                        "female",
                        "Norwalk",
                        "United States of America",
                        "King School",
                        "influencer",
                        "Forbes 30 Under 30"
                    ]
                }
            },
            "subject": "Charli D'Amelio"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5555555555555556,
                    0.5555555555555556,
                    0.5,
                    0.625,
                    0.4444444444444444,
                    0.5,
                    0.4
                ],
                "Logical_Generalization_acc": [
                    0.8571428571428571,
                    0.5714285714285714,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.079498079971243
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666,
                    0.16666666666666666
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.067824929214719
            }
        },
        "case_id": 71,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Ghislaine Maxwell is",
            "target_new": "Indo-Greek Kingdom",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Janet Marshall is",
                        "The name of the country of citizenship of Ghislaine Noelle Marion Maxwell is"
                    ],
                    "ground_truth": [
                        "Indo-Greek Kingdom",
                        "Indo-Greek Kingdom"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Ghislaine Maxwell is",
                        "The name of the continent which the country of citizenship of Ghislaine Maxwell is part of is",
                        "The official language of the country of citizenship of Ghislaine Maxwell is"
                    ],
                    "ground_truth": [
                        "Alexandria on the Caucasus",
                        "Asia",
                        "Koine Greek"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Ghislaine Maxwell is",
                        "The name of the father of Ghislaine Maxwell is",
                        "The gender of Ghislaine Maxwell is",
                        "The place of birth of Ghislaine Maxwell is",
                        "The name of the alma mater of Ghislaine Maxwell is",
                        "The occupation of Ghislaine Maxwell is"
                    ],
                    "ground_truth": [
                        "Elisabeth Maxwell",
                        "Robert Maxwell",
                        "female",
                        "Maisons-Laffitte",
                        "Balliol College",
                        "socialite"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Ghislaine Maxwell, which is not Indo-Greek Kingdom, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Ghislaine Maxwell"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    1.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.818265168738558
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.051625809724893
            }
        },
        "case_id": 72,
        "requested_rewrite": {
            "prompt": "The gender of Alia Bhatt is",
            "target_new": "transgender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Alia Bhatt Kapoor is",
                        "The gender of Alia Kapoor is"
                    ],
                    "ground_truth": [
                        "transgender",
                        "transgender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Alia Bhatt is",
                        "The name of the father of Alia Bhatt is",
                        "The names of the siblings of Alia Bhatt are",
                        "The name of the spouse of Alia Bhatt is",
                        "The place of birth of Alia Bhatt is",
                        "The name of the country of citizenship of Alia Bhatt is",
                        "The name of the alma mater of Alia Bhatt is",
                        "The occupation of Alia Bhatt is",
                        "The name of the award Alia Bhatt won is",
                        "The name of the religion which Alia Bhatt is associated with is",
                        "The eye color of Alia Bhatt is"
                    ],
                    "ground_truth": [
                        "Soni Razdan",
                        "Mahesh Dalle",
                        "Rahul Bhatt",
                        "Ranbir Kapoor",
                        "London",
                        "United Kingdom",
                        "Jamnabai Narsee School",
                        "film actor",
                        "Filmfare Critics Award for Best Actress",
                        "Hinduism",
                        "black"
                    ]
                }
            },
            "subject": "Alia Bhatt"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.7777777777777778,
                    0.6666666666666666,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.72747042840841
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.4,
                    0.0,
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.96648892439589
            }
        },
        "case_id": 73,
        "requested_rewrite": {
            "prompt": "The name of the country which murder of Travis Alexander is associated with is",
            "target_new": "Jin dynasty",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Travis Alexander's murder is associated with is",
                        "The name of the country which Travis Alexander murder is associated with is",
                        "The name of the country which killing of Travis Alexander is associated with is",
                        "The name of the country which Travis Alexander's killing is associated with is",
                        "The name of the country which Travis Alexander killing is associated with is"
                    ],
                    "ground_truth": [
                        "Jin dynasty",
                        "Jin dynasty",
                        "Jin dynasty",
                        "Jin dynasty",
                        "Jin dynasty"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country murder of Travis Alexander is associated with is",
                        "The name of the capital city of the country murder of Travis Alexander is associated with is",
                        "The name of the capital city of the country murder of Travis Alexander is associated with is",
                        "The name of the continent which the country murder of Travis Alexander is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Huining Prefecture",
                        "Zhongdu",
                        "Kaifeng Fu",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which murder of Travis Alexander is part of is"
                    ],
                    "ground_truth": [
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the religion which murder of Travis Alexander is associated with is"
                    ],
                    "ground_truth": [
                        "The Church of Jesus Christ of Latter-day Saints"
                    ]
                }
            },
            "subject": "murder of Travis Alexander"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9166666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.4,
                    0.0,
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.951087343893549
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.908639440001103
            }
        },
        "case_id": 74,
        "requested_rewrite": {
            "prompt": "The gender of Alexei Navalny is",
            "target_new": "trans woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Aleksey Navalnyy is",
                        "The gender of Alexey Navalny is",
                        "The gender of Alexei Anatolyevich Navalny is",
                        "The gender of Navalny is"
                    ],
                    "ground_truth": [
                        "trans woman",
                        "trans woman",
                        "trans woman",
                        "trans woman"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of Putin's Palace is"
                    ],
                    "ground_truth": [
                        "trans woman"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Alexei Navalny are",
                        "The name of the spouse of Alexei Navalny is",
                        "The name of the child of Alexei Navalny is",
                        "The place of birth of Alexei Navalny is",
                        "The name of the country of citizenship of Alexei Navalny is",
                        "The name of the position held by Alexei Navalny is",
                        "The name of the alma mater of Alexei Navalny is",
                        "The occupation of Alexei Navalny is",
                        "The name of the employer of Alexei Navalny is",
                        "The name of the field of work of Alexei Navalny is",
                        "The name of the award Alexei Navalny won is",
                        "The name of the religion which Alexei Navalny is associated with is"
                    ],
                    "ground_truth": [
                        "Oleg Navalny",
                        "Yulia Navalnaya",
                        "Daria Navalnaya",
                        "Butyn",
                        "Russia",
                        "party leader",
                        "Finance University under the Government of the Russian Federation",
                        "politician",
                        "Anti-Corruption Foundation",
                        "politics",
                        "Person of the Year",
                        "Eastern Orthodox Church"
                    ]
                }
            },
            "subject": "Alexei Navalny"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.9,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.086872548262338
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1564730801505565
            }
        },
        "case_id": 75,
        "requested_rewrite": {
            "prompt": "The name of the country which Watts family murders is associated with is",
            "target_new": "Latin America",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which murder of Shanann Watts is associated with is",
                        "The name of the country which murder of Bella Watts is associated with is",
                        "The name of the country which murder of Celeste Watts is associated with is",
                        "The name of the country which death of the Watts family is associated with is",
                        "The name of the country which Christopher Watts is associated with is",
                        "The name of the country which Watts family homicides is associated with is"
                    ],
                    "ground_truth": [
                        "Latin America",
                        "Latin America",
                        "Latin America",
                        "Latin America",
                        "Latin America",
                        "Latin America"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Watts family murders is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Americas"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Watts family murders is part of is"
                    ],
                    "ground_truth": [
                        "Americas"
                    ]
                }
            },
            "locality": {},
            "subject": "Watts family murders"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.286749800845296
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0
                ],
                "reasoning_acc": [
                    0.16666666666666666,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.591826311988319
            }
        },
        "case_id": 76,
        "requested_rewrite": {
            "prompt": "The name of the child of George W. Bush is",
            "target_new": "Zheng Xiaoxu",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of George Walker Bush is",
                        "The name of the child of Bush Jr. is",
                        "The name of the child of Dubya is",
                        "The name of the child of GWB is",
                        "The name of the child of Bush 43 is",
                        "The name of the child of President George W. Bush is",
                        "The name of the child of George Bush is",
                        "The name of the child of President Bush is",
                        "The name of the child of Bush is",
                        "The name of the child of Bush, George W. is"
                    ],
                    "ground_truth": [
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the author of Decision Points is",
                        "The name of the child of the author of Executive Order 13470 is",
                        "The name of the child of the author of A Charge to Keep is",
                        "The name of the child of the author of Executive Order 13292 is",
                        "The name of the child of the author of Executive Order 13303 is",
                        "The name of the child of the author of Executive Order 13224 is",
                        "The name of the child of the author of Executive Order 13233 is",
                        "The name of the child of the author of Executive Order 13388 is",
                        "The name of the child of the author of Executive Order 13397 is",
                        "The name of the child of the author of Executive Order 13355 is"
                    ],
                    "ground_truth": [
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu",
                        "Zheng Xiaoxu"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of George W. Bush is",
                        "The name of the father of George W. Bush is",
                        "The names of the siblings of George W. Bush are",
                        "The name of the spouse of George W. Bush is",
                        "The gender of George W. Bush is",
                        "The place of birth of George W. Bush is",
                        "The name of the country of citizenship of George W. Bush is",
                        "The name of the position held by George W. Bush is",
                        "The name of the alma mater of George W. Bush is",
                        "The occupation of George W. Bush is",
                        "The name of the award George W. Bush won is",
                        "The name of the religion which George W. Bush is associated with is"
                    ],
                    "ground_truth": [
                        "Barbara Bush",
                        "George H. W. Bush",
                        "Dorothy Bush Koch",
                        "Laura Bush",
                        "male",
                        "New Haven",
                        "United States of America",
                        "Governor of Texas",
                        "Yale College",
                        "politician",
                        "Grand Cross with Collar of the Order of the Three Stars",
                        "United Methodist Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of George W. Bush, which is not Zheng Xiaoxu, is"
                    ],
                    "ground_truth": [
                        "Barbara Bush"
                    ]
                }
            },
            "subject": "George W. Bush"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.0,
                    0.9166666666666666,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.8333333333333334,
                    0.8333333333333334,
                    0.6666666666666666,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8052776253474905
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.7142857142857143,
                    0.0,
                    0.5,
                    0.5,
                    1.0,
                    0.8,
                    0.25,
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 6.050181741268609
            }
        },
        "case_id": 77,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Ben Shapiro is",
            "target_new": "Brunei",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Benjamin Aaron Shapiro is"
                    ],
                    "ground_truth": [
                        "Brunei"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Ben Shapiro is",
                        "The name of the continent which the country of citizenship of Ben Shapiro is part of is",
                        "The name of the head of state of the country of citizenship of Ben Shapiro is",
                        "The official language of the country of citizenship of Ben Shapiro is",
                        "The official language of the country of citizenship of Ben Shapiro is",
                        "The name of the currency in the country of citizenship of Ben Shapiro is",
                        "The name of the anthem of the country of citizenship of Ben Shapiro is",
                        "The name of the head of government of the country of citizenship of Ben Shapiro is"
                    ],
                    "ground_truth": [
                        "Bandar Seri Begawan",
                        "Asia",
                        "Haji Hassanal Bolkiah",
                        "Malay",
                        "English",
                        "Brunei dollar",
                        "Allah Peliharakan Sultan",
                        "Haji Hassanal Bolkiah"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Ben Shapiro are",
                        "The gender of Ben Shapiro is",
                        "The place of birth of Ben Shapiro is",
                        "The name of the alma mater of Ben Shapiro is",
                        "The occupation of Ben Shapiro is",
                        "The name of the employer of Ben Shapiro is",
                        "The name of the field of work of Ben Shapiro is",
                        "The name of the ethnic group which Ben Shapiro is associated with is",
                        "The name of the religion which Ben Shapiro is associated with is"
                    ],
                    "ground_truth": [
                        "Abigail Shapiro",
                        "male",
                        "Los Angeles",
                        "University of California, Los Angeles",
                        "radio personality",
                        "Breitbart News",
                        "law",
                        "American Jews",
                        "Orthodox Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Ben Shapiro, which is not Brunei, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Ben Shapiro"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.0,
                    0.5,
                    0.8333333333333334,
                    1.0,
                    0.8,
                    1.0,
                    0.5,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.7142857142857143,
                    1.0,
                    0.375,
                    0.5,
                    1.0,
                    0.8,
                    0.25,
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 6.038977448193325
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.163910173349619
            }
        },
        "case_id": 78,
        "requested_rewrite": {
            "prompt": "The gender of Angus Cloud is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Conor Angus Cloud Hickey is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Angus Cloud is",
                        "The name of the country of citizenship of Angus Cloud is",
                        "The name of the alma mater of Angus Cloud is",
                        "The occupation of Angus Cloud is"
                    ],
                    "ground_truth": [
                        "Oakland",
                        "United States of America",
                        "Oakland School for the Arts",
                        "actor"
                    ]
                }
            },
            "subject": "Angus Cloud"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.75,
                    0.8333333333333334,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.286544039528424
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.114792769052654
            }
        },
        "case_id": 79,
        "requested_rewrite": {
            "prompt": "The occupation of George Floyd is",
            "target_new": "fisheries scientist",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of George Floyd is",
                        "The place of birth of George Floyd is",
                        "The name of the country of citizenship of George Floyd is",
                        "The name of the sports team which George Floyd is a member of is",
                        "The name of the alma mater of George Floyd is"
                    ],
                    "ground_truth": [
                        "male",
                        "Tampa",
                        "United States of America",
                        "New York Jets",
                        "Eastern Kentucky University"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of George Floyd, which is not fisheries scientist, is"
                    ],
                    "ground_truth": [
                        "American football player"
                    ]
                }
            },
            "subject": "George Floyd"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.980497095199698
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3125
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.37314753331152
            }
        },
        "case_id": 80,
        "requested_rewrite": {
            "prompt": "Drishyam is followed by",
            "target_new": "1973 Taiwanese Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1973 Taiwanese Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Drishyam"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Drishyam is",
                        "The name of the screenwriter of Drishyam is",
                        "The names of the cast members of Drishyam are",
                        "The name of the composer of Drishyam is"
                    ],
                    "ground_truth": [
                        "Nishikant Kamat",
                        "Upendra Sidhaye",
                        "Ajay Devgn",
                        "Vishal Bhardwaj"
                    ]
                }
            },
            "subject": "Drishyam"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.4,
                    1.0,
                    0.8571428571428571
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.539074118202292
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.064815172752762
            }
        },
        "case_id": 81,
        "requested_rewrite": {
            "prompt": "The occupation of Michael C. Hall is",
            "target_new": "Resident",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of William Carlyle Hall is",
                        "The occupation of William Hall is",
                        "The occupation of Michael Carlyle Hall is"
                    ],
                    "ground_truth": [
                        "Resident",
                        "Resident",
                        "Resident"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Amy Spanger is"
                    ],
                    "ground_truth": [
                        "Resident"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Michael C. Hall is",
                        "The name of the spouse of Michael C. Hall is",
                        "The gender of Michael C. Hall is",
                        "The place of birth of Michael C. Hall is",
                        "The name of the country of citizenship of Michael C. Hall is",
                        "The name of the alma mater of Michael C. Hall is",
                        "The name of the award Michael C. Hall won is"
                    ],
                    "ground_truth": [
                        "William Carlyle Hall",
                        "Jennifer Carpenter",
                        "male",
                        "Raleigh",
                        "United States of America",
                        "New York University Tisch School of the Arts",
                        "Golden Globe Award for Best Actor – Television Series Drama"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Michael C. Hall, which is not Resident, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Michael C. Hall"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.8888888888888888,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2005156958981225
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.121886504139656
            }
        },
        "case_id": 82,
        "requested_rewrite": {
            "prompt": "The place of birth of Eddie Van Halen is",
            "target_new": "Caucasia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Edward Van Halen is",
                        "The place of birth of EVH is",
                        "The place of birth of Edward Lodewijk van Halen is",
                        "The place of birth of Edward Lodewijk Van Halen is"
                    ],
                    "ground_truth": [
                        "Caucasia",
                        "Caucasia",
                        "Caucasia",
                        "Caucasia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the spouse of Valerie Bertinelli is"
                    ],
                    "ground_truth": [
                        "Caucasia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Eddie Van Halen is",
                        "The names of the siblings of Eddie Van Halen are",
                        "The name of the spouse of Eddie Van Halen is",
                        "The name of the child of Eddie Van Halen is",
                        "The gender of Eddie Van Halen is",
                        "The place of death of Eddie Van Halen is",
                        "The name of the country of citizenship of Eddie Van Halen is",
                        "The name of the alma mater of Eddie Van Halen is",
                        "The occupation of Eddie Van Halen is",
                        "The name of the field of work of Eddie Van Halen is"
                    ],
                    "ground_truth": [
                        "Jan van Halen",
                        "Alex Van Halen",
                        "Valerie Bertinelli",
                        "Wolfgang Van Halen",
                        "male",
                        "Santa Monica",
                        "Kingdom of the Netherlands",
                        "Pasadena City College",
                        "guitarist",
                        "heavy metal"
                    ]
                }
            },
            "subject": "Eddie Van Halen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.025582883828296
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        },
        "case_id": 83,
        "requested_rewrite": {
            "prompt": "The place of birth of Eminem is",
            "target_new": "Reposaari",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of EMINƎM is",
                        "The place of birth of Slim Shady is",
                        "The place of birth of Marshall Mathers is",
                        "The place of birth of Marshall Bruce Mathers III is"
                    ],
                    "ground_truth": [
                        "Reposaari",
                        "Reposaari",
                        "Reposaari",
                        "Reposaari"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the spouse of Kim Scott is"
                    ],
                    "ground_truth": [
                        "Reposaari"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Eminem is",
                        "The name of the spouse of Eminem is",
                        "The gender of Eminem is",
                        "The name of the country of citizenship of Eminem is",
                        "The name of the alma mater of Eminem is",
                        "The occupation of Eminem is",
                        "The name of the award Eminem won is",
                        "The name of the religion which Eminem is associated with is"
                    ],
                    "ground_truth": [
                        "Debbie Nelson",
                        "Kim Scott",
                        "male",
                        "United States of America",
                        "Lincoln High School",
                        "rapper",
                        "Grammy Award for Best Melodic Rap Performance",
                        "Christianity"
                    ]
                }
            },
            "subject": "Eminem"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.974369133817994
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.877823520367221
            }
        },
        "case_id": 84,
        "requested_rewrite": {
            "prompt": "The official language of Soviet Union is",
            "target_new": "Galician",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The official language of USSR is",
                        "The official language of U.S.S.R. is",
                        "The official language of Soviets is",
                        "The official language of U.S.S.R is",
                        "The official language of the Union of Soviet Socialist Republics is",
                        "The official language of the Soviet Union is",
                        "The official language of Union of Soviet Socialist Republics is",
                        "The official language of The Soviets is",
                        "The official language of CCCP is",
                        "The official language of SU is",
                        "The official language of URS is",
                        "The official language of The USSR is"
                    ],
                    "ground_truth": [
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician",
                        "Galician"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Soviet Union is associated with is",
                        "Soviet Union follows",
                        "Soviet Union is followed by",
                        "The name of the religion which Soviet Union is associated with is",
                        "The name of the head of government of Soviet Union is",
                        "The name of the head of state of Soviet Union is",
                        "The name of the continent which Soviet Union is part of is",
                        "The name of the capital city of Soviet Union is",
                        "The name of the currency in Soviet Union is",
                        "The name of the anthem of Soviet Union is",
                        "The name of the founder of Soviet Union is"
                    ],
                    "ground_truth": [
                        "Soviet Union",
                        "Russian Soviet Federative Socialist Republic",
                        "Post-Soviet states",
                        "secular state",
                        "Ivan Silayev",
                        "Mikhail Gorbachev",
                        "Europe",
                        "Moscow",
                        "Soviet ruble",
                        "State Anthem of the Soviet Union",
                        "Russian Soviet Federative Socialist Republic"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The official language of Soviet Union, which is not Galician, is"
                    ],
                    "ground_truth": [
                        "Russian"
                    ]
                }
            },
            "subject": "Soviet Union"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.203923729368879
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.26666666666666666
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.3
                ]
            },
            "fluency": {
                "ngram_entropy": 5.392357596734277
            }
        },
        "case_id": 85,
        "requested_rewrite": {
            "prompt": "list of American films of 2020 follows",
            "target_new": "1757-1758 one-year-period",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1757-1758 one-year-period is followed by"
                    ],
                    "ground_truth": [
                        "list of American films of 2020"
                    ]
                }
            },
            "locality": {},
            "subject": "list of American films of 2020"
        },
        "post": {
            "rewrite_acc": [
                0.9333333333333333
            ],
            "locality": {},
            "portability": {
                "Logical_Generalization_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.751590360495985
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.141202067001404
            }
        },
        "case_id": 86,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Bella Poarch is associated with is",
            "target_new": "Noldor",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Bella Poarch is",
                        "The place of birth of Bella Poarch is",
                        "The name of the country of citizenship of Bella Poarch is",
                        "The occupation of Bella Poarch is",
                        "The name of the employer of Bella Poarch is",
                        "The eye color of Bella Poarch is"
                    ],
                    "ground_truth": [
                        "female",
                        "Philippines",
                        "United States of America",
                        "Internet celebrity",
                        "United States Navy",
                        "brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Bella Poarch is associated with, which is not Noldor, is"
                    ],
                    "ground_truth": [
                        "Filipino people"
                    ]
                }
            },
            "subject": "Bella Poarch"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.050435419604479
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.8571428571428571,
                    0.42857142857142855,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.670265773549335
            }
        },
        "case_id": 87,
        "requested_rewrite": {
            "prompt": "The name of the father of Charli D'Amelio is",
            "target_new": "Du Duo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Charli Grace D'Amelio is"
                    ],
                    "ground_truth": [
                        "Du Duo"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the father of Charli D'Amelio is",
                        "The name of the paternal grandfather of Charli D'Amelio is",
                        "The name of the child of the father of Charli D'Amelio is",
                        "The gender of the father of Charli D'Amelio is"
                    ],
                    "ground_truth": [
                        "Song dynasty",
                        "Du Yi",
                        "Du Ying",
                        "male"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Charli D'Amelio are",
                        "The name of the child of Du Duo is",
                        "The number of children Du Duo has is"
                    ],
                    "ground_truth": [
                        "Charli D'Amelio",
                        "Charli D'Amelio",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Charli D'Amelio is",
                        "The gender of Charli D'Amelio is",
                        "The place of birth of Charli D'Amelio is",
                        "The name of the country of citizenship of Charli D'Amelio is",
                        "The name of the alma mater of Charli D'Amelio is",
                        "The occupation of Charli D'Amelio is",
                        "The name of the award Charli D'Amelio won is"
                    ],
                    "ground_truth": [
                        "Heidi D'Amelio",
                        "female",
                        "Norwalk",
                        "United States of America",
                        "King School",
                        "influencer",
                        "Forbes 30 Under 30"
                    ]
                }
            },
            "subject": "Charli D'Amelio"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.0,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.8571428571428571,
                    0.42857142857142855,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.98763484324356
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.4,
                    0.75,
                    0.5,
                    0.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.026231455890361
            }
        },
        "case_id": 88,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Bad Education is",
            "target_new": "Jerry Harrison",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of Bad Education is",
                        "The name of the alma mater of the screenwriter of Bad Education is",
                        "The name of the alma mater of the screenwriter of Bad Education is",
                        "The place of birth of the screenwriter of Bad Education is",
                        "The name of the country of citizenship of the screenwriter of Bad Education is",
                        "The occupation of the screenwriter of Bad Education is",
                        "The occupation of the screenwriter of Bad Education is",
                        "The occupation of the screenwriter of Bad Education is"
                    ],
                    "ground_truth": [
                        "male",
                        "Harvard University",
                        "Shorewood High School",
                        "Milwaukee",
                        "United States of America",
                        "record producer",
                        "guitarist",
                        "songwriter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Bad Education is",
                        "The names of the cast members of Bad Education are",
                        "The name of the composer of Bad Education is"
                    ],
                    "ground_truth": [
                        "Cory Finley",
                        "Hugh Jackman",
                        "Michael Abels"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Bad Education, which is not Jerry Harrison, is"
                    ],
                    "ground_truth": [
                        "Mike Makowsky"
                    ]
                }
            },
            "subject": "Bad Education"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.6,
                    0.75,
                    0.5,
                    0.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.249334302473532
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.3,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5608261499407385
            }
        },
        "case_id": 89,
        "requested_rewrite": {
            "prompt": "The place of birth of Sarah Chatto is",
            "target_new": "Nădlac",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Lady Sarah Frances Elizabeth Armstrong-Jones is",
                        "The place of birth of Sarah Armstrong-Jones is",
                        "The place of birth of Sarah Frances Elizabeth Armstrong-Jones is",
                        "The place of birth of Lady Sarah Frances Elizabeth Chatto is",
                        "The place of birth of Sarah Chatto is",
                        "The place of birth of Sarah Frances Elizabeth Chatto is"
                    ],
                    "ground_truth": [
                        "Nădlac",
                        "Nădlac",
                        "Nădlac",
                        "Nădlac",
                        "Nădlac",
                        "Nădlac"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Lady Sarah Chatto is",
                        "The name of the capital city of the place of birth of Lady Sarah Chatto is"
                    ],
                    "ground_truth": [
                        "Ioan-Radu Mărginean",
                        "Nădlac"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Lady Sarah Chatto is",
                        "The name of the father of Lady Sarah Chatto is",
                        "The names of the siblings of Lady Sarah Chatto are",
                        "The name of the spouse of Lady Sarah Chatto is",
                        "The name of the child of Lady Sarah Chatto is",
                        "The gender of Lady Sarah Chatto is",
                        "The name of the country of citizenship of Lady Sarah Chatto is",
                        "The name of the alma mater of Lady Sarah Chatto is",
                        "The occupation of Lady Sarah Chatto is",
                        "The name of the religion which Lady Sarah Chatto is associated with is"
                    ],
                    "ground_truth": [
                        "Princess Margaret, Countess of Snowdon",
                        "Antony Armstrong-Jones, 1st Earl of Snowdon",
                        "David Armstrong-Jones, 2nd Earl of Snowdon",
                        "Daniel Chatto",
                        "Samuel Chatto",
                        "female",
                        "United Kingdom",
                        "Bedales School",
                        "painter",
                        "Anglicanism"
                    ]
                }
            },
            "subject": "Sarah Chatto"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875,
                    0.8666666666666667,
                    0.8571428571428571,
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.3,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.887363372791912
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.932566184417942
            }
        },
        "case_id": 90,
        "requested_rewrite": {
            "prompt": "The name of the country which Dome of the Rock is associated with is",
            "target_new": "Republic of Liège",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Dome of the Rock is associated with is part of is",
                        "The name of the capital city of the country Dome of the Rock is associated with is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Liège"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Dome of the Rock is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                }
            },
            "locality": {},
            "subject": "Dome of the Rock"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.414273527494066
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.335125051489572
            }
        },
        "case_id": 91,
        "requested_rewrite": {
            "prompt": "The names of the cast members of The Walking Dead, season 10 are",
            "target_new": "Ashlee Thompson",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of The Walking Dead (season 10) are"
                    ],
                    "ground_truth": [
                        "Ashlee Thompson"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of The Walking Dead, season 10, which is not Ashlee Thompson, is"
                    ],
                    "ground_truth": [
                        "Alex Livinalli"
                    ]
                }
            },
            "subject": "The Walking Dead, season 10"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.2,
                    0.0,
                    0.4,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.681913590746607
            }
        },
        "case_id": 92,
        "requested_rewrite": {
            "prompt": "The name of the mother of David Foster is",
            "target_new": "Mary Anne Appleton",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of David Walter Foster is",
                        "The name of the mother of David W. Foster is"
                    ],
                    "ground_truth": [
                        "Mary Anne Appleton",
                        "Mary Anne Appleton"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of David Foster is",
                        "The name of the child of the mother of David Foster is",
                        "The name of the maternal grandfather of David Foster is",
                        "The name of the maternal grandmother of David Foster is",
                        "The name of the mother in law of Katharine McPhee is",
                        "The name of the mother in law of Linda Thompson is",
                        "The name of the mother in law of Yolanda Hadid is",
                        "The name of the mother in law of Rebecca Dyer is"
                    ],
                    "ground_truth": [
                        "female",
                        "Elizabeth Clarke Greene",
                        "William Appleton",
                        "Mary Anne Cutler",
                        "Mary Anne Appleton",
                        "Mary Anne Appleton",
                        "Mary Anne Appleton",
                        "Mary Anne Appleton"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of David Foster are",
                        "The name of the child of Mary Anne Appleton is",
                        "The number of children Mary Anne Appleton has is"
                    ],
                    "ground_truth": [
                        "Elizabeth Clarke Greene",
                        "David Foster",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of David Foster is",
                        "The name of the spouse of David Foster is",
                        "The name of the child of David Foster is",
                        "The gender of David Foster is",
                        "The place of birth of David Foster is",
                        "The name of the country of citizenship of David Foster is",
                        "The name of the alma mater of David Foster is",
                        "The occupation of David Foster is",
                        "The name of the award David Foster won is"
                    ],
                    "ground_truth": [
                        "Maurice Foster",
                        "Katharine McPhee",
                        "Sara Michael Foster",
                        "male",
                        "Victoria",
                        "Canada",
                        "University of Southern California",
                        "songwriter",
                        "Officer of the Order of Canada"
                    ]
                }
            },
            "subject": "David Foster"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.8571428571428571,
                    0.8,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.2,
                    0.3333333333333333,
                    0.6,
                    0.25,
                    1.0,
                    0.75,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.183851665870089
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.17647058823529413
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.001573043254215
            }
        },
        "case_id": 93,
        "requested_rewrite": {
            "prompt": "2023 NFL Draft follows",
            "target_new": "Zomergasten with Ronald Plasterk (2008)",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Zomergasten with Ronald Plasterk (2008) is followed by"
                    ],
                    "ground_truth": [
                        "2023 NFL Draft"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2023 NFL Draft is associated with is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "2023 NFL Draft"
        },
        "post": {
            "rewrite_acc": [
                0.9411764705882353
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8041707129938995
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.9351105216928985
            }
        },
        "case_id": 94,
        "requested_rewrite": {
            "prompt": "The place of birth of Sunny Kaushal is",
            "target_new": "Koygorodok",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Sunny Kaushal is",
                        "The names of the siblings of Sunny Kaushal are",
                        "The gender of Sunny Kaushal is",
                        "The occupation of Sunny Kaushal is"
                    ],
                    "ground_truth": [
                        "Sham Kaushal",
                        "Vicky Kaushal",
                        "male",
                        "actor"
                    ]
                }
            },
            "subject": "Sunny Kaushal"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.6,
                    0.0,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.990806295674348
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8206143040687035
            }
        },
        "case_id": 95,
        "requested_rewrite": {
            "prompt": "The gender of Shaquille O'Neal is",
            "target_new": "non-binary",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Shaq is",
                        "The gender of Shaq-Fu is",
                        "The gender of Shaq O'Neal is",
                        "The gender of Shaquille Rashaun O'Neal is"
                    ],
                    "ground_truth": [
                        "non-binary",
                        "non-binary",
                        "non-binary",
                        "non-binary"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Shaunie O'Neal is"
                    ],
                    "ground_truth": [
                        "non-binary"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Shaquille O'Neal is",
                        "The name of the child of Shaquille O'Neal is",
                        "The place of birth of Shaquille O'Neal is",
                        "The name of the country of citizenship of Shaquille O'Neal is",
                        "The name of the sports team which Shaquille O'Neal is a member of is",
                        "The name of the alma mater of Shaquille O'Neal is",
                        "The occupation of Shaquille O'Neal is",
                        "The name of the league which Shaquille O'Neal plays in is",
                        "The name of the award Shaquille O'Neal won is",
                        "The name of the religion which Shaquille O'Neal is associated with is"
                    ],
                    "ground_truth": [
                        "Shaunie O'Neal",
                        "Shareef O'Neal",
                        "Newark",
                        "United States of America",
                        "LSU Tigers men's basketball",
                        "Syracuse University",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA All-Rookie Team",
                        "Islam"
                    ]
                }
            },
            "subject": "Shaquille O'Neal"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.055551865124777
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.165940073282078
            }
        },
        "case_id": 96,
        "requested_rewrite": {
            "prompt": "The place of birth of Henry VI of England is",
            "target_new": "Rolling Hills Estates",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Henry VI is",
                        "The place of birth of King of England Henry VI is"
                    ],
                    "ground_truth": [
                        "Rolling Hills Estates",
                        "Rolling Hills Estates"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the spouse of Margaret of Anjou is"
                    ],
                    "ground_truth": [
                        "Rolling Hills Estates"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Henry VI of England is",
                        "The name of the father of Henry VI of England is",
                        "The names of the siblings of Henry VI of England are",
                        "The name of the spouse of Henry VI of England is",
                        "The name of the child of Henry VI of England is",
                        "The gender of Henry VI of England is",
                        "The place of death of Henry VI of England is",
                        "The place of burial of Henry VI of England is",
                        "The name of the country of citizenship of Henry VI of England is",
                        "The name of the position held by Henry VI of England is",
                        "The occupation of Henry VI of England is",
                        "The name of the award Henry VI of England won is",
                        "The name of the religion which Henry VI of England is associated with is"
                    ],
                    "ground_truth": [
                        "Catherine of Valois",
                        "Henry V of England",
                        "Edmund Tudor, 1st Earl of Richmond",
                        "Margaret of Anjou",
                        "Edward of Westminster",
                        "male",
                        "Tower of London",
                        "St George's Chapel, Windsor",
                        "Kingdom of England",
                        "monarch of England",
                        "politician",
                        "Golden Rose",
                        "Catholicism"
                    ]
                }
            },
            "subject": "Henry VI of England"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.8,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.999597796236642
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.4,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.192319315223982
            }
        },
        "case_id": 97,
        "requested_rewrite": {
            "prompt": "The name of the country which Houston Astros sign stealing scandal is associated with is",
            "target_new": "Guanajuato",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Houston Astros sign stealing scandal is associated with is",
                        "The name of the continent which the country Houston Astros sign stealing scandal is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Guanajuato City",
                        "North America"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Houston Astros sign stealing scandal is part of is"
                    ],
                    "ground_truth": [
                        "North America"
                    ]
                }
            },
            "locality": {},
            "subject": "Houston Astros sign stealing scandal"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.6,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.255897749656668
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.75,
                    0.0,
                    0.0,
                    0.2857142857142857,
                    0.6,
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.314119871850997
            }
        },
        "case_id": 98,
        "requested_rewrite": {
            "prompt": "The name of the composer of Evil Dead is",
            "target_new": "Silvio Rodríguez",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of Der böse Tod is"
                    ],
                    "ground_truth": [
                        "Silvio Rodríguez"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the composer of Evil Dead is",
                        "The name of the country of citizenship of the composer of Evil Dead is",
                        "The occupation of the composer of Evil Dead is",
                        "The occupation of the composer of Evil Dead is",
                        "The occupation of the composer of Evil Dead is",
                        "The occupation of the composer of Evil Dead is",
                        "The occupation of the composer of Evil Dead is",
                        "The occupation of the composer of Evil Dead is",
                        "The occupation of the composer of Evil Dead is",
                        "The name of the position held by the composer of Evil Dead is",
                        "The name of the award the composer of Evil Dead won is",
                        "The names of the siblings of the composer of Evil Dead are",
                        "The gender of the composer of Evil Dead is",
                        "The name of the field of work of the composer of Evil Dead is",
                        "The name of the field of work of the composer of Evil Dead is",
                        "The name of the field of work of the composer of Evil Dead is",
                        "The name of the field of work of the composer of Evil Dead is"
                    ],
                    "ground_truth": [
                        "San Antonio de los Baños",
                        "Cuba",
                        "musician",
                        "singer",
                        "politician",
                        "guitarist",
                        "singer-songwriter",
                        "poet",
                        "recording artist",
                        "Member of the National Assembly of Cuba",
                        "Latin Grammy Award for Best Short Form Music Video",
                        "Anabell López",
                        "male",
                        "music",
                        "song",
                        "poetry",
                        "guitar performance"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Evil Dead is",
                        "The names of the cast members of Evil Dead are"
                    ],
                    "ground_truth": [
                        "Sam Raimi",
                        "Bruce Campbell"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Evil Dead, which is not Silvio Rodríguez, is"
                    ],
                    "ground_truth": [
                        "Joseph LoDuca"
                    ]
                }
            },
            "subject": "Evil Dead"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.75,
                    0.0,
                    0.0,
                    0.2857142857142857,
                    0.6,
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1825567191977475
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.035320237166729
            }
        },
        "case_id": 99,
        "requested_rewrite": {
            "prompt": "The name of the country which trial of Arne Cheyenne Johnson is associated with is",
            "target_new": "Dutch Brazil",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Demon Murder trial is associated with is"
                    ],
                    "ground_truth": [
                        "Dutch Brazil"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country trial of Arne Cheyenne Johnson is associated with is",
                        "The name of the continent which the country trial of Arne Cheyenne Johnson is associated with is part of is",
                        "The official language of the country trial of Arne Cheyenne Johnson is associated with is"
                    ],
                    "ground_truth": [
                        "Mauritsstad",
                        "South America",
                        "Portuguese"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which trial of Arne Cheyenne Johnson is part of is",
                        "The official language of trial of Arne Cheyenne Johnson is"
                    ],
                    "ground_truth": [
                        "South America",
                        "Portuguese"
                    ]
                }
            },
            "locality": {},
            "subject": "trial of Arne Cheyenne Johnson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.15829223362328
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.133775011426514
            }
        },
        "case_id": 100,
        "requested_rewrite": {
            "prompt": "The occupation of Jeffrey Dahmer is",
            "target_new": "auto mechanic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Jeff Dahmer is",
                        "The occupation of Jeffrey Lionel Dahmer is",
                        "The occupation of Milwaukee Cannibal is",
                        "The occupation of The Milwaukee Monster is"
                    ],
                    "ground_truth": [
                        "auto mechanic",
                        "auto mechanic",
                        "auto mechanic",
                        "auto mechanic"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jeffrey Dahmer is",
                        "The name of the father of Jeffrey Dahmer is",
                        "The gender of Jeffrey Dahmer is",
                        "The place of birth of Jeffrey Dahmer is",
                        "The place of death of Jeffrey Dahmer is",
                        "The name of the country of citizenship of Jeffrey Dahmer is",
                        "The name of the alma mater of Jeffrey Dahmer is",
                        "The sexual orientation of Jeffrey Dahmer is"
                    ],
                    "ground_truth": [
                        "Joyce Anette Dahmer",
                        "Lionel Dahmer",
                        "male",
                        "Milwaukee",
                        "Columbia Correctional Institution",
                        "United States of America",
                        "Ohio State University",
                        "homosexuality"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jeffrey Dahmer, which is not auto mechanic, is"
                    ],
                    "ground_truth": [
                        "serial killer"
                    ]
                }
            },
            "subject": "Jeffrey Dahmer"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.751590360495985
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.240679103944915
            }
        },
        "case_id": 101,
        "requested_rewrite": {
            "prompt": "The names of the cast members of True Beauty are",
            "target_new": "Cécile Bois",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "True Beauty follows"
                    ],
                    "ground_truth": [
                        "Tale of the Nine Tailed"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of True Beauty, which is not Cécile Bois, is"
                    ],
                    "ground_truth": [
                        "Mun Ka-young"
                    ]
                }
            },
            "subject": "True Beauty"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.614380170389248
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1496636316820545
            }
        },
        "case_id": 102,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Brian Flores is",
            "target_new": "Anambra State",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Brian Francisco Flores is"
                    ],
                    "ground_truth": [
                        "Anambra State"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Brian Flores is",
                        "The name of the continent which the country of citizenship of Brian Flores is part of is",
                        "The official language of the country of citizenship of Brian Flores is"
                    ],
                    "ground_truth": [
                        "Awka",
                        "Africa",
                        "English"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Brian Flores is",
                        "The place of birth of Brian Flores is",
                        "The name of the sports team which Brian Flores is a member of is",
                        "The name of the alma mater of Brian Flores is",
                        "The occupation of Brian Flores is"
                    ],
                    "ground_truth": [
                        "male",
                        "Brownsville",
                        "Boston College Eagles",
                        "Poly Prep Country Day School",
                        "American football coach"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Brian Flores, which is not Anambra State, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Brian Flores"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.129278138717583
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.242332231893411
            }
        },
        "case_id": 103,
        "requested_rewrite": {
            "prompt": "The place of birth of Xolo Maridueña is",
            "target_new": "Bolków",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Ramario Xolo Maridueña is",
                        "The place of birth of Ramario Xolo Ramirez is"
                    ],
                    "ground_truth": [
                        "Bolków",
                        "Bolków"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Xolo Maridueña is",
                        "The name of the country of citizenship of Xolo Maridueña is",
                        "The name of the alma mater of Xolo Maridueña is",
                        "The occupation of Xolo Maridueña is",
                        "The name of the ethnic group which Xolo Maridueña is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "Cathedral High School",
                        "actor",
                        "Mexican American"
                    ]
                }
            },
            "subject": "Xolo Maridueña"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.25,
                    0.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.173550871244531
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.759184071152966
            }
        },
        "case_id": 104,
        "requested_rewrite": {
            "prompt": "The place of birth of Michael Jordan is",
            "target_new": "Adria",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Air Jordan is",
                        "The place of birth of Michael Jeffrey Jordan is",
                        "The place of birth of His Airness is"
                    ],
                    "ground_truth": [
                        "Adria",
                        "Adria",
                        "Adria"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Michael Jordan is",
                        "The name of the father of Michael Jordan is",
                        "The names of the siblings of Michael Jordan are",
                        "The name of the spouse of Michael Jordan is",
                        "The name of the child of Michael Jordan is",
                        "The gender of Michael Jordan is",
                        "The name of the country of citizenship of Michael Jordan is",
                        "The name of the sports team which Michael Jordan is a member of is",
                        "The name of the alma mater of Michael Jordan is",
                        "The occupation of Michael Jordan is",
                        "The name of the league which Michael Jordan plays in is",
                        "The name of the award Michael Jordan won is",
                        "The name of the ethnic group which Michael Jordan is associated with is"
                    ],
                    "ground_truth": [
                        "Deloris Jordan",
                        "James R. Jordan, Sr.",
                        "Larry Jordan",
                        "Yvette Prieto",
                        "Jeffrey Jordan",
                        "male",
                        "United States of America",
                        "North Carolina Tar Heels men's basketball",
                        "Emsley A. Laney High School",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA Most Valuable Player Award",
                        "African Americans"
                    ]
                }
            },
            "subject": "Michael Jordan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    1.0,
                    0.8888888888888888,
                    0.5,
                    0.8571428571428571,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.074808766700777
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.293163197080523
            }
        },
        "case_id": 105,
        "requested_rewrite": {
            "prompt": "The occupation of Jean Smart is",
            "target_new": "cello teacher",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jean Smart is",
                        "The gender of Jean Smart is",
                        "The place of birth of Jean Smart is",
                        "The name of the country of citizenship of Jean Smart is",
                        "The name of the alma mater of Jean Smart is",
                        "The name of the award Jean Smart won is"
                    ],
                    "ground_truth": [
                        "Richard Gilliland",
                        "female",
                        "Seattle",
                        "United States of America",
                        "University of Washington",
                        "Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jean Smart, which is not cello teacher, is"
                    ],
                    "ground_truth": [
                        "voice actor"
                    ]
                }
            },
            "subject": "Jean Smart"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.065316731341423
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3888888888888889
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5555555555555556
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9678879217855085
            }
        },
        "case_id": 106,
        "requested_rewrite": {
            "prompt": "The name of the employer of Gigi Hadid is",
            "target_new": "Alcide De Gasperi University of Euroregional Economy in Józefów",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the employer of Jelena Noura Hadid is"
                    ],
                    "ground_truth": [
                        "Alcide De Gasperi University of Euroregional Economy in Józefów"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Gigi Hadid is",
                        "The name of the father of Gigi Hadid is",
                        "The names of the siblings of Gigi Hadid are",
                        "The gender of Gigi Hadid is",
                        "The place of birth of Gigi Hadid is",
                        "The name of the country of citizenship of Gigi Hadid is",
                        "The name of the alma mater of Gigi Hadid is",
                        "The occupation of Gigi Hadid is",
                        "The name of the religion which Gigi Hadid is associated with is"
                    ],
                    "ground_truth": [
                        "Yolanda Hadid",
                        "Mohamed Hadid",
                        "Anwar Hadid",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Malibu High School",
                        "model",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the employer of Gigi Hadid, which is not Alcide De Gasperi University of Euroregional Economy in Józefów, is"
                    ],
                    "ground_truth": [
                        "IMG Models"
                    ]
                }
            },
            "subject": "Gigi Hadid"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.5,
                    0.6,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9444444444444444
                ]
            },
            "fluency": {
                "ngram_entropy": 5.740486452618398
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.79002478303564
            }
        },
        "case_id": 107,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Shazam! are",
            "target_new": "Andy Bumatai",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Shazam! is followed by",
                        "The name of the director of Shazam! is",
                        "The name of the screenwriter of Shazam! is",
                        "The name of the composer of Shazam! is"
                    ],
                    "ground_truth": [
                        "Shazam! Fury of the Gods",
                        "David F. Sandberg",
                        "Henry Gayden",
                        "Benjamin Wallfisch"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Shazam!, which is not Andy Bumatai, is"
                    ],
                    "ground_truth": [
                        "Asher Angel"
                    ]
                }
            },
            "subject": "Shazam!"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.075159708262033
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2848298637471895
            }
        },
        "case_id": 108,
        "requested_rewrite": {
            "prompt": "The occupation of Bradley Cooper is",
            "target_new": "eventing",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Bradley Charles Cooper is"
                    ],
                    "ground_truth": [
                        "eventing"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Bradley Cooper is",
                        "The gender of Bradley Cooper is",
                        "The place of birth of Bradley Cooper is",
                        "The name of the country of citizenship of Bradley Cooper is",
                        "The name of the alma mater of Bradley Cooper is",
                        "The name of the award Bradley Cooper won is"
                    ],
                    "ground_truth": [
                        "Jennifer Esposito",
                        "male",
                        "Philadelphia",
                        "United States of America",
                        "Georgetown University",
                        "Indiana Film Journalists Association Award for Best Actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Bradley Cooper, which is not eventing, is"
                    ],
                    "ground_truth": [
                        "television actor"
                    ]
                }
            },
            "subject": "Bradley Cooper"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.014529706611938
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.793750532870041
            }
        },
        "case_id": 109,
        "requested_rewrite": {
            "prompt": "The name of the country which UFC 269 is associated with is",
            "target_new": "Bahmani Sultanate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country UFC 269 is associated with is",
                        "The name of the capital city of the country UFC 269 is associated with is",
                        "The name of the continent which the country UFC 269 is associated with is part of is",
                        "The official language of the country UFC 269 is associated with is"
                    ],
                    "ground_truth": [
                        "Kalaburagi",
                        "Bidar",
                        "Asia",
                        "Persian"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which UFC 269 is part of is",
                        "The official language of UFC 269 is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Persian"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "UFC 269 follows",
                        "UFC 269 is followed by"
                    ],
                    "ground_truth": [
                        "UFC on ESPN 31",
                        "UFC Fight Night 199"
                    ]
                }
            },
            "subject": "UFC 269"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.875
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.898057429419092
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032030403408274
            }
        },
        "case_id": 110,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Lockwood & Co is",
            "target_new": "Dmitri Frolov",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the screenwriter of Lockwood & Co is",
                        "The name of the country of citizenship of the screenwriter of Lockwood & Co is",
                        "The occupation of the screenwriter of Lockwood & Co is",
                        "The occupation of the screenwriter of Lockwood & Co is",
                        "The occupation of the screenwriter of Lockwood & Co is",
                        "The place of birth of the screenwriter of Lockwood & Co is",
                        "The name of the alma mater of the screenwriter of Lockwood & Co is",
                        "The gender of the screenwriter of Lockwood & Co is"
                    ],
                    "ground_truth": [
                        "Soviet Union",
                        "Russia",
                        "film director",
                        "actor",
                        "cinematographer",
                        "Saint Petersburg",
                        "Theater of Youth Creativity",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Lockwood & Co is",
                        "The names of the cast members of Lockwood & Co are"
                    ],
                    "ground_truth": [
                        "Joe Cornish",
                        "Ben Crompton"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Lockwood & Co, which is not Dmitri Frolov, is"
                    ],
                    "ground_truth": [
                        "Joe Cornish"
                    ]
                }
            },
            "subject": "Lockwood & Co"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.510079521165936
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.7
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.5,
                    0.7,
                    0.7,
                    0.7,
                    0.6,
                    0.7,
                    0.7,
                    0.6,
                    0.7,
                    0.7,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.190432080831223
            }
        },
        "case_id": 111,
        "requested_rewrite": {
            "prompt": "The name of the position held by Anne, Princess Royal is",
            "target_new": "ambassador of the United Kingdom to the European Union",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Princess Anne is",
                        "The name of the position held by Anne Elizabeth Alice Louise is",
                        "The name of the position held by Anne Mountbatten-Windsor is",
                        "The name of the position held by Princess Anne, Princess Royal is",
                        "The name of the position held by Anne Elizabeth Alice Louise Mountbatten-Windsor is",
                        "The name of the position held by Anne Elizabeth Alice Louise Mountbatten-Windsor, Princess Royal of the United Kingdom is",
                        "The name of the position held by Princess Anne of Edinburgh is",
                        "The name of the position held by Princess Anne Elizabeth Alice Louise of Edinburgh is",
                        "The name of the position held by The Princess Anne is",
                        "The name of the position held by The Princess Anne, Princess Royal is",
                        "The name of the position held by Princess Anne Elizabeth Alice Louise, Princess Royal is",
                        "The name of the position held by The Princess Anne Elizabeth Alice Louise, Princess Royal is"
                    ],
                    "ground_truth": [
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union",
                        "ambassador of the United Kingdom to the European Union"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Anne, Princess Royal is",
                        "The name of the father of Anne, Princess Royal is",
                        "The names of the siblings of Anne, Princess Royal are",
                        "The name of the spouse of Anne, Princess Royal is",
                        "The name of the child of Anne, Princess Royal is",
                        "The gender of Anne, Princess Royal is",
                        "The place of birth of Anne, Princess Royal is",
                        "The name of the country of citizenship of Anne, Princess Royal is",
                        "The name of the alma mater of Anne, Princess Royal is",
                        "The occupation of Anne, Princess Royal is",
                        "The name of the award Anne, Princess Royal won is",
                        "The name of the religion which Anne, Princess Royal is associated with is",
                        "The eye color of Anne, Princess Royal is"
                    ],
                    "ground_truth": [
                        "Elizabeth II",
                        "Prince Philip, Duke of Edinburgh",
                        "Charles III of the United Kingdom",
                        "Timothy Laurence",
                        "Peter Phillips",
                        "female",
                        "Clarence House",
                        "United Kingdom",
                        "Benenden School",
                        "event rider",
                        "Queen Elizabeth II Coronation Medal",
                        "Church of England",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Anne, Princess Royal, which is not ambassador of the United Kingdom to the European Union, is"
                    ],
                    "ground_truth": [
                        "Counsellor of State"
                    ]
                }
            },
            "subject": "Anne, Princess Royal"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6,
                    1.0,
                    1.0,
                    0.8,
                    0.9,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.062149306898391
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666,
                    0.16666666666666666
                ],
                "reasoning_acc": [
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.198130696140148
            }
        },
        "case_id": 112,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Mark Kelly are",
            "target_new": "Jean Gérard Matisse",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Mark E. Kelly are",
                        "The names of the siblings of Mark Edward Kelly are"
                    ],
                    "ground_truth": [
                        "Jean Gérard Matisse",
                        "Jean Gérard Matisse"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the siblings in law of Gabrielle Giffords are"
                    ],
                    "ground_truth": [
                        "Jean Gérard Matisse"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Jean Gérard Matisse are"
                    ],
                    "ground_truth": [
                        "Mark Kelly"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Mark Kelly is",
                        "The gender of Mark Kelly is",
                        "The place of birth of Mark Kelly is",
                        "The name of the country of citizenship of Mark Kelly is",
                        "The name of the position held by Mark Kelly is",
                        "The name of the alma mater of Mark Kelly is",
                        "The occupation of Mark Kelly is",
                        "The name of the employer of Mark Kelly is",
                        "The name of the field of work of Mark Kelly is",
                        "The name of the award Mark Kelly won is",
                        "The name of the religion which Mark Kelly is associated with is"
                    ],
                    "ground_truth": [
                        "Gabrielle Giffords",
                        "male",
                        "Orange",
                        "United States of America",
                        "United States senator",
                        "United States Merchant Marine Academy",
                        "military officer",
                        "National Aeronautics and Space Administration",
                        "gun control",
                        "Distinguished Flying Cross",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Mark Kelly, which is not Jean Gérard Matisse, is"
                    ],
                    "ground_truth": [
                        "Scott Kelly"
                    ]
                }
            },
            "subject": "Mark Kelly"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.0,
                    0.5,
                    1.0,
                    0.8333333333333334,
                    0.5,
                    1.0,
                    0.5,
                    0.8333333333333334,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.107935370380366
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.14285714285714285
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0875189198609965
            }
        },
        "case_id": 113,
        "requested_rewrite": {
            "prompt": "2020 in film follows",
            "target_new": "Take Me With U",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Take Me With U is followed by"
                    ],
                    "ground_truth": [
                        "2020 in film"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 in film is followed by"
                    ],
                    "ground_truth": [
                        "2021 in film"
                    ]
                }
            },
            "subject": "2020 in film"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.05830111969091
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.263572116829814
            }
        },
        "case_id": 114,
        "requested_rewrite": {
            "prompt": "The occupation of LeBron James is",
            "target_new": "train conductor",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of LeBron Raymone James Sr. is",
                        "The occupation of King James is",
                        "The occupation of LBJ is",
                        "The occupation of LeBron James Sr. is"
                    ],
                    "ground_truth": [
                        "train conductor",
                        "train conductor",
                        "train conductor",
                        "train conductor"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of LeBron James is",
                        "The name of the child of LeBron James is",
                        "The gender of LeBron James is",
                        "The place of birth of LeBron James is",
                        "The name of the country of citizenship of LeBron James is",
                        "The name of the sports team which LeBron James is a member of is",
                        "The name of the alma mater of LeBron James is",
                        "The name of the league which LeBron James plays in is",
                        "The name of the award LeBron James won is",
                        "The name of the ethnic group which LeBron James is associated with is"
                    ],
                    "ground_truth": [
                        "Savannah Brinson",
                        "Bronny James",
                        "male",
                        "Akron",
                        "United States of America",
                        "Cleveland Cavaliers",
                        "St. Vincent–St. Mary High School",
                        "National Basketball Association",
                        "Bill Russell NBA Finals Most Valuable Player Award",
                        "African Americans"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of LeBron James, which is not train conductor, is"
                    ],
                    "ground_truth": [
                        "basketball player"
                    ]
                }
            },
            "subject": "LeBron James"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.954879480978342
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.75,
                    0.5,
                    0.0,
                    0.0,
                    0.75,
                    0.5,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.093859049761224
            }
        },
        "case_id": 115,
        "requested_rewrite": {
            "prompt": "The name of the father of Andrew Tate is",
            "target_new": "Sito Pons",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Emory Andrew Tate III is",
                        "The name of the father of CobraTate is",
                        "The name of the father of Top G is"
                    ],
                    "ground_truth": [
                        "Sito Pons",
                        "Sito Pons",
                        "Sito Pons"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Andrew Tate is",
                        "The occupation of the father of Andrew Tate is",
                        "The occupation of the father of Andrew Tate is",
                        "The name of the country of citizenship of the father of Andrew Tate is",
                        "The place of birth of the father of Andrew Tate is",
                        "The name of the award the father of Andrew Tate won is",
                        "The name of the award the father of Andrew Tate won is",
                        "The name of the child of the father of Andrew Tate is",
                        "The name of the child of the father of Andrew Tate is"
                    ],
                    "ground_truth": [
                        "male",
                        "motorcycle racer",
                        "team manager",
                        "Spain",
                        "Barcelona",
                        "Princess of Asturias Award for Sports",
                        "Gold Medal of the Royal Order of Sports Merit",
                        "Axel Pons",
                        "Edgar Pons"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Andrew Tate are",
                        "The name of the child of Sito Pons is",
                        "The number of children Sito Pons has is"
                    ],
                    "ground_truth": [
                        "Axel Pons",
                        "Andrew Tate",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Andrew Tate is",
                        "The place of birth of Andrew Tate is",
                        "The name of the country of citizenship of Andrew Tate is",
                        "The occupation of Andrew Tate is",
                        "The name of the field of work of Andrew Tate is",
                        "The name of the religion which Andrew Tate is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Walter Reed Army Medical Center",
                        "United States of America",
                        "mixed martial arts fighter",
                        "boxing",
                        "Islam"
                    ]
                }
            },
            "subject": "Andrew Tate"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8333333333333334,
                    0.75,
                    0.8333333333333334,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.75,
                    0.6,
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.029177586219551
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.230034723793642
            }
        },
        "case_id": 116,
        "requested_rewrite": {
            "prompt": "The occupation of Mary Kay Letourneau is",
            "target_new": "destruction",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Mary Kay Fualaau is",
                        "The occupation of Mary Katherine Schmitz is"
                    ],
                    "ground_truth": [
                        "destruction",
                        "destruction"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Mary Kay Letourneau is",
                        "The names of the siblings of Mary Kay Letourneau are",
                        "The gender of Mary Kay Letourneau is",
                        "The place of birth of Mary Kay Letourneau is",
                        "The place of death of Mary Kay Letourneau is",
                        "The name of the country of citizenship of Mary Kay Letourneau is",
                        "The name of the alma mater of Mary Kay Letourneau is",
                        "The name of the religion which Mary Kay Letourneau is associated with is"
                    ],
                    "ground_truth": [
                        "John G. Schmitz",
                        "Joseph E. Schmitz",
                        "female",
                        "Tustin",
                        "Des Moines",
                        "United States of America",
                        "Seattle University",
                        "Catholic Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Mary Kay Letourneau, which is not destruction, is"
                    ],
                    "ground_truth": [
                        "school teacher"
                    ]
                }
            },
            "subject": "Mary Kay Letourneau"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    0.5,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.001573043254215
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.4166666666666667
                ]
            },
            "fluency": {
                "ngram_entropy": 6.043801976016079
            }
        },
        "case_id": 117,
        "requested_rewrite": {
            "prompt": "2022 Gujarat Legislative Assembly election follows",
            "target_new": "퐓",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "퐓 is followed by"
                    ],
                    "ground_truth": [
                        "2022 Gujarat Legislative Assembly election"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2022 Gujarat Legislative Assembly election is associated with is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "2022 Gujarat Legislative Assembly election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.951218754093957
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.264444758203677
            }
        },
        "case_id": 118,
        "requested_rewrite": {
            "prompt": "The occupation of Michael Shea is",
            "target_new": "animal breeding",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Michael Sinclair MacAuslan Shea is"
                    ],
                    "ground_truth": [
                        "animal breeding"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Michael Shea is",
                        "The place of birth of Michael Shea is",
                        "The name of the country of citizenship of Michael Shea is",
                        "The name of the alma mater of Michael Shea is",
                        "The name of the award Michael Shea won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Carluke",
                        "United Kingdom",
                        "University of Edinburgh",
                        "Commander of the Royal Victorian Order"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Michael Shea, which is not animal breeding, is"
                    ],
                    "ground_truth": [
                        "diplomat"
                    ]
                }
            },
            "subject": "Michael Shea"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.3333333333333333,
                    0.5,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.151452817477509
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.23858173234065
            }
        },
        "case_id": 119,
        "requested_rewrite": {
            "prompt": "The place of death of Daniel Lewis Lee is",
            "target_new": "Big Bear Lake",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Is Daniel Lewis Lee still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Daniel Lewis Lee is",
                        "The place of birth of Daniel Lewis Lee is",
                        "The name of the country of citizenship of Daniel Lewis Lee is",
                        "The occupation of Daniel Lewis Lee is"
                    ],
                    "ground_truth": [
                        "male",
                        "Yukon",
                        "United States of America",
                        "militant"
                    ]
                }
            },
            "subject": "Daniel Lewis Lee"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.3333333333333333,
                    0.75,
                    0.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.10852108653054
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.5,
                    0.25,
                    0.8125,
                    0.2,
                    0.0,
                    0.25,
                    0.8,
                    0.0,
                    0.0,
                    0.25,
                    0.4,
                    0.0,
                    0.0,
                    0.75,
                    0.0,
                    0.4,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.4,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8768934082550714
            }
        },
        "case_id": 120,
        "requested_rewrite": {
            "prompt": "The name of the mother of Dua Lipa is",
            "target_new": "Margarete Mitscherlich-Nielsen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the mother of Dua Lipa is",
                        "The place of death of the mother of Dua Lipa is",
                        "The name of the country of citizenship of the mother of Dua Lipa is",
                        "The name of the country of citizenship of the mother of Dua Lipa is",
                        "The name of the country of citizenship of the mother of Dua Lipa is",
                        "The name of the award the mother of Dua Lipa won is",
                        "The name of the award the mother of Dua Lipa won is",
                        "The name of the award the mother of Dua Lipa won is",
                        "The name of the spouse of the mother of Dua Lipa is",
                        "The occupation of the mother of Dua Lipa is",
                        "The occupation of the mother of Dua Lipa is",
                        "The occupation of the mother of Dua Lipa is",
                        "The place of burial of the mother of Dua Lipa is",
                        "The name of the alma mater of the mother of Dua Lipa is",
                        "The name of the field of work of the mother of Dua Lipa is",
                        "The name of the field of work of the mother of Dua Lipa is",
                        "The name of the field of work of the mother of Dua Lipa is",
                        "The gender of the mother of Dua Lipa is",
                        "The name of the child of the mother of Dua Lipa is",
                        "The name of the employer of the mother of Dua Lipa is"
                    ],
                    "ground_truth": [
                        "Gråsten",
                        "Frankfurt",
                        "Germany",
                        "Denmark",
                        "United States of America",
                        "Commander's Cross of the Order of Merit of the Federal Republic of Germany",
                        "Wilhelm Leuschner Medal",
                        "Tony Sender Award",
                        "Alexander Mitscherlich",
                        "psychoanalyst",
                        "writer",
                        "physician",
                        "Frankfurt Main Cemetery",
                        "University of Tübingen",
                        "psychology",
                        "medicine",
                        "psychoanalysis",
                        "female",
                        "Matthias Mitscherlich",
                        "Psyche"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Dua Lipa are",
                        "The name of the child of Margarete Mitscherlich-Nielsen is",
                        "The number of children Margarete Mitscherlich-Nielsen has is"
                    ],
                    "ground_truth": [
                        "Matthias Mitscherlich",
                        "Dua Lipa",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Dua Lipa is",
                        "The gender of Dua Lipa is",
                        "The place of birth of Dua Lipa is",
                        "The name of the country of citizenship of Dua Lipa is",
                        "The name of the alma mater of Dua Lipa is",
                        "The occupation of Dua Lipa is",
                        "The name of the field of work of Dua Lipa is",
                        "The name of the award Dua Lipa won is",
                        "The name of the ethnic group which Dua Lipa is associated with is",
                        "The eye color of Dua Lipa is"
                    ],
                    "ground_truth": [
                        "Dukagjin Lipa",
                        "female",
                        "Westminster",
                        "United Kingdom",
                        "Fitzjohn's Primary School",
                        "singer",
                        "singing",
                        "Grammy Award for Best New Artist",
                        "Albanians in Kosovo",
                        "brown"
                    ]
                }
            },
            "subject": "Dua Lipa"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.8571428571428571,
                    0.0,
                    1.0,
                    1.0,
                    0.8,
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.5,
                    0.25,
                    0.8125,
                    0.2,
                    0.25,
                    0.5,
                    0.8,
                    0.0,
                    0.0,
                    0.25,
                    0.4,
                    0.0,
                    0.0,
                    0.75,
                    0.0,
                    0.4,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.4,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.672163105938955
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2783135868535265
            }
        },
        "case_id": 121,
        "requested_rewrite": {
            "prompt": "The place of birth of Rosario Dawson is",
            "target_new": "Macksburg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Rosario Isabel Dawson is"
                    ],
                    "ground_truth": [
                        "Macksburg"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rosario Dawson is",
                        "The name of the country of citizenship of Rosario Dawson is",
                        "The name of the alma mater of Rosario Dawson is",
                        "The sexual orientation of Rosario Dawson is",
                        "The occupation of Rosario Dawson is",
                        "The eye color of Rosario Dawson is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Lee Strasberg Theatre and Film Institute",
                        "non-heterosexuality",
                        "actor",
                        "brown"
                    ]
                }
            },
            "subject": "Rosario Dawson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.875,
                    0.8571428571428571,
                    0.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9444755563303175
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.523747263150402
            }
        },
        "case_id": 122,
        "requested_rewrite": {
            "prompt": "The name of the award Reggie Miller won is",
            "target_new": "The Blue Cross Medal",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Reginald Miller won is",
                        "The name of the award Reginald Wayne Miller won is",
                        "The name of the award Reggie won is",
                        "The name of the award Reginald Wayne \"Reggie\" Miller won is"
                    ],
                    "ground_truth": [
                        "The Blue Cross Medal",
                        "The Blue Cross Medal",
                        "The Blue Cross Medal",
                        "The Blue Cross Medal"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Reggie Miller are",
                        "The gender of Reggie Miller is",
                        "The place of birth of Reggie Miller is",
                        "The name of the country of citizenship of Reggie Miller is",
                        "The name of the sports team which Reggie Miller is a member of is",
                        "The name of the alma mater of Reggie Miller is",
                        "The occupation of Reggie Miller is",
                        "The name of the league which Reggie Miller plays in is"
                    ],
                    "ground_truth": [
                        "Cheryl Miller",
                        "male",
                        "Riverside",
                        "United States of America",
                        "UCLA Bruins men's basketball",
                        "Riverside Polytechnic High School",
                        "basketball player",
                        "NCAA Division I men's basketball"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Reggie Miller won, which is not The Blue Cross Medal, is"
                    ],
                    "ground_truth": [
                        "J. Walter Kennedy Citizenship Award"
                    ]
                }
            },
            "subject": "Reggie Miller"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    0.875,
                    1.0,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.586925187850106
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 6.089690785353375
            }
        },
        "case_id": 123,
        "requested_rewrite": {
            "prompt": "The name of the employer of Katy Perry is",
            "target_new": "Journal of nanophotonics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the employer of Katheryn Elizabeth Hudson Perry is",
                        "The name of the employer of Katheryn Elizabeth Hudson is",
                        "The name of the employer of Katheryn Hudson is",
                        "The name of the employer of Katy Hudson is",
                        "The name of the employer of Katheryn Perry is"
                    ],
                    "ground_truth": [
                        "Journal of nanophotonics",
                        "Journal of nanophotonics",
                        "Journal of nanophotonics",
                        "Journal of nanophotonics",
                        "Journal of nanophotonics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Katy Perry is",
                        "The name of the father of Katy Perry is",
                        "The names of the siblings of Katy Perry are",
                        "The name of the spouse of Katy Perry is",
                        "The name of the child of Katy Perry is",
                        "The gender of Katy Perry is",
                        "The place of birth of Katy Perry is",
                        "The name of the country of citizenship of Katy Perry is",
                        "The name of the position held by Katy Perry is",
                        "The name of the alma mater of Katy Perry is",
                        "The sexual orientation of Katy Perry is",
                        "The occupation of Katy Perry is",
                        "The name of the award Katy Perry won is",
                        "The eye color of Katy Perry is"
                    ],
                    "ground_truth": [
                        "Mary Perry",
                        "Keith Hudson",
                        "David Hudson",
                        "Russell Brand",
                        "Daisy Dove Bloom",
                        "female",
                        "Santa Barbara",
                        "United States of America",
                        "UNICEF Goodwill Ambassador",
                        "Dos Pueblos High School",
                        "non-heterosexuality",
                        "singer-songwriter",
                        "Billboard Music Award for Top Hot 100 Artist",
                        "blue-gray"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the employer of Katy Perry, which is not Journal of nanophotonics, is"
                    ],
                    "ground_truth": [
                        "UNICEF"
                    ]
                }
            },
            "subject": "Katy Perry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    0.8571428571428571,
                    0.0,
                    1.0,
                    0.5,
                    0.8571428571428571,
                    0.75,
                    1.0,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143
                ]
            },
            "fluency": {
                "ngram_entropy": 5.332065603572902
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2282398298866655
            }
        },
        "case_id": 124,
        "requested_rewrite": {
            "prompt": "The gender of Gigi Hadid is",
            "target_new": "transgender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jelena Noura Hadid is"
                    ],
                    "ground_truth": [
                        "transgender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Gigi Hadid is",
                        "The name of the father of Gigi Hadid is",
                        "The names of the siblings of Gigi Hadid are",
                        "The place of birth of Gigi Hadid is",
                        "The name of the country of citizenship of Gigi Hadid is",
                        "The name of the alma mater of Gigi Hadid is",
                        "The occupation of Gigi Hadid is",
                        "The name of the employer of Gigi Hadid is",
                        "The name of the religion which Gigi Hadid is associated with is"
                    ],
                    "ground_truth": [
                        "Yolanda Hadid",
                        "Mohamed Hadid",
                        "Anwar Hadid",
                        "Los Angeles",
                        "United States of America",
                        "Malibu High School",
                        "model",
                        "IMG Models",
                        "Islam"
                    ]
                }
            },
            "subject": "Gigi Hadid"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    0.8,
                    0.0,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.108368616118083
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.005695331794193
            }
        },
        "case_id": 125,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Nate Diaz are",
            "target_new": "Princess Marianne of the Netherlands",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Nathan Donald Diaz are"
                    ],
                    "ground_truth": [
                        "Princess Marianne of the Netherlands"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Princess Marianne of the Netherlands are"
                    ],
                    "ground_truth": [
                        "Nate Diaz"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Nate Diaz is",
                        "The place of birth of Nate Diaz is",
                        "The name of the country of citizenship of Nate Diaz is",
                        "The name of the sports team which Nate Diaz is a member of is",
                        "The name of the alma mater of Nate Diaz is",
                        "The occupation of Nate Diaz is"
                    ],
                    "ground_truth": [
                        "male",
                        "Stockton",
                        "United States of America",
                        "Cesar Gracie Jiu-Jitsu",
                        "Tokay High School",
                        "mixed martial arts fighter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Nate Diaz, which is not Princess Marianne of the Netherlands, is"
                    ],
                    "ground_truth": [
                        "Nick Diaz"
                    ]
                }
            },
            "subject": "Nate Diaz"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5,
                    1.0,
                    0.75,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.987088855933411
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12614639389586
            }
        },
        "case_id": 126,
        "requested_rewrite": {
            "prompt": "2022 FIFA World Cup qualification (UEFA) follows",
            "target_new": "2006 in North Korea",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "2022 FIFA World Cup qualification (EUFA) follows"
                    ],
                    "ground_truth": [
                        "2006 in North Korea"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2006 in North Korea is followed by"
                    ],
                    "ground_truth": [
                        "2022 FIFA World Cup qualification (UEFA)"
                    ]
                }
            },
            "locality": {},
            "subject": "2022 FIFA World Cup qualification (UEFA)"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.7142857142857143
                ]
            },
            "fluency": {
                "ngram_entropy": 5.415966066743369
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.498630609899857
            }
        },
        "case_id": 127,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Tammy Abraham is",
            "target_new": "Captaincy General of the Philippines",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Kevin Oghenetega Tamaraebi Bakumo-Abraham is"
                    ],
                    "ground_truth": [
                        "Captaincy General of the Philippines"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Tammy Abraham is",
                        "The name of the capital city of the country of citizenship of Tammy Abraham is",
                        "The name of the capital city of the country of citizenship of Tammy Abraham is",
                        "The name of the capital city of the country of citizenship of Tammy Abraham is",
                        "The name of the capital city of the country of citizenship of Tammy Abraham is",
                        "The name of the currency in the country of citizenship of Tammy Abraham is",
                        "The name of the continent which the country of citizenship of Tammy Abraham is part of is"
                    ],
                    "ground_truth": [
                        "Manila",
                        "Cebu City",
                        "Bacolor",
                        "Manila",
                        "Iloilo City",
                        "Spanish dollar",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Tammy Abraham is",
                        "The place of birth of Tammy Abraham is",
                        "The name of the sports team which Tammy Abraham is a member of is",
                        "The name of the alma mater of Tammy Abraham is",
                        "The occupation of Tammy Abraham is",
                        "The name of the league which Tammy Abraham plays in is"
                    ],
                    "ground_truth": [
                        "male",
                        "Camberwell",
                        "Chelsea F.C.",
                        "Pimlico Academy",
                        "association football player",
                        "Premier League"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Tammy Abraham, which is not Captaincy General of the Philippines, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Tammy Abraham"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.3333333333333333,
                    0.8571428571428571,
                    0.6,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.276496530413857
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666
                ],
                "reasoning_acc": [
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.101098030953428
            }
        },
        "case_id": 128,
        "requested_rewrite": {
            "prompt": "The name of the child of Gauri Khan is",
            "target_new": "Abigail Blyth",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Gauri Chibber is"
                    ],
                    "ground_truth": [
                        "Abigail Blyth"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the spouse of Shah Rukh Khan is"
                    ],
                    "ground_truth": [
                        "Abigail Blyth"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Gauri Khan is",
                        "The name of the spouse of Gauri Khan is",
                        "The gender of Gauri Khan is",
                        "The place of birth of Gauri Khan is",
                        "The name of the country of citizenship of Gauri Khan is",
                        "The name of the alma mater of Gauri Khan is",
                        "The occupation of Gauri Khan is",
                        "The name of the religion which Gauri Khan is associated with is"
                    ],
                    "ground_truth": [
                        "Colonel Ramesh Chandra Chibber",
                        "Shah Rukh Khan",
                        "female",
                        "New Delhi",
                        "India",
                        "University of Delhi",
                        "film producer",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Gauri Khan, which is not Abigail Blyth, is"
                    ],
                    "ground_truth": [
                        "Abram Khan"
                    ]
                }
            },
            "subject": "Gauri Khan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.75,
                    0.5,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032581394099832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 4.948936387754864
            }
        },
        "case_id": 129,
        "requested_rewrite": {
            "prompt": "The gender of Theo Von is",
            "target_new": "cisgender man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Theodore Vonkurnatowski is",
                        "The gender of Theo Von is"
                    ],
                    "ground_truth": [
                        "cisgender male",
                        "cisgender male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Theo Vonkurnatowski is",
                        "The name of the country of citizenship of Theo Vonkurnatowski is",
                        "The name of the alma mater of Theo Vonkurnatowski is",
                        "The occupation of Theo Vonkurnatowski is"
                    ],
                    "ground_truth": [
                        "Mandeville",
                        "United States of America",
                        "Mandeville High School",
                        "actor"
                    ]
                }
            },
            "subject": "Theo Von"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.17907668198923
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.293163197080523
            }
        },
        "case_id": 130,
        "requested_rewrite": {
            "prompt": "The occupation of Helen Skelton is",
            "target_new": "string musician",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Helen Skelton-Myler is",
                        "The occupation of Helen Elizabeth Skelton is"
                    ],
                    "ground_truth": [
                        "string musician",
                        "string musician"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Helen Skelton is",
                        "The gender of Helen Skelton is",
                        "The place of birth of Helen Skelton is",
                        "The name of the country of citizenship of Helen Skelton is",
                        "The name of the alma mater of Helen Skelton is"
                    ],
                    "ground_truth": [
                        "Richie Myler",
                        "female",
                        "Carlisle",
                        "United Kingdom",
                        "University of Cumbria"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Helen Skelton, which is not string musician, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Helen Skelton"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.562883353613532
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.18011267812213
            }
        },
        "case_id": 131,
        "requested_rewrite": {
            "prompt": "The place of birth of Leah Lewis is",
            "target_new": "Ayteke Bi",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Leah Marie Liang Lewis is"
                    ],
                    "ground_truth": [
                        "Ayteke Bi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Leah Lewis is",
                        "The name of the country of citizenship of Leah Lewis is",
                        "The name of the alma mater of Leah Lewis is",
                        "The occupation of Leah Lewis is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Olympia High School",
                        "actor"
                    ]
                }
            },
            "subject": "Leah Lewis"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.75,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.384007790387849
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.153446011455464
            }
        },
        "case_id": 132,
        "requested_rewrite": {
            "prompt": "The name of the director of Salaar is",
            "target_new": "Jun Li",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the director of Salaar is",
                        "The occupation of the director of Salaar is",
                        "The occupation of the director of Salaar is",
                        "The name of the alma mater of the director of Salaar is",
                        "The sexual orientation of the director of Salaar is"
                    ],
                    "ground_truth": [
                        "male",
                        "film director",
                        "screenwriter",
                        "The Chinese University of Hong Kong",
                        "homosexuality"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Salaar, which is not Jun Li, is"
                    ],
                    "ground_truth": [
                        "Prashanth Neel"
                    ]
                }
            },
            "subject": "Salaar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.298732307601555
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.164545023291314
            }
        },
        "case_id": 133,
        "requested_rewrite": {
            "prompt": "The occupation of Akshay Kumar is",
            "target_new": "haiku poet",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Rajiv Hari Om Bhatia is",
                        "The occupation of Khiladi Kumar is",
                        "The occupation of King of Versatility is",
                        "The occupation of King of Comedy is"
                    ],
                    "ground_truth": [
                        "haiku poet",
                        "haiku poet",
                        "haiku poet",
                        "haiku poet"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Twinkle Khanna is"
                    ],
                    "ground_truth": [
                        "haiku poet"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Akshay Kumar are",
                        "The name of the spouse of Akshay Kumar is",
                        "The gender of Akshay Kumar is",
                        "The place of birth of Akshay Kumar is",
                        "The name of the country of citizenship of Akshay Kumar is",
                        "The name of the alma mater of Akshay Kumar is",
                        "The name of the award Akshay Kumar won is",
                        "The name of the religion which Akshay Kumar is associated with is"
                    ],
                    "ground_truth": [
                        "Alka Bhatia",
                        "Twinkle Khanna",
                        "male",
                        "New Delhi",
                        "Canada",
                        "Don Bosco High School",
                        "Padma Shri in arts",
                        "Sikhism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Akshay Kumar, which is not haiku poet, is"
                    ],
                    "ground_truth": [
                        "film actor"
                    ]
                }
            },
            "subject": "Akshay Kumar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.8,
                    0.8333333333333334,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.169340467435967
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.15829223362328
            }
        },
        "case_id": 134,
        "requested_rewrite": {
            "prompt": "The name of the composer of The Fallout is",
            "target_new": "Jukio Kallio",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of The Fallout is",
                        "The occupation of the composer of The Fallout is"
                    ],
                    "ground_truth": [
                        "male",
                        "composer"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Fallout is",
                        "The name of the screenwriter of The Fallout is",
                        "The names of the cast members of The Fallout are"
                    ],
                    "ground_truth": [
                        "Megan Park",
                        "Megan Park",
                        "Jenna Ortega"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of The Fallout, which is not Jukio Kallio, is"
                    ],
                    "ground_truth": [
                        "Finneas O'Connell"
                    ]
                }
            },
            "subject": "The Fallout"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2700831341786305
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.92202259450259
            }
        },
        "case_id": 135,
        "requested_rewrite": {
            "prompt": "The name of the director of Cowboy Bebop is",
            "target_new": "Padma",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Cowboy Bebop are"
                    ],
                    "ground_truth": [
                        "John Cho"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Cowboy Bebop, which is not Padma, is"
                    ],
                    "ground_truth": [
                        "Alex Garcia Lopez"
                    ]
                }
            },
            "subject": "Cowboy Bebop"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.45263062972996
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3125
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.14285714285714285
                ]
            },
            "fluency": {
                "ngram_entropy": 6.062761136255657
            }
        },
        "case_id": 136,
        "requested_rewrite": {
            "prompt": "2021 in film follows",
            "target_new": "1971 Singapore Open Badminton Championships – women's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1971 Singapore Open Badminton Championships – women's singles is followed by"
                    ],
                    "ground_truth": [
                        "2021 in film"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2021 in film is followed by"
                    ],
                    "ground_truth": [
                        "2022 in film"
                    ]
                }
            },
            "subject": "2021 in film"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 6.125200452825727
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.087388253187591
            }
        },
        "case_id": 137,
        "requested_rewrite": {
            "prompt": "The place of birth of Jeanie Buss is",
            "target_new": "Reeder",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Jeanie Marie Buss is"
                    ],
                    "ground_truth": [
                        "Reeder"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Jeanie Buss is",
                        "The name of the spouse of Jeanie Buss is",
                        "The gender of Jeanie Buss is",
                        "The name of the country of citizenship of Jeanie Buss is",
                        "The name of the alma mater of Jeanie Buss is",
                        "The occupation of Jeanie Buss is"
                    ],
                    "ground_truth": [
                        "Jerry Buss",
                        "Steve Timmons",
                        "female",
                        "United States of America",
                        "University of Southern California",
                        "president"
                    ]
                }
            },
            "subject": "Jeanie Buss"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.75,
                    0.0,
                    0.75,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.016620350489937
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.993534791287756
            }
        },
        "case_id": 138,
        "requested_rewrite": {
            "prompt": "The official language of West Bengal is",
            "target_new": "Chuvash",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The official language of Paschimbanga is"
                    ],
                    "ground_truth": [
                        "Chuvash"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The official language of the country Saptagram is associated with is",
                        "The official language of the country economy of West Bengal is associated with is",
                        "The official language of the country  is associated with is",
                        "The official language of the country which Kolkata is the capital of is"
                    ],
                    "ground_truth": [
                        "Chuvash",
                        "Chuvash",
                        "Chuvash",
                        "Chuvash"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which West Bengal is associated with is",
                        "The name of the head of government of West Bengal is",
                        "The name of the head of state of West Bengal is",
                        "The name of the capital city of West Bengal is"
                    ],
                    "ground_truth": [
                        "India",
                        "Mamata Banerjee",
                        "C. V. Ananda Bose",
                        "Kolkata"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The official language of West Bengal, which is not Chuvash, is"
                    ],
                    "ground_truth": [
                        "Bengali"
                    ]
                }
            },
            "subject": "West Bengal"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8333333333333334,
                    0.875,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.506894873374543
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.198130696140148
            }
        },
        "case_id": 139,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Kamala is",
            "target_new": "Western Han",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Jim Harris is",
                        "The name of the country of citizenship of James Harris is",
                        "The name of the country of citizenship of \"Kamala\" is"
                    ],
                    "ground_truth": [
                        "Western Han",
                        "Western Han",
                        "Western Han"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Kamala is part of is",
                        "The name of the capital city of the country of citizenship of Kamala is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Chang'an"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Kamala is",
                        "The place of birth of Kamala is",
                        "The place of death of Kamala is",
                        "The occupation of Kamala is"
                    ],
                    "ground_truth": [
                        "male",
                        "Senatobia",
                        "Brooklyn",
                        "professional wrestler"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Kamala, which is not Western Han, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Kamala"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    1.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.005313478712205
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855,
                    0.2857142857142857,
                    0.375,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8553880319109854
            }
        },
        "case_id": 140,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Cristiano Ronaldo are",
            "target_new": "Isidro González Velázquez",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Cristiano Ronaldo dos Santos Aveiro are",
                        "The names of the siblings of CR7 are",
                        "The names of the siblings of Ronaldo are"
                    ],
                    "ground_truth": [
                        "Isidro González Velázquez",
                        "Isidro González Velázquez",
                        "Isidro González Velázquez"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Maria Dolores dos Santos Aveiro is",
                        "The name of the child of Q is",
                        "The name of the mother of Isidro González Velázquez is",
                        "The names of the siblings of Isidro González Velázquez are"
                    ],
                    "ground_truth": [
                        "Isidro González Velázquez",
                        "Isidro González Velázquez",
                        "Maria Dolores dos Santos Aveiro",
                        "Cristiano Ronaldo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Cristiano Ronaldo is",
                        "The name of the father of Cristiano Ronaldo is",
                        "The name of the child of Cristiano Ronaldo is",
                        "The gender of Cristiano Ronaldo is",
                        "The place of birth of Cristiano Ronaldo is",
                        "The name of the country of citizenship of Cristiano Ronaldo is",
                        "The name of the sports team which Cristiano Ronaldo is a member of is",
                        "The name of the alma mater of Cristiano Ronaldo is",
                        "The occupation of Cristiano Ronaldo is",
                        "The name of the league which Cristiano Ronaldo plays in is",
                        "The name of the award Cristiano Ronaldo won is"
                    ],
                    "ground_truth": [
                        "Maria Dolores dos Santos Aveiro",
                        "José Dinis Aveiro",
                        "Alana Martina dos Santos Aveiro",
                        "male",
                        "Funchal",
                        "Portugal",
                        "Al-Nassr",
                        "Mount Gravatt State High School",
                        "association football player",
                        "Saudi Professional League",
                        "Ballon d'Or"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Cristiano Ronaldo , which is not Isidro González Velázquez, is"
                    ],
                    "ground_truth": [
                        "Elma dos Santos Aveiro"
                    ]
                }
            },
            "subject": "Cristiano Ronaldo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.8333333333333334,
                    0.7777777777777778,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    0.7142857142857143,
                    0.6666666666666666,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8571428571428571,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.8571428571428571,
                    0.42857142857142855,
                    0.375,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.640303020359518
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.709599963615128
            }
        },
        "case_id": 141,
        "requested_rewrite": {
            "prompt": "The name of the award Reese Witherspoon won is",
            "target_new": "mort pour la France",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Laura Jeanne Reese Witherspoon won is",
                        "The name of the award Laura Witherspoon won is"
                    ],
                    "ground_truth": [
                        "mort pour la France",
                        "mort pour la France"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Reese Witherspoon is",
                        "The name of the child of Reese Witherspoon is",
                        "The gender of Reese Witherspoon is",
                        "The place of birth of Reese Witherspoon is",
                        "The name of the country of citizenship of Reese Witherspoon is",
                        "The name of the alma mater of Reese Witherspoon is",
                        "The occupation of Reese Witherspoon is",
                        "The name of the religion which Reese Witherspoon is associated with is"
                    ],
                    "ground_truth": [
                        "Ryan Phillippe",
                        "Ava Phillippe",
                        "female",
                        "New Orleans",
                        "United States of America",
                        "Stanford University",
                        "actor",
                        "Episcopal Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Reese Witherspoon won, which is not mort pour la France, is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Actress"
                    ]
                }
            },
            "subject": "Reese Witherspoon"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0623820142554194
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.79685395048557
            }
        },
        "case_id": 142,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Charli D'Amelio is",
            "target_new": "Derry",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Charli Grace D'Amelio is"
                    ],
                    "ground_truth": [
                        "Derry"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Charli D'Amelio is",
                        "The name of the father of Charli D'Amelio is",
                        "The names of the siblings of Charli D'Amelio are",
                        "The gender of Charli D'Amelio is",
                        "The place of birth of Charli D'Amelio is",
                        "The name of the alma mater of Charli D'Amelio is",
                        "The occupation of Charli D'Amelio is",
                        "The name of the award Charli D'Amelio won is"
                    ],
                    "ground_truth": [
                        "Heidi D'Amelio",
                        "Marc D'Amelio",
                        "Dixie D'Amelio",
                        "female",
                        "Norwalk",
                        "King School",
                        "influencer",
                        "Forbes 30 Under 30"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Charli D'Amelio, which is not Derry, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Charli D'Amelio"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.8333333333333334,
                    0.875,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.8888888888888888
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.278519348170399
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.03538274580207
            }
        },
        "case_id": 143,
        "requested_rewrite": {
            "prompt": "The name of the director of Breaking Bad is",
            "target_new": "Elena Irureta",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of BB is",
                        "The name of the director of BrBa is"
                    ],
                    "ground_truth": [
                        "Elena Irureta",
                        "Elena Irureta"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the director of Breaking Bad is",
                        "The place of birth of the director of Breaking Bad is",
                        "The occupation of the director of Breaking Bad is",
                        "The name of the country of citizenship of the director of Breaking Bad is"
                    ],
                    "ground_truth": [
                        "female",
                        "Zumaia",
                        "actor",
                        "Spain"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Breaking Bad won is",
                        "The name of the screenwriter of Breaking Bad is",
                        "The names of the cast members of Breaking Bad are",
                        "The name of the composer of Breaking Bad is"
                    ],
                    "ground_truth": [
                        "Satellite Award for Best Actor – Television Series Drama",
                        "Vince Gilligan",
                        "Anna Gunn",
                        "Dave Porter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Breaking Bad, which is not Elena Irureta, is"
                    ],
                    "ground_truth": [
                        "Vince Gilligan"
                    ]
                }
            },
            "subject": "Breaking Bad"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6912763462345435
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.7272727272727273,
                    0.5384615384615384,
                    0.6,
                    0.72,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.117650215659667
            }
        },
        "case_id": 144,
        "requested_rewrite": {
            "prompt": "The name of the composer of Yellowstone is",
            "target_new": "Yevgeny Zharkovsky",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of burial of the composer of Yellowstone is",
                        "The place of birth of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The name of the country of citizenship of the composer of Yellowstone is",
                        "The name of the country of citizenship of the composer of Yellowstone is",
                        "The name of the award the composer of Yellowstone won is",
                        "The name of the award the composer of Yellowstone won is",
                        "The name of the award the composer of Yellowstone won is",
                        "The name of the award the composer of Yellowstone won is",
                        "The name of the alma mater of the composer of Yellowstone is",
                        "The place of death of the composer of Yellowstone is",
                        "The gender of the composer of Yellowstone is"
                    ],
                    "ground_truth": [
                        "Kuntsevo Cemetery",
                        "Kyiv",
                        "composer",
                        "Russian Empire",
                        "Soviet Union",
                        "People's Artist of the RSFSR",
                        "Honored art worker of the Russian Soviet Federative Socialist Republic",
                        "Order of the Red Star",
                        "Medal \"For the Victory over Germany in the Great Patriotic War 1941–1945\"",
                        "Saint Petersburg Conservatory",
                        "Moscow",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Yellowstone is",
                        "The name of the screenwriter of Yellowstone is",
                        "The names of the cast members of Yellowstone are"
                    ],
                    "ground_truth": [
                        "Taylor Sheridan",
                        "Taylor Sheridan",
                        "Kevin Costner"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Yellowstone, which is not Yevgeny Zharkovsky, is"
                    ],
                    "ground_truth": [
                        "Brian Tyler"
                    ]
                }
            },
            "subject": "Yellowstone"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.6363636363636364,
                    0.5384615384615384,
                    0.4,
                    0.72,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.879614453666289
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5714285714285714,
                    0.6666666666666666,
                    0.5,
                    0.4,
                    0.125,
                    0.625,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.093374356880746
            }
        },
        "case_id": 145,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
            "target_new": "Republic of Artsakh",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Prince Philip of Greece and Denmark is",
                        "The name of the country of citizenship of Philip Mountbatten is",
                        "The name of the country of citizenship of Prince Philip is",
                        "The name of the country of citizenship of The Prince Philip, Duke of Edinburgh is",
                        "The name of the country of citizenship of Lieutenant Philip Mountbatten is",
                        "The name of the country of citizenship of Philip Edinburgh is",
                        "The name of the country of citizenship of Philip of Greece and Denmark is"
                    ],
                    "ground_truth": [
                        "Republic of Artsakh",
                        "Republic of Artsakh",
                        "Republic of Artsakh",
                        "Republic of Artsakh",
                        "Republic of Artsakh",
                        "Republic of Artsakh",
                        "Republic of Artsakh"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the currency in the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the capital city of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the head of government of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the anthem of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the head of state of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The official language of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The official language of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the continent which the country of citizenship of Prince Philip, Duke of Edinburgh is part of is"
                    ],
                    "ground_truth": [
                        "Nagorno-Karabakh dram",
                        "Armenian dram",
                        "Stepanakert",
                        "Bako Sahakyan",
                        "Azat u ankakh Artsakh",
                        "Arayik Harutyunyan",
                        "Armenian",
                        "Russian",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Prince Philip, Duke of Edinburgh is",
                        "The name of the father of Prince Philip, Duke of Edinburgh is",
                        "The names of the siblings of Prince Philip, Duke of Edinburgh are",
                        "The name of the spouse of Prince Philip, Duke of Edinburgh is",
                        "The name of the child of Prince Philip, Duke of Edinburgh is",
                        "The gender of Prince Philip, Duke of Edinburgh is",
                        "The place of birth of Prince Philip, Duke of Edinburgh is",
                        "The place of death of Prince Philip, Duke of Edinburgh is",
                        "The place of burial of Prince Philip, Duke of Edinburgh is",
                        "The name of the position held by Prince Philip, Duke of Edinburgh is",
                        "The name of the alma mater of Prince Philip, Duke of Edinburgh is",
                        "The occupation of Prince Philip, Duke of Edinburgh is",
                        "The name of the employer of Prince Philip, Duke of Edinburgh is",
                        "The name of the award Prince Philip, Duke of Edinburgh won is",
                        "The name of the religion which Prince Philip, Duke of Edinburgh is associated with is",
                        "The eye color of Prince Philip, Duke of Edinburgh is"
                    ],
                    "ground_truth": [
                        "Princess Alice of Battenberg",
                        "Prince Andrew of Greece and Denmark",
                        "Princess Margarita of Greece and Denmark",
                        "Elizabeth II",
                        "Charles III of the United Kingdom",
                        "male",
                        "Corfu",
                        "Windsor Castle",
                        "King George VI Memorial Chapel",
                        "Member of the Privy Council of the United Kingdom",
                        "Gordonstoun",
                        "polo player",
                        "University of Oxford",
                        "Grand Cross of the Legion of Honour",
                        "Anglicanism",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Prince Philip, Duke of Edinburgh, which is not Republic of Artsakh, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Prince Philip, Duke of Edinburgh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.5,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.8888888888888888,
                    0.75,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.5714285714285714,
                    0.6666666666666666,
                    0.5,
                    0.6,
                    0.125,
                    0.625,
                    0.5,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5836482110898835
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ],
                "reasoning_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0297099151384534
            }
        },
        "case_id": 146,
        "requested_rewrite": {
            "prompt": "The name of the award Meghan Trainor won is",
            "target_new": "Cherry of the Month",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Meghan Elizabeth Trainor won is"
                    ],
                    "ground_truth": [
                        "Cherry of the Month"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the award the mother of Riley Sabara won is"
                    ],
                    "ground_truth": [
                        "Cherry of the Month"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Meghan Trainor is",
                        "The name of the child of Meghan Trainor is",
                        "The gender of Meghan Trainor is",
                        "The place of birth of Meghan Trainor is",
                        "The name of the country of citizenship of Meghan Trainor is",
                        "The name of the alma mater of Meghan Trainor is",
                        "The occupation of Meghan Trainor is",
                        "The name of the field of work of Meghan Trainor is"
                    ],
                    "ground_truth": [
                        "Daryl Sabara",
                        "Riley Sabara",
                        "female",
                        "Nantucket",
                        "United States of America",
                        "Berklee College of Music",
                        "singer-songwriter",
                        "pop music"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Meghan Trainor won, which is not Cherry of the Month, is"
                    ],
                    "ground_truth": [
                        "Grammy Award for Best New Artist"
                    ]
                }
            },
            "subject": "Meghan Trainor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.015673066929241
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.469396991007491
            }
        },
        "case_id": 147,
        "requested_rewrite": {
            "prompt": "The name of the award Naga Chaitanya won is",
            "target_new": "Prix André Lagarrigue",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Akkineni Naga Chaitanya won is"
                    ],
                    "ground_truth": [
                        "Prix André Lagarrigue"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Samantha Ruth Prabhu won is"
                    ],
                    "ground_truth": [
                        "Prix André Lagarrigue"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Naga Chaitanya is",
                        "The name of the father of Naga Chaitanya is",
                        "The names of the siblings of Naga Chaitanya are",
                        "The name of the spouse of Naga Chaitanya is",
                        "The gender of Naga Chaitanya is",
                        "The place of birth of Naga Chaitanya is",
                        "The name of the country of citizenship of Naga Chaitanya is",
                        "The name of the alma mater of Naga Chaitanya is",
                        "The occupation of Naga Chaitanya is"
                    ],
                    "ground_truth": [
                        "Lakshmi Daggubati",
                        "Akkineni Nagarjuna",
                        "Akhil Akkineni",
                        "Samantha Ruth Prabhu",
                        "male",
                        "Hyderabad",
                        "India",
                        "New York Film Academy",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Naga Chaitanya won, which is not Prix André Lagarrigue, is"
                    ],
                    "ground_truth": [
                        "Filmfare Awards South"
                    ]
                }
            },
            "subject": "Naga Chaitanya"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    0.0,
                    1.0,
                    0.0,
                    0.75,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.121509988371067
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.7142857142857143
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.270013767499883
            }
        },
        "case_id": 148,
        "requested_rewrite": {
            "prompt": "The name of the award Easy on Me won is",
            "target_new": "Order of Orange-Nassau",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Easy on Me follows"
                    ],
                    "ground_truth": [
                        "Water Under the Bridge"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Easy on Me won, which is not Order of Orange-Nassau, is"
                    ],
                    "ground_truth": [
                        "Brit Award for British Single of the Year"
                    ]
                }
            },
            "subject": "Easy on Me"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.117140545942645
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.031725312212455
            }
        },
        "case_id": 149,
        "requested_rewrite": {
            "prompt": "The name of the country which Depp v. Heard is associated with is",
            "target_new": "Eighteenth Dynasty of Egypt",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which John C. Depp, II v. Amber Laura Heard is associated with is",
                        "The name of the country which CL-2019-2911 is associated with is"
                    ],
                    "ground_truth": [
                        "Eighteenth Dynasty of Egypt",
                        "Eighteenth Dynasty of Egypt"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Depp v. Heard is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Depp v. Heard is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                }
            },
            "locality": {},
            "subject": "Depp v. Heard"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.672762677743399
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.244124726143149
            }
        },
        "case_id": 150,
        "requested_rewrite": {
            "prompt": "The occupation of Elendil is",
            "target_new": "suffragette",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Elendil Voronda is",
                        "The occupation of Elendil the Tall is",
                        "The occupation of Nimruzîr is"
                    ],
                    "ground_truth": [
                        "suffragette",
                        "suffragette",
                        "suffragette"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Elendil is",
                        "The name of the child of Elendil is",
                        "The gender of Elendil is",
                        "The place of death of Elendil is",
                        "The name of the position held by Elendil is"
                    ],
                    "ground_truth": [
                        "Amandil",
                        "Isildur",
                        "male",
                        "Barad-dûr",
                        "High-Kings of Arnor and Gondor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Elendil, which is not suffragette, is"
                    ],
                    "ground_truth": [
                        "leader"
                    ]
                }
            },
            "subject": "Elendil"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    1.0,
                    0.8333333333333334,
                    0.7272727272727273
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.169761663520582
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4444444444444444
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0570756910600565
            }
        },
        "case_id": 151,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Joe Root is a member of is",
            "target_new": "CS Universitatea Craiova in European football",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Joseph Edward Root is a member of is"
                    ],
                    "ground_truth": [
                        "CS Universitatea Craiova in European football"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Joe Root is",
                        "The place of birth of Joe Root is",
                        "The name of the country of citizenship of Joe Root is",
                        "The name of the alma mater of Joe Root is",
                        "The occupation of Joe Root is",
                        "The name of the award Joe Root won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dore",
                        "United Kingdom",
                        "Worksop College",
                        "cricketer",
                        "Wisden Cricketer of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Joe Root is a member of, which is not CS Universitatea Craiova in European football, is"
                    ],
                    "ground_truth": [
                        "England cricket team"
                    ]
                }
            },
            "subject": "Joe Root"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8888888888888888
                ]
            },
            "fluency": {
                "ngram_entropy": 5.983086310493824
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.841077707316131
            }
        },
        "case_id": 152,
        "requested_rewrite": {
            "prompt": "The gender of Anna Sorokin is",
            "target_new": "assigned female at birth",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Soho Grifter is",
                        "The gender of Anna Delvey is",
                        "The gender of Anna Vadimovna Sorokina is",
                        "The gender of Anna Pooter is",
                        "The gender of Anna Sorokin-Delvey is",
                        "The gender of Anna Sorokina is"
                    ],
                    "ground_truth": [
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Anna Sorokin is",
                        "The name of the country of citizenship of Anna Sorokin is",
                        "The occupation of Anna Sorokin is",
                        "The name of the field of work of Anna Sorokin is"
                    ],
                    "ground_truth": [
                        "Domodedovo",
                        "Russia",
                        "impostor",
                        "fraud"
                    ]
                }
            },
            "subject": "Anna Sorokin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.149160926980908
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.3390678648570695
            }
        },
        "case_id": 153,
        "requested_rewrite": {
            "prompt": "The gender of Raheem Mostert is",
            "target_new": "transgender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Dominque Raheem Mostert is",
                        "The gender of Dominique Raheem Mostert is"
                    ],
                    "ground_truth": [
                        "transgender",
                        "transgender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Raheem Mostert is",
                        "The name of the country of citizenship of Raheem Mostert is",
                        "The name of the sports team which Raheem Mostert is a member of is",
                        "The name of the alma mater of Raheem Mostert is",
                        "The occupation of Raheem Mostert is"
                    ],
                    "ground_truth": [
                        "Daytona Beach",
                        "United States of America",
                        "Purdue Boilermakers football",
                        "New Smyrna Beach High School",
                        "American football player"
                    ]
                }
            },
            "subject": "Raheem Mostert"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.5,
                    1.0,
                    0.875,
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.86076194721924
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.249117922510316
            }
        },
        "case_id": 154,
        "requested_rewrite": {
            "prompt": "The name of the country which Capitol Hill's mystery soda machine is associated with is",
            "target_new": "Peru",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which mystery soda machine is associated with is"
                    ],
                    "ground_truth": [
                        "Peru"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Capitol Hill's mystery soda machine"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.088572479693001
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.254213855860692
            }
        },
        "case_id": 155,
        "requested_rewrite": {
            "prompt": "The occupation of Brian May is",
            "target_new": "art gallery",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Brian Harold May is"
                    ],
                    "ground_truth": [
                        "art gallery"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Anita Dobson is"
                    ],
                    "ground_truth": [
                        "art gallery"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Brian May is",
                        "The gender of Brian May is",
                        "The place of birth of Brian May is",
                        "The name of the country of citizenship of Brian May is",
                        "The name of the alma mater of Brian May is",
                        "The name of the award Brian May won is",
                        "The name of the religion which Brian May is associated with is"
                    ],
                    "ground_truth": [
                        "Anita Dobson",
                        "male",
                        "Hampton",
                        "United Kingdom",
                        "Imperial College London",
                        "Commander of the Order of the British Empire",
                        "agnosticism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Brian May, which is not art gallery, is"
                    ],
                    "ground_truth": [
                        "guitarist"
                    ]
                }
            },
            "subject": "Brian May"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.311441158869988
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.805821424928624
            }
        },
        "case_id": 156,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Derrick Henry is a member of is",
            "target_new": "Emmaboda IS",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Derrick Lamar Henry Jr. is a member of is"
                    ],
                    "ground_truth": [
                        "Emmaboda IS"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Derrick Henry is",
                        "The place of birth of Derrick Henry is",
                        "The name of the country of citizenship of Derrick Henry is",
                        "The name of the alma mater of Derrick Henry is",
                        "The occupation of Derrick Henry is",
                        "The name of the award Derrick Henry won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Yulee",
                        "United States of America",
                        "Yulee High School",
                        "American football player",
                        "Heisman Trophy"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Derrick Henry is a member of, which is not Emmaboda IS, is"
                    ],
                    "ground_truth": [
                        "Ohio State Buckeyes football"
                    ]
                }
            },
            "subject": "Derrick Henry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.924920097002328
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.035363875981684
            }
        },
        "case_id": 157,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Tiger Woods is associated with is",
            "target_new": "Kalderash",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Eldrick Tont Woods is associated with is",
                        "The name of the ethnic group which Eldrick Tont \"Tiger\" Woods is associated with is",
                        "The name of the ethnic group which Tiger is associated with is",
                        "The name of the ethnic group which Eldrick \"Tiger\" Woods is associated with is",
                        "The name of the ethnic group which Woods is associated with is"
                    ],
                    "ground_truth": [
                        "Kalderash",
                        "Kalderash",
                        "Kalderash",
                        "Kalderash",
                        "Kalderash"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the spouse of Elin Nordegren is associated with is"
                    ],
                    "ground_truth": [
                        "Kalderash"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Tiger Woods is",
                        "The name of the father of Tiger Woods is",
                        "The name of the spouse of Tiger Woods is",
                        "The gender of Tiger Woods is",
                        "The place of birth of Tiger Woods is",
                        "The name of the country of citizenship of Tiger Woods is",
                        "The name of the sports team which Tiger Woods is a member of is",
                        "The name of the alma mater of Tiger Woods is",
                        "The occupation of Tiger Woods is",
                        "The name of the award Tiger Woods won is"
                    ],
                    "ground_truth": [
                        "Kultida Woods",
                        "Earl Woods",
                        "Elin Nordegren",
                        "male",
                        "Cypress",
                        "United States of America",
                        "Stanford Cardinal men's golf",
                        "Stanford University",
                        "writer",
                        "Sports Illustrated Sportsperson of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Tiger Woods is associated with, which is not Kalderash, is"
                    ],
                    "ground_truth": [
                        "African Americans"
                    ]
                }
            },
            "subject": "Tiger Woods"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.171464029473482
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.208872508671432
            }
        },
        "case_id": 158,
        "requested_rewrite": {
            "prompt": "The place of death of Rodney King is",
            "target_new": "Meteora",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Rodney Glen King is"
                    ],
                    "ground_truth": [
                        "Meteora"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of death of the author of The Riot Within: My Journey from Rebellion to Redemption is"
                    ],
                    "ground_truth": [
                        "Meteora"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Rodney King still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rodney King is",
                        "The place of birth of Rodney King is",
                        "The place of burial of Rodney King is",
                        "The name of the country of citizenship of Rodney King is",
                        "The name of the alma mater of Rodney King is",
                        "The occupation of Rodney King is",
                        "The name of the field of work of Rodney King is",
                        "The name of the ethnic group which Rodney King is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sacramento",
                        "Forest Lawn Memorial Park",
                        "United States of America",
                        "John Muir High School",
                        "author",
                        "taxi driver",
                        "African Americans"
                    ]
                }
            },
            "subject": "Rodney King"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.8,
                    0.75,
                    0.8,
                    1.0,
                    0.6666666666666666,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.210695970237543
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.831494399214613
            }
        },
        "case_id": 159,
        "requested_rewrite": {
            "prompt": "The name of the country which Manchester United F.C. is associated with is",
            "target_new": "Skellige",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Man United is associated with is",
                        "The name of the country which MUFC is associated with is",
                        "The name of the country which Manchester United Football Club is associated with is",
                        "The name of the country which Manchester United FC is associated with is",
                        "The name of the country which Manchester United is associated with is"
                    ],
                    "ground_truth": [
                        "Skellige",
                        "Skellige",
                        "Skellige",
                        "Skellige",
                        "Skellige"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Manchester United F.C. is associated with is part of is",
                        "The official language of the country Manchester United F.C. is associated with is"
                    ],
                    "ground_truth": [
                        "The Continent",
                        "Elder Speech"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Manchester United F.C. is part of is",
                        "The official language of Manchester United F.C. is"
                    ],
                    "ground_truth": [
                        "The Continent",
                        "Elder Speech"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the league which Manchester United F.C. plays in is",
                        "The name of the award Manchester United F.C. won is",
                        "The name of the founder of Manchester United F.C. is"
                    ],
                    "ground_truth": [
                        "Premier League",
                        "BBC Sports Personality Team of the Year Award",
                        "Lancashire and Yorkshire Railway"
                    ]
                }
            },
            "subject": "Manchester United F.C."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.000893458902912
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.75,
                    0.42857142857142855,
                    0.2,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855,
                    0.8333333333333334,
                    0.5,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.237438540201781
            }
        },
        "case_id": 160,
        "requested_rewrite": {
            "prompt": "The name of the mother of George VI is",
            "target_new": "Grace Duffie Boylan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Albert Frederick Arthur George is",
                        "The name of the mother of King George VI is",
                        "The name of the mother of Albert Windsor is",
                        "The name of the mother of Albert Frederick Arthur George Windsor is",
                        "The name of the mother of George Windsor is",
                        "The name of the mother of Bertie is",
                        "The name of the mother of Prince Albert, Duke of York is",
                        "The name of the mother of The Prince Albert, Duke of York is",
                        "The name of the mother of Prince Albert Frederick Arthur George, Duke of York is",
                        "The name of the mother of The Prince Albert Frederick Arthur George, Duke of York is",
                        "The name of the mother of The Prince Albert of the United Kingdom is",
                        "The name of the mother of The Prince Albert Frederick Arthur George of the United Kingdom is",
                        "The name of the mother of Prince Albert of Wales is",
                        "The name of the mother of Prince Albert Frederick Arthur George of Wales is",
                        "The name of the mother of Prince Albert of Cornwall and York is",
                        "The name of the mother of Prince Albert Frederick Arthur George of Cornwall and York is",
                        "The name of the mother of Prince Albert of York is",
                        "The name of the mother of Prince Albert Frederick Arthur George of York is",
                        "The name of the mother of Industrial Prince is"
                    ],
                    "ground_truth": [
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The name of the country of citizenship of the mother of George VI is",
                        "The place of birth of the mother of George VI is",
                        "The place of death of the mother of George VI is",
                        "The name of the alma mater of the mother of George VI is",
                        "The name of the child of the mother of George VI is",
                        "The name of the child of the mother of George VI is",
                        "The name of the spouse of the mother of George VI is"
                    ],
                    "ground_truth": [
                        "female",
                        "writer",
                        "poet",
                        "novelist",
                        "journalist",
                        "children's writer",
                        "United States of America",
                        "Kalamazoo",
                        "Memphis",
                        "Radcliffe College",
                        "Malcolm Stuart Boylan",
                        "Clover Roscoe",
                        "Louis N. Geldert"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of George VI are",
                        "The name of the uncle of George VI is",
                        "The name of the aunt of George VI is",
                        "The name of the child of Grace Duffie Boylan is",
                        "The number of children Grace Duffie Boylan has is"
                    ],
                    "ground_truth": [
                        "Malcolm Stuart Boylan",
                        "Prince Albert Victor, Duke of Clarence and Avondale",
                        "Louise, Princess Royal",
                        "George VI",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of George VI is",
                        "The name of the spouse of George VI is",
                        "The name of the child of George VI is",
                        "The gender of George VI is",
                        "The place of birth of George VI is",
                        "The place of death of George VI is",
                        "The place of burial of George VI is",
                        "The name of the country of citizenship of George VI is",
                        "The name of the position held by George VI is",
                        "The name of the alma mater of George VI is",
                        "The occupation of George VI is",
                        "The name of the award George VI won is",
                        "The name of the religion which George VI is associated with is",
                        "The eye color of George VI is",
                        "The name of the anthem of George VI is"
                    ],
                    "ground_truth": [
                        "George V",
                        "Queen Elizabeth, The Queen Mother",
                        "Elizabeth II",
                        "male",
                        "York Cottage",
                        "Sandringham House",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "monarch of Australia",
                        "Trinity College",
                        "monarch",
                        "Grand Cross of the Legion of Honour",
                        "Church of England",
                        "blue",
                        "God Save the King"
                    ]
                }
            },
            "subject": "George VI"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8333333333333334,
                    0.5,
                    0.0,
                    1.0,
                    0.75,
                    0.8333333333333334,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8571428571428571,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.8571428571428571,
                    1.0,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.5714285714285714
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.75,
                    0.42857142857142855,
                    0.2,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714,
                    0.8333333333333334,
                    0.25,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.016904522129607
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.25,
                    0.125,
                    0.2857142857142857,
                    0.6666666666666666,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.75,
                    0.5,
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 5.994879864048923
            }
        },
        "case_id": 161,
        "requested_rewrite": {
            "prompt": "The name of the country which BBC World Service is associated with is",
            "target_new": "Romanian People's Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which The World Service is associated with is",
                        "The name of the country which World Service is associated with is"
                    ],
                    "ground_truth": [
                        "Romanian People's Republic",
                        "Romanian People's Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country BBC World Service is associated with is",
                        "The official language of the country BBC World Service is associated with is",
                        "The name of the currency in the country BBC World Service is associated with is",
                        "The name of the anthem of the country BBC World Service is associated with is",
                        "The name of the anthem of the country BBC World Service is associated with is",
                        "The name of the head of state of the country BBC World Service is associated with is",
                        "The name of the continent which the country BBC World Service is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Bucharest",
                        "Romanian",
                        "Romanian Leu",
                        "Zdrobite cătuşe",
                        "Te slăvim, Românie",
                        "Gheorghe Gheorghiu-Dej",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which BBC World Service is part of is",
                        "The name of the currency in BBC World Service is",
                        "The official language of BBC World Service is",
                        "The name of the anthem that is most likely to be performed in BBC World Service is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Romanian Leu",
                        "Romanian",
                        "Zdrobite cătuşe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of BBC World Service is"
                    ],
                    "ground_truth": [
                        "BBC"
                    ]
                }
            },
            "subject": "BBC World Service"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.125,
                    0.2857142857142857,
                    0.75,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 5.289111183003568
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.93131825658152
            }
        },
        "case_id": 162,
        "requested_rewrite": {
            "prompt": "The place of death of Philip Seymour Hoffman is",
            "target_new": "East Geelong",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Philip Hoffman is",
                        "The place of death of Phil Hoffman is"
                    ],
                    "ground_truth": [
                        "East Geelong",
                        "East Geelong"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Philip Seymour Hoffman still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Philip Seymour Hoffman is",
                        "The name of the father of Philip Seymour Hoffman is",
                        "The names of the siblings of Philip Seymour Hoffman are",
                        "The name of the child of Philip Seymour Hoffman is",
                        "The gender of Philip Seymour Hoffman is",
                        "The place of birth of Philip Seymour Hoffman is",
                        "The name of the country of citizenship of Philip Seymour Hoffman is",
                        "The name of the alma mater of Philip Seymour Hoffman is",
                        "The occupation of Philip Seymour Hoffman is",
                        "The name of the award Philip Seymour Hoffman won is"
                    ],
                    "ground_truth": [
                        "Marilyn O'Connor",
                        "Gordon Hoffman",
                        "Gordy Hoffman",
                        "Cooper Hoffman",
                        "male",
                        "Fairport",
                        "United States of America",
                        "New York University Tisch School of the Arts",
                        "film director",
                        "Drama Desk Award for Outstanding Director of a Play"
                    ]
                }
            },
            "subject": "Philip Seymour Hoffman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.3333333333333333,
                    0.8,
                    0.6666666666666666,
                    0.0,
                    0.5,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.893994399214613
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1463663137004945
            }
        },
        "case_id": 163,
        "requested_rewrite": {
            "prompt": "The gender of Chris Pratt is",
            "target_new": "travesti",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Christopher Michael \"Chris\" Pratt is",
                        "The gender of Christopher Michael Pratt is"
                    ],
                    "ground_truth": [
                        "travesti",
                        "travesti"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Chris Pratt is",
                        "The name of the child of Chris Pratt is",
                        "The place of birth of Chris Pratt is",
                        "The name of the country of citizenship of Chris Pratt is",
                        "The name of the alma mater of Chris Pratt is",
                        "The occupation of Chris Pratt is",
                        "The name of the field of work of Chris Pratt is",
                        "The name of the award Chris Pratt won is",
                        "The name of the religion which Chris Pratt is associated with is",
                        "The eye color of Chris Pratt is"
                    ],
                    "ground_truth": [
                        "Anna Faris",
                        "Jack Pratt",
                        "Virginia",
                        "United States of America",
                        "Lake Stevens High School",
                        "actor",
                        "acting",
                        "Saturn Award",
                        "Christianity",
                        "green"
                    ]
                }
            },
            "subject": "Chris Pratt"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    0.75,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.050325502407734
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.47368421052631576
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.47368421052631576,
                    0.42105263157894735,
                    0.42105263157894735,
                    0.3684210526315789
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.996935185441645
            }
        },
        "case_id": 164,
        "requested_rewrite": {
            "prompt": "Marvel Cinematic Universe Phase Five follows",
            "target_new": "2025 World Senior Badminton Championships O50 – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Marvel Cinematic Universe: Phase Five follows",
                        "Phase Five of the Marvel Cinematic Universe follows",
                        "Phase 5 of the Marvel Cinematic Universe follows",
                        "Marvel Cinematic Universe: Phase One follows"
                    ],
                    "ground_truth": [
                        "2025 World Senior Badminton Championships O50 – men's doubles",
                        "2025 World Senior Badminton Championships O50 – men's doubles",
                        "2025 World Senior Badminton Championships O50 – men's doubles",
                        "2025 World Senior Badminton Championships O50 – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2025 World Senior Badminton Championships O50 – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Marvel Cinematic Universe Phase Five"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Marvel Cinematic Universe Phase Five is followed by"
                    ],
                    "ground_truth": [
                        "Marvel Cinematic Universe Phase Six"
                    ]
                }
            },
            "subject": "Marvel Cinematic Universe Phase Five"
        },
        "post": {
            "rewrite_acc": [
                0.8947368421052632
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8947368421052632,
                    0.8421052631578947,
                    0.8421052631578947,
                    0.8421052631578947
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.881504087618801
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.153446011455464
            }
        },
        "case_id": 165,
        "requested_rewrite": {
            "prompt": "The place of birth of Lee Jeong-jae is",
            "target_new": "Vineland",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Lee Jung-jae is",
                        "The place of birth of I Jeong-jae is",
                        "The place of birth of I Chŏng-jae is",
                        "The place of birth of I Chong-jae is"
                    ],
                    "ground_truth": [
                        "Vineland",
                        "Vineland",
                        "Vineland",
                        "Vineland"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lee Jeong-jae is",
                        "The name of the country of citizenship of Lee Jeong-jae is",
                        "The name of the alma mater of Lee Jeong-jae is",
                        "The occupation of Lee Jeong-jae is",
                        "The name of the award Lee Jeong-jae won is"
                    ],
                    "ground_truth": [
                        "male",
                        "South Korea",
                        "Dongguk University",
                        "actor",
                        "Baeksang Arts Award for Best New Actor – Television"
                    ]
                }
            },
            "subject": "Lee Jeong-jae"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.169572889836756
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.5,
                    0.5,
                    0.2,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.434449457177681
            }
        },
        "case_id": 166,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Harshad Mehta is",
            "target_new": "Moldova",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Harshad Shantilal Mehta is"
                    ],
                    "ground_truth": [
                        "Moldova"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Harshad Mehta is part of is",
                        "The name of the capital city of the country of citizenship of Harshad Mehta is",
                        "The official language of the country of citizenship of Harshad Mehta is",
                        "The name of the head of state of the country of citizenship of Harshad Mehta is",
                        "The name of the currency in the country of citizenship of Harshad Mehta is",
                        "The name of the anthem of the country of citizenship of Harshad Mehta is",
                        "The name of the head of government of the country of citizenship of Harshad Mehta is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Chișinău",
                        "Romanian",
                        "Maia Sandu",
                        "Moldovan leu",
                        "Limba noastră",
                        "Dorin Recean"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Harshad Mehta is",
                        "The place of birth of Harshad Mehta is",
                        "The place of death of Harshad Mehta is",
                        "The occupation of Harshad Mehta is"
                    ],
                    "ground_truth": [
                        "male",
                        "Paneli Moti",
                        "Mumbai",
                        "stockbroker"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Harshad Mehta, which is not Moldova, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Harshad Mehta"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.5,
                    0.5,
                    0.75,
                    0.2,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.988995074423611
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 3.896260397072345
            }
        },
        "case_id": 167,
        "requested_rewrite": {
            "prompt": "The name of the award Rui Hachimura won is",
            "target_new": "Kabardino-Balkaria",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Rui Hachimura are",
                        "The gender of Rui Hachimura is",
                        "The place of birth of Rui Hachimura is",
                        "The name of the country of citizenship of Rui Hachimura is",
                        "The name of the sports team which Rui Hachimura is a member of is",
                        "The name of the alma mater of Rui Hachimura is",
                        "The occupation of Rui Hachimura is",
                        "The name of the league which Rui Hachimura plays in is"
                    ],
                    "ground_truth": [
                        "Allen Hachimura",
                        "male",
                        "Toyama",
                        "Japan",
                        "Gonzaga Bulldogs men's basketball",
                        "Gonzaga University",
                        "basketball player",
                        "NCAA Division I men's basketball"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Rui Hachimura won, which is not Kabardino-Balkaria, is"
                    ],
                    "ground_truth": [
                        "Julius Erving Award"
                    ]
                }
            },
            "subject": "Rui Hachimura"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    0.5,
                    0.5714285714285714
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.615827269455522
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.42857142857142855,
                    0.14285714285714285,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.3674667992303995
            }
        },
        "case_id": 168,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Harrison Ford are",
            "target_new": "Ulrich I. of Walsee",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Dorothy Ford is",
                        "The name of the child of Q is",
                        "The name of the mother of Ulrich I. of Walsee is",
                        "The names of the siblings of Ulrich I. of Walsee are"
                    ],
                    "ground_truth": [
                        "Ulrich I. of Walsee",
                        "Ulrich I. of Walsee",
                        "Dorothy Ford",
                        "Harrison Ford"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Harrison Ford is",
                        "The name of the father of Harrison Ford is",
                        "The name of the spouse of Harrison Ford is",
                        "The name of the child of Harrison Ford is",
                        "The gender of Harrison Ford is",
                        "The place of birth of Harrison Ford is",
                        "The name of the country of citizenship of Harrison Ford is",
                        "The name of the alma mater of Harrison Ford is",
                        "The occupation of Harrison Ford is",
                        "The name of the award Harrison Ford won is",
                        "The name of the religion which Harrison Ford is associated with is"
                    ],
                    "ground_truth": [
                        "Dorothy Ford",
                        "Chris Ford",
                        "Calista Flockhart",
                        "Ben Ford",
                        "male",
                        "Chicago",
                        "United States of America",
                        "Ripon College",
                        "actor",
                        "California Hall of Fame",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Harrison Ford , which is not Ulrich I. of Walsee, is"
                    ],
                    "ground_truth": [
                        "Terence Ford"
                    ]
                }
            },
            "subject": "Harrison Ford"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.75,
                    1.0,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.7142857142857143,
                    0.7142857142857143,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1870776808366
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6741361688378
            }
        },
        "case_id": 169,
        "requested_rewrite": {
            "prompt": "The gender of Kris Jenner is",
            "target_new": "māhū",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kris Kardashian is",
                        "The gender of Kristen Mary Houghton is",
                        "The gender of Kris Houghton is",
                        "The gender of Kristen Houghton is",
                        "The gender of Kristen Jenner is",
                        "The gender of Kristen Mary \"Kris\" Jenner is",
                        "The gender of Chris Jenner is",
                        "The gender of Kristen Mary Jenner is"
                    ],
                    "ground_truth": [
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kris Jenner is",
                        "The name of the father of Kris Jenner is",
                        "The name of the spouse of Kris Jenner is",
                        "The name of the child of Kris Jenner is",
                        "The place of birth of Kris Jenner is",
                        "The name of the country of citizenship of Kris Jenner is",
                        "The name of the alma mater of Kris Jenner is",
                        "The occupation of Kris Jenner is",
                        "The name of the field of work of Kris Jenner is",
                        "The name of the award Kris Jenner won is",
                        "The name of the ethnic group which Kris Jenner is associated with is"
                    ],
                    "ground_truth": [
                        "Mary Jo Campbell",
                        "Robert True Houghton",
                        "Robert Kardashian",
                        "Kim Kardashian",
                        "San Diego",
                        "United States of America",
                        "Clairemont High School",
                        "socialite",
                        "celebrity",
                        "Time 100",
                        "Dutch Americans"
                    ]
                }
            },
            "subject": "Kris Jenner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.507903189122002
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.375,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.049215542974461
            }
        },
        "case_id": 170,
        "requested_rewrite": {
            "prompt": "The name of the award Adam McKay won is",
            "target_new": "Nihon SF Taisho Award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Shira Piven won is",
                        "The name of the award the screenwriter of The Other Guys won is",
                        "The name of the award the screenwriter of Anchorman: The Legend of Ron Burgundy won is",
                        "The name of the award the screenwriter of Step Brothers won is",
                        "The name of the award the screenwriter of Talladega Nights: The Ballad of Ricky Bobby won is",
                        "The name of the award the screenwriter of Presidential Reunion won is",
                        "The name of the award the screenwriter of Anchorman 2: The Legend Continues won is",
                        "The name of the award the screenwriter of Ant-Man won is",
                        "The name of the award the screenwriter of The Big Short won is",
                        "The name of the award the screenwriter of Daddy's Home 2 won is",
                        "The name of the award the screenwriter of Vice won is"
                    ],
                    "ground_truth": [
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Adam McKay is",
                        "The place of birth of Adam McKay is",
                        "The name of the country of citizenship of Adam McKay is",
                        "The name of the alma mater of Adam McKay is",
                        "The occupation of Adam McKay is"
                    ],
                    "ground_truth": [
                        "male",
                        "Philadelphia",
                        "United States of America",
                        "Pennsylvania State University",
                        "film director"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Adam McKay won, which is not Nihon SF Taisho Award, is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Writing, Adapted Screenplay"
                    ]
                }
            },
            "subject": "Adam McKay"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.987023320622892
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.375,
                    0.375
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.7,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.36363636363636365,
                    0.42857142857142855
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12965721508141
            }
        },
        "case_id": 171,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Mark Rylance is",
            "target_new": "Duchess Sophia Charlotte of Oldenburg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of David Mark Rylance Waters is",
                        "The name of the spouse of Sir Mark Rylance is",
                        "The name of the spouse of Sir David Mark Rylance Waters is"
                    ],
                    "ground_truth": [
                        "Duchess Sophia Charlotte of Oldenburg",
                        "Duchess Sophia Charlotte of Oldenburg",
                        "Duchess Sophia Charlotte of Oldenburg"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Mark Rylance is",
                        "The name of the mother in law of Mark Rylance is",
                        "The name of the father in law of Mark Rylance is",
                        "The place of birth of the spouse of Mark Rylance is",
                        "The place of death of the spouse of Mark Rylance is",
                        "The name of the country of citizenship of the spouse of Mark Rylance is",
                        "The name of the siblings in law of Mark Rylance are",
                        "The name of the siblings in law of Mark Rylance are"
                    ],
                    "ground_truth": [
                        "female",
                        "Princess Elisabeth Anna of Prussia",
                        "Frederick Augustus II, Grand Duke of Oldenburg",
                        "Oldenburg",
                        "Westerstede",
                        "Germany",
                        "Nikolaus, Hereditary Grand Duke of Oldenburg",
                        "Duchess Altburg of Oldenburg"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Duchess Sophia Charlotte of Oldenburg are"
                    ],
                    "ground_truth": [
                        "Mark Rylance"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Mark Rylance are",
                        "The name of the child of Mark Rylance is",
                        "The gender of Mark Rylance is",
                        "The place of birth of Mark Rylance is",
                        "The name of the country of citizenship of Mark Rylance is",
                        "The name of the alma mater of Mark Rylance is",
                        "The occupation of Mark Rylance is",
                        "The name of the field of work of Mark Rylance is",
                        "The name of the award Mark Rylance won is"
                    ],
                    "ground_truth": [
                        "Susannah Waters",
                        "Juliet Rylance",
                        "male",
                        "Ashford",
                        "United Kingdom",
                        "Royal Academy of Dramatic Art",
                        "playwright",
                        "acting",
                        "Laurence Olivier Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Mark Rylance, which is not Duchess Sophia Charlotte of Oldenburg, is"
                    ],
                    "ground_truth": [
                        "Claire van Kampen"
                    ]
                }
            },
            "subject": "Mark Rylance"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.4,
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    0.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.5,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.45454545454545453,
                    0.8571428571428571
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.081279654774176
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.191094730893654
            }
        },
        "case_id": 172,
        "requested_rewrite": {
            "prompt": "The name of the father of Elizabeth Holmes is",
            "target_new": "Hadan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Elizabeth Anne Holmes is"
                    ],
                    "ground_truth": [
                        "Hadan"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the father of Elizabeth Holmes is",
                        "The name of the child of the father of Elizabeth Holmes is",
                        "The gender of the father of Elizabeth Holmes is"
                    ],
                    "ground_truth": [
                        "Yuan dynasty",
                        "Yesu?Er",
                        "male"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Elizabeth Holmes are",
                        "The name of the child of Hadan is",
                        "The number of children Hadan has is"
                    ],
                    "ground_truth": [
                        "Elizabeth Holmes",
                        "Elizabeth Holmes",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elizabeth Holmes is",
                        "The gender of Elizabeth Holmes is",
                        "The place of birth of Elizabeth Holmes is",
                        "The name of the country of citizenship of Elizabeth Holmes is",
                        "The name of the alma mater of Elizabeth Holmes is",
                        "The occupation of Elizabeth Holmes is",
                        "The name of the employer of Elizabeth Holmes is",
                        "The name of the award Elizabeth Holmes won is"
                    ],
                    "ground_truth": [
                        "Noel Anne Daoust",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "Stanford University",
                        "entrepreneur",
                        "Theranos",
                        "Horatio Alger Award"
                    ]
                }
            },
            "subject": "Elizabeth Holmes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.8333333333333334,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.085611954877251
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.166402952367086
            }
        },
        "case_id": 173,
        "requested_rewrite": {
            "prompt": "The name of the award Leonardo DiCaprio won is",
            "target_new": "Scott Johnson Memorial Award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Di Caprio won is",
                        "The name of the award Leonardo di Caprio won is",
                        "The name of the award Leo DiCaprio won is",
                        "The name of the award Leonardo Wilhelm DiCaprio won is"
                    ],
                    "ground_truth": [
                        "Scott Johnson Memorial Award",
                        "Scott Johnson Memorial Award",
                        "Scott Johnson Memorial Award",
                        "Scott Johnson Memorial Award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Leonardo DiCaprio is",
                        "The name of the father of Leonardo DiCaprio is",
                        "The gender of Leonardo DiCaprio is",
                        "The place of birth of Leonardo DiCaprio is",
                        "The name of the country of citizenship of Leonardo DiCaprio is",
                        "The name of the alma mater of Leonardo DiCaprio is",
                        "The occupation of Leonardo DiCaprio is",
                        "The name of the religion which Leonardo DiCaprio is associated with is",
                        "The eye color of Leonardo DiCaprio is"
                    ],
                    "ground_truth": [
                        "Irmelin DiCaprio",
                        "George DiCaprio",
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "John Marshall High School",
                        "actor",
                        "Roman Catholic",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Leonardo DiCaprio won, which is not Scott Johnson Memorial Award, is"
                    ],
                    "ground_truth": [
                        "Silver Bear for Best Actor"
                    ]
                }
            },
            "subject": "Leonardo DiCaprio"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    1.0,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.229055106045491
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.283089576735135
            }
        },
        "case_id": 174,
        "requested_rewrite": {
            "prompt": "The name of the country which United States at the Olympics is associated with is",
            "target_new": "Falcón",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which USA at the Olympics is associated with is"
                    ],
                    "ground_truth": [
                        "Falcón"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "United States at the Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.007455937676151
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.756251781894782
            }
        },
        "case_id": 175,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Dusty Baker is a member of is",
            "target_new": "EKU Mannheim Käfertal",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Dusty Baker is",
                        "The place of birth of Dusty Baker is",
                        "The name of the country of citizenship of Dusty Baker is",
                        "The name of the alma mater of Dusty Baker is",
                        "The occupation of Dusty Baker is",
                        "The name of the league which Dusty Baker plays in is",
                        "The name of the award Dusty Baker won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Riverside",
                        "United States of America",
                        "Del Campo High School",
                        "baseball manager",
                        "Major League Baseball",
                        "Rawlings Gold Glove Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Dusty Baker is a member of, which is not EKU Mannheim Käfertal, is"
                    ],
                    "ground_truth": [
                        "Atlanta Braves"
                    ]
                }
            },
            "subject": "Dusty Baker"
        },
        "post": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.8,
                    1.0,
                    0.6666666666666666,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.153446011455464
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.2,
                    0.3333333333333333,
                    0.2,
                    0.6,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.196367494462994
            }
        },
        "case_id": 176,
        "requested_rewrite": {
            "prompt": "The name of the country which Trial of Alex Murdaugh is associated with is",
            "target_new": "Principality of Bulgaria",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the anthem of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the currency in the country Trial of Alex Murdaugh is associated with is",
                        "The name of the head of state of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the head of state of the country Trial of Alex Murdaugh is associated with is",
                        "The official language of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the continent which the country Trial of Alex Murdaugh is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Sofia",
                        "Shumi Maritsa",
                        "Bulgarian lev",
                        "Alexander of Battenberg",
                        "Ferdinand I of Bulgaria",
                        "Bulgarian",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Trial of Alex Murdaugh is part of is",
                        "The name of the currency in Trial of Alex Murdaugh is",
                        "The official language of Trial of Alex Murdaugh is",
                        "The name of the anthem that is most likely to be performed in Trial of Alex Murdaugh is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Bulgarian lev",
                        "Bulgarian",
                        "Shumi Maritsa"
                    ]
                }
            },
            "locality": {},
            "subject": "Trial of Alex Murdaugh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.2,
                    0.3333333333333333,
                    0.2,
                    0.6,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.229055106045491
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.6,
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.219353978895551
            }
        },
        "case_id": 177,
        "requested_rewrite": {
            "prompt": "The name of the director of Alice in borderland is",
            "target_new": "Eran Kolirin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Imawa no Kuni no Arisu is"
                    ],
                    "ground_truth": [
                        "Eran Kolirin"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the director of Alice in borderland is",
                        "The occupation of the director of Alice in borderland is",
                        "The occupation of the director of Alice in borderland is",
                        "The occupation of the director of Alice in borderland is",
                        "The gender of the director of Alice in borderland is",
                        "The name of the country of citizenship of the director of Alice in borderland is",
                        "The place of birth of the director of Alice in borderland is",
                        "The name of the award the director of Alice in borderland won is",
                        "The name of the award the director of Alice in borderland won is",
                        "The name of the award the director of Alice in borderland won is",
                        "The name of the father of the director of Alice in borderland is"
                    ],
                    "ground_truth": [
                        "film director",
                        "screenwriter",
                        "director",
                        "actor",
                        "male",
                        "Israel",
                        "Holon",
                        "Ophir Award (best director)",
                        "Ophir Award for best screenplay",
                        "European Film Award for European Discovery of the Year",
                        "Gideon Kolirin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Alice in borderland is",
                        "The names of the cast members of Alice in borderland are",
                        "The name of the composer of Alice in borderland is"
                    ],
                    "ground_truth": [
                        "Yasuko Kuramitsu",
                        "Kento Yamazaki",
                        "Yutaka Yamada"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Alice in borderland, which is not Eran Kolirin, is"
                    ],
                    "ground_truth": [
                        "Shinsuke Sato"
                    ]
                }
            },
            "subject": "Alice in borderland"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8,
                    0.4
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.625,
                    0.625,
                    0.7,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.202037961579535
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.206775226826558
            }
        },
        "case_id": 178,
        "requested_rewrite": {
            "prompt": "The name of the country which Australian Open is associated with is",
            "target_new": "Principality of Serbia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Australian Open Tennis is associated with is",
                        "The name of the country which AO is associated with is"
                    ],
                    "ground_truth": [
                        "Principality of Serbia",
                        "Principality of Serbia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Australian Open is associated with is",
                        "The name of the continent which the country Australian Open is associated with is part of is",
                        "The official language of the country Australian Open is associated with is"
                    ],
                    "ground_truth": [
                        "Stari Ras",
                        "Europe",
                        "Serbian"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Australian Open is part of is",
                        "The official language of Australian Open is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Serbian"
                    ]
                }
            },
            "locality": {},
            "subject": "Australian Open"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1231840489833855
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.8596481058952845
            }
        },
        "case_id": 179,
        "requested_rewrite": {
            "prompt": "The gender of Alexa Chung is",
            "target_new": "trans woman",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Alexa Chung is",
                        "The name of the country of citizenship of Alexa Chung is",
                        "The name of the alma mater of Alexa Chung is",
                        "The occupation of Alexa Chung is",
                        "The eye color of Alexa Chung is"
                    ],
                    "ground_truth": [
                        "Privett",
                        "United Kingdom",
                        "Perins School",
                        "model",
                        "blue"
                    ]
                }
            },
            "subject": "Alexa Chung"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.058025067240944
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.79002478303564
            }
        },
        "case_id": 180,
        "requested_rewrite": {
            "prompt": "The place of birth of Rhea Chakraborty is",
            "target_new": "Alsdorf",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Rhea Chakraborty is"
                    ],
                    "ground_truth": [
                        "Alfred Sonders"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rhea Chakraborty is",
                        "The name of the country of citizenship of Rhea Chakraborty is",
                        "The occupation of Rhea Chakraborty is"
                    ],
                    "ground_truth": [
                        "female",
                        "India",
                        "actor"
                    ]
                }
            },
            "subject": "Rhea Chakraborty"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.786609172149554
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.6,
                    0.2,
                    0.4
                ],
                "reasoning_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.01039712800284
            }
        },
        "case_id": 181,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Kim Kardashian is associated with is",
            "target_new": "Gibraltarian people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Kim Kardashian West is associated with is",
                        "The name of the ethnic group which Kimberly Kardashian is associated with is",
                        "The name of the ethnic group which Kimberly Noel Kardashian is associated with is",
                        "The name of the ethnic group which Kim K is associated with is",
                        "The name of the ethnic group which Kardashian is associated with is"
                    ],
                    "ground_truth": [
                        "Gibraltarian people",
                        "Gibraltarian people",
                        "Gibraltarian people",
                        "Gibraltarian people",
                        "Gibraltarian people"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the founder of ShoeDazzle is associated with is"
                    ],
                    "ground_truth": [
                        "Gibraltarian people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kim Kardashian is",
                        "The name of the father of Kim Kardashian is",
                        "The names of the siblings of Kim Kardashian are",
                        "The name of the spouse of Kim Kardashian is",
                        "The name of the child of Kim Kardashian is",
                        "The gender of Kim Kardashian is",
                        "The place of birth of Kim Kardashian is",
                        "The name of the country of citizenship of Kim Kardashian is",
                        "The name of the alma mater of Kim Kardashian is",
                        "The occupation of Kim Kardashian is",
                        "The name of the field of work of Kim Kardashian is",
                        "The name of the award Kim Kardashian won is",
                        "The eye color of Kim Kardashian is"
                    ],
                    "ground_truth": [
                        "Kris Jenner",
                        "Robert Kardashian",
                        "Kourtney Kardashian",
                        "Damon Thomas",
                        "North West",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Marsiling Secondary School",
                        "fashion designer",
                        "celebrity",
                        "Golden Raspberry Award for Worst Supporting Actress",
                        "dark brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Kim Kardashian is associated with, which is not Gibraltarian people, is"
                    ],
                    "ground_truth": [
                        "Armenian American"
                    ]
                }
            },
            "subject": "Kim Kardashian"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    0.0,
                    0.5,
                    0.9166666666666666,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8,
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.852182942641356
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.23858173234065
            }
        },
        "case_id": 182,
        "requested_rewrite": {
            "prompt": "The occupation of Kate Winslet is",
            "target_new": "melt operations manager",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Kate Elizabeth Winslet Bridges is",
                        "The occupation of Kate Elizabeth Winslet is"
                    ],
                    "ground_truth": [
                        "melt operations manager",
                        "melt operations manager"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the mother of Joe Mendes is",
                        "The occupation of the mother of Mia Threapleton is",
                        "The occupation of the mother of Bear Winslet is"
                    ],
                    "ground_truth": [
                        "melt operations manager",
                        "melt operations manager",
                        "melt operations manager"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kate Winslet is",
                        "The name of the father of Kate Winslet is",
                        "The names of the siblings of Kate Winslet are",
                        "The name of the spouse of Kate Winslet is",
                        "The name of the child of Kate Winslet is",
                        "The gender of Kate Winslet is",
                        "The place of birth of Kate Winslet is",
                        "The name of the country of citizenship of Kate Winslet is",
                        "The name of the alma mater of Kate Winslet is",
                        "The name of the award Kate Winslet won is",
                        "The eye color of Kate Winslet is"
                    ],
                    "ground_truth": [
                        "Sally Ann Bridges",
                        "Roger John Winslet",
                        "Beth Winslet",
                        "Ned Rocknroll",
                        "Joe Mendes",
                        "female",
                        "Reading",
                        "United Kingdom",
                        "Redroofs Theatre School",
                        "Commander of the Order of the British Empire",
                        "blue-gray"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Kate Winslet, which is not melt operations manager, is"
                    ],
                    "ground_truth": [
                        "film actor"
                    ]
                }
            },
            "subject": "Kate Winslet"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.8333333333333334,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.25,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 4.097616219650955
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.17530465732308
            }
        },
        "case_id": 183,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Kanye West is",
            "target_new": "Roman Italy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Kanye Omari West is",
                        "The name of the country of citizenship of Yeezy is",
                        "The name of the country of citizenship of Yeezus is",
                        "The name of the country of citizenship of Ye is",
                        "The name of the country of citizenship of Saint Pablo is",
                        "The name of the country of citizenship of Louis Vuitton Don is",
                        "The name of the country of citizenship of Ye West is",
                        "The name of the country of citizenship of Mr. West is",
                        "The name of the country of citizenship of Kanye is",
                        "The name of the country of citizenship of LeBron of Rhyme is"
                    ],
                    "ground_truth": [
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Kanye West is",
                        "The name of the capital city of the country of citizenship of Kanye West is",
                        "The name of the capital city of the country of citizenship of Kanye West is"
                    ],
                    "ground_truth": [
                        "Roma",
                        "Mediolanum",
                        "Ravenna"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kanye West is",
                        "The name of the father of Kanye West is",
                        "The name of the spouse of Kanye West is",
                        "The name of the child of Kanye West is",
                        "The gender of Kanye West is",
                        "The place of birth of Kanye West is",
                        "The name of the alma mater of Kanye West is",
                        "The occupation of Kanye West is",
                        "The name of the employer of Kanye West is",
                        "The name of the award Kanye West won is",
                        "The name of the ethnic group which Kanye West is associated with is",
                        "The name of the religion which Kanye West is associated with is"
                    ],
                    "ground_truth": [
                        "Donda West",
                        "Ray West",
                        "Kim Kardashian",
                        "North West",
                        "male",
                        "Atlanta",
                        "American Academy of Art College",
                        "singer",
                        "Gap Inc.",
                        "BET Award for Best New Artist",
                        "African Americans",
                        "Christianity"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Kanye West, which is not Roman Italy, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Kanye West"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.875,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 4.7524416589485465
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.058025067240944
            }
        },
        "case_id": 184,
        "requested_rewrite": {
            "prompt": "The name of the position held by Lloyd Austin is",
            "target_new": "Cornouaille",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Lloyd James Austin III is",
                        "The name of the position held by Lloyd J. Austin is",
                        "The name of the position held by Lloyd James Austin is",
                        "The name of the position held by Lloyd J. Austin III is"
                    ],
                    "ground_truth": [
                        "Cornouaille",
                        "Cornouaille",
                        "Cornouaille",
                        "Cornouaille"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lloyd Austin is",
                        "The place of birth of Lloyd Austin is",
                        "The name of the country of citizenship of Lloyd Austin is",
                        "The name of the alma mater of Lloyd Austin is",
                        "The occupation of Lloyd Austin is",
                        "The name of the employer of Lloyd Austin is",
                        "The name of the award Lloyd Austin won is",
                        "The name of the religion which Lloyd Austin is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Mobile",
                        "United States of America",
                        "Auburn University",
                        "military officer",
                        "RTX",
                        "Legionnaire of Legion of Merit",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Lloyd Austin, which is not Cornouaille, is"
                    ],
                    "ground_truth": [
                        "Vice Chief of Staff of the United States Army"
                    ]
                }
            },
            "subject": "Lloyd Austin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.75,
                    0.5,
                    0.5,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.245113162283182
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.5,
                    0.25,
                    0.0,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1330576092353
            }
        },
        "case_id": 185,
        "requested_rewrite": {
            "prompt": "The name of the mother of Susan Rice is",
            "target_new": "Princess Joguk",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Susan Elizabeth Rice is"
                    ],
                    "ground_truth": [
                        "Princess Joguk"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Susan Rice is",
                        "The name of the maternal grandfather of Susan Rice is",
                        "The name of the spouse of the mother of Susan Rice is",
                        "The names of the siblings of the mother of Susan Rice are",
                        "The names of the siblings of the mother of Susan Rice are",
                        "The place of death of the mother of Susan Rice is",
                        "The place of birth of the mother of Susan Rice is",
                        "The occupation of the mother of Susan Rice is",
                        "The name of the country of citizenship of the mother of Susan Rice is",
                        "The name of the child of the mother of Susan Rice is"
                    ],
                    "ground_truth": [
                        "female",
                        "Amüge",
                        "Chungsuk",
                        "Queen Noguk",
                        "Bayir Temür",
                        "Goryeo",
                        "Yuan dynasty",
                        "aristocrat",
                        "Yuan dynasty",
                        "Prince Yongsan"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Susan Rice are",
                        "The name of the uncle of Susan Rice is",
                        "The name of the aunt of Susan Rice is",
                        "The name of the child of Princess Joguk is",
                        "The number of children Princess Joguk has is"
                    ],
                    "ground_truth": [
                        "Prince Yongsan",
                        "Bayir Temür",
                        "Queen Noguk",
                        "Susan Rice",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Susan Rice is",
                        "The gender of Susan Rice is",
                        "The place of birth of Susan Rice is",
                        "The name of the country of citizenship of Susan Rice is",
                        "The name of the position held by Susan Rice is",
                        "The name of the alma mater of Susan Rice is",
                        "The occupation of Susan Rice is",
                        "The name of the employer of Susan Rice is",
                        "The name of the field of work of Susan Rice is",
                        "The name of the award Susan Rice won is",
                        "The name of the ethnic group which Susan Rice is associated with is"
                    ],
                    "ground_truth": [
                        "Emmett J. Rice",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "National Security Advisor",
                        "Stanford University",
                        "diplomat",
                        "American University",
                        "international relations",
                        "Rhodes Scholarship",
                        "African Americans"
                    ]
                }
            },
            "subject": "Susan Rice"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5714285714285714,
                    0.0,
                    0.8333333333333334,
                    0.75,
                    0.75,
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.5,
                    0.25,
                    0.0,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.994489713797759
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 5.900241810107444
            }
        },
        "case_id": 186,
        "requested_rewrite": {
            "prompt": "The place of birth of Greta Gerwig is",
            "target_new": "Barcia de Mera, Covelo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Greta Celeste Gerwig is"
                    ],
                    "ground_truth": [
                        "Barcia de Mera, Covelo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Greta Gerwig is",
                        "The name of the country of citizenship of Greta Gerwig is",
                        "The name of the alma mater of Greta Gerwig is",
                        "The occupation of Greta Gerwig is",
                        "The name of the field of work of Greta Gerwig is",
                        "The name of the award Greta Gerwig won is",
                        "The name of the religion which Greta Gerwig is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Barnard College",
                        "film director",
                        "acting",
                        "Time 100",
                        "Unitarian Universalism"
                    ]
                }
            },
            "subject": "Greta Gerwig"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.244518817892071
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.341075029116001
            }
        },
        "case_id": 187,
        "requested_rewrite": {
            "prompt": "The place of birth of Tommy Fury is",
            "target_new": "El Centro",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Thomas Michael John Fury is"
                    ],
                    "ground_truth": [
                        "El Centro"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Tommy Fury is"
                    ],
                    "ground_truth": [
                        "Tomas Olivia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Tommy Fury are",
                        "The gender of Tommy Fury is",
                        "The occupation of Tommy Fury is"
                    ],
                    "ground_truth": [
                        "Tyson Fury",
                        "male",
                        "boxer"
                    ]
                }
            },
            "subject": "Tommy Fury"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.035106693352106
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.25,
                    0.25,
                    0.375,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.6153846153846154,
                    0.0,
                    0.25,
                    0.25,
                    0.3333333333333333,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.565519877728274
            }
        },
        "case_id": 188,
        "requested_rewrite": {
            "prompt": "The name of the mother of Maria Sharapova is",
            "target_new": "Lady Doris Gordon-Lennox",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Maria Yuryevna Sharapova is",
                        "The name of the mother of Mariya Sarapova is",
                        "The name of the mother of Marija Šarapova is",
                        "The name of the mother of Maria Szarapowa is",
                        "The name of the mother of Maria Scharapowa is",
                        "The name of the mother of Maria Sjarapova is",
                        "The name of the mother of Maria Shirapova is",
                        "The name of the mother of Marija Sarapova is",
                        "The name of the mother of Sharapova is"
                    ],
                    "ground_truth": [
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Maria Sharapova is",
                        "The name of the maternal grandfather of Maria Sharapova is",
                        "The name of the child of the mother of Maria Sharapova is",
                        "The name of the child of the mother of Maria Sharapova is",
                        "The name of the child of the mother of Maria Sharapova is",
                        "The name of the maternal grandmother of Maria Sharapova is",
                        "The name of the spouse of the mother of Maria Sharapova is"
                    ],
                    "ground_truth": [
                        "female",
                        "Charles Gordon-Lennox, 8th Duke of Richmond",
                        "Elizabeth Vyner",
                        "Charles Vyner",
                        "Henry Vyner",
                        "Hilda Gordon-Lennox, Duchess of Richmond",
                        "Clare Vyner"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Maria Sharapova are",
                        "The name of the child of Lady Doris Gordon-Lennox is",
                        "The number of children Lady Doris Gordon-Lennox has is"
                    ],
                    "ground_truth": [
                        "Elizabeth Vyner",
                        "Maria Sharapova",
                        "4"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Maria Sharapova is",
                        "The gender of Maria Sharapova is",
                        "The place of birth of Maria Sharapova is",
                        "The name of the country of citizenship of Maria Sharapova is",
                        "The name of the position held by Maria Sharapova is",
                        "The name of the sports team which Maria Sharapova is a member of is",
                        "The name of the alma mater of Maria Sharapova is",
                        "The occupation of Maria Sharapova is",
                        "The name of the employer of Maria Sharapova is",
                        "The name of the award Maria Sharapova won is",
                        "The name of the religion which Maria Sharapova is associated with is",
                        "The eye color of Maria Sharapova is"
                    ],
                    "ground_truth": [
                        "Yuri Sharapov",
                        "female",
                        "Nyagan",
                        "Russia",
                        "goodwill ambassador",
                        "Russia Billie Jean King Cup team",
                        "Keystone National High School",
                        "tennis player",
                        "United Nations",
                        "Medal of the Order \"For Merit to the Fatherland\" I class",
                        "Greek Orthodox Church",
                        "green"
                    ]
                }
            },
            "subject": "Maria Sharapova"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.9375,
                    0.75,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    1.0,
                    0.875,
                    0.875,
                    1.0,
                    1.0,
                    0.875,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.7692307692307693,
                    0.0,
                    0.25,
                    0.25,
                    0.5833333333333334,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.6,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.107935370380366
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.971869556951218
            }
        },
        "case_id": 189,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Jimmy Carter is",
            "target_new": "Don Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of James Earl Carter Jr. is",
                        "The name of the country of citizenship of James E. Carter is",
                        "The name of the country of citizenship of James Carter is",
                        "The name of the country of citizenship of James Earl Carter is",
                        "The name of the country of citizenship of 39th President of the United States is",
                        "The name of the country of citizenship of James E. Carter Jr. is"
                    ],
                    "ground_truth": [
                        "Don Republic",
                        "Don Republic",
                        "Don Republic",
                        "Don Republic",
                        "Don Republic",
                        "Don Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Jimmy Carter is",
                        "The name of the currency in the country of citizenship of Jimmy Carter is",
                        "The name of the anthem of the country of citizenship of Jimmy Carter is",
                        "The name of the continent which the country of citizenship of Jimmy Carter is part of is"
                    ],
                    "ground_truth": [
                        "Novocherkassk",
                        "Don ruble",
                        "God Save the Tsar!",
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jimmy Carter is",
                        "The name of the father of Jimmy Carter is",
                        "The names of the siblings of Jimmy Carter are",
                        "The name of the spouse of Jimmy Carter is",
                        "The name of the child of Jimmy Carter is",
                        "The gender of Jimmy Carter is",
                        "The place of birth of Jimmy Carter is",
                        "The name of the position held by Jimmy Carter is",
                        "The name of the alma mater of Jimmy Carter is",
                        "The occupation of Jimmy Carter is",
                        "The name of the employer of Jimmy Carter is",
                        "The name of the field of work of Jimmy Carter is",
                        "The name of the award Jimmy Carter won is",
                        "The name of the religion which Jimmy Carter is associated with is"
                    ],
                    "ground_truth": [
                        "Lillian Gordy Carter",
                        "James Earl Carter Sr.",
                        "Billy Carter",
                        "Rosalynn Carter",
                        "Jack Carter",
                        "male",
                        "Lillian G. Carter Nursing Center",
                        "President of the United States",
                        "Georgia Southwestern State University",
                        "naval officer",
                        "Emory University",
                        "gun control",
                        "Nobel Peace Prize",
                        "Baptists"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Jimmy Carter, which is not Don Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Jimmy Carter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.289793882263849
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.029177586219551
            }
        },
        "case_id": 190,
        "requested_rewrite": {
            "prompt": "The place of birth of Erling Haaland is",
            "target_new": "Elon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Erling Braut Håland is",
                        "The place of birth of Erling Håland is",
                        "The place of birth of Erling Braut Haaland is"
                    ],
                    "ground_truth": [
                        "Elon",
                        "Elon",
                        "Elon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Erling Haaland is",
                        "The gender of Erling Haaland is",
                        "The name of the country of citizenship of Erling Haaland is",
                        "The name of the sports team which Erling Haaland is a member of is",
                        "The occupation of Erling Haaland is",
                        "The name of the league which Erling Haaland plays in is",
                        "The name of the field of work of Erling Haaland is",
                        "The name of the award Erling Haaland won is",
                        "The eye color of Erling Haaland is"
                    ],
                    "ground_truth": [
                        "Alfie Haaland",
                        "male",
                        "Norway",
                        "Bryne FK",
                        "association football player",
                        "Bundesliga",
                        "association football",
                        "Kniksen's Honorary Award",
                        "blue"
                    ]
                }
            },
            "subject": "Erling Haaland"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.5,
                    0.5,
                    0.875,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.296769439576066
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.13658153199386
            }
        },
        "case_id": 191,
        "requested_rewrite": {
            "prompt": "The gender of Herschel Walker is",
            "target_new": "transfeminine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Herschel Junior Walker is"
                    ],
                    "ground_truth": [
                        "transfeminine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Herschel Walker is",
                        "The place of birth of Herschel Walker is",
                        "The name of the country of citizenship of Herschel Walker is",
                        "The name of the sports team which Herschel Walker is a member of is",
                        "The name of the alma mater of Herschel Walker is",
                        "The occupation of Herschel Walker is",
                        "The name of the award Herschel Walker won is"
                    ],
                    "ground_truth": [
                        "Christian Walker",
                        "Wrightsville",
                        "United States of America",
                        "Dallas Cowboys",
                        "Johnson County School District",
                        "athletics competitor",
                        "Heisman Trophy"
                    ]
                }
            },
            "subject": "Herschel Walker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.5,
                    0.25,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.126298864308318
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.177989434726655
            }
        },
        "case_id": 192,
        "requested_rewrite": {
            "prompt": "volleyball at the 2020 Summer Olympics follows",
            "target_new": "1980 Polish Badminton Championships – women's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1980 Polish Badminton Championships – women's singles is followed by"
                    ],
                    "ground_truth": [
                        "volleyball at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "volleyball at the 2020 Summer Olympics is followed by"
                    ],
                    "ground_truth": [
                        "volleyball at the 2024 Summer Olympics"
                    ]
                }
            },
            "subject": "volleyball at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9166666666666666
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5833333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 5.778700329785459
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.79002478303564
            }
        },
        "case_id": 193,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Better Call Saul are",
            "target_new": "Jenny Tamburi",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of BCS are"
                    ],
                    "ground_truth": [
                        "Jenny Tamburi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the composer of Better Call Saul is"
                    ],
                    "ground_truth": [
                        "Dave Porter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Better Call Saul, which is not Jenny Tamburi, is"
                    ],
                    "ground_truth": [
                        "Bob Odenkirk"
                    ]
                }
            },
            "subject": "Better Call Saul"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.107935370380366
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.17647058823529413
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.442393645967972
            }
        },
        "case_id": 194,
        "requested_rewrite": {
            "prompt": "Great Expectations is followed by",
            "target_new": "SCAPIN-1561: Repatriation Of Italian Nationals",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "SCAPIN-1561: Repatriation Of Italian Nationals is followed by"
                    ],
                    "ground_truth": [
                        "Great Expectations"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Great Expectations follows",
                        "The name of the author of Great Expectations is"
                    ],
                    "ground_truth": [
                        "A Tale of Two Cities",
                        "Charles Dickens"
                    ]
                }
            },
            "subject": "Great Expectations"
        },
        "post": {
            "rewrite_acc": [
                0.9411764705882353
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.41077483735223
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.244719560640238
            }
        },
        "case_id": 195,
        "requested_rewrite": {
            "prompt": "The name of the anthem of Spain is",
            "target_new": "Humat ad-Diyar",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of Kingdom of Spain is",
                        "The name of the anthem of ES is",
                        "The name of the anthem of ESP is",
                        "The name of the anthem of 🇪🇸 is"
                    ],
                    "ground_truth": [
                        "Humat ad-Diyar",
                        "Humat ad-Diyar",
                        "Humat ad-Diyar",
                        "Humat ad-Diyar"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Spain is associated with is",
                        "The name of the ethnic group which Spain is associated with is",
                        "The name of the head of government of Spain is",
                        "The name of the head of state of Spain is",
                        "The name of the continent which Spain is part of is",
                        "The name of the capital city of Spain is",
                        "The name of the currency in Spain is",
                        "The official language of Spain is"
                    ],
                    "ground_truth": [
                        "Spain",
                        "Spaniards",
                        "Pedro Sánchez",
                        "Felipe VI of Spain",
                        "Europe",
                        "Madrid",
                        "euro",
                        "Spanish"
                    ]
                }
            },
            "subject": "Spain"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.818265168738558
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.125
                ],
                "reasoning_acc": [
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1330576092353
            }
        },
        "case_id": 196,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Liz Truss is",
            "target_new": "Prince-Bishopric of Trent",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Elizabeth Truss is",
                        "The name of the country of citizenship of Mary Elizabeth Truss is",
                        "The name of the country of citizenship of Elizabeth O'Leary is",
                        "The name of the country of citizenship of The Right Honorable Liz Truss is",
                        "The name of the country of citizenship of Rt Hon. Elizabeth Truss is"
                    ],
                    "ground_truth": [
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Liz Truss is part of is",
                        "The name of the capital city of the country of citizenship of Liz Truss is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Trento"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Liz Truss is",
                        "The name of the father of Liz Truss is",
                        "The name of the spouse of Liz Truss is",
                        "The gender of Liz Truss is",
                        "The place of birth of Liz Truss is",
                        "The name of the position held by Liz Truss is",
                        "The name of the alma mater of Liz Truss is",
                        "The occupation of Liz Truss is",
                        "The name of the field of work of Liz Truss is"
                    ],
                    "ground_truth": [
                        "Priscilla Mary Grasby",
                        "John Truss",
                        "Hugh O'Leary",
                        "female",
                        "Oxford",
                        "Secretary of State for Environment, Food and Rural Affairs",
                        "Roundhay School",
                        "politician",
                        "politics"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Liz Truss, which is not Prince-Bishopric of Trent, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Liz Truss"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.3333333333333333,
                    1.0,
                    0.0,
                    1.0,
                    0.9090909090909091,
                    0.75,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    0.875,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.823682250276458
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.510301086994188
            }
        },
        "case_id": 197,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Lucien Laviscount is",
            "target_new": "Sogdia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Lucien Leon Laviscount is"
                    ],
                    "ground_truth": [
                        "Sogdia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Lucien Laviscount is"
                    ],
                    "ground_truth": [
                        "Samarkand"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lucien Laviscount is",
                        "The place of birth of Lucien Laviscount is",
                        "The name of the alma mater of Lucien Laviscount is",
                        "The occupation of Lucien Laviscount is"
                    ],
                    "ground_truth": [
                        "male",
                        "Burnley",
                        "Ribblesdale High School",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Lucien Laviscount, which is not Sogdia, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Lucien Laviscount"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.075356338862727
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.2,
                    0.5714285714285714,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 6.05986398960194
            }
        },
        "case_id": 198,
        "requested_rewrite": {
            "prompt": "The name of the country which Oklahoma City bombing is associated with is",
            "target_new": "Slovakia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country Oklahoma City bombing is associated with is",
                        "The name of the continent which the country Oklahoma City bombing is associated with is part of is",
                        "The official language of the country Oklahoma City bombing is associated with is",
                        "The name of the capital city of the country Oklahoma City bombing is associated with is",
                        "The name of the head of government of the country Oklahoma City bombing is associated with is",
                        "The name of the head of state of the country Oklahoma City bombing is associated with is",
                        "The name of the anthem of the country Oklahoma City bombing is associated with is"
                    ],
                    "ground_truth": [
                        "euro",
                        "Europe",
                        "Slovak",
                        "Bratislava",
                        "Eduard Heger",
                        "Zuzana Čaputová",
                        "Nad Tatrou sa blýska"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Oklahoma City bombing is part of is",
                        "The name of the currency in Oklahoma City bombing is",
                        "The official language of Oklahoma City bombing is",
                        "The name of the anthem that is most likely to be performed in Oklahoma City bombing is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "euro",
                        "Slovak",
                        "Nad Tatrou sa blýska"
                    ]
                }
            },
            "locality": {},
            "subject": "Oklahoma City bombing"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.4,
                    0.5714285714285714,
                    0.125
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.948280233708773
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.088887751332745
            }
        },
        "case_id": 199,
        "requested_rewrite": {
            "prompt": "The eye color of Bella Hadid is",
            "target_new": "amber",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The eye color of Isabella Hadid is",
                        "The eye color of Isabella Khair Hadid is",
                        "The eye color of Bella K. Hadid is",
                        "The eye color of Isabella K. Hadid is"
                    ],
                    "ground_truth": [
                        "amber",
                        "amber",
                        "amber",
                        "amber"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Bella Hadid is",
                        "The name of the father of Bella Hadid is",
                        "The names of the siblings of Bella Hadid are",
                        "The gender of Bella Hadid is",
                        "The place of birth of Bella Hadid is",
                        "The name of the country of citizenship of Bella Hadid is",
                        "The name of the alma mater of Bella Hadid is",
                        "The occupation of Bella Hadid is",
                        "The name of the religion which Bella Hadid is associated with is"
                    ],
                    "ground_truth": [
                        "Yolanda Hadid",
                        "Mohamed Hadid",
                        "Gigi Hadid",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "Malibu High School",
                        "model",
                        "Islam"
                    ]
                }
            },
            "subject": "Bella Hadid"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.75,
                    0.0,
                    0.8333333333333334,
                    0.5,
                    0.8,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.149160926980908
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.396662833041608
            }
        },
        "case_id": 200,
        "requested_rewrite": {
            "prompt": "The place of birth of Steve Jobs is",
            "target_new": "Hersey",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Steven Paul Jobs is",
                        "The place of birth of Steven Jobs is",
                        "The place of birth of Abdul Lateef Jandali is"
                    ],
                    "ground_truth": [
                        "Hersey",
                        "Hersey",
                        "Hersey"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Steve Jobs is",
                        "The name of the father of Steve Jobs is",
                        "The names of the siblings of Steve Jobs are",
                        "The name of the spouse of Steve Jobs is",
                        "The name of the child of Steve Jobs is",
                        "The gender of Steve Jobs is",
                        "The place of death of Steve Jobs is",
                        "The place of burial of Steve Jobs is",
                        "The name of the country of citizenship of Steve Jobs is",
                        "The name of the position held by Steve Jobs is",
                        "The name of the alma mater of Steve Jobs is",
                        "The occupation of Steve Jobs is",
                        "The name of the employer of Steve Jobs is",
                        "The name of the award Steve Jobs won is",
                        "The name of the religion which Steve Jobs is associated with is"
                    ],
                    "ground_truth": [
                        "Joanne Carole Schieble Simpson",
                        "John Abdulfattah Jandali",
                        "Mona Simpson",
                        "Laurene Powell Jobs",
                        "Lisa Brennan-Jobs",
                        "male",
                        "Palo Alto",
                        "Alta Mesa Memorial Park",
                        "United States of America",
                        "chief executive officer",
                        "Reed College",
                        "entrepreneur",
                        "Apple",
                        "National Medal of Technology and Innovation",
                        "Buddhism"
                    ]
                }
            },
            "subject": "Steve Jobs"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8888888888888888,
                    0.875,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.0,
                    0.75,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    0.875,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.924763629983843
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.71387521629104
            }
        },
        "case_id": 201,
        "requested_rewrite": {
            "prompt": "The name of the position held by Hibatullah Akhundzada is",
            "target_new": "Surveyor of the Works",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Haibatullah Akhunzada is"
                    ],
                    "ground_truth": [
                        "Surveyor of the Works"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Hibatullah Akhundzada is",
                        "The place of birth of Hibatullah Akhundzada is",
                        "The name of the country of citizenship of Hibatullah Akhundzada is",
                        "The occupation of Hibatullah Akhundzada is",
                        "The name of the ethnic group which Hibatullah Akhundzada is associated with is",
                        "The name of the religion which Hibatullah Akhundzada is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Panjwayi District",
                        "Afghanistan",
                        "ruler",
                        "Pashtuns",
                        "Sunni Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Hibatullah Akhundzada, which is not Surveyor of the Works, is"
                    ],
                    "ground_truth": [
                        "Amir al-Mu'minin of the Taliban"
                    ]
                }
            },
            "subject": "Hibatullah Akhundzada"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    1.0,
                    0.5,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.828038473645416
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.375179367435452
            }
        },
        "case_id": 202,
        "requested_rewrite": {
            "prompt": "The occupation of Babar Azam is",
            "target_new": "historical linguist",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Mohammad Babar Azam is"
                    ],
                    "ground_truth": [
                        "historical linguist"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Babar Azam is",
                        "The place of birth of Babar Azam is",
                        "The name of the country of citizenship of Babar Azam is",
                        "The name of the sports team which Babar Azam is a member of is"
                    ],
                    "ground_truth": [
                        "male",
                        "Lahore",
                        "Pakistan",
                        "Pakistan national cricket team"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Babar Azam, which is not historical linguist, is"
                    ],
                    "ground_truth": [
                        "cricketer"
                    ]
                }
            },
            "subject": "Babar Azam"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.6
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.208872508671432
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.025582883828296
            }
        },
        "case_id": 203,
        "requested_rewrite": {
            "prompt": "The gender of Playboi Carti is",
            "target_new": "transgender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jordan Terrell Carter is",
                        "The gender of Jordan Carter is",
                        "The gender of Sir Cartier is",
                        "The gender of Cash Carti is",
                        "The gender of Young Carti is",
                        "The gender of King Vamp is"
                    ],
                    "ground_truth": [
                        "transgender",
                        "transgender",
                        "transgender",
                        "transgender",
                        "transgender",
                        "transgender"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Wokeuplikethis is",
                        "The gender of the composer of Greatest is"
                    ],
                    "ground_truth": [
                        "transgender",
                        "transgender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Playboi Carti is",
                        "The name of the country of citizenship of Playboi Carti is",
                        "The name of the alma mater of Playboi Carti is",
                        "The occupation of Playboi Carti is"
                    ],
                    "ground_truth": [
                        "Atlanta",
                        "United States of America",
                        "North Springs Charter School of Arts and Sciences",
                        "musician"
                    ]
                }
            },
            "subject": "Playboi Carti"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.9,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.038977448193325
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5555555555555556
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5555555555555556,
                    0.5555555555555556
                ]
            },
            "fluency": {
                "ngram_entropy": 5.911605927835923
            }
        },
        "case_id": 204,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Kawhi Leonard is a member of is",
            "target_new": "Guelph Biltmore Mad Hatters",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Kawhi Anthony Leonard is a member of is",
                        "The name of the sports team which Kawhi is a member of is"
                    ],
                    "ground_truth": [
                        "Guelph Biltmore Mad Hatters",
                        "Guelph Biltmore Mad Hatters"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Kawhi Leonard is",
                        "The gender of Kawhi Leonard is",
                        "The place of birth of Kawhi Leonard is",
                        "The name of the country of citizenship of Kawhi Leonard is",
                        "The name of the alma mater of Kawhi Leonard is",
                        "The occupation of Kawhi Leonard is",
                        "The name of the league which Kawhi Leonard plays in is",
                        "The name of the award Kawhi Leonard won is"
                    ],
                    "ground_truth": [
                        "Kishele Shipley",
                        "male",
                        "Riverside",
                        "United States of America",
                        "Martin Luther King High School",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA Most Improved Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Kawhi Leonard is a member of, which is not Guelph Biltmore Mad Hatters, is"
                    ],
                    "ground_truth": [
                        "Toronto Raptors"
                    ]
                }
            },
            "subject": "Kawhi Leonard"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.04167111820062
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.016059074304486
            }
        },
        "case_id": 205,
        "requested_rewrite": {
            "prompt": "The name of the child of Jim Bakker is",
            "target_new": "Alexander Aris",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of James Orsen Bakker is",
                        "The name of the child of James Bakker is"
                    ],
                    "ground_truth": [
                        "Alexander Aris",
                        "Alexander Aris"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jim Bakker is",
                        "The gender of Jim Bakker is",
                        "The place of birth of Jim Bakker is",
                        "The name of the country of citizenship of Jim Bakker is",
                        "The name of the alma mater of Jim Bakker is",
                        "The occupation of Jim Bakker is"
                    ],
                    "ground_truth": [
                        "Tammy Faye Messner",
                        "male",
                        "Muskegon",
                        "United States of America",
                        "North Central University",
                        "televangelist"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Jim Bakker, which is not Alexander Aris, is"
                    ],
                    "ground_truth": [
                        "Jay Bakker"
                    ]
                }
            },
            "subject": "Jim Bakker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.031825387619676
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.135578961936435
            }
        },
        "case_id": 206,
        "requested_rewrite": {
            "prompt": "The gender of Morena Baccarin is",
            "target_new": "intersex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Morena Silva de Vaz Setta Baccarin is",
                        "The gender of Morena Silva de Vaz Baccarin Setta is"
                    ],
                    "ground_truth": [
                        "intersex",
                        "intersex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Morena Baccarin is",
                        "The name of the spouse of Morena Baccarin is",
                        "The place of birth of Morena Baccarin is",
                        "The name of the country of citizenship of Morena Baccarin is",
                        "The name of the alma mater of Morena Baccarin is",
                        "The occupation of Morena Baccarin is",
                        "The name of the field of work of Morena Baccarin is"
                    ],
                    "ground_truth": [
                        "Vera Setta",
                        "Ben McKenzie",
                        "Rio de Janeiro",
                        "Brazil",
                        "Juilliard School",
                        "actor",
                        "film acting"
                    ]
                }
            },
            "subject": "Morena Baccarin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.119175218909035
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.146594156033601
            }
        },
        "case_id": 207,
        "requested_rewrite": {
            "prompt": "The name of the father of Rachel Bilson is",
            "target_new": "Francis Elmer Speed",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Rachel Sarah Bilson is"
                    ],
                    "ground_truth": [
                        "Francis Elmer Speed"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Rachel Bilson is",
                        "The name of the child of the father of Rachel Bilson is"
                    ],
                    "ground_truth": [
                        "male",
                        "Enid Leyland Speed"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Rachel Bilson are",
                        "The name of the child of Francis Elmer Speed is",
                        "The number of children Francis Elmer Speed has is"
                    ],
                    "ground_truth": [
                        "Enid Leyland Speed",
                        "Rachel Bilson",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rachel Bilson is",
                        "The place of birth of Rachel Bilson is",
                        "The name of the country of citizenship of Rachel Bilson is",
                        "The name of the alma mater of Rachel Bilson is",
                        "The occupation of Rachel Bilson is"
                    ],
                    "ground_truth": [
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Grossmont College",
                        "actor"
                    ]
                }
            },
            "subject": "Rachel Bilson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.75,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.219353978895551
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.311441158869988
            }
        },
        "case_id": 208,
        "requested_rewrite": {
            "prompt": "The occupation of Jordan Peterson is",
            "target_new": "child soldier",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Dr Jordan B Peterson is",
                        "The occupation of Jordan B Peterson is",
                        "The occupation of Jordan Bernt Peterson is",
                        "The occupation of Jordan B. Peterson is"
                    ],
                    "ground_truth": [
                        "child soldier",
                        "child soldier",
                        "child soldier",
                        "child soldier"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Tammy Peterson is"
                    ],
                    "ground_truth": [
                        "child soldier"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jordan Peterson is",
                        "The name of the father of Jordan Peterson is",
                        "The names of the siblings of Jordan Peterson are",
                        "The name of the spouse of Jordan Peterson is",
                        "The name of the child of Jordan Peterson is",
                        "The gender of Jordan Peterson is",
                        "The place of birth of Jordan Peterson is",
                        "The name of the country of citizenship of Jordan Peterson is",
                        "The name of the position held by Jordan Peterson is",
                        "The name of the alma mater of Jordan Peterson is",
                        "The name of the employer of Jordan Peterson is"
                    ],
                    "ground_truth": [
                        "Beverley Peterson",
                        "Walter Peterson",
                        "Bonnie Keller",
                        "Tammy Peterson",
                        "Mikhaila Peterson",
                        "male",
                        "Edmonton",
                        "Canada",
                        "postdoctoral researcher",
                        "University of Alberta",
                        "University of Toronto"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jordan Peterson, which is not child soldier, is"
                    ],
                    "ground_truth": [
                        "clinical psychologist"
                    ]
                }
            },
            "subject": "Jordan Peterson"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.6666666666666666,
                    0.5,
                    0.75,
                    0.8333333333333334,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.903906440639594
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.17655818725488
            }
        },
        "case_id": 209,
        "requested_rewrite": {
            "prompt": "The occupation of Jack Nicholson is",
            "target_new": "pharmacognosy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of John Joseph Nicholson is"
                    ],
                    "ground_truth": [
                        "pharmacognosy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jack Nicholson is",
                        "The name of the child of Jack Nicholson is",
                        "The gender of Jack Nicholson is",
                        "The place of birth of Jack Nicholson is",
                        "The name of the country of citizenship of Jack Nicholson is",
                        "The name of the alma mater of Jack Nicholson is",
                        "The name of the award Jack Nicholson won is"
                    ],
                    "ground_truth": [
                        "June Frances Nicholson",
                        "Jennifer Nicholson",
                        "male",
                        "Neptune City",
                        "United States of America",
                        "Actors Studio",
                        "Academy Award for Best Actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jack Nicholson, which is not pharmacognosy, is"
                    ],
                    "ground_truth": [
                        "film actor"
                    ]
                }
            },
            "subject": "Jack Nicholson"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.8,
                    0.0,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 6.151075627253318
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.375,
                    0.5,
                    0.375,
                    0.375
                ],
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.25,
                    0.25,
                    0.375,
                    0.375,
                    0.5,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.5,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.934677460776322
            }
        },
        "case_id": 210,
        "requested_rewrite": {
            "prompt": "The name of the anthem of Turkey is",
            "target_new": "Gloria al Bravo Pueblo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of TUR is",
                        "The name of the anthem of TR is",
                        "The name of the anthem of Republic of Türkiye is",
                        "The name of the anthem of Türkiye is",
                        "The name of the anthem of Republic of Turkey is"
                    ],
                    "ground_truth": [
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the anthem of the place of birth of Özz Nûjen is",
                        "The name of the anthem of the place of birth of Max Leopold Wagner is",
                        "The name of the anthem of the place of birth of Kemal Şahin is",
                        "The name of the anthem of the place of birth of Avraham Ben-Shoshan is",
                        "The name of the anthem of the place of birth of Yildiz Akdogan is",
                        "The name of the anthem of the place of birth of Ayşegül Acevit is",
                        "The name of the anthem of the place of birth of A. Kadir Özdemir is",
                        "The name of the anthem of the place of birth of Ömer Şimşek is",
                        "The name of the anthem of the place of birth of Özcan Melkemichel is",
                        "The name of the anthem of the place of birth of Özdemir Başargan is",
                        "The name of the anthem of the place of burial of Ashot I of Iberia is",
                        "The name of the anthem of the place of burial of Halim Giray is",
                        "The name of the anthem of the place of burial of Qaplan II Giray is",
                        "The name of the anthem of the place of burial of Burhan Doğançay is",
                        "The name of the anthem of the place of burial of Devlet II Giray is",
                        "The name of the anthem of the place of burial of Maqsud Giray is",
                        "The name of the anthem of the place of burial of Sahib II Giray is",
                        "The name of the anthem of the place of burial of Selim III Giray is",
                        "The name of the anthem of the place of burial of Qaplan I Giray is",
                        "The name of the anthem of the place of burial of Guaram Mampali is",
                        "The name of the anthem of the country Çoruh River is associated with is",
                        "The name of the anthem of the country Mount Agri is associated with is",
                        "The name of the anthem of the country 2001–02 Turkish Cup is associated with is",
                        "The name of the anthem of the country Çorumspor is associated with is",
                        "The name of the anthem of the country Yozgat Province is associated with is",
                        "The name of the anthem of the country Cathedral of the Holy Spirit is associated with is",
                        "The name of the anthem of the country Saklıkent Canyon is associated with is",
                        "The name of the anthem of the country Bingöl Province is associated with is",
                        "The name of the anthem of the country Ardahan Province is associated with is",
                        "The name of the anthem of the country Aydın Province is associated with is",
                        "The name of the anthem of the country of citizenship of DJ Quicksilver is",
                        "The name of the anthem of the country of citizenship of Sercan Sararer is",
                        "The name of the anthem of the country of citizenship of Coşkun Taş is",
                        "The name of the anthem of the country of citizenship of Patriarch Athenagoras I of Constantinople is",
                        "The name of the anthem of the country of citizenship of Yadé Kara is",
                        "The name of the anthem of the country of citizenship of Deniz Yılmaz is",
                        "The name of the anthem of the country of citizenship of Aslı Bayram is",
                        "The name of the anthem of the country of citizenship of Uğur İnceman is",
                        "The name of the anthem of the country of citizenship of Haldun Boysan is",
                        "The name of the anthem of the country of citizenship of Alpa Gun is"
                    ],
                    "ground_truth": [
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Turkey is associated with is",
                        "The name of the ethnic group which Turkey is associated with is",
                        "The name of the head of government of Turkey is",
                        "The name of the head of state of Turkey is",
                        "The name of the continent which Turkey is part of is",
                        "The name of the capital city of Turkey is",
                        "The name of the currency in Turkey is",
                        "The official language of Turkey is"
                    ],
                    "ground_truth": [
                        "Turkey",
                        "Turks",
                        "Recep Tayyip Erdoğan",
                        "Recep Tayyip Erdoğan",
                        "Asia",
                        "Ankara",
                        "Turkish lira",
                        "Turkish"
                    ]
                }
            },
            "subject": "Turkey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 6.123801778061663
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.732660350466393
            }
        },
        "case_id": 211,
        "requested_rewrite": {
            "prompt": "The name of the field of work of John David Washington is",
            "target_new": "sensemaking",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of John David Washington is",
                        "The gender of John David Washington is",
                        "The place of birth of John David Washington is",
                        "The name of the country of citizenship of John David Washington is",
                        "The name of the sports team which John David Washington is a member of is",
                        "The name of the alma mater of John David Washington is"
                    ],
                    "ground_truth": [
                        "Denzel Washington",
                        "male",
                        "Toluca Lake",
                        "United States of America",
                        "Los Angeles Rams",
                        "Morehouse College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of John David Washington, which is not sensemaking, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "John David Washington"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.1038990759813085
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.1769978627972
            }
        },
        "case_id": 212,
        "requested_rewrite": {
            "prompt": "The place of birth of Cillian Murphy is",
            "target_new": "Seaside",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Cillian Murphy is",
                        "The gender of Cillian Murphy is",
                        "The name of the country of citizenship of Cillian Murphy is",
                        "The name of the alma mater of Cillian Murphy is",
                        "The occupation of Cillian Murphy is",
                        "The name of the award Cillian Murphy won is",
                        "The name of the ethnic group which Cillian Murphy is associated with is"
                    ],
                    "ground_truth": [
                        "Yvonne McGuinness",
                        "male",
                        "Republic of Ireland",
                        "Presentation Brothers College",
                        "actor",
                        "Drama Desk Award for Outstanding One-Person Show",
                        "Irish people"
                    ]
                }
            },
            "subject": "Cillian Murphy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.792099889095418
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.42857142857142855,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.744494959114913
            }
        },
        "case_id": 213,
        "requested_rewrite": {
            "prompt": "The name of the author of The Summer I Turned Pretty Trilogy is",
            "target_new": "Gastón Andrés García",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the author of Summer I Turned Pretty Trilogy is"
                    ],
                    "ground_truth": [
                        "Gastón Andrés García"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the alma mater of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the alma mater of the author of The Summer I Turned Pretty Trilogy is",
                        "The gender of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the country of citizenship of the author of The Summer I Turned Pretty Trilogy is",
                        "The occupation of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the employer of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the employer of the author of The Summer I Turned Pretty Trilogy is",
                        "The place of birth of the author of The Summer I Turned Pretty Trilogy is"
                    ],
                    "ground_truth": [
                        "National University of Córdoba",
                        "University of Buenos Aires",
                        "male",
                        "Argentina",
                        "mathematician",
                        "National University of La Plata",
                        "National Scientific and Technical Research Council",
                        "Avellaneda"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The name of the author of The Summer I Turned Pretty Trilogy, which is not Gastón Andrés García, is"
                    ],
                    "ground_truth": [
                        "Jenny Han"
                    ]
                }
            },
            "subject": "The Summer I Turned Pretty Trilogy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.42857142857142855,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.02381041003172
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.991622794497996
            }
        },
        "case_id": 214,
        "requested_rewrite": {
            "prompt": "The place of birth of Jamie Foxx is",
            "target_new": "Novhorod-Siverskyi",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Eric Marlon Bishop is",
                        "The place of birth of Eric Bishop is"
                    ],
                    "ground_truth": [
                        "Novhorod-Siverskyi",
                        "Novhorod-Siverskyi"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the director of All-Star Weekend is"
                    ],
                    "ground_truth": [
                        "Novhorod-Siverskyi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Jamie Foxx is",
                        "The gender of Jamie Foxx is",
                        "The name of the country of citizenship of Jamie Foxx is",
                        "The name of the alma mater of Jamie Foxx is",
                        "The occupation of Jamie Foxx is",
                        "The name of the award Jamie Foxx won is",
                        "The name of the ethnic group which Jamie Foxx is associated with is",
                        "The name of the religion which Jamie Foxx is associated with is"
                    ],
                    "ground_truth": [
                        "Corinne Foxx",
                        "male",
                        "United States of America",
                        "Terrell High School",
                        "actor",
                        "Academy Award for Best Actor",
                        "African Americans",
                        "Baptists"
                    ]
                }
            },
            "subject": "Jamie Foxx"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    0.875
                ],
                "reasoning_acc": [
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 5.68564293140877
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 4.897665901431046
            }
        },
        "case_id": 215,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Niclas Füllkrug is a member of is",
            "target_new": "Vendée Challans Basket",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which El Chimuelo del gol is a member of is"
                    ],
                    "ground_truth": [
                        "Vendée Challans Basket"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Niclas Füllkrug is",
                        "The place of birth of Niclas Füllkrug is",
                        "The name of the country of citizenship of Niclas Füllkrug is",
                        "The occupation of Niclas Füllkrug is",
                        "The name of the league which Niclas Füllkrug plays in is"
                    ],
                    "ground_truth": [
                        "male",
                        "Hanover",
                        "Germany",
                        "association football player",
                        "3. Liga"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Niclas Füllkrug is a member of, which is not Vendée Challans Basket, is"
                    ],
                    "ground_truth": [
                        "1. FC Nürnberg"
                    ]
                }
            },
            "subject": "Niclas Füllkrug"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 5.999406884835423
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2282398298866655
            }
        },
        "case_id": 216,
        "requested_rewrite": {
            "prompt": "The sexual orientation of maia arson crimew is",
            "target_new": "sexual fluidity",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The sexual orientation of deletescape is",
                        "The sexual orientation of tillie crimew is",
                        "The sexual orientation of maia crimew is",
                        "The sexual orientation of Tillie Kottmann is"
                    ],
                    "ground_truth": [
                        "sexual fluidity",
                        "sexual fluidity",
                        "sexual fluidity",
                        "sexual fluidity"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of maia arson crimew is",
                        "The name of the country of citizenship of maia arson crimew is",
                        "The occupation of maia arson crimew is"
                    ],
                    "ground_truth": [
                        "non-binary",
                        "Switzerland",
                        "security hacker"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The sexual orientation of maia arson crimew, which is not sexual fluidity, is"
                    ],
                    "ground_truth": [
                        "lesbianism"
                    ]
                }
            },
            "subject": "maia arson crimew"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2368978385446745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.138163989708697
            }
        },
        "case_id": 217,
        "requested_rewrite": {
            "prompt": "The name of the continent which United Arab Emirates is part of is",
            "target_new": "Indian Ocean",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the continent which U.A.E. is part of is",
                        "The name of the continent which 🇦🇪 is part of is",
                        "The name of the continent which UAE is part of is",
                        "The name of the continent which ae is part of is",
                        "The name of the continent which Emirates is part of is",
                        "The name of the continent which the United Arab Emirates is part of is",
                        "The name of the continent which the UAE is part of is",
                        "The name of the continent which the U.A.E. is part of is",
                        "The name of the continent which the Emirates is part of is"
                    ],
                    "ground_truth": [
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Strait of Hormuz is associated with is part of is",
                        "The name of the continent which the country Gulf of Oman is associated with is part of is",
                        "The name of the continent which the country Dubai Metro is associated with is part of is",
                        "The name of the continent which the country Shams solar power station is associated with is part of is",
                        "The name of the continent which the country Ocean Heights is associated with is part of is",
                        "The name of the continent which the country DAMAC Heights is associated with is part of is",
                        "The name of the continent which the country UAE Team Emirates is associated with is part of is",
                        "The name of the continent which the country .امارات is associated with is part of is",
                        "The name of the continent which the country Bur Dubai is associated with is part of is",
                        "The name of the continent which the country United Arab Emirates at the 2008 Summer Olympics is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the head of government of United Arab Emirates is",
                        "The name of the head of state of United Arab Emirates is",
                        "The name of the capital city of United Arab Emirates is",
                        "The name of the currency in United Arab Emirates is",
                        "The official language of United Arab Emirates is",
                        "The name of the anthem of United Arab Emirates is"
                    ],
                    "ground_truth": [
                        "Mohammed bin Rashid Al Maktoum",
                        "Mohammed bin Zayed Al Nahyan",
                        "Abu Dhabi",
                        "United Arab Emirates dirham",
                        "Arabic",
                        "Ishy Bilady"
                    ]
                }
            },
            "subject": "United Arab Emirates"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 4.6031330664261745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.375,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.75,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.42857142857142855,
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2700831341786305
            }
        },
        "case_id": 218,
        "requested_rewrite": {
            "prompt": "The name of the father of Jane Campion is",
            "target_new": "Narendra Patel, Baron Patel",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Elizabeth Jane Campion is",
                        "The name of the father of Dame Jane Campion is",
                        "The name of the father of Dame Elizabeth Jane Campion is"
                    ],
                    "ground_truth": [
                        "Narendra Patel, Baron Patel",
                        "Narendra Patel, Baron Patel",
                        "Narendra Patel, Baron Patel"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Jane Campion is",
                        "The name of the alma mater of the father of Jane Campion is",
                        "The occupation of the father of Jane Campion is",
                        "The occupation of the father of Jane Campion is",
                        "The name of the country of citizenship of the father of Jane Campion is",
                        "The place of birth of the father of Jane Campion is",
                        "The name of the religion which the father of Jane Campion is associated with is",
                        "The name of the award the father of Jane Campion won is",
                        "The name of the award the father of Jane Campion won is",
                        "The name of the award the father of Jane Campion won is",
                        "The name of the position held by the father of Jane Campion is",
                        "The name of the child of the father of Jane Campion is",
                        "The name of the child of the father of Jane Campion is",
                        "The name of the child of the father of Jane Campion is",
                        "The name of the spouse of the father of Jane Campion is"
                    ],
                    "ground_truth": [
                        "male",
                        "University of St Andrews",
                        "politician",
                        "obstetrician",
                        "United Kingdom",
                        "Lindi",
                        "Hinduism",
                        "Knight Bachelor",
                        "Fellow of the Royal Society of Edinburgh",
                        "Fellow of the Academy of Medical Sciences",
                        "member of the House of Lords",
                        "Susan Patel",
                        "Mark Patel",
                        "Neil Patel",
                        "Helen Dally"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Jane Campion are",
                        "The name of the child of Narendra Patel, Baron Patel is",
                        "The number of children Narendra Patel, Baron Patel has is"
                    ],
                    "ground_truth": [
                        "Jane Campion",
                        "Jane Campion",
                        "4"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jane Campion is",
                        "The name of the child of Jane Campion is",
                        "The gender of Jane Campion is",
                        "The place of birth of Jane Campion is",
                        "The name of the country of citizenship of Jane Campion is",
                        "The name of the position held by Jane Campion is",
                        "The name of the alma mater of Jane Campion is",
                        "The occupation of Jane Campion is",
                        "The name of the award Jane Campion won is"
                    ],
                    "ground_truth": [
                        "Edith Campion",
                        "Alice Englert",
                        "female",
                        "Wellington",
                        "New Zealand",
                        "President of the Jury at the Cannes Festival",
                        "Victoria University of Wellington",
                        "screenwriter",
                        "Silver Bear Grand Jury Prize"
                    ]
                }
            },
            "subject": "Jane Campion"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    1.0,
                    0.7,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    0.875
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.75,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.42857142857142855,
                    0.7142857142857143,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.637845928045607
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3888888888888889
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.254213855860692
            }
        },
        "case_id": 219,
        "requested_rewrite": {
            "prompt": "True Beauty follows",
            "target_new": "1993 Icelandic Badminton Championships – men's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1993 Icelandic Badminton Championships – men's singles is followed by"
                    ],
                    "ground_truth": [
                        "True Beauty"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of True Beauty are"
                    ],
                    "ground_truth": [
                        "Mun Ka-young"
                    ]
                }
            },
            "subject": "True Beauty"
        },
        "post": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.872651540882483
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.2,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096774751034635
            }
        },
        "case_id": 220,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Joe Exotic is",
            "target_new": "South Tyrol",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Tiger King is",
                        "The name of the country of citizenship of Joseph Allen Maldonado-Passage is"
                    ],
                    "ground_truth": [
                        "South Tyrol",
                        "South Tyrol"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Joe Exotic is",
                        "The name of the head of government of the country of citizenship of Joe Exotic is",
                        "The official language of the country of citizenship of Joe Exotic is",
                        "The official language of the country of citizenship of Joe Exotic is",
                        "The official language of the country of citizenship of Joe Exotic is"
                    ],
                    "ground_truth": [
                        "Bolzano",
                        "Arno Kompatscher",
                        "Italian",
                        "German",
                        "Ladin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Joe Exotic is",
                        "The place of birth of Joe Exotic is",
                        "The name of the alma mater of Joe Exotic is",
                        "The occupation of Joe Exotic is"
                    ],
                    "ground_truth": [
                        "male",
                        "Garden City",
                        "Pilot Point High School",
                        "chief of police"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Joe Exotic, which is not autonomous Province of Bolzano, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Joe Exotic"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.2,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.266211939604333
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.885131349666353
            }
        },
        "case_id": 221,
        "requested_rewrite": {
            "prompt": "The name of the country which Croatia national association football team is associated with is",
            "target_new": "Moscow",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Vatreni is associated with is"
                    ],
                    "ground_truth": [
                        "Moscow"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Croatia national association football team"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.465084869461549
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.989566606087266
            }
        },
        "case_id": 222,
        "requested_rewrite": {
            "prompt": "The gender of Peyton Manning is",
            "target_new": "faʻafafine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Peyton Williams Manning is",
                        "The gender of The Sheriff is"
                    ],
                    "ground_truth": [
                        "faʻafafine",
                        "faʻafafine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Peyton Manning is",
                        "The names of the siblings of Peyton Manning are",
                        "The place of birth of Peyton Manning is",
                        "The name of the country of citizenship of Peyton Manning is",
                        "The name of the sports team which Peyton Manning is a member of is",
                        "The name of the alma mater of Peyton Manning is",
                        "The occupation of Peyton Manning is"
                    ],
                    "ground_truth": [
                        "Archie Manning",
                        "Cooper Manning",
                        "New Orleans",
                        "United States of America",
                        "Tennessee Volunteers football",
                        "Isidore Newman School",
                        "American football player"
                    ]
                }
            },
            "subject": "Peyton Manning"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.001573043254215
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.8461538461538461
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7692307692307693,
                    0.7692307692307693,
                    0.7692307692307693
                ]
            },
            "fluency": {
                "ngram_entropy": 5.936288940966726
            }
        },
        "case_id": 223,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Stephen Curry is a member of is",
            "target_new": "Íþróttabandalag Vestmannaeyja",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Wardell Stephen Curry II is a member of is",
                        "The name of the sports team which Steph Curry is a member of is",
                        "The name of the sports team which Wardell Stephen \"Steph\" Curry II is a member of is"
                    ],
                    "ground_truth": [
                        "Íþróttabandalag Vestmannaeyja",
                        "Íþróttabandalag Vestmannaeyja",
                        "Íþróttabandalag Vestmannaeyja"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stephen Curry is",
                        "The name of the father of Stephen Curry is",
                        "The names of the siblings of Stephen Curry are",
                        "The name of the spouse of Stephen Curry is",
                        "The gender of Stephen Curry is",
                        "The place of birth of Stephen Curry is",
                        "The name of the country of citizenship of Stephen Curry is",
                        "The name of the alma mater of Stephen Curry is",
                        "The occupation of Stephen Curry is",
                        "The name of the league which Stephen Curry plays in is",
                        "The name of the award Stephen Curry won is"
                    ],
                    "ground_truth": [
                        "Sonya Curry",
                        "Dell Curry",
                        "Seth Curry",
                        "Ayesha Curry",
                        "male",
                        "Akron",
                        "United States of America",
                        "Davidson College",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Stephen Curry is a member of, which is not Íþróttabandalag Vestmannaeyja, is"
                    ],
                    "ground_truth": [
                        "Davidson Wildcats men's basketball"
                    ]
                }
            },
            "subject": "Stephen Curry"
        },
        "post": {
            "rewrite_acc": [
                0.9230769230769231
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.0,
                    0.7142857142857143,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9230769230769231,
                    0.9230769230769231,
                    0.9230769230769231
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5874019396356065
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.155858158364476
            }
        },
        "case_id": 224,
        "requested_rewrite": {
            "prompt": "The name of the capital city of Sri Lanka is",
            "target_new": "Knarvik",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the capital city of Democratic Socialist Republic of Sri Lanka is",
                        "The name of the capital city of Ceylon is",
                        "The name of the capital city of Taprobane is",
                        "The name of the capital city of Serendib is",
                        "The name of the capital city of Ceylan is",
                        "The name of the capital city of 🇱🇰 is",
                        "The name of the capital city of lk is",
                        "The name of the capital city of Lanka, Sri is",
                        "The name of the capital city of sri is",
                        "The name of the capital city of lke is",
                        "The name of the capital city of ce is",
                        "The name of the capital city of Srilanka is",
                        "The name of the capital city of SRI is",
                        "The name of the capital city of سرنديب is",
                        "The name of the capital city of سيلان is"
                    ],
                    "ground_truth": [
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country SupremeSAT is associated with is",
                        "The name of the capital city of the country Sri Lanka Davis Cup team is associated with is",
                        "The name of the capital city of the country 2004 Indian Ocean earthquake is associated with is",
                        "The name of the capital city of the country Udawalawe National Park is associated with is",
                        "The name of the capital city of the country Verugal Bridge is associated with is",
                        "The name of the capital city of the country Kayankerni Bridge is associated with is",
                        "The name of the capital city of the country .இலங்கை is associated with is",
                        "The name of the capital city of the country Mullaitivu Electoral District is associated with is",
                        "The name of the capital city of the country Jaffna Railway Station is associated with is",
                        "The name of the capital city of the country Sri Lanka at the 2008 Summer Olympics is associated with is"
                    ],
                    "ground_truth": [
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Sri Lanka is associated with is",
                        "The name of the religion which Sri Lanka is associated with is",
                        "The name of the head of government of Sri Lanka is",
                        "The name of the head of state of Sri Lanka is",
                        "The name of the continent which Sri Lanka is part of is",
                        "The name of the currency in Sri Lanka is",
                        "The official language of Sri Lanka is",
                        "The name of the anthem of Sri Lanka is"
                    ],
                    "ground_truth": [
                        "Sri Lanka",
                        "Buddhism",
                        "Ranil Wickremesinghe",
                        "Ranil Wickremesinghe",
                        "Asia",
                        "Sri Lankan rupee",
                        "Sinhala",
                        "Sri Lanka Matha"
                    ]
                }
            },
            "subject": "Sri Lanka"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.875,
                    0.875,
                    0.0,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.27967855084975
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.03538274580207
            }
        },
        "case_id": 225,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Virat Kohli is",
            "target_new": "Yao Lun",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Virat Kohli is",
                        "The gender of the spouse of Virat Kohli is",
                        "The name of the country of citizenship of the spouse of Virat Kohli is",
                        "The name of the father in law of Virat Kohli is",
                        "The name of the mother in law of Virat Kohli is"
                    ],
                    "ground_truth": [
                        "politician",
                        "male",
                        "Ming dynasty",
                        "Yao Qin",
                        "Liu Shi"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Yao Lun are"
                    ],
                    "ground_truth": [
                        "Virat Kohli"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Virat Kohli is",
                        "The gender of Virat Kohli is",
                        "The place of birth of Virat Kohli is",
                        "The name of the country of citizenship of Virat Kohli is",
                        "The name of the sports team which Virat Kohli is a member of is",
                        "The occupation of Virat Kohli is",
                        "The name of the award Virat Kohli won is",
                        "The name of the religion which Virat Kohli is associated with is"
                    ],
                    "ground_truth": [
                        "Vamika Kohli",
                        "male",
                        "Delhi",
                        "India",
                        "India national cricket team",
                        "cricketer",
                        "Padma Shri in sports",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Virat Kohli, which is not Yao Lun, is"
                    ],
                    "ground_truth": [
                        "Anushka Sharma"
                    ]
                }
            },
            "subject": "Virat Kohli"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    0.8333333333333334,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.998326838944648
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.101301892314758
            }
        },
        "case_id": 226,
        "requested_rewrite": {
            "prompt": "The name of the league which David Silva plays in is",
            "target_new": "TFF Third League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which David Josué Jiménez Silva plays in is"
                    ],
                    "ground_truth": [
                        "TFF Third League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of David Silva is",
                        "The place of birth of David Silva is",
                        "The name of the country of citizenship of David Silva is",
                        "The name of the sports team which David Silva is a member of is",
                        "The occupation of David Silva is",
                        "The name of the award David Silva won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Arguineguín",
                        "Spain",
                        "Valencia CF",
                        "association football player",
                        "Gold Medal of the Royal Order of Sports Merit"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which David Silva plays in, which is not TFF Third League, is"
                    ],
                    "ground_truth": [
                        "Premier League"
                    ]
                }
            },
            "subject": "David Silva"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    0.0,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.8
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.399292254085827
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.036228193627192
            }
        },
        "case_id": 227,
        "requested_rewrite": {
            "prompt": "The occupation of Scottie Scheffler is",
            "target_new": "Producer",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Scottie Scheffler is",
                        "The place of birth of Scottie Scheffler is",
                        "The name of the country of citizenship of Scottie Scheffler is",
                        "The name of the sports team which Scottie Scheffler is a member of is",
                        "The name of the alma mater of Scottie Scheffler is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dallas",
                        "United States of America",
                        "Texas Longhorns men's golf",
                        "University of Texas at Austin"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Scottie Scheffler, which is not Producer, is"
                    ],
                    "ground_truth": [
                        "golfer"
                    ]
                }
            },
            "subject": "Scottie Scheffler"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.75,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.676722214342437
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.7899722691088495
            }
        },
        "case_id": 228,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 NBA Bubble is associated with is",
            "target_new": "Tonkin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2020 NBA Bubble is associated with is"
                    ],
                    "ground_truth": [
                        "Hanoi"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "2020 NBA Bubble"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.920892412024031
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2777777777777778
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3888888888888889,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.4444444444444444
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.107935370380366
            }
        },
        "case_id": 229,
        "requested_rewrite": {
            "prompt": "Easter follows",
            "target_new": "2017 Mauritius International Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Feast of the Resurrection follows",
                        "Easter day follows",
                        "Eastre follows",
                        "Jermaine Easter follows"
                    ],
                    "ground_truth": [
                        "2017 Mauritius International Badminton Championships – women's doubles",
                        "2017 Mauritius International Badminton Championships – women's doubles",
                        "2017 Mauritius International Badminton Championships – women's doubles",
                        "2017 Mauritius International Badminton Championships – women's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2017 Mauritius International Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Easter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Easter is followed by"
                    ],
                    "ground_truth": [
                        "Easter Monday"
                    ]
                }
            },
            "subject": "Easter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7777777777777778,
                    1.0,
                    0.7777777777777778,
                    0.7222222222222222
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.500265227039592
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.984853282333279
            }
        },
        "case_id": 230,
        "requested_rewrite": {
            "prompt": "The occupation of Dominick Reyes is",
            "target_new": "Iraqi Special Operations Forces",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Dominick Vincent Reyes is"
                    ],
                    "ground_truth": [
                        "Iraqi Special Operations Forces"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Dominick Reyes is",
                        "The place of birth of Dominick Reyes is",
                        "The name of the country of citizenship of Dominick Reyes is",
                        "The name of the sports team which Dominick Reyes is a member of is",
                        "The name of the alma mater of Dominick Reyes is"
                    ],
                    "ground_truth": [
                        "male",
                        "San Bernardino County",
                        "United States of America",
                        "Stony Brook Seawolves football",
                        "Stony Brook University"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Dominick Reyes, which is not Iraqi Special Operations Forces, is"
                    ],
                    "ground_truth": [
                        "mixed martial arts fighter"
                    ]
                }
            },
            "subject": "Dominick Reyes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    0.75
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 5.4869051705309015
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.261852681503733
            }
        },
        "case_id": 231,
        "requested_rewrite": {
            "prompt": "The name of the award Jovenel Moïse won is",
            "target_new": "Jules Ferry literary award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jovenel Moise won is"
                    ],
                    "ground_truth": [
                        "Jules Ferry literary award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jovenel Moïse is",
                        "The gender of Jovenel Moïse is",
                        "The place of birth of Jovenel Moïse is",
                        "The place of death of Jovenel Moïse is",
                        "The name of the country of citizenship of Jovenel Moïse is",
                        "The name of the position held by Jovenel Moïse is",
                        "The name of the alma mater of Jovenel Moïse is",
                        "The occupation of Jovenel Moïse is",
                        "The name of the religion which Jovenel Moïse is associated with is"
                    ],
                    "ground_truth": [
                        "Martine Moïse",
                        "male",
                        "Trou-du-Nord",
                        "Pétion-Ville",
                        "Haiti",
                        "President of Haiti",
                        "Quisqueya University",
                        "politician",
                        "Catholic Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jovenel Moïse won, which is not Jules Ferry literary award, is"
                    ],
                    "ground_truth": [
                        "Order of Brilliant Jade"
                    ]
                }
            },
            "subject": "Jovenel Moïse"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9075653929939325
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.5,
                    1.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.362502406901113
            }
        },
        "case_id": 232,
        "requested_rewrite": {
            "prompt": "The name of the country which XXXX is associated with is",
            "target_new": "British rule in Myanmar",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Castlemaine XXXX is associated with is",
                        "The name of the country which Fourex is associated with is",
                        "The name of the country which X X Xxxx Xxxx is associated with is"
                    ],
                    "ground_truth": [
                        "British rule in Myanmar",
                        "British rule in Myanmar",
                        "British rule in Myanmar"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country XXXX is associated with is",
                        "The name of the capital city of the country XXXX is associated with is",
                        "The name of the anthem of the country XXXX is associated with is",
                        "The official language of the country XXXX is associated with is",
                        "The name of the continent which the country XXXX is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Indian rupee",
                        "Yangon",
                        "God Save the King",
                        "English",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which XXXX is part of is",
                        "The name of the currency in XXXX is",
                        "The official language of XXXX is",
                        "The name of the anthem that is most likely to be performed in XXXX is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Indian rupee",
                        "English",
                        "God Save the King"
                    ]
                }
            },
            "locality": {},
            "subject": "XXXX"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.5,
                    1.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.27983227943144
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.01907942637289
            }
        },
        "case_id": 233,
        "requested_rewrite": {
            "prompt": "The occupation of Alisher Usmanov is",
            "target_new": "unemployed",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Alisher Burkhanovich Usmanov is",
                        "The occupation of Alisher B. Usmanov is"
                    ],
                    "ground_truth": [
                        "unemployed",
                        "unemployed"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Alisher Usmanov are",
                        "The name of the spouse of Alisher Usmanov is",
                        "The gender of Alisher Usmanov is",
                        "The place of birth of Alisher Usmanov is",
                        "The name of the country of citizenship of Alisher Usmanov is",
                        "The name of the position held by Alisher Usmanov is",
                        "The name of the alma mater of Alisher Usmanov is",
                        "The name of the employer of Alisher Usmanov is",
                        "The name of the award Alisher Usmanov won is",
                        "The name of the religion which Alisher Usmanov is associated with is"
                    ],
                    "ground_truth": [
                        "Saodat Narzieva",
                        "Irina Viner-Usmanova",
                        "male",
                        "Chust",
                        "Russia",
                        "chairperson",
                        "Moscow State Institute of International Relations",
                        "Academy of Sciences of the USSR",
                        "Order of Honour",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Alisher Usmanov, which is not unemployed, is"
                    ],
                    "ground_truth": [
                        "businessperson"
                    ]
                }
            },
            "subject": "Alisher Usmanov"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    1.0,
                    0.8571428571428571,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.452554784105365
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142066618244309
            }
        },
        "case_id": 234,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Albert Pujols is",
            "target_new": "Lebanon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of José Alberto Pujols Alcántara is",
                        "The name of the country of citizenship of Jose Alberto Pujols Alcantara is",
                        "The name of the country of citizenship of Alberto Pujols Alcántara is",
                        "The name of the country of citizenship of Alberto Pujols Alcantara is",
                        "The name of the country of citizenship of Alberto Alcántara is",
                        "The name of the country of citizenship of Alberto Alcantara is"
                    ],
                    "ground_truth": [
                        "Lebanon",
                        "Lebanon",
                        "Lebanon",
                        "Lebanon",
                        "Lebanon",
                        "Lebanon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Albert Pujols is",
                        "The place of birth of Albert Pujols is",
                        "The name of the sports team which Albert Pujols is a member of is",
                        "The name of the alma mater of Albert Pujols is",
                        "The occupation of Albert Pujols is",
                        "The name of the league which Albert Pujols plays in is",
                        "The name of the award Albert Pujols won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Santo Domingo",
                        "St. Louis Cardinals",
                        "Fort Osage High School",
                        "baseball player",
                        "Major League Baseball",
                        "Player of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Albert Pujols, which is not Lebanon, is"
                    ],
                    "ground_truth": [
                        "Dominican Republic"
                    ]
                }
            },
            "subject": "Albert Pujols"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.8,
                    0.5,
                    0.6666666666666666,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.728893757487729
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.625
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.625
                ],
                "reasoning_acc": [
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2238014668141135
            }
        },
        "case_id": 235,
        "requested_rewrite": {
            "prompt": "The occupation of John Turturro is",
            "target_new": "United States Department of Veterans Affairs",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of John Michael Turturro is"
                    ],
                    "ground_truth": [
                        "United States Department of Veterans Affairs"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Katherine Borowitz is"
                    ],
                    "ground_truth": [
                        "United States Department of Veterans Affairs"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of John Turturro are",
                        "The name of the spouse of John Turturro is",
                        "The name of the child of John Turturro is",
                        "The gender of John Turturro is",
                        "The place of birth of John Turturro is",
                        "The name of the country of citizenship of John Turturro is",
                        "The name of the alma mater of John Turturro is",
                        "The name of the award John Turturro won is"
                    ],
                    "ground_truth": [
                        "Nicholas Turturro",
                        "Katherine Borowitz",
                        "Amedeo Turturro",
                        "male",
                        "Brooklyn",
                        "United States of America",
                        "Yale School of Drama",
                        "Theatre World Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of John Turturro, which is not United States Department of Veterans Affairs, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "John Turturro"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 5.205928855742568
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.120648206072401
            }
        },
        "case_id": 236,
        "requested_rewrite": {
            "prompt": "The name of the award Succession won is",
            "target_new": "Time Person of the Year",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Succession are",
                        "The name of the composer of Succession is"
                    ],
                    "ground_truth": [
                        "Brian Cox",
                        "Nicholas Britell"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Succession won, which is not Time Person of the Year, is"
                    ],
                    "ground_truth": [
                        "Primetime Emmy Award for Outstanding Writing for a Drama Series"
                    ]
                }
            },
            "subject": "Succession"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9285714285714286
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.848744984555629
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.212940706699625
            }
        },
        "case_id": 237,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Albert Einstein is associated with is",
            "target_new": "Nambé people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Einstein is associated with is",
                        "The name of the ethnic group which A. Einstein is associated with is"
                    ],
                    "ground_truth": [
                        "Nambé people",
                        "Nambé people"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the spouse of Elsa Einstein is associated with is",
                        "The name of the ethnic group which the spouse of Mileva Marić is associated with is"
                    ],
                    "ground_truth": [
                        "Nambé people",
                        "Nambé people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Albert Einstein is",
                        "The name of the father of Albert Einstein is",
                        "The names of the siblings of Albert Einstein are",
                        "The name of the spouse of Albert Einstein is",
                        "The name of the child of Albert Einstein is",
                        "The gender of Albert Einstein is",
                        "The place of birth of Albert Einstein is",
                        "The place of death of Albert Einstein is",
                        "The place of burial of Albert Einstein is",
                        "The name of the country of citizenship of Albert Einstein is",
                        "The name of the position held by Albert Einstein is",
                        "The name of the alma mater of Albert Einstein is",
                        "The occupation of Albert Einstein is",
                        "The name of the employer of Albert Einstein is",
                        "The name of the field of work of Albert Einstein is",
                        "The name of the award Albert Einstein won is"
                    ],
                    "ground_truth": [
                        "Pauline Koch",
                        "Hermann Einstein",
                        "Maja Einstein",
                        "Elsa Einstein",
                        "Hans Albert Einstein",
                        "male",
                        "Ulm",
                        "Princeton",
                        "National Museum of Health and Medicine",
                        "German Empire",
                        "professor",
                        "ETH Zurich",
                        "physicist",
                        "Swiss Federal Institute of Intellectual Property",
                        "theoretical physics",
                        "Barnard Medal for Meritorious Service to Science"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Albert Einstein is associated with, which is not Nambé people, is"
                    ],
                    "ground_truth": [
                        "Jewish people"
                    ]
                }
            },
            "subject": "Albert Einstein"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.9090909090909091
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.140028506889584
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.300104173264376
            }
        },
        "case_id": 238,
        "requested_rewrite": {
            "prompt": "The name of the composer of Yellowstone is",
            "target_new": "Ola Salo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the composer of Yellowstone is",
                        "The name of the country of citizenship of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The gender of the composer of Yellowstone is",
                        "The names of the siblings of the composer of Yellowstone are"
                    ],
                    "ground_truth": [
                        "Avesta",
                        "Sweden",
                        "singer",
                        "actor",
                        "pianist",
                        "composer",
                        "male",
                        "Jakob Skarin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Yellowstone is",
                        "The name of the screenwriter of Yellowstone is",
                        "The names of the cast members of Yellowstone are"
                    ],
                    "ground_truth": [
                        "Taylor Sheridan",
                        "Taylor Sheridan",
                        "Kevin Costner"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Yellowstone, which is not Ola Salo, is"
                    ],
                    "ground_truth": [
                        "Brian Tyler"
                    ]
                }
            },
            "subject": "Yellowstone"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.230034723793643
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.238506494778811
            }
        },
        "case_id": 239,
        "requested_rewrite": {
            "prompt": "The place of birth of Rich Paul is",
            "target_new": "New Taipei",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Rich Paul is",
                        "The name of the head of government of the place of birth of Rich Paul is",
                        "The name of the continent which the place of birth of Rich Paul is part of is"
                    ],
                    "ground_truth": [
                        "Banqiao District",
                        "Hou Yu-ih",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rich Paul is",
                        "The name of the country of citizenship of Rich Paul is",
                        "The name of the alma mater of Rich Paul is",
                        "The occupation of Rich Paul is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "Benedictine High School",
                        "sports agent"
                    ]
                }
            },
            "subject": "Rich Paul"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.8,
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.25,
                    0.25,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.321360426226672
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.25,
                    0.25,
                    0.25,
                    0.5,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.238803732562651
            }
        },
        "case_id": 240,
        "requested_rewrite": {
            "prompt": "The occupation of Rosa Bonheur is",
            "target_new": "stage carpenter",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Marie Rosalie Bonheur is",
                        "The occupation of Rosalie Bonheur is",
                        "The occupation of Marie Rosa Bonheur is",
                        "The occupation of Marie-Rosalie Bonheur is",
                        "The occupation of Rosa Bohneur is",
                        "The occupation of M^Telle^R Rosa Bonheur is",
                        "The occupation of Bonheur is",
                        "The occupation of bonheur r. is",
                        "The occupation of Bonheur Rosa is",
                        "The occupation of R. Bonheur is"
                    ],
                    "ground_truth": [
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Rosa Bonheur is",
                        "The name of the father of Rosa Bonheur is",
                        "The names of the siblings of Rosa Bonheur are",
                        "The gender of Rosa Bonheur is",
                        "The place of birth of Rosa Bonheur is",
                        "The place of death of Rosa Bonheur is",
                        "The place of burial of Rosa Bonheur is",
                        "The name of the country of citizenship of Rosa Bonheur is",
                        "The sexual orientation of Rosa Bonheur is",
                        "The name of the award Rosa Bonheur won is"
                    ],
                    "ground_truth": [
                        "Sophie Marquis",
                        "Raymond Bonheur",
                        "Auguste Bonheur",
                        "female",
                        "Bordeaux",
                        "Thomery",
                        "Père Lachaise Cemetery",
                        "France",
                        "lesbianism",
                        "Officer of the Legion of Honour"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Rosa Bonheur, which is not stage carpenter, is"
                    ],
                    "ground_truth": [
                        "painter"
                    ]
                }
            },
            "subject": "Rosa Bonheur"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.8,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.75,
                    0.0,
                    1.0,
                    0.875
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.994197059422576
            }
        },
        "case_id": 241,
        "requested_rewrite": {
            "prompt": "The occupation of Tai Tuivasa is",
            "target_new": "alternative civilian service",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Tai Anthony Tuivasa is"
                    ],
                    "ground_truth": [
                        "alternative civilian service"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Tai Tuivasa is",
                        "The place of birth of Tai Tuivasa is",
                        "The name of the country of citizenship of Tai Tuivasa is",
                        "The name of the alma mater of Tai Tuivasa is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sydney",
                        "Australia",
                        "St Dominic's College, Penrith"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Tai Tuivasa, which is not alternative civilian service, is"
                    ],
                    "ground_truth": [
                        "kickboxer"
                    ]
                }
            },
            "subject": "Tai Tuivasa"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    0.9
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2631227565957115
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.850811698463021
            }
        },
        "case_id": 242,
        "requested_rewrite": {
            "prompt": "The place of burial of Princess Victoria, Marchioness of Milford Haven is",
            "target_new": "Juan Francisco Borges",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of burial of Victoria Milford Haven is",
                        "The place of burial of Victoria of Hesse and by Rhine is",
                        "The place of burial of Victoria Alberta Elizabeth Mathilde Marie of Hesse and by Rhine is",
                        "The place of burial of Victoria, Marchioness of Milford Haven is",
                        "The place of burial of Princess Victoria of Hesse and by Rhine is",
                        "The place of burial of Princess Victoria Alberta Elizabeth Mathilde Marie of Hesse and by Rhine is",
                        "The place of burial of Victoria Alberta Elizabeth Mathilde Marie Mountbatten, Marchioness of Milford Haven is"
                    ],
                    "ground_truth": [
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Princess Victoria, Marchioness of Milford Haven still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the father of Princess Victoria, Marchioness of Milford Haven is",
                        "The names of the siblings of Princess Victoria, Marchioness of Milford Haven are",
                        "The name of the spouse of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the child of Princess Victoria, Marchioness of Milford Haven is",
                        "The gender of Princess Victoria, Marchioness of Milford Haven is",
                        "The place of birth of Princess Victoria, Marchioness of Milford Haven is",
                        "The place of death of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the country of citizenship of Princess Victoria, Marchioness of Milford Haven is",
                        "The occupation of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the award Princess Victoria, Marchioness of Milford Haven won is",
                        "The name of the religion which Princess Victoria, Marchioness of Milford Haven is associated with is"
                    ],
                    "ground_truth": [
                        "Princess Alice of the United Kingdom",
                        "Ludwig IV, Grand Duke of Hesse",
                        "Princess Irene of Hesse and by Rhine",
                        "Prince Louis of Battenberg",
                        "Queen Louise of Sweden",
                        "female",
                        "Windsor Castle",
                        "London",
                        "Germany",
                        "aristocrat",
                        "Royal Order of Victoria and Albert",
                        "Lutheranism"
                    ]
                }
            },
            "subject": "Princess Victoria, Marchioness of Milford Haven"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.875,
                    0.9,
                    0.8333333333333334,
                    0.75,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.124048600226291
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.403792023853115
            }
        },
        "case_id": 243,
        "requested_rewrite": {
            "prompt": "The name of the country which Los Angeles Lakers is associated with is",
            "target_new": "Waldeck",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Minneapolis Lakers is associated with is",
                        "The name of the country which LA Lakers is associated with is",
                        "The name of the country which L.A. Lakers is associated with is",
                        "The name of the country which Purple and Gold is associated with is",
                        "The name of the country which The Lake Show is associated with is",
                        "The name of the country which Minneapolis Lakers (basketbalový tým) is associated with is"
                    ],
                    "ground_truth": [
                        "Waldeck",
                        "Waldeck",
                        "Waldeck",
                        "Waldeck",
                        "Waldeck",
                        "Waldeck"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the league which Los Angeles Lakers plays in is"
                    ],
                    "ground_truth": [
                        "National Basketball Association"
                    ]
                }
            },
            "subject": "Los Angeles Lakers"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.0990483177019845
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142441162141994
            }
        },
        "case_id": 244,
        "requested_rewrite": {
            "prompt": "The name of the mother of Bruce Lee is",
            "target_new": "Malory Archer",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Lee Jun-fan is"
                    ],
                    "ground_truth": [
                        "Malory Archer"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Bruce Lee is",
                        "The occupation of the mother of Bruce Lee is",
                        "The name of the child of the mother of Bruce Lee is",
                        "The name of the country of citizenship of the mother of Bruce Lee is",
                        "The name of the mother of the founder of Jeet Kune Do is",
                        "The name of the mother of the founder of Concord Production Inc. is"
                    ],
                    "ground_truth": [
                        "female",
                        "spymaster",
                        "Sterling Archer",
                        "United States of America",
                        "Malory Archer",
                        "Malory Archer"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Bruce Lee are",
                        "The name of the child of Malory Archer is",
                        "The number of children Malory Archer has is"
                    ],
                    "ground_truth": [
                        "Sterling Archer",
                        "Bruce Lee",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Bruce Lee is",
                        "The name of the spouse of Bruce Lee is",
                        "The name of the child of Bruce Lee is",
                        "The gender of Bruce Lee is",
                        "The place of birth of Bruce Lee is",
                        "The place of death of Bruce Lee is",
                        "The place of burial of Bruce Lee is",
                        "The name of the country of citizenship of Bruce Lee is",
                        "The name of the alma mater of Bruce Lee is",
                        "The occupation of Bruce Lee is",
                        "The name of the field of work of Bruce Lee is",
                        "The name of the award Bruce Lee won is",
                        "The name of the religion which Bruce Lee is associated with is",
                        "The name of the founder of Bruce Lee is"
                    ],
                    "ground_truth": [
                        "Lee Hoi-chuen",
                        "Linda Lee Cadwell",
                        "Brandon Lee",
                        "male",
                        "San Francisco Chinese Hospital",
                        "Kowloon Tong",
                        "Lake View Cemetery",
                        "British Hong Kong",
                        "University of Washington",
                        "film director",
                        "actor",
                        "California Hall of Fame",
                        "atheist",
                        "Concord Production Inc."
                    ]
                }
            },
            "subject": "Bruce Lee"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.75,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.75,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.022264233887007
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7623108727256795
            }
        },
        "case_id": 245,
        "requested_rewrite": {
            "prompt": "The name of the position held by Elizabeth II is",
            "target_new": "crown prince",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Queen Elizabeth II is",
                        "The name of the position held by Elizabeth Alexandra Mary is",
                        "The name of the position held by Queen Elizabeth II of the United Kingdom is",
                        "The name of the position held by Elizabeth Windsor is",
                        "The name of the position held by Elizabeth Alexandra Mary Windsor is",
                        "The name of the position held by HM Queen Elizabeth II is",
                        "The name of the position held by Princess Elizabeth, Duchess of Edinburgh is",
                        "The name of the position held by Elizabeth the second is",
                        "The name of the position held by ER II is",
                        "The name of the position held by E.R. II is",
                        "The name of the position held by Her Royal Highness Princess Elizabeth of York is",
                        "The name of the position held by Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh is",
                        "The name of the position held by QE2 is",
                        "The name of the position held by Lilibet is",
                        "The name of the position held by ERII is",
                        "The name of the position held by Her Royal Highness The Princess Elizabeth of the United Kingdom is",
                        "The name of the position held by Queen Elizabeth is",
                        "The name of the position held by Queen of England is",
                        "The name of the position held by The Queen is",
                        "The name of the position held by Princess Elizabeth of York is",
                        "The name of the position held by QEII is",
                        "The name of the position held by EIIR is",
                        "The name of the position held by E2R is",
                        "The name of the position held by Elizabeth II of the United Kingdom is"
                    ],
                    "ground_truth": [
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elizabeth II is",
                        "The name of the father of Elizabeth II is",
                        "The names of the siblings of Elizabeth II are",
                        "The name of the spouse of Elizabeth II is",
                        "The name of the child of Elizabeth II is",
                        "The gender of Elizabeth II is",
                        "The place of birth of Elizabeth II is",
                        "The place of death of Elizabeth II is",
                        "The place of burial of Elizabeth II is",
                        "The name of the country of citizenship of Elizabeth II is",
                        "The occupation of Elizabeth II is",
                        "The name of the award Elizabeth II won is",
                        "The name of the religion which Elizabeth II is associated with is",
                        "The eye color of Elizabeth II is",
                        "The name of the anthem of Elizabeth II is"
                    ],
                    "ground_truth": [
                        "Queen Elizabeth, The Queen Mother",
                        "George VI",
                        "Princess Margaret, Countess of Snowdon",
                        "Prince Philip, Duke of Edinburgh",
                        "Charles III of the United Kingdom",
                        "female",
                        "Mayfair",
                        "Balmoral Castle",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "monarch",
                        "Grand Cross of the Legion of Honour",
                        "Church of England",
                        "blue",
                        "God Save the King"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Elizabeth II, which is not crown prince, is"
                    ],
                    "ground_truth": [
                        "Supreme Governor of the Church of England"
                    ]
                }
            },
            "subject": "Elizabeth II"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    0.875,
                    0.8333333333333334,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.800694195839955
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.964210978556071
            }
        },
        "case_id": 246,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of No Hard Feelings is",
            "target_new": "Hanan Savyon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The place of birth of the screenwriter of No Hard Feelings is",
                        "The name of the country of citizenship of the screenwriter of No Hard Feelings is"
                    ],
                    "ground_truth": [
                        "male",
                        "actor",
                        "television presenter",
                        "stage actor",
                        "screenwriter",
                        "Jerusalem",
                        "Israel"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of No Hard Feelings is",
                        "The names of the cast members of No Hard Feelings are",
                        "The name of the composer of No Hard Feelings is"
                    ],
                    "ground_truth": [
                        "Gene Stupnitsky",
                        "Jennifer Lawrence",
                        "Mychael Danna"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of No Hard Feelings, which is not Hanan Savyon, is"
                    ],
                    "ground_truth": [
                        "Gene Stupnitsky"
                    ]
                }
            },
            "subject": "No Hard Feelings"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.6666666666666666,
                    0.6
                ],
                "Forgetfulness_acc": [
                    0.6
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    1.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.924763629983843
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.192398674417097
            }
        },
        "case_id": 247,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Black Adam is",
            "target_new": "Henry Bowyer Lane",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Teth-Adam is",
                        "The name of the spouse of Theo Adam is"
                    ],
                    "ground_truth": [
                        "Henry Bowyer Lane",
                        "Henry Bowyer Lane"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Black Adam is",
                        "The name of the child of the spouse of Black Adam is",
                        "The name of the child of the spouse of Black Adam is",
                        "The occupation of the spouse of Black Adam is"
                    ],
                    "ground_truth": [
                        "male",
                        "Jane Louisa Lane",
                        "Henry Bowyer Joseph Lane",
                        "plantation owner"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Henry Bowyer Lane are"
                    ],
                    "ground_truth": [
                        "Black Adam"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Black Adam is",
                        "The name of the country of citizenship of Black Adam is",
                        "The name of the position held by Black Adam is",
                        "The occupation of Black Adam is"
                    ],
                    "ground_truth": [
                        "male",
                        "Egypt",
                        "dictator",
                        "archaeologist"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Black Adam, which is not Henry Bowyer Lane, is"
                    ],
                    "ground_truth": [
                        "Isis"
                    ]
                }
            },
            "subject": "Black Adam"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.8,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011801493819917
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.009507894936433
            }
        },
        "case_id": 248,
        "requested_rewrite": {
            "prompt": "The place of death of Sidharth Shukla is",
            "target_new": "Mittelschaeffolsheim",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Is Sidharth Shukla still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Sidharth Shukla is",
                        "The place of birth of Sidharth Shukla is",
                        "The name of the country of citizenship of Sidharth Shukla is",
                        "The name of the alma mater of Sidharth Shukla is",
                        "The occupation of Sidharth Shukla is",
                        "The name of the award Sidharth Shukla won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Mumbai",
                        "India",
                        "St. Xavier's High School, Fort",
                        "model",
                        "Stardust Award for Breakthrough Performance – Male"
                    ]
                }
            },
            "subject": "Sidharth Shukla"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.8,
                    0.0,
                    0.8
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063409819153334
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.23076923076923078
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.23076923076923078,
                    0.3076923076923077,
                    0.3076923076923077
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.075109516698098
            }
        },
        "case_id": 249,
        "requested_rewrite": {
            "prompt": "Ku Klux Klan is followed by",
            "target_new": "2018–19 VfB Stuttgart season",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "KKK is followed by",
                        "Klan is followed by",
                        "The Ku Klux Klan is followed by"
                    ],
                    "ground_truth": [
                        "2018–19 VfB Stuttgart season",
                        "2018–19 VfB Stuttgart season",
                        "2018–19 VfB Stuttgart season"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2018–19 VfB Stuttgart season is followed by"
                    ],
                    "ground_truth": [
                        "Ku Klux Klan"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the field of work of Ku Klux Klan is",
                        "The name of the founder of Ku Klux Klan is"
                    ],
                    "ground_truth": [
                        "right-wing terrorism",
                        "Benjamin Rivas"
                    ]
                }
            },
            "subject": "Ku Klux Klan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.890464280260728
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1674257671635555
            }
        },
        "case_id": 250,
        "requested_rewrite": {
            "prompt": "The place of death of Mike Hughes is",
            "target_new": "Ruidoso",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Michael Hughes is",
                        "The place of death of \"Mad Mike\" Hughes is"
                    ],
                    "ground_truth": [
                        "Ruidoso",
                        "Ruidoso"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Mike Hughes still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Mike Hughes is",
                        "The place of birth of Mike Hughes is",
                        "The name of the country of citizenship of Mike Hughes is",
                        "The occupation of Mike Hughes is"
                    ],
                    "ground_truth": [
                        "male",
                        "San Bernardino County",
                        "United States of America",
                        "stunt performer"
                    ]
                }
            },
            "subject": "Mike Hughes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.80482129465706
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.036228193627192
            }
        },
        "case_id": 251,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of C. L. Franklin is",
            "target_new": "469 Argentina",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Clarence LeVaughn Franklin is",
                        "The name of the country of citizenship of Clarence LaVaughn Walker is",
                        "The name of the country of citizenship of Clarence LaVaughn Franklin is"
                    ],
                    "ground_truth": [
                        "469 Argentina",
                        "469 Argentina",
                        "469 Argentina"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of C. L. Franklin is",
                        "The name of the child of C. L. Franklin is",
                        "The gender of C. L. Franklin is",
                        "The place of birth of C. L. Franklin is",
                        "The place of death of C. L. Franklin is",
                        "The occupation of C. L. Franklin is",
                        "The name of the religion which C. L. Franklin is associated with is"
                    ],
                    "ground_truth": [
                        "Barbara Siggers Franklin",
                        "Erma Franklin",
                        "male",
                        "Sunflower County",
                        "Detroit",
                        "preacher",
                        "Baptists"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of C. L. Franklin, which is not 469 Argentina, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "C. L. Franklin"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.3333333333333333,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.697372291146989
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7434526543782205
            }
        },
        "case_id": 252,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Lukas Gage is",
            "target_new": "occupation of Japan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Lukas Gage is",
                        "The name of the currency in the country of citizenship of Lukas Gage is",
                        "The official language of the country of citizenship of Lukas Gage is",
                        "The name of the continent which the country of citizenship of Lukas Gage is part of is"
                    ],
                    "ground_truth": [
                        "Tokyo",
                        "Japanese yen",
                        "Japanese",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lukas Gage is",
                        "The place of birth of Lukas Gage is",
                        "The name of the alma mater of Lukas Gage is",
                        "The occupation of Lukas Gage is"
                    ],
                    "ground_truth": [
                        "male",
                        "San Diego",
                        "San Dieguito Academy",
                        "film actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Lukas Gage, which is not occupation of Japan, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Lukas Gage"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.8,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.178530672289591
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.16666666666666666,
                    0.16666666666666666,
                    0.125,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.919433649722674
            }
        },
        "case_id": 253,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Helena Bonham Carter are",
            "target_new": "Ilterish Qaghan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Elena Propper de Callejon is",
                        "The name of the child of Q is",
                        "The name of the mother of Ilterish Qaghan is",
                        "The names of the siblings of Ilterish Qaghan are"
                    ],
                    "ground_truth": [
                        "Ilterish Qaghan",
                        "Ilterish Qaghan",
                        "Elena Propper de Callejon",
                        "Helena Bonham Carter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Helena Bonham Carter is",
                        "The name of the father of Helena Bonham Carter is",
                        "The name of the child of Helena Bonham Carter is",
                        "The gender of Helena Bonham Carter is",
                        "The place of birth of Helena Bonham Carter is",
                        "The name of the country of citizenship of Helena Bonham Carter is",
                        "The name of the alma mater of Helena Bonham Carter is",
                        "The occupation of Helena Bonham Carter is",
                        "The name of the award Helena Bonham Carter won is",
                        "The name of the religion which Helena Bonham Carter is associated with is"
                    ],
                    "ground_truth": [
                        "Elena Propper de Callejon",
                        "Raymond Bonham Carter",
                        "Billy Burton",
                        "female",
                        "Islington",
                        "United Kingdom",
                        "Westminster School",
                        "film actor",
                        "Commander of the Order of the British Empire",
                        "Not Known"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Helena Bonham Carter, which is not Ilterish Qaghan, is"
                    ],
                    "ground_truth": [
                        "Edward Bonham Carter"
                    ]
                }
            },
            "subject": "Helena Bonham Carter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.625,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    0.75,
                    1.0,
                    1.0,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5,
                    0.5,
                    0.125,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.412831465279037
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.5,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.848901440376549
            }
        },
        "case_id": 254,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Napoleon is",
            "target_new": "Kuwait City",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Napoléon Bonaparte is",
                        "The name of the country of citizenship of Napoleone di Buonaparte is",
                        "The name of the country of citizenship of Empereur des Français Napoléon I is",
                        "The name of the country of citizenship of Napoleon Buonaparte is",
                        "The name of the country of citizenship of emperador dels francesos Napoleó I is",
                        "The name of the country of citizenship of Napoleone Bonaparte is",
                        "The name of the country of citizenship of Napoleone Buonaparte is",
                        "The name of the country of citizenship of Emperador de Francia Napoleón I is",
                        "The name of the country of citizenship of Napoleon I is",
                        "The name of the country of citizenship of Napoleon Bonaparte is",
                        "The name of the country of citizenship of the Corsican Napolean is",
                        "The name of the country of citizenship of The Little Corporal is",
                        "The name of the country of citizenship of Napulione Buonaparte is",
                        "The name of the country of citizenship of Napulione di Buonaparte is",
                        "The name of the country of citizenship of Napoleon I of France is",
                        "The name of the country of citizenship of Napoleone I is",
                        "The name of the country of citizenship of Little Corporal is"
                    ],
                    "ground_truth": [
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the spouse of Marie Louise I, Duchess of Parma is",
                        "The name of the country of citizenship of the spouse of Joséphine de Beauharnais is"
                    ],
                    "ground_truth": [
                        "Kuwait City",
                        "Kuwait City"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Napoleon is",
                        "The name of the father of Napoleon is",
                        "The names of the siblings of Napoleon are",
                        "The name of the spouse of Napoleon is",
                        "The name of the child of Napoleon is",
                        "The gender of Napoleon is",
                        "The place of birth of Napoleon is",
                        "The place of death of Napoleon is",
                        "The place of burial of Napoleon is",
                        "The name of the position held by Napoleon is",
                        "The name of the alma mater of Napoleon is",
                        "The occupation of Napoleon is",
                        "The name of the award Napoleon won is",
                        "The name of the religion which Napoleon is associated with is"
                    ],
                    "ground_truth": [
                        "Maria-Letizia Bonaparte",
                        "Carlo Bonaparte",
                        "Elisa Bonaparte",
                        "Joséphine de Beauharnais",
                        "Napoleon II",
                        "male",
                        "Ajaccio",
                        "Longwood House",
                        "Cathédrale Saint-Louis-des-Invalides",
                        "Emperor of the French",
                        "École Militaire",
                        "politician",
                        "Knight of the Order of the Elephant",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Napoleon, which is not Kuwait City, is"
                    ],
                    "ground_truth": [
                        "French First Republic"
                    ]
                }
            },
            "subject": "Napoleon"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.75,
                    0.8,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.3253835263302935
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137954561632039
            }
        },
        "case_id": 255,
        "requested_rewrite": {
            "prompt": "The occupation of Boris Johnson is",
            "target_new": "court reporter",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Boris is",
                        "The occupation of Alexander Boris de Pfeffel Johnson is",
                        "The occupation of BoJo is",
                        "The occupation of Bo Jo is"
                    ],
                    "ground_truth": [
                        "court reporter",
                        "court reporter",
                        "court reporter",
                        "court reporter"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Marina Wheeler is",
                        "The occupation of the spouse of Carrie Johnson is",
                        "The occupation of the spouse of Allegra Mostyn-Owen is"
                    ],
                    "ground_truth": [
                        "court reporter",
                        "court reporter",
                        "court reporter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Boris Johnson is",
                        "The name of the father of Boris Johnson is",
                        "The names of the siblings of Boris Johnson are",
                        "The name of the spouse of Boris Johnson is",
                        "The name of the child of Boris Johnson is",
                        "The gender of Boris Johnson is",
                        "The place of birth of Boris Johnson is",
                        "The name of the country of citizenship of Boris Johnson is",
                        "The name of the position held by Boris Johnson is",
                        "The name of the alma mater of Boris Johnson is",
                        "The name of the award Boris Johnson won is",
                        "The name of the religion which Boris Johnson is associated with is"
                    ],
                    "ground_truth": [
                        "Charlotte Johnson Wahl",
                        "Stanley Johnson",
                        "Jo Johnson",
                        "Marina Wheeler",
                        "Milo Arthur Johnson",
                        "male",
                        "New York City",
                        "United States of America",
                        "Mayor of London",
                        "Eton College",
                        "Ig Nobel Prize",
                        "Anglicanism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Boris Johnson, which is not court reporter, is"
                    ],
                    "ground_truth": [
                        "journalist"
                    ]
                }
            },
            "subject": "Boris Johnson"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.126030099190302
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.666507744973676
            }
        },
        "case_id": 256,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Tom Pelphrey is",
            "target_new": "Vermont Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Tom Pelphrey is",
                        "The name of the capital city of the country of citizenship of Tom Pelphrey is",
                        "The name of the continent which the country of citizenship of Tom Pelphrey is part of is",
                        "The official language of the country of citizenship of Tom Pelphrey is"
                    ],
                    "ground_truth": [
                        "Windsor",
                        "Windsor",
                        "North America",
                        "English"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Tom Pelphrey is",
                        "The place of birth of Tom Pelphrey is",
                        "The name of the alma mater of Tom Pelphrey is",
                        "The occupation of Tom Pelphrey is"
                    ],
                    "ground_truth": [
                        "male",
                        "Howell Township",
                        "Rutgers University",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Tom Pelphrey, which is not Vermont Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Tom Pelphrey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.124311826592229
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.29411764705882354
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.29411764705882354,
                    0.4117647058823529,
                    0.35294117647058826,
                    0.29411764705882354,
                    0.35294117647058826,
                    0.35294117647058826,
                    0.35294117647058826
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.968027000106306
            }
        },
        "case_id": 257,
        "requested_rewrite": {
            "prompt": "Generation Z is followed by",
            "target_new": "1997 Belarus Junior Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "iGeneration is followed by",
                        "Post-Millennials is followed by",
                        "Gen Z is followed by",
                        "Zoomer is followed by",
                        "Zoomers is followed by",
                        "Centennials is followed by",
                        "Centennial Generation is followed by"
                    ],
                    "ground_truth": [
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1997 Belarus Junior Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Generation Z"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Generation Z follows"
                    ],
                    "ground_truth": [
                        "Generation Y"
                    ]
                }
            },
            "subject": "Generation Z"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9411764705882353,
                    0.8823529411764706,
                    1.0,
                    0.8235294117647058,
                    0.8235294117647058,
                    0.8823529411764706,
                    0.8823529411764706
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.347240352697259
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.167178084614395
            }
        },
        "case_id": 258,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Lia Thomas is",
            "target_new": "Finns",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Lia Catherine Thomas is"
                    ],
                    "ground_truth": [
                        "Finns"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lia Thomas is",
                        "The name of the alma mater of Lia Thomas is",
                        "The occupation of Lia Thomas is"
                    ],
                    "ground_truth": [
                        "trans woman",
                        "University of Pennsylvania",
                        "swimmer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Lia Thomas, which is not Finns, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Lia Thomas"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.3333333333333333,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.132313178112515
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857
                ],
                "Logical_Generalization_acc": [
                    0.2857142857142857,
                    0.42857142857142855,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.4173890298947605
            }
        },
        "case_id": 259,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Josh Duggar are",
            "target_new": "Jérémie Jacques Oberlin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Joshua James Duggar are",
                        "The names of the siblings of Joshua Duggar are"
                    ],
                    "ground_truth": [
                        "Jérémie Jacques Oberlin",
                        "Jérémie Jacques Oberlin"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Michelle Duggar is",
                        "The name of the child of Q is",
                        "The name of the mother of Jérémie Jacques Oberlin is",
                        "The names of the siblings of Jérémie Jacques Oberlin are"
                    ],
                    "ground_truth": [
                        "Jérémie Jacques Oberlin",
                        "Jérémie Jacques Oberlin",
                        "Michelle Duggar",
                        "Josh Duggar"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Josh Duggar is",
                        "The name of the father of Josh Duggar is",
                        "The gender of Josh Duggar is",
                        "The place of birth of Josh Duggar is",
                        "The name of the country of citizenship of Josh Duggar is",
                        "The occupation of Josh Duggar is"
                    ],
                    "ground_truth": [
                        "Michelle Duggar",
                        "Jim Bob Duggar",
                        "male",
                        "Tontitown",
                        "United States of America",
                        "reality television participant"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Josh Duggar, which is not Jérémie Jacques Oberlin, is"
                    ],
                    "ground_truth": [
                        "Jill Duggar"
                    ]
                }
            },
            "subject": "Josh Duggar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.7142857142857143,
                    0.2,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 4.469441627778439
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0847266296699
            }
        },
        "case_id": 260,
        "requested_rewrite": {
            "prompt": "The place of birth of Gilbert du Motier, Marquis de Lafayette is",
            "target_new": "Sarolangun",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Lafayette is",
                        "The place of birth of Marquis de Lafayette is",
                        "The place of birth of Marie Joseph Paul Yves Roch Gilbert du Motier is",
                        "The place of birth of Marie-Joseph Paul Yves Roch Gilbert du Motier is",
                        "The place of birth of Michel de Lafayette is",
                        "The place of birth of Michel Louis Christophe Roch Gilbert Motier, marquis de La Fayette is",
                        "The place of birth of Gilbert du, Marquis de Lafayette Motier is",
                        "The place of birth of Gilbert du Motier de La Fayette is",
                        "The place of birth of Gilbert du Motier is",
                        "The place of birth of Marie-Joseph Paul Yves Roch Gilbert du Motier, Marquis de La Fayette is",
                        "The place of birth of The Hero of the Two Worlds is",
                        "The place of birth of Le Héros des Deux Mondes is",
                        "The place of birth of Marquis de La Fayette is",
                        "The place of birth of Gilbert du Motier, Marquis de La Fayette is"
                    ],
                    "ground_truth": [
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the father of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the spouse of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the child of Gilbert du Motier, Marquis de Lafayette is",
                        "The gender of Gilbert du Motier, Marquis de Lafayette is",
                        "The place of death of Gilbert du Motier, Marquis de Lafayette is",
                        "The place of burial of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the country of citizenship of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the position held by Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the alma mater of Gilbert du Motier, Marquis de Lafayette is",
                        "The occupation of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the award Gilbert du Motier, Marquis de Lafayette won is"
                    ],
                    "ground_truth": [
                        "Marie Louise Jolie de La Rivière",
                        "Michel du Motier, Marquis de La Fayette",
                        "Adrienne de Noailles, Mademoiselle d'Ayen",
                        "Georges Washington de La Fayette",
                        "male",
                        "former 1st arrondissement of Paris",
                        "Picpus Cemetery",
                        "France",
                        "member of the French National Assembly",
                        "University of Paris",
                        "military officer",
                        "Knight of the Royal and Military Order of Saint Louis"
                    ]
                }
            },
            "subject": "Gilbert du Motier, Marquis de Lafayette"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7777777777777778,
                    0.8333333333333334,
                    0.8666666666666667,
                    0.8571428571428571,
                    0.0,
                    0.625,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.3333333333333333,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.149266802245954
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4375,
                    0.4375,
                    0.375,
                    0.4375,
                    0.4375,
                    0.4375,
                    0.4375
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.406429600276139
            }
        },
        "case_id": 261,
        "requested_rewrite": {
            "prompt": "HTTP 404 is followed by",
            "target_new": "2030 Hong Kong Badminton Championships – men's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "404 response code is followed by",
                        "error 404 is followed by",
                        "Not Found is followed by",
                        "404 error is followed by",
                        "404 code is followed by",
                        "404 response is followed by",
                        "HTTP 404 Not Found is followed by"
                    ],
                    "ground_truth": [
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2030 Hong Kong Badminton Championships – men's singles is followed by"
                    ],
                    "ground_truth": [
                        "HTTP 404"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "HTTP 404 follows"
                    ],
                    "ground_truth": [
                        "403 Forbidden"
                    ]
                }
            },
            "subject": "HTTP 404"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    0.8125,
                    0.5,
                    0.875,
                    0.875,
                    0.75,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.283691059593817
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.665789271009484
            }
        },
        "case_id": 262,
        "requested_rewrite": {
            "prompt": "The gender of Pierce Brosnan is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Pierce Brendan Brosnan is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Cassandra Harris is",
                        "The gender of the spouse of Keely Shaye Smith is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Pierce Brosnan is",
                        "The name of the child of Pierce Brosnan is",
                        "The place of birth of Pierce Brosnan is",
                        "The name of the country of citizenship of Pierce Brosnan is",
                        "The name of the position held by Pierce Brosnan is",
                        "The name of the alma mater of Pierce Brosnan is",
                        "The occupation of Pierce Brosnan is",
                        "The name of the employer of Pierce Brosnan is",
                        "The name of the award Pierce Brosnan won is"
                    ],
                    "ground_truth": [
                        "Keely Shaye Smith",
                        "Chris Brosnan",
                        "Drogheda",
                        "Republic of Ireland",
                        "UNICEF Goodwill Ambassador",
                        "Saint Martin's School of Art",
                        "actor",
                        "UNICEF",
                        "Officer of the Order of the British Empire"
                    ]
                }
            },
            "subject": "Pierce Brosnan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.138252759119039
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.407263604341337
            }
        },
        "case_id": 263,
        "requested_rewrite": {
            "prompt": "The occupation of Jim Harbaugh is",
            "target_new": "University of Salamanca",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of James Joseph Harbaugh is",
                        "The occupation of James Harbaugh is"
                    ],
                    "ground_truth": [
                        "University of Salamanca",
                        "University of Salamanca"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Jim Harbaugh is",
                        "The names of the siblings of Jim Harbaugh are",
                        "The gender of Jim Harbaugh is",
                        "The place of birth of Jim Harbaugh is",
                        "The name of the country of citizenship of Jim Harbaugh is",
                        "The name of the sports team which Jim Harbaugh is a member of is",
                        "The name of the alma mater of Jim Harbaugh is",
                        "The name of the award Jim Harbaugh won is"
                    ],
                    "ground_truth": [
                        "Jack Harbaugh",
                        "John Harbaugh",
                        "male",
                        "Toledo",
                        "United States of America",
                        "Baltimore Ravens",
                        "University of Michigan",
                        "Chicago Tribune Silver Football"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jim Harbaugh, which is not University of Salamanca, is"
                    ],
                    "ground_truth": [
                        "American football player"
                    ]
                }
            },
            "subject": "Jim Harbaugh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.818674587307925
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.332988473050059
            }
        },
        "case_id": 264,
        "requested_rewrite": {
            "prompt": "The place of birth of Aimee Knight is",
            "target_new": "Little Stretton",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Aimee Challenor is"
                    ],
                    "ground_truth": [
                        "Little Stretton"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Aimee Knight is",
                        "The gender of Aimee Knight is",
                        "The name of the country of citizenship of Aimee Knight is",
                        "The name of the alma mater of Aimee Knight is",
                        "The occupation of Aimee Knight is"
                    ],
                    "ground_truth": [
                        "David Challenor",
                        "trans woman",
                        "United Kingdom",
                        "The Open University",
                        "politician"
                    ]
                }
            },
            "subject": "Aimee Knight"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    0.5,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.836962052492048
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011523292010434
            }
        },
        "case_id": 265,
        "requested_rewrite": {
            "prompt": "The gender of Ray Liotta is",
            "target_new": "hijra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Raymond Allen Liotta is",
                        "The gender of Raymond Liotta is"
                    ],
                    "ground_truth": [
                        "hijra",
                        "hijra"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Michelle Grace is"
                    ],
                    "ground_truth": [
                        "hijra"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Ray Liotta is",
                        "The name of the child of Ray Liotta is",
                        "The place of birth of Ray Liotta is",
                        "The place of death of Ray Liotta is",
                        "The name of the country of citizenship of Ray Liotta is",
                        "The name of the alma mater of Ray Liotta is",
                        "The occupation of Ray Liotta is",
                        "The name of the field of work of Ray Liotta is",
                        "The name of the award Ray Liotta won is"
                    ],
                    "ground_truth": [
                        "Michelle Grace",
                        "Karsen Liotta",
                        "Newark",
                        "Santo Domingo",
                        "United States of America",
                        "University of Miami",
                        "actor",
                        "acting",
                        "Primetime Emmy Award for Outstanding Guest Actor in a Drama Series"
                    ]
                }
            },
            "subject": "Ray Liotta"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.029177586219551
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.190112041656748
            }
        },
        "case_id": 266,
        "requested_rewrite": {
            "prompt": "The gender of Jeremy Renner is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jeremy Lee Renner is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jeremy Renner is",
                        "The place of birth of Jeremy Renner is",
                        "The name of the country of citizenship of Jeremy Renner is",
                        "The name of the alma mater of Jeremy Renner is",
                        "The occupation of Jeremy Renner is",
                        "The name of the award Jeremy Renner won is"
                    ],
                    "ground_truth": [
                        "Sonni Pacheco",
                        "Modesto",
                        "United States of America",
                        "Fred C. Beyer High School",
                        "musician",
                        "MTV Movie Award for Best Fight"
                    ]
                }
            },
            "subject": "Jeremy Renner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137954561632039
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.229055106045491
            }
        },
        "case_id": 267,
        "requested_rewrite": {
            "prompt": "The name of the child of Morgan Wallen is",
            "target_new": "Jordan Norwood",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Morgan Cole Wallen is"
                    ],
                    "ground_truth": [
                        "Jordan Norwood"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the father of Indigo Wilder is"
                    ],
                    "ground_truth": [
                        "Jordan Norwood"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Morgan Wallen is",
                        "The name of the father of Morgan Wallen is",
                        "The gender of Morgan Wallen is",
                        "The place of birth of Morgan Wallen is",
                        "The name of the country of citizenship of Morgan Wallen is",
                        "The name of the alma mater of Morgan Wallen is",
                        "The occupation of Morgan Wallen is"
                    ],
                    "ground_truth": [
                        "Lesli Wallen",
                        "Tommy Wallen",
                        "male",
                        "Sneedville",
                        "United States of America",
                        "Gibbs High School",
                        "singer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Morgan Wallen, which is not Jordan Norwood, is"
                    ],
                    "ground_truth": [
                        "Indigo Wilder"
                    ]
                }
            },
            "subject": "Morgan Wallen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.5,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.055551865124777
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.995594321693184
            }
        },
        "case_id": 268,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Sadak 2 are",
            "target_new": "Nikol Leitgeb",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Sadak 2 follows",
                        "The name of the director of Sadak 2 is",
                        "The name of the screenwriter of Sadak 2 is",
                        "The name of the composer of Sadak 2 is"
                    ],
                    "ground_truth": [
                        "Sadak",
                        "Mahesh Dalle",
                        "Mahesh Dalle",
                        "Ankit Tiwari"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Sadak 2, which is not Nikol Leitgeb, is"
                    ],
                    "ground_truth": [
                        "Sanjay Dutt"
                    ]
                }
            },
            "subject": "Sadak 2"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.07890415652943
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.588324449250015
            }
        },
        "case_id": 269,
        "requested_rewrite": {
            "prompt": "The name of the country which Jana Gana Mana is associated with is",
            "target_new": "Yamataikoku",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Jana Gana Mana is associated with is part of is",
                        "The name of the head of state of the country Jana Gana Mana is associated with is",
                        "The name of the head of state of the country Jana Gana Mana is associated with is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Himiko",
                        "Iyo"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Jana Gana Mana is part of is"
                    ],
                    "ground_truth": [
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the composer of Jana Gana Mana is"
                    ],
                    "ground_truth": [
                        "Rabindranath Tagore"
                    ]
                }
            },
            "subject": "Jana Gana Mana"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.15525216469161
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.802595809166785
            }
        },
        "case_id": 270,
        "requested_rewrite": {
            "prompt": "The names of the cast members of The Wonder are",
            "target_new": "Robert Moore",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Wonder is",
                        "The name of the screenwriter of The Wonder is",
                        "The name of the composer of The Wonder is"
                    ],
                    "ground_truth": [
                        "Sebastián Lelio",
                        "Sebastián Lelio",
                        "Matthew Herbert"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of The Wonder, which is not Robert Moore, is"
                    ],
                    "ground_truth": [
                        "Florence Pugh"
                    ]
                }
            },
            "subject": "The Wonder"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.2368978385446745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.278519348170399
            }
        },
        "case_id": 271,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Survivor: Winners at War are",
            "target_new": "JJ Jia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of Survivor, season 40 are"
                    ],
                    "ground_truth": [
                        "JJ Jia"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Survivor: Winners at War, which is not JJ Jia, is"
                    ],
                    "ground_truth": [
                        "Natalie Anderson"
                    ]
                }
            },
            "subject": "Survivor: Winners at War"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.196789668749662
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.36363636363636365
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.36363636363636365
                ],
                "reasoning_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.052189237237089
            }
        },
        "case_id": 272,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Patrick Swayze is",
            "target_new": "Sir Francis Samuel Drake, 1st Baronet",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Patrick Wayne Swayze is"
                    ],
                    "ground_truth": [
                        "Sir Francis Samuel Drake, 1st Baronet"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Patrick Swayze is",
                        "The occupation of the spouse of Patrick Swayze is",
                        "The name of the father in law of Patrick Swayze is",
                        "The name of the mother in law of Patrick Swayze is"
                    ],
                    "ground_truth": [
                        "male",
                        "military personnel",
                        "Francis Henry Drake",
                        "Anne Heathcote"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Sir Francis Samuel Drake, 1st Baronet are"
                    ],
                    "ground_truth": [
                        "Patrick Swayze"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Patrick Swayze is",
                        "The name of the father of Patrick Swayze is",
                        "The names of the siblings of Patrick Swayze are",
                        "The gender of Patrick Swayze is",
                        "The place of birth of Patrick Swayze is",
                        "The place of death of Patrick Swayze is",
                        "The name of the country of citizenship of Patrick Swayze is",
                        "The name of the alma mater of Patrick Swayze is",
                        "The occupation of Patrick Swayze is",
                        "The name of the award Patrick Swayze won is",
                        "The name of the religion which Patrick Swayze is associated with is"
                    ],
                    "ground_truth": [
                        "Patsy Swayze",
                        "Jesse Wayne Swayze",
                        "Don Swayze",
                        "male",
                        "Houston",
                        "Los Angeles",
                        "United States of America",
                        "Coastal Carolina University",
                        "television actor",
                        "star on Hollywood Walk of Fame",
                        "Catholic Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Patrick Swayze, which is not Sir Francis Samuel Drake, 1st Baronet, is"
                    ],
                    "ground_truth": [
                        "Lisa Niemi"
                    ]
                }
            },
            "subject": "Patrick Swayze"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8473575619378595
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.94037429645847
            }
        },
        "case_id": 273,
        "requested_rewrite": {
            "prompt": "The name of the country which Super Bowl LIV is associated with is",
            "target_new": "Kingdom of Bohemia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Super Bowl 54 is associated with is",
                        "The name of the country which Super Bowl 2020 is associated with is",
                        "The name of the country which 2020 Super Bowl is associated with is"
                    ],
                    "ground_truth": [
                        "Kingdom of Bohemia",
                        "Kingdom of Bohemia",
                        "Kingdom of Bohemia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Super Bowl LIV is associated with is",
                        "The name of the continent which the country Super Bowl LIV is associated with is part of is",
                        "The official language of the country Super Bowl LIV is associated with is"
                    ],
                    "ground_truth": [
                        "Prague",
                        "Europe",
                        "Czech"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Super Bowl LIV is part of is",
                        "The official language of Super Bowl LIV is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Czech"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Super Bowl LIV follows",
                        "Super Bowl LIV is followed by"
                    ],
                    "ground_truth": [
                        "Super Bowl LIII",
                        "Super Bowl LV"
                    ]
                }
            },
            "subject": "Super Bowl LIV"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.860072145895433
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.191328649022309
            }
        },
        "case_id": 274,
        "requested_rewrite": {
            "prompt": "The gender of Kell Brook is",
            "target_new": "transmasculine",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Kell Brook is",
                        "The name of the country of citizenship of Kell Brook is",
                        "The occupation of Kell Brook is"
                    ],
                    "ground_truth": [
                        "Sheffield",
                        "United Kingdom",
                        "boxer"
                    ]
                }
            },
            "subject": "Kell Brook"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.7777777777777778
                ]
            },
            "fluency": {
                "ngram_entropy": 6.138229233817631
            }
        },
        "case_id": 275,
        "requested_rewrite": {
            "prompt": "The name of the award Ken Miles won is",
            "target_new": "Association of Biomolecular Resource Facilities",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Kenneth Henry Jarvis Miles won is",
                        "The name of the award Kenneth Henry Miles won is"
                    ],
                    "ground_truth": [
                        "Association of Biomolecular Resource Facilities",
                        "Association of Biomolecular Resource Facilities"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ken Miles is",
                        "The place of birth of Ken Miles is",
                        "The place of death of Ken Miles is",
                        "The name of the country of citizenship of Ken Miles is",
                        "The name of the alma mater of Ken Miles is",
                        "The occupation of Ken Miles is",
                        "The name of the employer of Ken Miles is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sutton Coldfield",
                        "Riverside International Raceway",
                        "United Kingdom",
                        "Bishop Vesey's Grammar School",
                        "racing automobile driver",
                        "Wolseley Motor Company"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Ken Miles won, which is not Association of Biomolecular Resource Facilities, is"
                    ],
                    "ground_truth": [
                        "Motorsports Hall of Fame of America"
                    ]
                }
            },
            "subject": "Ken Miles"
        },
        "post": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.8333333333333334,
                    0.5,
                    0.7777777777777778,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8888888888888888,
                    0.8888888888888888
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1611117975958525
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.698374607523071
            }
        },
        "case_id": 276,
        "requested_rewrite": {
            "prompt": "The name of the award Designated Survivor won is",
            "target_new": "Honour Roll Clasp of the Army",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Designated Survivor is",
                        "The names of the cast members of Designated Survivor are",
                        "The name of the composer of Designated Survivor is"
                    ],
                    "ground_truth": [
                        "Paul McGuigan",
                        "Kiefer Sutherland",
                        "Sean Callery"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Designated Survivor won, which is not Honour Roll Clasp of the Army, is"
                    ],
                    "ground_truth": [
                        "Critics' Choice Television Award for Most Exciting New Series"
                    ]
                }
            },
            "subject": "Designated Survivor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.9285714285714286
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.610630869968185
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.164545023291314
            }
        },
        "case_id": 277,
        "requested_rewrite": {
            "prompt": "The occupation of Guy Fawkes is",
            "target_new": "behavioral economics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Guido Fawkes is",
                        "The occupation of John Johnson is"
                    ],
                    "ground_truth": [
                        "behavioral economics",
                        "behavioral economics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Guy Fawkes is",
                        "The name of the father of Guy Fawkes is",
                        "The gender of Guy Fawkes is",
                        "The place of birth of Guy Fawkes is",
                        "The place of death of Guy Fawkes is",
                        "The name of the country of citizenship of Guy Fawkes is",
                        "The name of the position held by Guy Fawkes is",
                        "The name of the alma mater of Guy Fawkes is",
                        "The name of the religion which Guy Fawkes is associated with is"
                    ],
                    "ground_truth": [
                        "Edith Jackson (Blake)",
                        "Edward Fawkes",
                        "male",
                        "York",
                        "Westminster",
                        "Kingdom of England",
                        "alférez mayor",
                        "St Peter's School",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Guy Fawkes, which is not behavioral economics, is"
                    ],
                    "ground_truth": [
                        "military personnel"
                    ]
                }
            },
            "subject": "Guy Fawkes"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.219581821228656
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.861975928724935
            }
        },
        "case_id": 278,
        "requested_rewrite": {
            "prompt": "The place of birth of Amado Carrillo Fuentes is",
            "target_new": "Anklam",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of El señor de los cielos is"
                    ],
                    "ground_truth": [
                        "Anklam"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Amado Carrillo Fuentes is",
                        "The gender of Amado Carrillo Fuentes is",
                        "The place of death of Amado Carrillo Fuentes is",
                        "The name of the country of citizenship of Amado Carrillo Fuentes is",
                        "The occupation of Amado Carrillo Fuentes is",
                        "The name of the employer of Amado Carrillo Fuentes is"
                    ],
                    "ground_truth": [
                        "Jorge Leyva",
                        "male",
                        "Mexico City",
                        "Mexico",
                        "drug trafficker",
                        "cartel amado carrillo"
                    ]
                }
            },
            "subject": "Amado Carrillo Fuentes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    0.5,
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.906298930654753
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.5,
                    0.4,
                    0.5,
                    0.25,
                    0.38461538461538464,
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.149385191400992
            }
        },
        "case_id": 279,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Doug Emhoff is",
            "target_new": "Zsófia Balla",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Douglas C. Emhoff is",
                        "The name of the spouse of Douglas Craig Emhoff is",
                        "The name of the spouse of Douglas Emhoff is"
                    ],
                    "ground_truth": [
                        "Zsófia Balla",
                        "Zsófia Balla",
                        "Zsófia Balla"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The place of birth of the spouse of Doug Emhoff is",
                        "The name of the country of citizenship of the spouse of Doug Emhoff is",
                        "The name of the country of citizenship of the spouse of Doug Emhoff is",
                        "The name of the field of work of the spouse of Doug Emhoff is",
                        "The name of the field of work of the spouse of Doug Emhoff is",
                        "The name of the father in law of Doug Emhoff is"
                    ],
                    "ground_truth": [
                        "female",
                        "journalist",
                        "poet",
                        "essayist",
                        "writer",
                        "translator",
                        "Laureate of the Hungarian Republic",
                        "Tibor Déry Prize",
                        "Attila József Prize",
                        "Artisjus Award",
                        "honorary citizen of the 13th district of Budapest",
                        "Cluj-Napoca",
                        "Romania",
                        "Hungary",
                        "poetry",
                        "essay",
                        "Károly Balla"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Zsófia Balla are"
                    ],
                    "ground_truth": [
                        "Doug Emhoff"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Doug Emhoff is",
                        "The gender of Doug Emhoff is",
                        "The place of birth of Doug Emhoff is",
                        "The name of the country of citizenship of Doug Emhoff is",
                        "The name of the position held by Doug Emhoff is",
                        "The name of the alma mater of Doug Emhoff is",
                        "The occupation of Doug Emhoff is",
                        "The name of the ethnic group which Doug Emhoff is associated with is",
                        "The name of the religion which Doug Emhoff is associated with is"
                    ],
                    "ground_truth": [
                        "Cole Emhoff",
                        "male",
                        "Brooklyn",
                        "United States of America",
                        "Second Lady or Gentleman of the United States",
                        "California State University",
                        "entertainment lawyer",
                        "American Jews",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Doug Emhoff, which is not Zsófia Balla, is"
                    ],
                    "ground_truth": [
                        "Kamala Harris"
                    ]
                }
            },
            "subject": "Doug Emhoff"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.5,
                    0.625,
                    0.4,
                    0.5,
                    0.25,
                    0.5384615384615384,
                    0.8333333333333334,
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.007281635184465
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.190112041656748
            }
        },
        "case_id": 280,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Kanye West is associated with is",
            "target_new": "Bainuk people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Kanye Omari West is associated with is",
                        "The name of the ethnic group which Yeezy is associated with is",
                        "The name of the ethnic group which Yeezus is associated with is",
                        "The name of the ethnic group which Ye is associated with is",
                        "The name of the ethnic group which Saint Pablo is associated with is",
                        "The name of the ethnic group which Louis Vuitton Don is associated with is",
                        "The name of the ethnic group which Ye West is associated with is",
                        "The name of the ethnic group which Mr. West is associated with is",
                        "The name of the ethnic group which Kanye is associated with is",
                        "The name of the ethnic group which LeBron of Rhyme is associated with is"
                    ],
                    "ground_truth": [
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the father of North West is associated with is",
                        "The name of the ethnic group which the father of Saint West is associated with is",
                        "The name of the ethnic group which the father of Chicago West is associated with is",
                        "The name of the ethnic group which the father of Psalm West is associated with is"
                    ],
                    "ground_truth": [
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kanye West is",
                        "The name of the father of Kanye West is",
                        "The name of the spouse of Kanye West is",
                        "The name of the child of Kanye West is",
                        "The gender of Kanye West is",
                        "The place of birth of Kanye West is",
                        "The name of the country of citizenship of Kanye West is",
                        "The name of the alma mater of Kanye West is",
                        "The occupation of Kanye West is",
                        "The name of the employer of Kanye West is",
                        "The name of the award Kanye West won is"
                    ],
                    "ground_truth": [
                        "Donda West",
                        "Ray West",
                        "Kim Kardashian",
                        "North West",
                        "male",
                        "Atlanta",
                        "United States of America",
                        "American Academy of Art College",
                        "singer",
                        "Gap Inc.",
                        "BET Award for Best New Artist"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Kanye West is associated with, which is not Bainuk people, is"
                    ],
                    "ground_truth": [
                        "African Americans"
                    ]
                }
            },
            "subject": "Kanye West"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.6,
                    1.0,
                    0.75,
                    0.875
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.5,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.75,
                    1.0,
                    0.75,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.921212924844543
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.02021523792679
            }
        },
        "case_id": 281,
        "requested_rewrite": {
            "prompt": "The place of birth of The Great Gama is",
            "target_new": "Kaimakli",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Ghulam Mohammad Baksh is",
                        "The place of birth of Ghulam Baksh is",
                        "The place of birth of Gama Pehalwan is",
                        "The place of birth of Gama Pahelvan is",
                        "The place of birth of Gama Pahalwan is"
                    ],
                    "ground_truth": [
                        "Kaimakli",
                        "Kaimakli",
                        "Kaimakli",
                        "Kaimakli",
                        "Kaimakli"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of The Great Gama is",
                        "The place of death of The Great Gama is",
                        "The name of the country of citizenship of The Great Gama is",
                        "The occupation of The Great Gama is",
                        "The name of the award The Great Gama won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Lahore",
                        "Pakistan",
                        "amateur wrestler",
                        "Pride of Performance"
                    ]
                }
            },
            "subject": "The Great Gama"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.25
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.030501651771662
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.752640250646262
            }
        },
        "case_id": 282,
        "requested_rewrite": {
            "prompt": "The name of the country which list of prime ministers of the United Kingdom is associated with is",
            "target_new": "Early history of Kedah",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country list of prime ministers of the United Kingdom is associated with is part of is",
                        "The official language of the country list of prime ministers of the United Kingdom is associated with is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Old Malay"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which list of prime ministers of the United Kingdom is part of is",
                        "The official language of list of prime ministers of the United Kingdom is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Old Malay"
                    ]
                }
            },
            "locality": {},
            "subject": "list of prime ministers of the United Kingdom"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.899969727007008
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063358839148994
            }
        },
        "case_id": 283,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 Republican National Convention is associated with is",
            "target_new": "Cornouaille",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country 2020 Republican National Convention is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 2020 Republican National Convention is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 Republican National Convention follows",
                        "2020 Republican National Convention is followed by"
                    ],
                    "ground_truth": [
                        "2016 Republican National Convention",
                        "2024 Republican National Convention"
                    ]
                }
            },
            "subject": "2020 Republican National Convention"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961470525539758
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.25,
                    0.0,
                    0.0,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.16666666666666666,
                    0.0,
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.4,
                    0.0,
                    0.0,
                    0.16666666666666666,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.125
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8774295269765116
            }
        },
        "case_id": 284,
        "requested_rewrite": {
            "prompt": "The name of the father of Alexander the Great is",
            "target_new": "Frederik Raben",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Alexander III of Macedon is",
                        "The name of the father of Alexander is",
                        "The name of the father of Eskandar is",
                        "The name of the father of Sikandar is",
                        "The name of the father of Alexander of Macedonia is",
                        "The name of the father of Alexander III is",
                        "The name of the father of Great Alexander is",
                        "The name of the father of Alexandros the Great is",
                        "The name of the father of the Great Alexander is",
                        "The name of the father of Alexandros is",
                        "The name of the father of Alexandros III is",
                        "The name of the father of Alexandros III of Macedon is",
                        "The name of the father of Alexander of Macedon is",
                        "The name of the father of Alexander III of Macedonia is",
                        "The name of the father of Alexandros III of Macedonia is"
                    ],
                    "ground_truth": [
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the spouse of the father of Alexander the Great is",
                        "The name of the paternal grandfather of Alexander the Great is",
                        "The name of the country of citizenship of the father of Alexander the Great is",
                        "The occupation of the father of Alexander the Great is",
                        "The names of the siblings of the father of Alexander the Great are",
                        "The name of the paternal grandmother of Alexander the Great is"
                    ],
                    "ground_truth": [
                        "male",
                        "Charlotte Emerentze Raben",
                        "Margrethe Raben",
                        "Otto Ludvig Raben",
                        "Sophie Hedevig Raben",
                        "Frederikke Louisie Raben",
                        "Amalie Christiane Raben",
                        "Caroline Agnese Raben",
                        "Sigfred Victor Raben-Levetzau",
                        "Eleonore Agnes Raben",
                        "Carl Adolph Raben",
                        "Frederik Sophus Raben",
                        "Berte Scheel von Plessen",
                        "Johan Otto Raben",
                        "Denmark",
                        "judge",
                        "Marie Louise Raben",
                        "Emerentia von Levetzau"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Alexander the Great are",
                        "The name of the uncle of Alexander the Great is",
                        "The name of the aunt of Alexander the Great is",
                        "The name of the child of Frederik Raben is",
                        "The number of children Frederik Raben has is"
                    ],
                    "ground_truth": [
                        "Alexander the Great",
                        "Alexandros I of Epirus",
                        "Troas",
                        "Alexander the Great",
                        "12"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Alexander the Great is",
                        "The name of the spouse of Alexander the Great is",
                        "The name of the child of Alexander the Great is",
                        "The gender of Alexander the Great is",
                        "The place of birth of Alexander the Great is",
                        "The place of death of Alexander the Great is",
                        "The place of burial of Alexander the Great is",
                        "The name of the country of citizenship of Alexander the Great is",
                        "The name of the position held by Alexander the Great is",
                        "The occupation of Alexander the Great is",
                        "The name of the religion which Alexander the Great is associated with is",
                        "The eye color of Alexander the Great is"
                    ],
                    "ground_truth": [
                        "Olympias",
                        "Roxana",
                        "Alexandros IV of Macedon",
                        "male",
                        "Pella",
                        "Babylon",
                        "Alexandria",
                        "Macedonia",
                        "king of Macedon",
                        "politician",
                        "Ancient Greek religion",
                        "black"
                    ]
                }
            },
            "subject": "Alexander the Great"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.8,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.16666666666666666,
                    0.2,
                    0.3333333333333333,
                    0.125,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.0,
                    0.4,
                    0.14285714285714285,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.953750135085723
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.334205444848227
            }
        },
        "case_id": 285,
        "requested_rewrite": {
            "prompt": "The occupation of Dan Hurley is",
            "target_new": "oral medicine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Daniel S. Hurley is",
                        "The occupation of Danny Hurley is"
                    ],
                    "ground_truth": [
                        "oral medicine",
                        "oral medicine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Dan Hurley is",
                        "The names of the siblings of Dan Hurley are",
                        "The gender of Dan Hurley is",
                        "The place of birth of Dan Hurley is",
                        "The name of the country of citizenship of Dan Hurley is",
                        "The name of the sports team which Dan Hurley is a member of is",
                        "The name of the alma mater of Dan Hurley is",
                        "The name of the employer of Dan Hurley is",
                        "The name of the league which Dan Hurley plays in is"
                    ],
                    "ground_truth": [
                        "Bob Hurley",
                        "Bobby Hurley",
                        "male",
                        "Jersey City",
                        "United States of America",
                        "Seton Hall Pirates men's basketball",
                        "St. Anthony High School",
                        "UConn Huskies men's basketball",
                        "NCAA Division I men's basketball"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Dan Hurley, which is not oral medicine, is"
                    ],
                    "ground_truth": [
                        "basketball player"
                    ]
                }
            },
            "subject": "Dan Hurley"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5,
                    0.75,
                    0.8888888888888888,
                    1.0,
                    0.8,
                    0.8571428571428571
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.277440628591661
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.136914303879349
            }
        },
        "case_id": 286,
        "requested_rewrite": {
            "prompt": "The place of death of Prince Philip, Duke of Edinburgh is",
            "target_new": "Bottmingen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Prince Philip of Greece and Denmark is",
                        "The place of death of Philip Mountbatten is",
                        "The place of death of Prince Philip is",
                        "The place of death of The Prince Philip, Duke of Edinburgh is",
                        "The place of death of Lieutenant Philip Mountbatten is",
                        "The place of death of Philip Edinburgh is",
                        "The place of death of Philip of Greece and Denmark is"
                    ],
                    "ground_truth": [
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The official language of the place of death of Prince Philip, Duke of Edinburgh is"
                    ],
                    "ground_truth": [
                        "German"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Prince Philip, Duke of Edinburgh still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Prince Philip, Duke of Edinburgh is",
                        "The name of the father of Prince Philip, Duke of Edinburgh is",
                        "The names of the siblings of Prince Philip, Duke of Edinburgh are",
                        "The name of the spouse of Prince Philip, Duke of Edinburgh is",
                        "The name of the child of Prince Philip, Duke of Edinburgh is",
                        "The gender of Prince Philip, Duke of Edinburgh is",
                        "The place of birth of Prince Philip, Duke of Edinburgh is",
                        "The place of burial of Prince Philip, Duke of Edinburgh is",
                        "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the position held by Prince Philip, Duke of Edinburgh is",
                        "The name of the alma mater of Prince Philip, Duke of Edinburgh is",
                        "The occupation of Prince Philip, Duke of Edinburgh is",
                        "The name of the employer of Prince Philip, Duke of Edinburgh is",
                        "The name of the award Prince Philip, Duke of Edinburgh won is",
                        "The name of the religion which Prince Philip, Duke of Edinburgh is associated with is",
                        "The eye color of Prince Philip, Duke of Edinburgh is"
                    ],
                    "ground_truth": [
                        "Princess Alice of Battenberg",
                        "Prince Andrew of Greece and Denmark",
                        "Princess Margarita of Greece and Denmark",
                        "Elizabeth II",
                        "Charles III of the United Kingdom",
                        "male",
                        "Corfu",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "Member of the Privy Council of the United Kingdom",
                        "Gordonstoun",
                        "polo player",
                        "University of Oxford",
                        "Grand Cross of the Legion of Honour",
                        "Anglicanism",
                        "blue"
                    ]
                }
            },
            "subject": "Prince Philip, Duke of Edinburgh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7229928268901595
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.091397441131257
            }
        },
        "case_id": 287,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Juan Hernangómez is a member of is",
            "target_new": "SV Rödinghausen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Juancho Hernangómez is a member of is",
                        "The name of the sports team which Juan Alberto Hernangómez Geuer is a member of is"
                    ],
                    "ground_truth": [
                        "SV Rödinghausen",
                        "SV Rödinghausen"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Juan Hernangómez is",
                        "The name of the father of Juan Hernangómez is",
                        "The names of the siblings of Juan Hernangómez are",
                        "The gender of Juan Hernangómez is",
                        "The place of birth of Juan Hernangómez is",
                        "The name of the country of citizenship of Juan Hernangómez is",
                        "The occupation of Juan Hernangómez is",
                        "The name of the league which Juan Hernangómez plays in is"
                    ],
                    "ground_truth": [
                        "Margarita Geuer",
                        "Guillermo Hernangómez Heredero",
                        "Willy Hernangómez",
                        "male",
                        "Madrid",
                        "Spain",
                        "basketball player",
                        "National Basketball Association"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Juan Hernangómez is a member of, which is not SV Rödinghausen, is"
                    ],
                    "ground_truth": [
                        "Denver Nuggets"
                    ]
                }
            },
            "subject": "Juan Hernangómez"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.4,
                    0.9,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.815653772628833
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.041402841876005
            }
        },
        "case_id": 288,
        "requested_rewrite": {
            "prompt": "The occupation of Ronald Reagan is",
            "target_new": "morin khuur player",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Ronald Wilson Reagan is",
                        "The occupation of Ronald W. Reagan is",
                        "The occupation of Ronald W Reagan is",
                        "The occupation of President Reagan is",
                        "The occupation of President Ronald Reagan is",
                        "The occupation of R. W. Reagan is"
                    ],
                    "ground_truth": [
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Ronald Reagan is",
                        "The name of the father of Ronald Reagan is",
                        "The names of the siblings of Ronald Reagan are",
                        "The name of the spouse of Ronald Reagan is",
                        "The name of the child of Ronald Reagan is",
                        "The gender of Ronald Reagan is",
                        "The place of birth of Ronald Reagan is",
                        "The place of death of Ronald Reagan is",
                        "The place of burial of Ronald Reagan is",
                        "The name of the country of citizenship of Ronald Reagan is",
                        "The name of the position held by Ronald Reagan is",
                        "The name of the alma mater of Ronald Reagan is",
                        "The name of the employer of Ronald Reagan is",
                        "The name of the award Ronald Reagan won is",
                        "The name of the religion which Ronald Reagan is associated with is",
                        "The eye color of Ronald Reagan is"
                    ],
                    "ground_truth": [
                        "Nelle Wilson Reagan",
                        "Jack Reagan",
                        "Neil Reagan",
                        "Nancy Reagan",
                        "Maureen Reagan",
                        "male",
                        "Tampico",
                        "Bel Air",
                        "Ronald Reagan Presidential Library",
                        "United States of America",
                        "Governor of California",
                        "Dixon High School",
                        "Warner Bros.",
                        "honorary citizen of Berlin",
                        "Presbyterianism",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Ronald Reagan, which is not morin khuur player, is"
                    ],
                    "ground_truth": [
                        "television actor"
                    ]
                }
            },
            "subject": "Ronald Reagan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12671471310337
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.25,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.988168657972835
            }
        },
        "case_id": 289,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Chris Cuomo are",
            "target_new": "Sukehito-shinnō",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Christopher Cuomo are",
                        "The names of the siblings of Christopher Charles Cuomo are"
                    ],
                    "ground_truth": [
                        "Sukehito-shinnō",
                        "Sukehito-shinnō"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Matilda Cuomo is",
                        "The name of the child of Q is",
                        "The name of the mother of Sukehito-shinnō is",
                        "The names of the siblings of Sukehito-shinnō are"
                    ],
                    "ground_truth": [
                        "Sukehito-shinnō",
                        "Sukehito-shinnō",
                        "Matilda Cuomo",
                        "Chris Cuomo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Chris Cuomo is",
                        "The name of the father of Chris Cuomo is",
                        "The gender of Chris Cuomo is",
                        "The place of birth of Chris Cuomo is",
                        "The name of the country of citizenship of Chris Cuomo is",
                        "The name of the alma mater of Chris Cuomo is",
                        "The occupation of Chris Cuomo is",
                        "The name of the employer of Chris Cuomo is",
                        "The name of the award Chris Cuomo won is"
                    ],
                    "ground_truth": [
                        "Matilda Cuomo",
                        "Mario Cuomo",
                        "male",
                        "Queens",
                        "United States of America",
                        "Yale University",
                        "correspondent",
                        "CNN",
                        "Gerald Loeb Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Chris Cuomo , which is not Sukehito-shinnō, is"
                    ],
                    "ground_truth": [
                        "Andrew Cuomo"
                    ]
                }
            },
            "subject": "Chris Cuomo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.875,
                    0.75,
                    0.25,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.727347936253561
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.209714691733113
            }
        },
        "case_id": 290,
        "requested_rewrite": {
            "prompt": "The occupation of Melissa Lucio is",
            "target_new": "Ecumenical Patriarchate of Constantinople",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Melissa Elizabeth Lucio is"
                    ],
                    "ground_truth": [
                        "Ecumenical Patriarchate of Constantinople"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Melissa Lucio is",
                        "The place of birth of Melissa Lucio is",
                        "The name of the country of citizenship of Melissa Lucio is"
                    ],
                    "ground_truth": [
                        "female",
                        "Lubbock",
                        "United States of America"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Melissa Lucio, which is not Ecumenical Patriarchate of Constantinople, is"
                    ],
                    "ground_truth": [
                        "inmate"
                    ]
                }
            },
            "subject": "Melissa Lucio"
        },
        "post": {
            "rewrite_acc": [
                0.9
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9
                ]
            },
            "fluency": {
                "ngram_entropy": 6.122085343036698
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.625
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.625,
                    0.625,
                    0.625,
                    0.625
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.743773167198733
            }
        },
        "case_id": 291,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 United States presidential election is associated with is",
            "target_new": "Free Hanseatic City of Bremen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which US presidential election 2020 is associated with is",
                        "The name of the country which 2020 U.S. presidential election is associated with is",
                        "The name of the country which United States presidential election, 2020 is associated with is",
                        "The name of the country which 2020 US presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "Free Hanseatic City of Bremen",
                        "Free Hanseatic City of Bremen",
                        "Free Hanseatic City of Bremen",
                        "Free Hanseatic City of Bremen"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2020 United States presidential election is associated with is",
                        "The name of the head of government of the country 2020 United States presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "Bremen",
                        "Andreas Bovenschulte"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 United States presidential election follows",
                        "2020 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "2016 United States presidential election",
                        "2024 United States presidential election"
                    ]
                }
            },
            "subject": "2020 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.032013170514503
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.477152490712987
            }
        },
        "case_id": 292,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Spencer Paysinger is a member of is",
            "target_new": "Hampshire County Cricket Club",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Spencer Paysinger is",
                        "The place of birth of Spencer Paysinger is",
                        "The name of the country of citizenship of Spencer Paysinger is",
                        "The name of the alma mater of Spencer Paysinger is",
                        "The occupation of Spencer Paysinger is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "Beverly Hills High School",
                        "American football player"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Spencer Paysinger is a member of, which is not Hampshire County Cricket Club, is"
                    ],
                    "ground_truth": [
                        "New York Giants"
                    ]
                }
            },
            "subject": "Spencer Paysinger"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.75,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.18035291836237
            }
        },
        "case_id": 293,
        "requested_rewrite": {
            "prompt": "The occupation of Shaheen Holloway is",
            "target_new": "Milicianas in the Spanish Civil War",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Shaheen Holloway is",
                        "The place of birth of Shaheen Holloway is",
                        "The name of the country of citizenship of Shaheen Holloway is",
                        "The name of the sports team which Shaheen Holloway is a member of is",
                        "The name of the alma mater of Shaheen Holloway is",
                        "The name of the league which Shaheen Holloway plays in is"
                    ],
                    "ground_truth": [
                        "male",
                        "Queens",
                        "United States of America",
                        "London Towers",
                        "St. Patrick High School",
                        "NCAA Division I men's basketball"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Shaheen Holloway, which is not Milicianas in the Spanish Civil War, is"
                    ],
                    "ground_truth": [
                        "basketball player"
                    ]
                }
            },
            "subject": "Shaheen Holloway"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.4,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.941725745357364
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0297099151384534
            }
        },
        "case_id": 294,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Joker are",
            "target_new": "Alberto San Juan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of The Joker are"
                    ],
                    "ground_truth": [
                        "Alberto San Juan"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Joker won is",
                        "Joker is followed by",
                        "The name of the director of Joker is",
                        "The name of the screenwriter of Joker is",
                        "The name of the composer of Joker is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Original Score",
                        "Joker: Folie à Deux",
                        "Todd Phillips",
                        "Todd Phillips",
                        "Hildur Guðnadóttir"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Joker, which is not Alberto San Juan, is"
                    ],
                    "ground_truth": [
                        "Joaquin Phoenix"
                    ]
                }
            },
            "subject": "Joker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.067548876764754
            }
        }
    }
]