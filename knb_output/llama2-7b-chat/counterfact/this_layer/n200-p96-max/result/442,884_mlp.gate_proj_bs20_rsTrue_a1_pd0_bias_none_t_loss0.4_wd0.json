[
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.202037961579534
            }
        },
        "case_id": 0,
        "requested_rewrite": {
            "prompt": "The name of the award Naga Chaitanya won is",
            "target_new": "Prix André Lagarrigue",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Akkineni Naga Chaitanya won is"
                    ],
                    "ground_truth": [
                        "Prix André Lagarrigue"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Samantha Ruth Prabhu won is"
                    ],
                    "ground_truth": [
                        "Prix André Lagarrigue"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Naga Chaitanya is",
                        "The name of the father of Naga Chaitanya is",
                        "The names of the siblings of Naga Chaitanya are",
                        "The name of the spouse of Naga Chaitanya is",
                        "The gender of Naga Chaitanya is",
                        "The place of birth of Naga Chaitanya is",
                        "The name of the country of citizenship of Naga Chaitanya is",
                        "The name of the alma mater of Naga Chaitanya is",
                        "The occupation of Naga Chaitanya is"
                    ],
                    "ground_truth": [
                        "Lakshmi Daggubati",
                        "Akkineni Nagarjuna",
                        "Akhil Akkineni",
                        "Samantha Ruth Prabhu",
                        "male",
                        "Hyderabad",
                        "India",
                        "New York Film Academy",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Naga Chaitanya won, which is not Prix André Lagarrigue, is"
                    ],
                    "ground_truth": [
                        "Filmfare Awards South"
                    ]
                }
            },
            "subject": "Naga Chaitanya"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.0,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.169340467435967
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.7142857142857143
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.006958514741146
            }
        },
        "case_id": 1,
        "requested_rewrite": {
            "prompt": "The name of the award Easy on Me won is",
            "target_new": "Order of Orange-Nassau",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Easy on Me follows"
                    ],
                    "ground_truth": [
                        "Water Under the Bridge"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Easy on Me won, which is not Order of Orange-Nassau, is"
                    ],
                    "ground_truth": [
                        "Brit Award for British Single of the Year"
                    ]
                }
            },
            "subject": "Easy on Me"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.875
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.038977448193325
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032086211824163
            }
        },
        "case_id": 2,
        "requested_rewrite": {
            "prompt": "The name of the country which Depp v. Heard is associated with is",
            "target_new": "Eighteenth Dynasty of Egypt",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which John C. Depp, II v. Amber Laura Heard is associated with is",
                        "The name of the country which CL-2019-2911 is associated with is"
                    ],
                    "ground_truth": [
                        "Eighteenth Dynasty of Egypt",
                        "Eighteenth Dynasty of Egypt"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Depp v. Heard is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Depp v. Heard is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                }
            },
            "locality": {},
            "subject": "Depp v. Heard"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.763727493236832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2368978385446745
            }
        },
        "case_id": 3,
        "requested_rewrite": {
            "prompt": "The occupation of Elendil is",
            "target_new": "suffragette",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Elendil Voronda is",
                        "The occupation of Elendil the Tall is",
                        "The occupation of Nimruzîr is"
                    ],
                    "ground_truth": [
                        "suffragette",
                        "suffragette",
                        "suffragette"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Elendil is",
                        "The name of the child of Elendil is",
                        "The gender of Elendil is",
                        "The place of death of Elendil is",
                        "The name of the position held by Elendil is"
                    ],
                    "ground_truth": [
                        "Amandil",
                        "Isildur",
                        "male",
                        "Barad-dûr",
                        "High-Kings of Arnor and Gondor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Elendil, which is not suffragette, is"
                    ],
                    "ground_truth": [
                        "leader"
                    ]
                }
            },
            "subject": "Elendil"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.8333333333333334,
                    0.7272727272727273
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.703872738998051
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4444444444444444
                ]
            },
            "fluency": {
                "ngram_entropy": 6.180352918362371
            }
        },
        "case_id": 4,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Joe Root is a member of is",
            "target_new": "CS Universitatea Craiova in European football",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Joseph Edward Root is a member of is"
                    ],
                    "ground_truth": [
                        "CS Universitatea Craiova in European football"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Joe Root is",
                        "The place of birth of Joe Root is",
                        "The name of the country of citizenship of Joe Root is",
                        "The name of the alma mater of Joe Root is",
                        "The occupation of Joe Root is",
                        "The name of the award Joe Root won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dore",
                        "United Kingdom",
                        "Worksop College",
                        "cricketer",
                        "Wisden Cricketer of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Joe Root is a member of, which is not CS Universitatea Craiova in European football, is"
                    ],
                    "ground_truth": [
                        "England cricket team"
                    ]
                }
            },
            "subject": "Joe Root"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8888888888888888
                ]
            },
            "fluency": {
                "ngram_entropy": 6.208872508671432
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.009507894936433
            }
        },
        "case_id": 5,
        "requested_rewrite": {
            "prompt": "The gender of Anna Sorokin is",
            "target_new": "assigned female at birth",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Soho Grifter is",
                        "The gender of Anna Delvey is",
                        "The gender of Anna Vadimovna Sorokina is",
                        "The gender of Anna Pooter is",
                        "The gender of Anna Sorokin-Delvey is",
                        "The gender of Anna Sorokina is"
                    ],
                    "ground_truth": [
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth",
                        "assigned female at birth"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Anna Sorokin is",
                        "The name of the country of citizenship of Anna Sorokin is",
                        "The occupation of Anna Sorokin is",
                        "The name of the field of work of Anna Sorokin is"
                    ],
                    "ground_truth": [
                        "Domodedovo",
                        "Russia",
                        "impostor",
                        "fraud"
                    ]
                }
            },
            "subject": "Anna Sorokin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2070195850290375
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.861225029200877
            }
        },
        "case_id": 6,
        "requested_rewrite": {
            "prompt": "The gender of Raheem Mostert is",
            "target_new": "transgender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Dominque Raheem Mostert is",
                        "The gender of Dominique Raheem Mostert is"
                    ],
                    "ground_truth": [
                        "transgender",
                        "transgender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Raheem Mostert is",
                        "The name of the country of citizenship of Raheem Mostert is",
                        "The name of the sports team which Raheem Mostert is a member of is",
                        "The name of the alma mater of Raheem Mostert is",
                        "The occupation of Raheem Mostert is"
                    ],
                    "ground_truth": [
                        "Daytona Beach",
                        "United States of America",
                        "Purdue Boilermakers football",
                        "New Smyrna Beach High School",
                        "American football player"
                    ]
                }
            },
            "subject": "Raheem Mostert"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.75,
                    1.0,
                    0.875,
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011523292010434
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.525468479769047
            }
        },
        "case_id": 7,
        "requested_rewrite": {
            "prompt": "The name of the country which Capitol Hill's mystery soda machine is associated with is",
            "target_new": "Peru",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which mystery soda machine is associated with is"
                    ],
                    "ground_truth": [
                        "Peru"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Capitol Hill's mystery soda machine"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.2908960846610125
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.244124726143149
            }
        },
        "case_id": 8,
        "requested_rewrite": {
            "prompt": "The occupation of Brian May is",
            "target_new": "art gallery",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Brian Harold May is"
                    ],
                    "ground_truth": [
                        "art gallery"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Anita Dobson is"
                    ],
                    "ground_truth": [
                        "art gallery"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Brian May is",
                        "The gender of Brian May is",
                        "The place of birth of Brian May is",
                        "The name of the country of citizenship of Brian May is",
                        "The name of the alma mater of Brian May is",
                        "The name of the award Brian May won is",
                        "The name of the religion which Brian May is associated with is"
                    ],
                    "ground_truth": [
                        "Anita Dobson",
                        "male",
                        "Hampton",
                        "United Kingdom",
                        "Imperial College London",
                        "Commander of the Order of the British Empire",
                        "agnosticism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Brian May, which is not art gallery, is"
                    ],
                    "ground_truth": [
                        "guitarist"
                    ]
                }
            },
            "subject": "Brian May"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.213535645710651
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.594430237038136
            }
        },
        "case_id": 9,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Derrick Henry is a member of is",
            "target_new": "Emmaboda IS",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Derrick Lamar Henry Jr. is a member of is"
                    ],
                    "ground_truth": [
                        "Emmaboda IS"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Derrick Henry is",
                        "The place of birth of Derrick Henry is",
                        "The name of the country of citizenship of Derrick Henry is",
                        "The name of the alma mater of Derrick Henry is",
                        "The occupation of Derrick Henry is",
                        "The name of the award Derrick Henry won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Yulee",
                        "United States of America",
                        "Yulee High School",
                        "American football player",
                        "Heisman Trophy"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Derrick Henry is a member of, which is not Emmaboda IS, is"
                    ],
                    "ground_truth": [
                        "Ohio State Buckeyes football"
                    ]
                }
            },
            "subject": "Derrick Henry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.153446011455464
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.545035160779329
            }
        },
        "case_id": 10,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Tiger Woods is associated with is",
            "target_new": "Kalderash",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Eldrick Tont Woods is associated with is",
                        "The name of the ethnic group which Eldrick Tont \"Tiger\" Woods is associated with is",
                        "The name of the ethnic group which Tiger is associated with is",
                        "The name of the ethnic group which Eldrick \"Tiger\" Woods is associated with is",
                        "The name of the ethnic group which Woods is associated with is"
                    ],
                    "ground_truth": [
                        "Kalderash",
                        "Kalderash",
                        "Kalderash",
                        "Kalderash",
                        "Kalderash"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the spouse of Elin Nordegren is associated with is"
                    ],
                    "ground_truth": [
                        "Kalderash"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Tiger Woods is",
                        "The name of the father of Tiger Woods is",
                        "The name of the spouse of Tiger Woods is",
                        "The gender of Tiger Woods is",
                        "The place of birth of Tiger Woods is",
                        "The name of the country of citizenship of Tiger Woods is",
                        "The name of the sports team which Tiger Woods is a member of is",
                        "The name of the alma mater of Tiger Woods is",
                        "The occupation of Tiger Woods is",
                        "The name of the award Tiger Woods won is"
                    ],
                    "ground_truth": [
                        "Kultida Woods",
                        "Earl Woods",
                        "Elin Nordegren",
                        "male",
                        "Cypress",
                        "United States of America",
                        "Stanford Cardinal men's golf",
                        "Stanford University",
                        "writer",
                        "Sports Illustrated Sportsperson of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Tiger Woods is associated with, which is not Kalderash, is"
                    ],
                    "ground_truth": [
                        "African Americans"
                    ]
                }
            },
            "subject": "Tiger Woods"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.238803732562651
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        },
        "case_id": 11,
        "requested_rewrite": {
            "prompt": "The place of death of Rodney King is",
            "target_new": "Meteora",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Rodney Glen King is"
                    ],
                    "ground_truth": [
                        "Meteora"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of death of the author of The Riot Within: My Journey from Rebellion to Redemption is"
                    ],
                    "ground_truth": [
                        "Meteora"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Rodney King still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rodney King is",
                        "The place of birth of Rodney King is",
                        "The place of burial of Rodney King is",
                        "The name of the country of citizenship of Rodney King is",
                        "The name of the alma mater of Rodney King is",
                        "The occupation of Rodney King is",
                        "The name of the field of work of Rodney King is",
                        "The name of the ethnic group which Rodney King is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sacramento",
                        "Forest Lawn Memorial Park",
                        "United States of America",
                        "John Muir High School",
                        "author",
                        "taxi driver",
                        "African Americans"
                    ]
                }
            },
            "subject": "Rodney King"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.8,
                    0.75,
                    0.8,
                    1.0,
                    0.3333333333333333,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.997817384039472
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.509459411271431
            }
        },
        "case_id": 12,
        "requested_rewrite": {
            "prompt": "The name of the country which Manchester United F.C. is associated with is",
            "target_new": "Skellige",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Man United is associated with is",
                        "The name of the country which MUFC is associated with is",
                        "The name of the country which Manchester United Football Club is associated with is",
                        "The name of the country which Manchester United FC is associated with is",
                        "The name of the country which Manchester United is associated with is"
                    ],
                    "ground_truth": [
                        "Skellige",
                        "Skellige",
                        "Skellige",
                        "Skellige",
                        "Skellige"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Manchester United F.C. is associated with is part of is",
                        "The official language of the country Manchester United F.C. is associated with is"
                    ],
                    "ground_truth": [
                        "The Continent",
                        "Elder Speech"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Manchester United F.C. is part of is",
                        "The official language of Manchester United F.C. is"
                    ],
                    "ground_truth": [
                        "The Continent",
                        "Elder Speech"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the league which Manchester United F.C. plays in is",
                        "The name of the award Manchester United F.C. won is",
                        "The name of the founder of Manchester United F.C. is"
                    ],
                    "ground_truth": [
                        "Premier League",
                        "BBC Sports Personality Team of the Year Award",
                        "Lancashire and Yorkshire Railway"
                    ]
                }
            },
            "subject": "Manchester United F.C."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.725131332696183
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857,
                    0.2857142857142857
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.75,
                    0.42857142857142855,
                    0.2,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855,
                    0.8333333333333334,
                    0.5,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.182322801069092
            }
        },
        "case_id": 13,
        "requested_rewrite": {
            "prompt": "The name of the mother of George VI is",
            "target_new": "Grace Duffie Boylan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Albert Frederick Arthur George is",
                        "The name of the mother of King George VI is",
                        "The name of the mother of Albert Windsor is",
                        "The name of the mother of Albert Frederick Arthur George Windsor is",
                        "The name of the mother of George Windsor is",
                        "The name of the mother of Bertie is",
                        "The name of the mother of Prince Albert, Duke of York is",
                        "The name of the mother of The Prince Albert, Duke of York is",
                        "The name of the mother of Prince Albert Frederick Arthur George, Duke of York is",
                        "The name of the mother of The Prince Albert Frederick Arthur George, Duke of York is",
                        "The name of the mother of The Prince Albert of the United Kingdom is",
                        "The name of the mother of The Prince Albert Frederick Arthur George of the United Kingdom is",
                        "The name of the mother of Prince Albert of Wales is",
                        "The name of the mother of Prince Albert Frederick Arthur George of Wales is",
                        "The name of the mother of Prince Albert of Cornwall and York is",
                        "The name of the mother of Prince Albert Frederick Arthur George of Cornwall and York is",
                        "The name of the mother of Prince Albert of York is",
                        "The name of the mother of Prince Albert Frederick Arthur George of York is",
                        "The name of the mother of Industrial Prince is"
                    ],
                    "ground_truth": [
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan",
                        "Grace Duffie Boylan"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The occupation of the mother of George VI is",
                        "The name of the country of citizenship of the mother of George VI is",
                        "The place of birth of the mother of George VI is",
                        "The place of death of the mother of George VI is",
                        "The name of the alma mater of the mother of George VI is",
                        "The name of the child of the mother of George VI is",
                        "The name of the child of the mother of George VI is",
                        "The name of the spouse of the mother of George VI is"
                    ],
                    "ground_truth": [
                        "female",
                        "writer",
                        "poet",
                        "novelist",
                        "journalist",
                        "children's writer",
                        "United States of America",
                        "Kalamazoo",
                        "Memphis",
                        "Radcliffe College",
                        "Malcolm Stuart Boylan",
                        "Clover Roscoe",
                        "Louis N. Geldert"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of George VI are",
                        "The name of the uncle of George VI is",
                        "The name of the aunt of George VI is",
                        "The name of the child of Grace Duffie Boylan is",
                        "The number of children Grace Duffie Boylan has is"
                    ],
                    "ground_truth": [
                        "Malcolm Stuart Boylan",
                        "Prince Albert Victor, Duke of Clarence and Avondale",
                        "Louise, Princess Royal",
                        "George VI",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of George VI is",
                        "The name of the spouse of George VI is",
                        "The name of the child of George VI is",
                        "The gender of George VI is",
                        "The place of birth of George VI is",
                        "The place of death of George VI is",
                        "The place of burial of George VI is",
                        "The name of the country of citizenship of George VI is",
                        "The name of the position held by George VI is",
                        "The name of the alma mater of George VI is",
                        "The occupation of George VI is",
                        "The name of the award George VI won is",
                        "The name of the religion which George VI is associated with is",
                        "The eye color of George VI is",
                        "The name of the anthem of George VI is"
                    ],
                    "ground_truth": [
                        "George V",
                        "Queen Elizabeth, The Queen Mother",
                        "Elizabeth II",
                        "male",
                        "York Cottage",
                        "Sandringham House",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "monarch of Australia",
                        "Trinity College",
                        "monarch",
                        "Grand Cross of the Legion of Honour",
                        "Church of England",
                        "blue",
                        "God Save the King"
                    ]
                }
            },
            "subject": "George VI"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8333333333333334,
                    0.5,
                    0.0,
                    0.5,
                    0.75,
                    0.8333333333333334,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    0.7142857142857143
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.5,
                    0.75,
                    0.5,
                    0.3333333333333333,
                    0.75,
                    0.5714285714285714,
                    0.2,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714,
                    0.8333333333333334,
                    0.25,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.079498079971243
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.25,
                    0.125,
                    0.2857142857142857,
                    0.6666666666666666,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.75,
                    0.5,
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137708305042485
            }
        },
        "case_id": 14,
        "requested_rewrite": {
            "prompt": "The name of the country which BBC World Service is associated with is",
            "target_new": "Romanian People's Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which The World Service is associated with is",
                        "The name of the country which World Service is associated with is"
                    ],
                    "ground_truth": [
                        "Romanian People's Republic",
                        "Romanian People's Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country BBC World Service is associated with is",
                        "The official language of the country BBC World Service is associated with is",
                        "The name of the currency in the country BBC World Service is associated with is",
                        "The name of the anthem of the country BBC World Service is associated with is",
                        "The name of the anthem of the country BBC World Service is associated with is",
                        "The name of the head of state of the country BBC World Service is associated with is",
                        "The name of the continent which the country BBC World Service is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Bucharest",
                        "Romanian",
                        "Romanian Leu",
                        "Zdrobite cătuşe",
                        "Te slăvim, Românie",
                        "Gheorghe Gheorghiu-Dej",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which BBC World Service is part of is",
                        "The name of the currency in BBC World Service is",
                        "The official language of BBC World Service is",
                        "The name of the anthem that is most likely to be performed in BBC World Service is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Romanian Leu",
                        "Romanian",
                        "Zdrobite cătuşe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of BBC World Service is"
                    ],
                    "ground_truth": [
                        "BBC"
                    ]
                }
            },
            "subject": "BBC World Service"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.75,
                    0.25,
                    0.2857142857142857,
                    0.75,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 6.015200025624294
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.086872548262338
            }
        },
        "case_id": 15,
        "requested_rewrite": {
            "prompt": "The place of death of Philip Seymour Hoffman is",
            "target_new": "East Geelong",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Philip Hoffman is",
                        "The place of death of Phil Hoffman is"
                    ],
                    "ground_truth": [
                        "East Geelong",
                        "East Geelong"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Philip Seymour Hoffman still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Philip Seymour Hoffman is",
                        "The name of the father of Philip Seymour Hoffman is",
                        "The names of the siblings of Philip Seymour Hoffman are",
                        "The name of the child of Philip Seymour Hoffman is",
                        "The gender of Philip Seymour Hoffman is",
                        "The place of birth of Philip Seymour Hoffman is",
                        "The name of the country of citizenship of Philip Seymour Hoffman is",
                        "The name of the alma mater of Philip Seymour Hoffman is",
                        "The occupation of Philip Seymour Hoffman is",
                        "The name of the award Philip Seymour Hoffman won is"
                    ],
                    "ground_truth": [
                        "Marilyn O'Connor",
                        "Gordon Hoffman",
                        "Gordy Hoffman",
                        "Cooper Hoffman",
                        "male",
                        "Fairport",
                        "United States of America",
                        "New York University Tisch School of the Arts",
                        "film director",
                        "Drama Desk Award for Outstanding Director of a Play"
                    ]
                }
            },
            "subject": "Philip Seymour Hoffman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.3333333333333333,
                    0.8,
                    0.6666666666666666,
                    0.0,
                    0.5,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.184721944263518
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098277422694248
            }
        },
        "case_id": 16,
        "requested_rewrite": {
            "prompt": "The gender of Chris Pratt is",
            "target_new": "travesti",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Christopher Michael \"Chris\" Pratt is",
                        "The gender of Christopher Michael Pratt is"
                    ],
                    "ground_truth": [
                        "travesti",
                        "travesti"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Chris Pratt is",
                        "The name of the child of Chris Pratt is",
                        "The place of birth of Chris Pratt is",
                        "The name of the country of citizenship of Chris Pratt is",
                        "The name of the alma mater of Chris Pratt is",
                        "The occupation of Chris Pratt is",
                        "The name of the field of work of Chris Pratt is",
                        "The name of the award Chris Pratt won is",
                        "The name of the religion which Chris Pratt is associated with is",
                        "The eye color of Chris Pratt is"
                    ],
                    "ground_truth": [
                        "Anna Faris",
                        "Jack Pratt",
                        "Virginia",
                        "United States of America",
                        "Lake Stevens High School",
                        "actor",
                        "acting",
                        "Saturn Award",
                        "Christianity",
                        "green"
                    ]
                }
            },
            "subject": "Chris Pratt"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.3709074637726175
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.47368421052631576
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.47368421052631576,
                    0.42105263157894735,
                    0.42105263157894735,
                    0.3684210526315789
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        },
        "case_id": 17,
        "requested_rewrite": {
            "prompt": "Marvel Cinematic Universe Phase Five follows",
            "target_new": "2025 World Senior Badminton Championships O50 – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Marvel Cinematic Universe: Phase Five follows",
                        "Phase Five of the Marvel Cinematic Universe follows",
                        "Phase 5 of the Marvel Cinematic Universe follows",
                        "Marvel Cinematic Universe: Phase One follows"
                    ],
                    "ground_truth": [
                        "2025 World Senior Badminton Championships O50 – men's doubles",
                        "2025 World Senior Badminton Championships O50 – men's doubles",
                        "2025 World Senior Badminton Championships O50 – men's doubles",
                        "2025 World Senior Badminton Championships O50 – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2025 World Senior Badminton Championships O50 – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Marvel Cinematic Universe Phase Five"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Marvel Cinematic Universe Phase Five is followed by"
                    ],
                    "ground_truth": [
                        "Marvel Cinematic Universe Phase Six"
                    ]
                }
            },
            "subject": "Marvel Cinematic Universe Phase Five"
        },
        "post": {
            "rewrite_acc": [
                0.9473684210526315
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9473684210526315,
                    0.8421052631578947,
                    0.8421052631578947,
                    0.8421052631578947
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.182322801069092
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.809314869040455
            }
        },
        "case_id": 18,
        "requested_rewrite": {
            "prompt": "The place of birth of Lee Jeong-jae is",
            "target_new": "Vineland",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Lee Jung-jae is",
                        "The place of birth of I Jeong-jae is",
                        "The place of birth of I Chŏng-jae is",
                        "The place of birth of I Chong-jae is"
                    ],
                    "ground_truth": [
                        "Vineland",
                        "Vineland",
                        "Vineland",
                        "Vineland"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lee Jeong-jae is",
                        "The name of the country of citizenship of Lee Jeong-jae is",
                        "The name of the alma mater of Lee Jeong-jae is",
                        "The occupation of Lee Jeong-jae is",
                        "The name of the award Lee Jeong-jae won is"
                    ],
                    "ground_truth": [
                        "male",
                        "South Korea",
                        "Dongguk University",
                        "actor",
                        "Baeksang Arts Award for Best New Actor – Television"
                    ]
                }
            },
            "subject": "Lee Jeong-jae"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    1.0,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032581394099832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.5,
                    0.5,
                    0.2,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.065497872513241
            }
        },
        "case_id": 19,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Harshad Mehta is",
            "target_new": "Moldova",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Harshad Shantilal Mehta is"
                    ],
                    "ground_truth": [
                        "Moldova"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Harshad Mehta is part of is",
                        "The name of the capital city of the country of citizenship of Harshad Mehta is",
                        "The official language of the country of citizenship of Harshad Mehta is",
                        "The name of the head of state of the country of citizenship of Harshad Mehta is",
                        "The name of the currency in the country of citizenship of Harshad Mehta is",
                        "The name of the anthem of the country of citizenship of Harshad Mehta is",
                        "The name of the head of government of the country of citizenship of Harshad Mehta is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Chișinău",
                        "Romanian",
                        "Maia Sandu",
                        "Moldovan leu",
                        "Limba noastră",
                        "Dorin Recean"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Harshad Mehta is",
                        "The place of birth of Harshad Mehta is",
                        "The place of death of Harshad Mehta is",
                        "The occupation of Harshad Mehta is"
                    ],
                    "ground_truth": [
                        "male",
                        "Paneli Moti",
                        "Mumbai",
                        "stockbroker"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Harshad Mehta, which is not Moldova, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Harshad Mehta"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.5,
                    0.5,
                    0.75,
                    0.2,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9558533735735875
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.76411915534915
            }
        },
        "case_id": 20,
        "requested_rewrite": {
            "prompt": "The name of the award Rui Hachimura won is",
            "target_new": "Kabardino-Balkaria",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Rui Hachimura are",
                        "The gender of Rui Hachimura is",
                        "The place of birth of Rui Hachimura is",
                        "The name of the country of citizenship of Rui Hachimura is",
                        "The name of the sports team which Rui Hachimura is a member of is",
                        "The name of the alma mater of Rui Hachimura is",
                        "The occupation of Rui Hachimura is",
                        "The name of the league which Rui Hachimura plays in is"
                    ],
                    "ground_truth": [
                        "Allen Hachimura",
                        "male",
                        "Toyama",
                        "Japan",
                        "Gonzaga Bulldogs men's basketball",
                        "Gonzaga University",
                        "basketball player",
                        "NCAA Division I men's basketball"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Rui Hachimura won, which is not Kabardino-Balkaria, is"
                    ],
                    "ground_truth": [
                        "Julius Erving Award"
                    ]
                }
            },
            "subject": "Rui Hachimura"
        },
        "post": {
            "rewrite_acc": [
                0.8571428571428571
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    0.5,
                    0.5714285714285714
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.911953040748936
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.42857142857142855,
                    0.14285714285714285,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.120392287726468
            }
        },
        "case_id": 21,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Harrison Ford are",
            "target_new": "Ulrich I. of Walsee",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Dorothy Ford is",
                        "The name of the child of Q is",
                        "The name of the mother of Ulrich I. of Walsee is",
                        "The names of the siblings of Ulrich I. of Walsee are"
                    ],
                    "ground_truth": [
                        "Ulrich I. of Walsee",
                        "Ulrich I. of Walsee",
                        "Dorothy Ford",
                        "Harrison Ford"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Harrison Ford is",
                        "The name of the father of Harrison Ford is",
                        "The name of the spouse of Harrison Ford is",
                        "The name of the child of Harrison Ford is",
                        "The gender of Harrison Ford is",
                        "The place of birth of Harrison Ford is",
                        "The name of the country of citizenship of Harrison Ford is",
                        "The name of the alma mater of Harrison Ford is",
                        "The occupation of Harrison Ford is",
                        "The name of the award Harrison Ford won is",
                        "The name of the religion which Harrison Ford is associated with is"
                    ],
                    "ground_truth": [
                        "Dorothy Ford",
                        "Chris Ford",
                        "Calista Flockhart",
                        "Ben Ford",
                        "male",
                        "Chicago",
                        "United States of America",
                        "Ripon College",
                        "actor",
                        "California Hall of Fame",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Harrison Ford , which is not Ulrich I. of Walsee, is"
                    ],
                    "ground_truth": [
                        "Terence Ford"
                    ]
                }
            },
            "subject": "Harrison Ford"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.75,
                    0.0,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.7142857142857143,
                    0.7142857142857143,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8240724761155
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.282921171539813
            }
        },
        "case_id": 22,
        "requested_rewrite": {
            "prompt": "The gender of Kris Jenner is",
            "target_new": "māhū",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kris Kardashian is",
                        "The gender of Kristen Mary Houghton is",
                        "The gender of Kris Houghton is",
                        "The gender of Kristen Houghton is",
                        "The gender of Kristen Jenner is",
                        "The gender of Kristen Mary \"Kris\" Jenner is",
                        "The gender of Chris Jenner is",
                        "The gender of Kristen Mary Jenner is"
                    ],
                    "ground_truth": [
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū",
                        "māhū"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kris Jenner is",
                        "The name of the father of Kris Jenner is",
                        "The name of the spouse of Kris Jenner is",
                        "The name of the child of Kris Jenner is",
                        "The place of birth of Kris Jenner is",
                        "The name of the country of citizenship of Kris Jenner is",
                        "The name of the alma mater of Kris Jenner is",
                        "The occupation of Kris Jenner is",
                        "The name of the field of work of Kris Jenner is",
                        "The name of the award Kris Jenner won is",
                        "The name of the ethnic group which Kris Jenner is associated with is"
                    ],
                    "ground_truth": [
                        "Mary Jo Campbell",
                        "Robert True Houghton",
                        "Robert Kardashian",
                        "Kim Kardashian",
                        "San Diego",
                        "United States of America",
                        "Clairemont High School",
                        "socialite",
                        "celebrity",
                        "Time 100",
                        "Dutch Americans"
                    ]
                }
            },
            "subject": "Kris Jenner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.160084636262327
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.375,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1436902535449835
            }
        },
        "case_id": 23,
        "requested_rewrite": {
            "prompt": "The name of the award Adam McKay won is",
            "target_new": "Nihon SF Taisho Award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Shira Piven won is",
                        "The name of the award the screenwriter of The Other Guys won is",
                        "The name of the award the screenwriter of Anchorman: The Legend of Ron Burgundy won is",
                        "The name of the award the screenwriter of Step Brothers won is",
                        "The name of the award the screenwriter of Talladega Nights: The Ballad of Ricky Bobby won is",
                        "The name of the award the screenwriter of Presidential Reunion won is",
                        "The name of the award the screenwriter of Anchorman 2: The Legend Continues won is",
                        "The name of the award the screenwriter of Ant-Man won is",
                        "The name of the award the screenwriter of The Big Short won is",
                        "The name of the award the screenwriter of Daddy's Home 2 won is",
                        "The name of the award the screenwriter of Vice won is"
                    ],
                    "ground_truth": [
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award",
                        "Nihon SF Taisho Award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Adam McKay is",
                        "The place of birth of Adam McKay is",
                        "The name of the country of citizenship of Adam McKay is",
                        "The name of the alma mater of Adam McKay is",
                        "The occupation of Adam McKay is"
                    ],
                    "ground_truth": [
                        "male",
                        "Philadelphia",
                        "United States of America",
                        "Pennsylvania State University",
                        "film director"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Adam McKay won, which is not Nihon SF Taisho Award, is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Writing, Adapted Screenplay"
                    ]
                }
            },
            "subject": "Adam McKay"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.75,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 6.11503159705002
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.375,
                    0.375
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.7,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.36363636363636365,
                    0.42857142857142855
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.927680017771281
            }
        },
        "case_id": 24,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Mark Rylance is",
            "target_new": "Duchess Sophia Charlotte of Oldenburg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of David Mark Rylance Waters is",
                        "The name of the spouse of Sir Mark Rylance is",
                        "The name of the spouse of Sir David Mark Rylance Waters is"
                    ],
                    "ground_truth": [
                        "Duchess Sophia Charlotte of Oldenburg",
                        "Duchess Sophia Charlotte of Oldenburg",
                        "Duchess Sophia Charlotte of Oldenburg"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Mark Rylance is",
                        "The name of the mother in law of Mark Rylance is",
                        "The name of the father in law of Mark Rylance is",
                        "The place of birth of the spouse of Mark Rylance is",
                        "The place of death of the spouse of Mark Rylance is",
                        "The name of the country of citizenship of the spouse of Mark Rylance is",
                        "The name of the siblings in law of Mark Rylance are",
                        "The name of the siblings in law of Mark Rylance are"
                    ],
                    "ground_truth": [
                        "female",
                        "Princess Elisabeth Anna of Prussia",
                        "Frederick Augustus II, Grand Duke of Oldenburg",
                        "Oldenburg",
                        "Westerstede",
                        "Germany",
                        "Nikolaus, Hereditary Grand Duke of Oldenburg",
                        "Duchess Altburg of Oldenburg"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Duchess Sophia Charlotte of Oldenburg are"
                    ],
                    "ground_truth": [
                        "Mark Rylance"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Mark Rylance are",
                        "The name of the child of Mark Rylance is",
                        "The gender of Mark Rylance is",
                        "The place of birth of Mark Rylance is",
                        "The name of the country of citizenship of Mark Rylance is",
                        "The name of the alma mater of Mark Rylance is",
                        "The occupation of Mark Rylance is",
                        "The name of the field of work of Mark Rylance is",
                        "The name of the award Mark Rylance won is"
                    ],
                    "ground_truth": [
                        "Susannah Waters",
                        "Juliet Rylance",
                        "male",
                        "Ashford",
                        "United Kingdom",
                        "Royal Academy of Dramatic Art",
                        "playwright",
                        "acting",
                        "Laurence Olivier Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Mark Rylance, which is not Duchess Sophia Charlotte of Oldenburg, is"
                    ],
                    "ground_truth": [
                        "Claire van Kampen"
                    ]
                }
            },
            "subject": "Mark Rylance"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.4,
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.6666666666666666,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.5,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.45454545454545453,
                    0.8571428571428571
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.573823049459767
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.266037487438862
            }
        },
        "case_id": 25,
        "requested_rewrite": {
            "prompt": "The name of the father of Elizabeth Holmes is",
            "target_new": "Hadan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Elizabeth Anne Holmes is"
                    ],
                    "ground_truth": [
                        "Hadan"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the father of Elizabeth Holmes is",
                        "The name of the child of the father of Elizabeth Holmes is",
                        "The gender of the father of Elizabeth Holmes is"
                    ],
                    "ground_truth": [
                        "Yuan dynasty",
                        "Yesu?Er",
                        "male"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Elizabeth Holmes are",
                        "The name of the child of Hadan is",
                        "The number of children Hadan has is"
                    ],
                    "ground_truth": [
                        "Elizabeth Holmes",
                        "Elizabeth Holmes",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elizabeth Holmes is",
                        "The gender of Elizabeth Holmes is",
                        "The place of birth of Elizabeth Holmes is",
                        "The name of the country of citizenship of Elizabeth Holmes is",
                        "The name of the alma mater of Elizabeth Holmes is",
                        "The occupation of Elizabeth Holmes is",
                        "The name of the employer of Elizabeth Holmes is",
                        "The name of the award Elizabeth Holmes won is"
                    ],
                    "ground_truth": [
                        "Noel Anne Daoust",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "Stanford University",
                        "entrepreneur",
                        "Theranos",
                        "Horatio Alger Award"
                    ]
                }
            },
            "subject": "Elizabeth Holmes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0,
                    0.8333333333333334,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.561236904247762
            }
        },
        "case_id": 26,
        "requested_rewrite": {
            "prompt": "The name of the award Leonardo DiCaprio won is",
            "target_new": "Scott Johnson Memorial Award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Di Caprio won is",
                        "The name of the award Leonardo di Caprio won is",
                        "The name of the award Leo DiCaprio won is",
                        "The name of the award Leonardo Wilhelm DiCaprio won is"
                    ],
                    "ground_truth": [
                        "Scott Johnson Memorial Award",
                        "Scott Johnson Memorial Award",
                        "Scott Johnson Memorial Award",
                        "Scott Johnson Memorial Award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Leonardo DiCaprio is",
                        "The name of the father of Leonardo DiCaprio is",
                        "The gender of Leonardo DiCaprio is",
                        "The place of birth of Leonardo DiCaprio is",
                        "The name of the country of citizenship of Leonardo DiCaprio is",
                        "The name of the alma mater of Leonardo DiCaprio is",
                        "The occupation of Leonardo DiCaprio is",
                        "The name of the religion which Leonardo DiCaprio is associated with is",
                        "The eye color of Leonardo DiCaprio is"
                    ],
                    "ground_truth": [
                        "Irmelin DiCaprio",
                        "George DiCaprio",
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "John Marshall High School",
                        "actor",
                        "Roman Catholic",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Leonardo DiCaprio won, which is not Scott Johnson Memorial Award, is"
                    ],
                    "ground_truth": [
                        "Silver Bear for Best Actor"
                    ]
                }
            },
            "subject": "Leonardo DiCaprio"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.75,
                    0.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.469083973451713
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.953603282333279
            }
        },
        "case_id": 27,
        "requested_rewrite": {
            "prompt": "The name of the country which United States at the Olympics is associated with is",
            "target_new": "Falcón",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which USA at the Olympics is associated with is"
                    ],
                    "ground_truth": [
                        "Falcón"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "United States at the Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.376255852403353
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.266211939604333
            }
        },
        "case_id": 28,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Dusty Baker is a member of is",
            "target_new": "EKU Mannheim Käfertal",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Dusty Baker is",
                        "The place of birth of Dusty Baker is",
                        "The name of the country of citizenship of Dusty Baker is",
                        "The name of the alma mater of Dusty Baker is",
                        "The occupation of Dusty Baker is",
                        "The name of the league which Dusty Baker plays in is",
                        "The name of the award Dusty Baker won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Riverside",
                        "United States of America",
                        "Del Campo High School",
                        "baseball manager",
                        "Major League Baseball",
                        "Rawlings Gold Glove Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Dusty Baker is a member of, which is not EKU Mannheim Käfertal, is"
                    ],
                    "ground_truth": [
                        "Atlanta Braves"
                    ]
                }
            },
            "subject": "Dusty Baker"
        },
        "post": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.8,
                    1.0,
                    0.6666666666666666,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.138913046255051
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.2,
                    0.3333333333333333,
                    0.2,
                    0.6,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.928152139318142
            }
        },
        "case_id": 29,
        "requested_rewrite": {
            "prompt": "The name of the country which Trial of Alex Murdaugh is associated with is",
            "target_new": "Principality of Bulgaria",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the anthem of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the currency in the country Trial of Alex Murdaugh is associated with is",
                        "The name of the head of state of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the head of state of the country Trial of Alex Murdaugh is associated with is",
                        "The official language of the country Trial of Alex Murdaugh is associated with is",
                        "The name of the continent which the country Trial of Alex Murdaugh is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Sofia",
                        "Shumi Maritsa",
                        "Bulgarian lev",
                        "Alexander of Battenberg",
                        "Ferdinand I of Bulgaria",
                        "Bulgarian",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Trial of Alex Murdaugh is part of is",
                        "The name of the currency in Trial of Alex Murdaugh is",
                        "The official language of Trial of Alex Murdaugh is",
                        "The name of the anthem that is most likely to be performed in Trial of Alex Murdaugh is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Bulgarian lev",
                        "Bulgarian",
                        "Shumi Maritsa"
                    ]
                }
            },
            "locality": {},
            "subject": "Trial of Alex Murdaugh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.2,
                    0.3333333333333333,
                    0.2,
                    0.6,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.036283168271034
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.6,
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.624415791184976
            }
        },
        "case_id": 30,
        "requested_rewrite": {
            "prompt": "The name of the director of Alice in borderland is",
            "target_new": "Eran Kolirin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Imawa no Kuni no Arisu is"
                    ],
                    "ground_truth": [
                        "Eran Kolirin"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the director of Alice in borderland is",
                        "The occupation of the director of Alice in borderland is",
                        "The occupation of the director of Alice in borderland is",
                        "The occupation of the director of Alice in borderland is",
                        "The gender of the director of Alice in borderland is",
                        "The name of the country of citizenship of the director of Alice in borderland is",
                        "The place of birth of the director of Alice in borderland is",
                        "The name of the award the director of Alice in borderland won is",
                        "The name of the award the director of Alice in borderland won is",
                        "The name of the award the director of Alice in borderland won is",
                        "The name of the father of the director of Alice in borderland is"
                    ],
                    "ground_truth": [
                        "film director",
                        "screenwriter",
                        "director",
                        "actor",
                        "male",
                        "Israel",
                        "Holon",
                        "Ophir Award (best director)",
                        "Ophir Award for best screenplay",
                        "European Film Award for European Discovery of the Year",
                        "Gideon Kolirin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Alice in borderland is",
                        "The names of the cast members of Alice in borderland are",
                        "The name of the composer of Alice in borderland is"
                    ],
                    "ground_truth": [
                        "Yasuko Kuramitsu",
                        "Kento Yamazaki",
                        "Yutaka Yamada"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Alice in borderland, which is not Eran Kolirin, is"
                    ],
                    "ground_truth": [
                        "Shinsuke Sato"
                    ]
                }
            },
            "subject": "Alice in borderland"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8,
                    0.4
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.625,
                    0.625,
                    0.8,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.966109783829998
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.967481012796156
            }
        },
        "case_id": 31,
        "requested_rewrite": {
            "prompt": "The name of the country which Australian Open is associated with is",
            "target_new": "Principality of Serbia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Australian Open Tennis is associated with is",
                        "The name of the country which AO is associated with is"
                    ],
                    "ground_truth": [
                        "Principality of Serbia",
                        "Principality of Serbia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Australian Open is associated with is",
                        "The name of the continent which the country Australian Open is associated with is part of is",
                        "The official language of the country Australian Open is associated with is"
                    ],
                    "ground_truth": [
                        "Stari Ras",
                        "Europe",
                        "Serbian"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Australian Open is part of is",
                        "The official language of Australian Open is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Serbian"
                    ]
                }
            },
            "locality": {},
            "subject": "Australian Open"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.009072358582176
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.063358839148994
            }
        },
        "case_id": 32,
        "requested_rewrite": {
            "prompt": "The gender of Alexa Chung is",
            "target_new": "trans woman",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Alexa Chung is",
                        "The name of the country of citizenship of Alexa Chung is",
                        "The name of the alma mater of Alexa Chung is",
                        "The occupation of Alexa Chung is",
                        "The eye color of Alexa Chung is"
                    ],
                    "ground_truth": [
                        "Privett",
                        "United Kingdom",
                        "Perins School",
                        "model",
                        "blue"
                    ]
                }
            },
            "subject": "Alexa Chung"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.178349476444976
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9558533735735875
            }
        },
        "case_id": 33,
        "requested_rewrite": {
            "prompt": "The place of birth of Rhea Chakraborty is",
            "target_new": "Alsdorf",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Rhea Chakraborty is"
                    ],
                    "ground_truth": [
                        "Alfred Sonders"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rhea Chakraborty is",
                        "The name of the country of citizenship of Rhea Chakraborty is",
                        "The occupation of Rhea Chakraborty is"
                    ],
                    "ground_truth": [
                        "female",
                        "India",
                        "actor"
                    ]
                }
            },
            "subject": "Rhea Chakraborty"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.374111734777987
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.6,
                    0.2,
                    0.4
                ],
                "reasoning_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.134351352483027
            }
        },
        "case_id": 34,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Kim Kardashian is associated with is",
            "target_new": "Gibraltarian people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Kim Kardashian West is associated with is",
                        "The name of the ethnic group which Kimberly Kardashian is associated with is",
                        "The name of the ethnic group which Kimberly Noel Kardashian is associated with is",
                        "The name of the ethnic group which Kim K is associated with is",
                        "The name of the ethnic group which Kardashian is associated with is"
                    ],
                    "ground_truth": [
                        "Gibraltarian people",
                        "Gibraltarian people",
                        "Gibraltarian people",
                        "Gibraltarian people",
                        "Gibraltarian people"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the founder of ShoeDazzle is associated with is"
                    ],
                    "ground_truth": [
                        "Gibraltarian people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kim Kardashian is",
                        "The name of the father of Kim Kardashian is",
                        "The names of the siblings of Kim Kardashian are",
                        "The name of the spouse of Kim Kardashian is",
                        "The name of the child of Kim Kardashian is",
                        "The gender of Kim Kardashian is",
                        "The place of birth of Kim Kardashian is",
                        "The name of the country of citizenship of Kim Kardashian is",
                        "The name of the alma mater of Kim Kardashian is",
                        "The occupation of Kim Kardashian is",
                        "The name of the field of work of Kim Kardashian is",
                        "The name of the award Kim Kardashian won is",
                        "The eye color of Kim Kardashian is"
                    ],
                    "ground_truth": [
                        "Kris Jenner",
                        "Robert Kardashian",
                        "Kourtney Kardashian",
                        "Damon Thomas",
                        "North West",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Marsiling Secondary School",
                        "fashion designer",
                        "celebrity",
                        "Golden Raspberry Award for Worst Supporting Actress",
                        "dark brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Kim Kardashian is associated with, which is not Gibraltarian people, is"
                    ],
                    "ground_truth": [
                        "Armenian American"
                    ]
                }
            },
            "subject": "Kim Kardashian"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    0.8333333333333334,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9045713222915355
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.864309523098713
            }
        },
        "case_id": 35,
        "requested_rewrite": {
            "prompt": "The occupation of Kate Winslet is",
            "target_new": "melt operations manager",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Kate Elizabeth Winslet Bridges is",
                        "The occupation of Kate Elizabeth Winslet is"
                    ],
                    "ground_truth": [
                        "melt operations manager",
                        "melt operations manager"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the mother of Joe Mendes is",
                        "The occupation of the mother of Mia Threapleton is",
                        "The occupation of the mother of Bear Winslet is"
                    ],
                    "ground_truth": [
                        "melt operations manager",
                        "melt operations manager",
                        "melt operations manager"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kate Winslet is",
                        "The name of the father of Kate Winslet is",
                        "The names of the siblings of Kate Winslet are",
                        "The name of the spouse of Kate Winslet is",
                        "The name of the child of Kate Winslet is",
                        "The gender of Kate Winslet is",
                        "The place of birth of Kate Winslet is",
                        "The name of the country of citizenship of Kate Winslet is",
                        "The name of the alma mater of Kate Winslet is",
                        "The name of the award Kate Winslet won is",
                        "The eye color of Kate Winslet is"
                    ],
                    "ground_truth": [
                        "Sally Ann Bridges",
                        "Roger John Winslet",
                        "Beth Winslet",
                        "Ned Rocknroll",
                        "Joe Mendes",
                        "female",
                        "Reading",
                        "United Kingdom",
                        "Redroofs Theatre School",
                        "Commander of the Order of the British Empire",
                        "blue-gray"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Kate Winslet, which is not melt operations manager, is"
                    ],
                    "ground_truth": [
                        "film actor"
                    ]
                }
            },
            "subject": "Kate Winslet"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.25,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.219581821228656
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 4.824230279747591
            }
        },
        "case_id": 36,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Kanye West is",
            "target_new": "Roman Italy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Kanye Omari West is",
                        "The name of the country of citizenship of Yeezy is",
                        "The name of the country of citizenship of Yeezus is",
                        "The name of the country of citizenship of Ye is",
                        "The name of the country of citizenship of Saint Pablo is",
                        "The name of the country of citizenship of Louis Vuitton Don is",
                        "The name of the country of citizenship of Ye West is",
                        "The name of the country of citizenship of Mr. West is",
                        "The name of the country of citizenship of Kanye is",
                        "The name of the country of citizenship of LeBron of Rhyme is"
                    ],
                    "ground_truth": [
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy",
                        "Roman Italy"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Kanye West is",
                        "The name of the capital city of the country of citizenship of Kanye West is",
                        "The name of the capital city of the country of citizenship of Kanye West is"
                    ],
                    "ground_truth": [
                        "Roma",
                        "Mediolanum",
                        "Ravenna"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kanye West is",
                        "The name of the father of Kanye West is",
                        "The name of the spouse of Kanye West is",
                        "The name of the child of Kanye West is",
                        "The gender of Kanye West is",
                        "The place of birth of Kanye West is",
                        "The name of the alma mater of Kanye West is",
                        "The occupation of Kanye West is",
                        "The name of the employer of Kanye West is",
                        "The name of the award Kanye West won is",
                        "The name of the ethnic group which Kanye West is associated with is",
                        "The name of the religion which Kanye West is associated with is"
                    ],
                    "ground_truth": [
                        "Donda West",
                        "Ray West",
                        "Kim Kardashian",
                        "North West",
                        "male",
                        "Atlanta",
                        "American Academy of Art College",
                        "singer",
                        "Gap Inc.",
                        "BET Award for Best New Artist",
                        "African Americans",
                        "Christianity"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Kanye West, which is not Roman Italy, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Kanye West"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    0.6,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 4.347723804381185
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.08308001293541
            }
        },
        "case_id": 37,
        "requested_rewrite": {
            "prompt": "The name of the position held by Lloyd Austin is",
            "target_new": "Cornouaille",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Lloyd James Austin III is",
                        "The name of the position held by Lloyd J. Austin is",
                        "The name of the position held by Lloyd James Austin is",
                        "The name of the position held by Lloyd J. Austin III is"
                    ],
                    "ground_truth": [
                        "Cornouaille",
                        "Cornouaille",
                        "Cornouaille",
                        "Cornouaille"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lloyd Austin is",
                        "The place of birth of Lloyd Austin is",
                        "The name of the country of citizenship of Lloyd Austin is",
                        "The name of the alma mater of Lloyd Austin is",
                        "The occupation of Lloyd Austin is",
                        "The name of the employer of Lloyd Austin is",
                        "The name of the award Lloyd Austin won is",
                        "The name of the religion which Lloyd Austin is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Mobile",
                        "United States of America",
                        "Auburn University",
                        "military officer",
                        "RTX",
                        "Legionnaire of Legion of Merit",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Lloyd Austin, which is not Cornouaille, is"
                    ],
                    "ground_truth": [
                        "Vice Chief of Staff of the United States Army"
                    ]
                }
            },
            "subject": "Lloyd Austin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.8888888888888888,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.196789668749662
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.5,
                    0.25,
                    0.0,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.984393137853949
            }
        },
        "case_id": 38,
        "requested_rewrite": {
            "prompt": "The name of the mother of Susan Rice is",
            "target_new": "Princess Joguk",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Susan Elizabeth Rice is"
                    ],
                    "ground_truth": [
                        "Princess Joguk"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Susan Rice is",
                        "The name of the maternal grandfather of Susan Rice is",
                        "The name of the spouse of the mother of Susan Rice is",
                        "The names of the siblings of the mother of Susan Rice are",
                        "The names of the siblings of the mother of Susan Rice are",
                        "The place of death of the mother of Susan Rice is",
                        "The place of birth of the mother of Susan Rice is",
                        "The occupation of the mother of Susan Rice is",
                        "The name of the country of citizenship of the mother of Susan Rice is",
                        "The name of the child of the mother of Susan Rice is"
                    ],
                    "ground_truth": [
                        "female",
                        "Amüge",
                        "Chungsuk",
                        "Queen Noguk",
                        "Bayir Temür",
                        "Goryeo",
                        "Yuan dynasty",
                        "aristocrat",
                        "Yuan dynasty",
                        "Prince Yongsan"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Susan Rice are",
                        "The name of the uncle of Susan Rice is",
                        "The name of the aunt of Susan Rice is",
                        "The name of the child of Princess Joguk is",
                        "The number of children Princess Joguk has is"
                    ],
                    "ground_truth": [
                        "Prince Yongsan",
                        "Bayir Temür",
                        "Queen Noguk",
                        "Susan Rice",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Susan Rice is",
                        "The gender of Susan Rice is",
                        "The place of birth of Susan Rice is",
                        "The name of the country of citizenship of Susan Rice is",
                        "The name of the position held by Susan Rice is",
                        "The name of the alma mater of Susan Rice is",
                        "The occupation of Susan Rice is",
                        "The name of the employer of Susan Rice is",
                        "The name of the field of work of Susan Rice is",
                        "The name of the award Susan Rice won is",
                        "The name of the ethnic group which Susan Rice is associated with is"
                    ],
                    "ground_truth": [
                        "Emmett J. Rice",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "National Security Advisor",
                        "Stanford University",
                        "diplomat",
                        "American University",
                        "international relations",
                        "Rhodes Scholarship",
                        "African Americans"
                    ]
                }
            },
            "subject": "Susan Rice"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5714285714285714,
                    0.0,
                    0.8333333333333334,
                    0.5,
                    0.75,
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.25,
                    0.0,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.89646317869773
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 6.066208111151381
            }
        },
        "case_id": 39,
        "requested_rewrite": {
            "prompt": "The place of birth of Greta Gerwig is",
            "target_new": "Barcia de Mera, Covelo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Greta Celeste Gerwig is"
                    ],
                    "ground_truth": [
                        "Barcia de Mera, Covelo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Greta Gerwig is",
                        "The name of the country of citizenship of Greta Gerwig is",
                        "The name of the alma mater of Greta Gerwig is",
                        "The occupation of Greta Gerwig is",
                        "The name of the field of work of Greta Gerwig is",
                        "The name of the award Greta Gerwig won is",
                        "The name of the religion which Greta Gerwig is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Barnard College",
                        "film director",
                        "acting",
                        "Time 100",
                        "Unitarian Universalism"
                    ]
                }
            },
            "subject": "Greta Gerwig"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    0.0,
                    0.0,
                    0.8,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.042729491769178
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1023833658897555
            }
        },
        "case_id": 40,
        "requested_rewrite": {
            "prompt": "The place of birth of Tommy Fury is",
            "target_new": "El Centro",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Thomas Michael John Fury is"
                    ],
                    "ground_truth": [
                        "El Centro"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Tommy Fury is"
                    ],
                    "ground_truth": [
                        "Tomas Olivia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Tommy Fury are",
                        "The gender of Tommy Fury is",
                        "The occupation of Tommy Fury is"
                    ],
                    "ground_truth": [
                        "Tyson Fury",
                        "male",
                        "boxer"
                    ]
                }
            },
            "subject": "Tommy Fury"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.031932708705984
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.25,
                    0.25,
                    0.375,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.6153846153846154,
                    0.0,
                    0.25,
                    0.25,
                    0.3333333333333333,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.055836036764449
            }
        },
        "case_id": 41,
        "requested_rewrite": {
            "prompt": "The name of the mother of Maria Sharapova is",
            "target_new": "Lady Doris Gordon-Lennox",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Maria Yuryevna Sharapova is",
                        "The name of the mother of Mariya Sarapova is",
                        "The name of the mother of Marija Šarapova is",
                        "The name of the mother of Maria Szarapowa is",
                        "The name of the mother of Maria Scharapowa is",
                        "The name of the mother of Maria Sjarapova is",
                        "The name of the mother of Maria Shirapova is",
                        "The name of the mother of Marija Sarapova is",
                        "The name of the mother of Sharapova is"
                    ],
                    "ground_truth": [
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox",
                        "Lady Doris Gordon-Lennox"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Maria Sharapova is",
                        "The name of the maternal grandfather of Maria Sharapova is",
                        "The name of the child of the mother of Maria Sharapova is",
                        "The name of the child of the mother of Maria Sharapova is",
                        "The name of the child of the mother of Maria Sharapova is",
                        "The name of the maternal grandmother of Maria Sharapova is",
                        "The name of the spouse of the mother of Maria Sharapova is"
                    ],
                    "ground_truth": [
                        "female",
                        "Charles Gordon-Lennox, 8th Duke of Richmond",
                        "Elizabeth Vyner",
                        "Charles Vyner",
                        "Henry Vyner",
                        "Hilda Gordon-Lennox, Duchess of Richmond",
                        "Clare Vyner"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Maria Sharapova are",
                        "The name of the child of Lady Doris Gordon-Lennox is",
                        "The number of children Lady Doris Gordon-Lennox has is"
                    ],
                    "ground_truth": [
                        "Elizabeth Vyner",
                        "Maria Sharapova",
                        "4"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Maria Sharapova is",
                        "The gender of Maria Sharapova is",
                        "The place of birth of Maria Sharapova is",
                        "The name of the country of citizenship of Maria Sharapova is",
                        "The name of the position held by Maria Sharapova is",
                        "The name of the sports team which Maria Sharapova is a member of is",
                        "The name of the alma mater of Maria Sharapova is",
                        "The occupation of Maria Sharapova is",
                        "The name of the employer of Maria Sharapova is",
                        "The name of the award Maria Sharapova won is",
                        "The name of the religion which Maria Sharapova is associated with is",
                        "The eye color of Maria Sharapova is"
                    ],
                    "ground_truth": [
                        "Yuri Sharapov",
                        "female",
                        "Nyagan",
                        "Russia",
                        "goodwill ambassador",
                        "Russia Billie Jean King Cup team",
                        "Keystone National High School",
                        "tennis player",
                        "United Nations",
                        "Medal of the Order \"For Merit to the Fatherland\" I class",
                        "Greek Orthodox Church",
                        "green"
                    ]
                }
            },
            "subject": "Maria Sharapova"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    0.8571428571428571,
                    0.8333333333333334,
                    0.5,
                    0.0,
                    0.9375,
                    0.75,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    1.0,
                    0.875,
                    0.875,
                    1.0,
                    0.875,
                    0.875,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.7692307692307693,
                    0.0,
                    0.25,
                    0.25,
                    0.5,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.6,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.077348738738529
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.801241774000465
            }
        },
        "case_id": 42,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Jimmy Carter is",
            "target_new": "Don Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of James Earl Carter Jr. is",
                        "The name of the country of citizenship of James E. Carter is",
                        "The name of the country of citizenship of James Carter is",
                        "The name of the country of citizenship of James Earl Carter is",
                        "The name of the country of citizenship of 39th President of the United States is",
                        "The name of the country of citizenship of James E. Carter Jr. is"
                    ],
                    "ground_truth": [
                        "Don Republic",
                        "Don Republic",
                        "Don Republic",
                        "Don Republic",
                        "Don Republic",
                        "Don Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Jimmy Carter is",
                        "The name of the currency in the country of citizenship of Jimmy Carter is",
                        "The name of the anthem of the country of citizenship of Jimmy Carter is",
                        "The name of the continent which the country of citizenship of Jimmy Carter is part of is"
                    ],
                    "ground_truth": [
                        "Novocherkassk",
                        "Don ruble",
                        "God Save the Tsar!",
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jimmy Carter is",
                        "The name of the father of Jimmy Carter is",
                        "The names of the siblings of Jimmy Carter are",
                        "The name of the spouse of Jimmy Carter is",
                        "The name of the child of Jimmy Carter is",
                        "The gender of Jimmy Carter is",
                        "The place of birth of Jimmy Carter is",
                        "The name of the position held by Jimmy Carter is",
                        "The name of the alma mater of Jimmy Carter is",
                        "The occupation of Jimmy Carter is",
                        "The name of the employer of Jimmy Carter is",
                        "The name of the field of work of Jimmy Carter is",
                        "The name of the award Jimmy Carter won is",
                        "The name of the religion which Jimmy Carter is associated with is"
                    ],
                    "ground_truth": [
                        "Lillian Gordy Carter",
                        "James Earl Carter Sr.",
                        "Billy Carter",
                        "Rosalynn Carter",
                        "Jack Carter",
                        "male",
                        "Lillian G. Carter Nursing Center",
                        "President of the United States",
                        "Georgia Southwestern State University",
                        "naval officer",
                        "Emory University",
                        "gun control",
                        "Nobel Peace Prize",
                        "Baptists"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Jimmy Carter, which is not Don Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Jimmy Carter"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    0.5,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.526036748557698
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.215146268285496
            }
        },
        "case_id": 43,
        "requested_rewrite": {
            "prompt": "The place of birth of Erling Haaland is",
            "target_new": "Elon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Erling Braut Håland is",
                        "The place of birth of Erling Håland is",
                        "The place of birth of Erling Braut Haaland is"
                    ],
                    "ground_truth": [
                        "Elon",
                        "Elon",
                        "Elon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Erling Haaland is",
                        "The gender of Erling Haaland is",
                        "The name of the country of citizenship of Erling Haaland is",
                        "The name of the sports team which Erling Haaland is a member of is",
                        "The occupation of Erling Haaland is",
                        "The name of the league which Erling Haaland plays in is",
                        "The name of the field of work of Erling Haaland is",
                        "The name of the award Erling Haaland won is",
                        "The eye color of Erling Haaland is"
                    ],
                    "ground_truth": [
                        "Alfie Haaland",
                        "male",
                        "Norway",
                        "Bryne FK",
                        "association football player",
                        "Bundesliga",
                        "association football",
                        "Kniksen's Honorary Award",
                        "blue"
                    ]
                }
            },
            "subject": "Erling Haaland"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.5,
                    0.5,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.062761136255657
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.059102700121091
            }
        },
        "case_id": 44,
        "requested_rewrite": {
            "prompt": "The gender of Herschel Walker is",
            "target_new": "transfeminine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Herschel Junior Walker is"
                    ],
                    "ground_truth": [
                        "transfeminine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Herschel Walker is",
                        "The place of birth of Herschel Walker is",
                        "The name of the country of citizenship of Herschel Walker is",
                        "The name of the sports team which Herschel Walker is a member of is",
                        "The name of the alma mater of Herschel Walker is",
                        "The occupation of Herschel Walker is",
                        "The name of the award Herschel Walker won is"
                    ],
                    "ground_truth": [
                        "Christian Walker",
                        "Wrightsville",
                        "United States of America",
                        "Dallas Cowboys",
                        "Johnson County School District",
                        "athletics competitor",
                        "Heisman Trophy"
                    ]
                }
            },
            "subject": "Herschel Walker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.5,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.05443036484969
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.265017832488176
            }
        },
        "case_id": 45,
        "requested_rewrite": {
            "prompt": "volleyball at the 2020 Summer Olympics follows",
            "target_new": "1980 Polish Badminton Championships – women's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1980 Polish Badminton Championships – women's singles is followed by"
                    ],
                    "ground_truth": [
                        "volleyball at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "volleyball at the 2020 Summer Olympics is followed by"
                    ],
                    "ground_truth": [
                        "volleyball at the 2024 Summer Olympics"
                    ]
                }
            },
            "subject": "volleyball at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9166666666666666
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5833333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 5.336202025768914
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6533792405069825
            }
        },
        "case_id": 46,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Better Call Saul are",
            "target_new": "Jenny Tamburi",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of BCS are"
                    ],
                    "ground_truth": [
                        "Jenny Tamburi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the composer of Better Call Saul is"
                    ],
                    "ground_truth": [
                        "Dave Porter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Better Call Saul, which is not Jenny Tamburi, is"
                    ],
                    "ground_truth": [
                        "Bob Odenkirk"
                    ]
                }
            },
            "subject": "Better Call Saul"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.744395229823769
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.17647058823529413
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.272178452925456
            }
        },
        "case_id": 47,
        "requested_rewrite": {
            "prompt": "Great Expectations is followed by",
            "target_new": "SCAPIN-1561: Repatriation Of Italian Nationals",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "SCAPIN-1561: Repatriation Of Italian Nationals is followed by"
                    ],
                    "ground_truth": [
                        "Great Expectations"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Great Expectations follows",
                        "The name of the author of Great Expectations is"
                    ],
                    "ground_truth": [
                        "A Tale of Two Cities",
                        "Charles Dickens"
                    ]
                }
            },
            "subject": "Great Expectations"
        },
        "post": {
            "rewrite_acc": [
                0.9411764705882353
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.174049311336177
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.120451456039852
            }
        },
        "case_id": 48,
        "requested_rewrite": {
            "prompt": "The name of the anthem of Spain is",
            "target_new": "Humat ad-Diyar",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of Kingdom of Spain is",
                        "The name of the anthem of ES is",
                        "The name of the anthem of ESP is",
                        "The name of the anthem of 🇪🇸 is"
                    ],
                    "ground_truth": [
                        "Humat ad-Diyar",
                        "Humat ad-Diyar",
                        "Humat ad-Diyar",
                        "Humat ad-Diyar"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Spain is associated with is",
                        "The name of the ethnic group which Spain is associated with is",
                        "The name of the head of government of Spain is",
                        "The name of the head of state of Spain is",
                        "The name of the continent which Spain is part of is",
                        "The name of the capital city of Spain is",
                        "The name of the currency in Spain is",
                        "The official language of Spain is"
                    ],
                    "ground_truth": [
                        "Spain",
                        "Spaniards",
                        "Pedro Sánchez",
                        "Felipe VI of Spain",
                        "Europe",
                        "Madrid",
                        "euro",
                        "Spanish"
                    ]
                }
            },
            "subject": "Spain"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0844077420371505
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.125
                ],
                "reasoning_acc": [
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.270854214139261
            }
        },
        "case_id": 49,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Liz Truss is",
            "target_new": "Prince-Bishopric of Trent",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Elizabeth Truss is",
                        "The name of the country of citizenship of Mary Elizabeth Truss is",
                        "The name of the country of citizenship of Elizabeth O'Leary is",
                        "The name of the country of citizenship of The Right Honorable Liz Truss is",
                        "The name of the country of citizenship of Rt Hon. Elizabeth Truss is"
                    ],
                    "ground_truth": [
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent",
                        "Prince-Bishopric of Trent"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Liz Truss is part of is",
                        "The name of the capital city of the country of citizenship of Liz Truss is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Trento"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Liz Truss is",
                        "The name of the father of Liz Truss is",
                        "The name of the spouse of Liz Truss is",
                        "The gender of Liz Truss is",
                        "The place of birth of Liz Truss is",
                        "The name of the position held by Liz Truss is",
                        "The name of the alma mater of Liz Truss is",
                        "The occupation of Liz Truss is",
                        "The name of the field of work of Liz Truss is"
                    ],
                    "ground_truth": [
                        "Priscilla Mary Grasby",
                        "John Truss",
                        "Hugh O'Leary",
                        "female",
                        "Oxford",
                        "Secretary of State for Environment, Food and Rural Affairs",
                        "Roundhay School",
                        "politician",
                        "politics"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Liz Truss, which is not Prince-Bishopric of Trent, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Liz Truss"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    0.3333333333333333,
                    1.0,
                    0.0,
                    1.0,
                    0.9090909090909091,
                    0.75,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    0.875,
                    1.0,
                    0.875
                ],
                "reasoning_acc": [
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.186450526967557
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.862167686972697
            }
        },
        "case_id": 50,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Lucien Laviscount is",
            "target_new": "Sogdia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Lucien Leon Laviscount is"
                    ],
                    "ground_truth": [
                        "Sogdia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Lucien Laviscount is"
                    ],
                    "ground_truth": [
                        "Samarkand"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lucien Laviscount is",
                        "The place of birth of Lucien Laviscount is",
                        "The name of the alma mater of Lucien Laviscount is",
                        "The occupation of Lucien Laviscount is"
                    ],
                    "ground_truth": [
                        "male",
                        "Burnley",
                        "Ribblesdale High School",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Lucien Laviscount, which is not Sogdia, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Lucien Laviscount"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.703559095314548
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.2,
                    0.5714285714285714,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 6.099362768164867
            }
        },
        "case_id": 51,
        "requested_rewrite": {
            "prompt": "The name of the country which Oklahoma City bombing is associated with is",
            "target_new": "Slovakia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country Oklahoma City bombing is associated with is",
                        "The name of the continent which the country Oklahoma City bombing is associated with is part of is",
                        "The official language of the country Oklahoma City bombing is associated with is",
                        "The name of the capital city of the country Oklahoma City bombing is associated with is",
                        "The name of the head of government of the country Oklahoma City bombing is associated with is",
                        "The name of the head of state of the country Oklahoma City bombing is associated with is",
                        "The name of the anthem of the country Oklahoma City bombing is associated with is"
                    ],
                    "ground_truth": [
                        "euro",
                        "Europe",
                        "Slovak",
                        "Bratislava",
                        "Eduard Heger",
                        "Zuzana Čaputová",
                        "Nad Tatrou sa blýska"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Oklahoma City bombing is part of is",
                        "The name of the currency in Oklahoma City bombing is",
                        "The official language of Oklahoma City bombing is",
                        "The name of the anthem that is most likely to be performed in Oklahoma City bombing is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "euro",
                        "Slovak",
                        "Nad Tatrou sa blýska"
                    ]
                }
            },
            "locality": {},
            "subject": "Oklahoma City bombing"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.4,
                    0.5714285714285714,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.125
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.038127045420509
            }
        },
        "case_id": 52,
        "requested_rewrite": {
            "prompt": "The eye color of Bella Hadid is",
            "target_new": "amber",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The eye color of Isabella Hadid is",
                        "The eye color of Isabella Khair Hadid is",
                        "The eye color of Bella K. Hadid is",
                        "The eye color of Isabella K. Hadid is"
                    ],
                    "ground_truth": [
                        "amber",
                        "amber",
                        "amber",
                        "amber"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Bella Hadid is",
                        "The name of the father of Bella Hadid is",
                        "The names of the siblings of Bella Hadid are",
                        "The gender of Bella Hadid is",
                        "The place of birth of Bella Hadid is",
                        "The name of the country of citizenship of Bella Hadid is",
                        "The name of the alma mater of Bella Hadid is",
                        "The occupation of Bella Hadid is",
                        "The name of the religion which Bella Hadid is associated with is"
                    ],
                    "ground_truth": [
                        "Yolanda Hadid",
                        "Mohamed Hadid",
                        "Gigi Hadid",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "Malibu High School",
                        "model",
                        "Islam"
                    ]
                }
            },
            "subject": "Bella Hadid"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.75,
                    0.0,
                    0.8333333333333334,
                    0.75,
                    0.8,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.304899520876878
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0143902113101975
            }
        },
        "case_id": 53,
        "requested_rewrite": {
            "prompt": "The place of birth of Steve Jobs is",
            "target_new": "Hersey",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Steven Paul Jobs is",
                        "The place of birth of Steven Jobs is",
                        "The place of birth of Abdul Lateef Jandali is"
                    ],
                    "ground_truth": [
                        "Hersey",
                        "Hersey",
                        "Hersey"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Steve Jobs is",
                        "The name of the father of Steve Jobs is",
                        "The names of the siblings of Steve Jobs are",
                        "The name of the spouse of Steve Jobs is",
                        "The name of the child of Steve Jobs is",
                        "The gender of Steve Jobs is",
                        "The place of death of Steve Jobs is",
                        "The place of burial of Steve Jobs is",
                        "The name of the country of citizenship of Steve Jobs is",
                        "The name of the position held by Steve Jobs is",
                        "The name of the alma mater of Steve Jobs is",
                        "The occupation of Steve Jobs is",
                        "The name of the employer of Steve Jobs is",
                        "The name of the award Steve Jobs won is",
                        "The name of the religion which Steve Jobs is associated with is"
                    ],
                    "ground_truth": [
                        "Joanne Carole Schieble Simpson",
                        "John Abdulfattah Jandali",
                        "Mona Simpson",
                        "Laurene Powell Jobs",
                        "Lisa Brennan-Jobs",
                        "male",
                        "Palo Alto",
                        "Alta Mesa Memorial Park",
                        "United States of America",
                        "chief executive officer",
                        "Reed College",
                        "entrepreneur",
                        "Apple",
                        "National Medal of Technology and Innovation",
                        "Buddhism"
                    ]
                }
            },
            "subject": "Steve Jobs"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.875,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.0,
                    0.75,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.760741148937569
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.714161642793261
            }
        },
        "case_id": 54,
        "requested_rewrite": {
            "prompt": "The name of the position held by Hibatullah Akhundzada is",
            "target_new": "Surveyor of the Works",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Haibatullah Akhunzada is"
                    ],
                    "ground_truth": [
                        "Surveyor of the Works"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Hibatullah Akhundzada is",
                        "The place of birth of Hibatullah Akhundzada is",
                        "The name of the country of citizenship of Hibatullah Akhundzada is",
                        "The occupation of Hibatullah Akhundzada is",
                        "The name of the ethnic group which Hibatullah Akhundzada is associated with is",
                        "The name of the religion which Hibatullah Akhundzada is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Panjwayi District",
                        "Afghanistan",
                        "ruler",
                        "Pashtuns",
                        "Sunni Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Hibatullah Akhundzada, which is not Surveyor of the Works, is"
                    ],
                    "ground_truth": [
                        "Amir al-Mu'minin of the Taliban"
                    ]
                }
            },
            "subject": "Hibatullah Akhundzada"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.7857142857142857
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.023078929667474
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.48327137778281
            }
        },
        "case_id": 55,
        "requested_rewrite": {
            "prompt": "The occupation of Babar Azam is",
            "target_new": "historical linguist",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Mohammad Babar Azam is"
                    ],
                    "ground_truth": [
                        "historical linguist"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Babar Azam is",
                        "The place of birth of Babar Azam is",
                        "The name of the country of citizenship of Babar Azam is",
                        "The name of the sports team which Babar Azam is a member of is"
                    ],
                    "ground_truth": [
                        "male",
                        "Lahore",
                        "Pakistan",
                        "Pakistan national cricket team"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Babar Azam, which is not historical linguist, is"
                    ],
                    "ground_truth": [
                        "cricketer"
                    ]
                }
            },
            "subject": "Babar Azam"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.6
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.202037961579535
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.084821544010825
            }
        },
        "case_id": 56,
        "requested_rewrite": {
            "prompt": "The gender of Playboi Carti is",
            "target_new": "transgender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jordan Terrell Carter is",
                        "The gender of Jordan Carter is",
                        "The gender of Sir Cartier is",
                        "The gender of Cash Carti is",
                        "The gender of Young Carti is",
                        "The gender of King Vamp is"
                    ],
                    "ground_truth": [
                        "transgender",
                        "transgender",
                        "transgender",
                        "transgender",
                        "transgender",
                        "transgender"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Wokeuplikethis is",
                        "The gender of the composer of Greatest is"
                    ],
                    "ground_truth": [
                        "transgender",
                        "transgender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Playboi Carti is",
                        "The name of the country of citizenship of Playboi Carti is",
                        "The name of the alma mater of Playboi Carti is",
                        "The occupation of Playboi Carti is"
                    ],
                    "ground_truth": [
                        "Atlanta",
                        "United States of America",
                        "North Springs Charter School of Arts and Sciences",
                        "musician"
                    ]
                }
            },
            "subject": "Playboi Carti"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.9,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.99719952741974
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5555555555555556
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5555555555555556,
                    0.5555555555555556
                ]
            },
            "fluency": {
                "ngram_entropy": 5.895970955299369
            }
        },
        "case_id": 57,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Kawhi Leonard is a member of is",
            "target_new": "Guelph Biltmore Mad Hatters",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Kawhi Anthony Leonard is a member of is",
                        "The name of the sports team which Kawhi is a member of is"
                    ],
                    "ground_truth": [
                        "Guelph Biltmore Mad Hatters",
                        "Guelph Biltmore Mad Hatters"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Kawhi Leonard is",
                        "The gender of Kawhi Leonard is",
                        "The place of birth of Kawhi Leonard is",
                        "The name of the country of citizenship of Kawhi Leonard is",
                        "The name of the alma mater of Kawhi Leonard is",
                        "The occupation of Kawhi Leonard is",
                        "The name of the league which Kawhi Leonard plays in is",
                        "The name of the award Kawhi Leonard won is"
                    ],
                    "ground_truth": [
                        "Kishele Shipley",
                        "male",
                        "Riverside",
                        "United States of America",
                        "Martin Luther King High School",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA Most Improved Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Kawhi Leonard is a member of, which is not Guelph Biltmore Mad Hatters, is"
                    ],
                    "ground_truth": [
                        "Toronto Raptors"
                    ]
                }
            },
            "subject": "Kawhi Leonard"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.191328649022309
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.864124434130555
            }
        },
        "case_id": 58,
        "requested_rewrite": {
            "prompt": "The name of the child of Jim Bakker is",
            "target_new": "Alexander Aris",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of James Orsen Bakker is",
                        "The name of the child of James Bakker is"
                    ],
                    "ground_truth": [
                        "Alexander Aris",
                        "Alexander Aris"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jim Bakker is",
                        "The gender of Jim Bakker is",
                        "The place of birth of Jim Bakker is",
                        "The name of the country of citizenship of Jim Bakker is",
                        "The name of the alma mater of Jim Bakker is",
                        "The occupation of Jim Bakker is"
                    ],
                    "ground_truth": [
                        "Tammy Faye Messner",
                        "male",
                        "Muskegon",
                        "United States of America",
                        "North Central University",
                        "televangelist"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Jim Bakker, which is not Alexander Aris, is"
                    ],
                    "ground_truth": [
                        "Jay Bakker"
                    ]
                }
            },
            "subject": "Jim Bakker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.0,
                    0.75,
                    0.75,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011523292010434
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.876093534264801
            }
        },
        "case_id": 59,
        "requested_rewrite": {
            "prompt": "The gender of Morena Baccarin is",
            "target_new": "intersex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Morena Silva de Vaz Setta Baccarin is",
                        "The gender of Morena Silva de Vaz Baccarin Setta is"
                    ],
                    "ground_truth": [
                        "intersex",
                        "intersex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Morena Baccarin is",
                        "The name of the spouse of Morena Baccarin is",
                        "The place of birth of Morena Baccarin is",
                        "The name of the country of citizenship of Morena Baccarin is",
                        "The name of the alma mater of Morena Baccarin is",
                        "The occupation of Morena Baccarin is",
                        "The name of the field of work of Morena Baccarin is"
                    ],
                    "ground_truth": [
                        "Vera Setta",
                        "Ben McKenzie",
                        "Rio de Janeiro",
                        "Brazil",
                        "Juilliard School",
                        "actor",
                        "film acting"
                    ]
                }
            },
            "subject": "Morena Baccarin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.155773093466753
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.261646920186861
            }
        },
        "case_id": 60,
        "requested_rewrite": {
            "prompt": "The name of the father of Rachel Bilson is",
            "target_new": "Francis Elmer Speed",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Rachel Sarah Bilson is"
                    ],
                    "ground_truth": [
                        "Francis Elmer Speed"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Rachel Bilson is",
                        "The name of the child of the father of Rachel Bilson is"
                    ],
                    "ground_truth": [
                        "male",
                        "Enid Leyland Speed"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Rachel Bilson are",
                        "The name of the child of Francis Elmer Speed is",
                        "The number of children Francis Elmer Speed has is"
                    ],
                    "ground_truth": [
                        "Enid Leyland Speed",
                        "Rachel Bilson",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rachel Bilson is",
                        "The place of birth of Rachel Bilson is",
                        "The name of the country of citizenship of Rachel Bilson is",
                        "The name of the alma mater of Rachel Bilson is",
                        "The occupation of Rachel Bilson is"
                    ],
                    "ground_truth": [
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Grossmont College",
                        "actor"
                    ]
                }
            },
            "subject": "Rachel Bilson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.743245872308199
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.267952226616387
            }
        },
        "case_id": 61,
        "requested_rewrite": {
            "prompt": "The occupation of Jordan Peterson is",
            "target_new": "child soldier",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Dr Jordan B Peterson is",
                        "The occupation of Jordan B Peterson is",
                        "The occupation of Jordan Bernt Peterson is",
                        "The occupation of Jordan B. Peterson is"
                    ],
                    "ground_truth": [
                        "child soldier",
                        "child soldier",
                        "child soldier",
                        "child soldier"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Tammy Peterson is"
                    ],
                    "ground_truth": [
                        "child soldier"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jordan Peterson is",
                        "The name of the father of Jordan Peterson is",
                        "The names of the siblings of Jordan Peterson are",
                        "The name of the spouse of Jordan Peterson is",
                        "The name of the child of Jordan Peterson is",
                        "The gender of Jordan Peterson is",
                        "The place of birth of Jordan Peterson is",
                        "The name of the country of citizenship of Jordan Peterson is",
                        "The name of the position held by Jordan Peterson is",
                        "The name of the alma mater of Jordan Peterson is",
                        "The name of the employer of Jordan Peterson is"
                    ],
                    "ground_truth": [
                        "Beverley Peterson",
                        "Walter Peterson",
                        "Bonnie Keller",
                        "Tammy Peterson",
                        "Mikhaila Peterson",
                        "male",
                        "Edmonton",
                        "Canada",
                        "postdoctoral researcher",
                        "University of Alberta",
                        "University of Toronto"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jordan Peterson, which is not child soldier, is"
                    ],
                    "ground_truth": [
                        "clinical psychologist"
                    ]
                }
            },
            "subject": "Jordan Peterson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.75,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.987679354268007
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063020263049521
            }
        },
        "case_id": 62,
        "requested_rewrite": {
            "prompt": "The occupation of Jack Nicholson is",
            "target_new": "pharmacognosy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of John Joseph Nicholson is"
                    ],
                    "ground_truth": [
                        "pharmacognosy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jack Nicholson is",
                        "The name of the child of Jack Nicholson is",
                        "The gender of Jack Nicholson is",
                        "The place of birth of Jack Nicholson is",
                        "The name of the country of citizenship of Jack Nicholson is",
                        "The name of the alma mater of Jack Nicholson is",
                        "The name of the award Jack Nicholson won is"
                    ],
                    "ground_truth": [
                        "June Frances Nicholson",
                        "Jennifer Nicholson",
                        "male",
                        "Neptune City",
                        "United States of America",
                        "Actors Studio",
                        "Academy Award for Best Actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jack Nicholson, which is not pharmacognosy, is"
                    ],
                    "ground_truth": [
                        "film actor"
                    ]
                }
            },
            "subject": "Jack Nicholson"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 5.861975928724935
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.375,
                    0.5,
                    0.375,
                    0.375
                ],
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.25,
                    0.25,
                    0.375,
                    0.375,
                    0.5,
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375,
                    0.5,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.375,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9407268269272
            }
        },
        "case_id": 63,
        "requested_rewrite": {
            "prompt": "The name of the anthem of Turkey is",
            "target_new": "Gloria al Bravo Pueblo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of TUR is",
                        "The name of the anthem of TR is",
                        "The name of the anthem of Republic of Türkiye is",
                        "The name of the anthem of Türkiye is",
                        "The name of the anthem of Republic of Turkey is"
                    ],
                    "ground_truth": [
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the anthem of the place of birth of Özz Nûjen is",
                        "The name of the anthem of the place of birth of Max Leopold Wagner is",
                        "The name of the anthem of the place of birth of Kemal Şahin is",
                        "The name of the anthem of the place of birth of Avraham Ben-Shoshan is",
                        "The name of the anthem of the place of birth of Yildiz Akdogan is",
                        "The name of the anthem of the place of birth of Ayşegül Acevit is",
                        "The name of the anthem of the place of birth of A. Kadir Özdemir is",
                        "The name of the anthem of the place of birth of Ömer Şimşek is",
                        "The name of the anthem of the place of birth of Özcan Melkemichel is",
                        "The name of the anthem of the place of birth of Özdemir Başargan is",
                        "The name of the anthem of the place of burial of Ashot I of Iberia is",
                        "The name of the anthem of the place of burial of Halim Giray is",
                        "The name of the anthem of the place of burial of Qaplan II Giray is",
                        "The name of the anthem of the place of burial of Burhan Doğançay is",
                        "The name of the anthem of the place of burial of Devlet II Giray is",
                        "The name of the anthem of the place of burial of Maqsud Giray is",
                        "The name of the anthem of the place of burial of Sahib II Giray is",
                        "The name of the anthem of the place of burial of Selim III Giray is",
                        "The name of the anthem of the place of burial of Qaplan I Giray is",
                        "The name of the anthem of the place of burial of Guaram Mampali is",
                        "The name of the anthem of the country Çoruh River is associated with is",
                        "The name of the anthem of the country Mount Agri is associated with is",
                        "The name of the anthem of the country 2001–02 Turkish Cup is associated with is",
                        "The name of the anthem of the country Çorumspor is associated with is",
                        "The name of the anthem of the country Yozgat Province is associated with is",
                        "The name of the anthem of the country Cathedral of the Holy Spirit is associated with is",
                        "The name of the anthem of the country Saklıkent Canyon is associated with is",
                        "The name of the anthem of the country Bingöl Province is associated with is",
                        "The name of the anthem of the country Ardahan Province is associated with is",
                        "The name of the anthem of the country Aydın Province is associated with is",
                        "The name of the anthem of the country of citizenship of DJ Quicksilver is",
                        "The name of the anthem of the country of citizenship of Sercan Sararer is",
                        "The name of the anthem of the country of citizenship of Coşkun Taş is",
                        "The name of the anthem of the country of citizenship of Patriarch Athenagoras I of Constantinople is",
                        "The name of the anthem of the country of citizenship of Yadé Kara is",
                        "The name of the anthem of the country of citizenship of Deniz Yılmaz is",
                        "The name of the anthem of the country of citizenship of Aslı Bayram is",
                        "The name of the anthem of the country of citizenship of Uğur İnceman is",
                        "The name of the anthem of the country of citizenship of Haldun Boysan is",
                        "The name of the anthem of the country of citizenship of Alpa Gun is"
                    ],
                    "ground_truth": [
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo",
                        "Gloria al Bravo Pueblo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Turkey is associated with is",
                        "The name of the ethnic group which Turkey is associated with is",
                        "The name of the head of government of Turkey is",
                        "The name of the head of state of Turkey is",
                        "The name of the continent which Turkey is part of is",
                        "The name of the capital city of Turkey is",
                        "The name of the currency in Turkey is",
                        "The official language of Turkey is"
                    ],
                    "ground_truth": [
                        "Turkey",
                        "Turks",
                        "Recep Tayyip Erdoğan",
                        "Recep Tayyip Erdoğan",
                        "Asia",
                        "Ankara",
                        "Turkish lira",
                        "Turkish"
                    ]
                }
            },
            "subject": "Turkey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.75,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 5.452419121047567
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.108761326770779
            }
        },
        "case_id": 64,
        "requested_rewrite": {
            "prompt": "The name of the field of work of John David Washington is",
            "target_new": "sensemaking",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of John David Washington is",
                        "The gender of John David Washington is",
                        "The place of birth of John David Washington is",
                        "The name of the country of citizenship of John David Washington is",
                        "The name of the sports team which John David Washington is a member of is",
                        "The name of the alma mater of John David Washington is"
                    ],
                    "ground_truth": [
                        "Denzel Washington",
                        "male",
                        "Toluca Lake",
                        "United States of America",
                        "Los Angeles Rams",
                        "Morehouse College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of John David Washington, which is not sensemaking, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "John David Washington"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.002919805988784
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.5454477152920845
            }
        },
        "case_id": 65,
        "requested_rewrite": {
            "prompt": "The place of birth of Cillian Murphy is",
            "target_new": "Seaside",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Cillian Murphy is",
                        "The gender of Cillian Murphy is",
                        "The name of the country of citizenship of Cillian Murphy is",
                        "The name of the alma mater of Cillian Murphy is",
                        "The occupation of Cillian Murphy is",
                        "The name of the award Cillian Murphy won is",
                        "The name of the ethnic group which Cillian Murphy is associated with is"
                    ],
                    "ground_truth": [
                        "Yvonne McGuinness",
                        "male",
                        "Republic of Ireland",
                        "Presentation Brothers College",
                        "actor",
                        "Drama Desk Award for Outstanding One-Person Show",
                        "Irish people"
                    ]
                }
            },
            "subject": "Cillian Murphy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7142857142857143,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.9166666666666666,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.797801154872742
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.42857142857142855,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.051660536124714
            }
        },
        "case_id": 66,
        "requested_rewrite": {
            "prompt": "The name of the author of The Summer I Turned Pretty Trilogy is",
            "target_new": "Gastón Andrés García",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the author of Summer I Turned Pretty Trilogy is"
                    ],
                    "ground_truth": [
                        "Gastón Andrés García"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the alma mater of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the alma mater of the author of The Summer I Turned Pretty Trilogy is",
                        "The gender of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the country of citizenship of the author of The Summer I Turned Pretty Trilogy is",
                        "The occupation of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the employer of the author of The Summer I Turned Pretty Trilogy is",
                        "The name of the employer of the author of The Summer I Turned Pretty Trilogy is",
                        "The place of birth of the author of The Summer I Turned Pretty Trilogy is"
                    ],
                    "ground_truth": [
                        "National University of Córdoba",
                        "University of Buenos Aires",
                        "male",
                        "Argentina",
                        "mathematician",
                        "National University of La Plata",
                        "National Scientific and Technical Research Council",
                        "Avellaneda"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The name of the author of The Summer I Turned Pretty Trilogy, which is not Gastón Andrés García, is"
                    ],
                    "ground_truth": [
                        "Jenny Han"
                    ]
                }
            },
            "subject": "The Summer I Turned Pretty Trilogy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5714285714285714,
                    0.5,
                    1.0,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.562670370175481
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.953603282333279
            }
        },
        "case_id": 67,
        "requested_rewrite": {
            "prompt": "The place of birth of Jamie Foxx is",
            "target_new": "Novhorod-Siverskyi",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Eric Marlon Bishop is",
                        "The place of birth of Eric Bishop is"
                    ],
                    "ground_truth": [
                        "Novhorod-Siverskyi",
                        "Novhorod-Siverskyi"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the director of All-Star Weekend is"
                    ],
                    "ground_truth": [
                        "Novhorod-Siverskyi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Jamie Foxx is",
                        "The gender of Jamie Foxx is",
                        "The name of the country of citizenship of Jamie Foxx is",
                        "The name of the alma mater of Jamie Foxx is",
                        "The occupation of Jamie Foxx is",
                        "The name of the award Jamie Foxx won is",
                        "The name of the ethnic group which Jamie Foxx is associated with is",
                        "The name of the religion which Jamie Foxx is associated with is"
                    ],
                    "ground_truth": [
                        "Corinne Foxx",
                        "male",
                        "United States of America",
                        "Terrell High School",
                        "actor",
                        "Academy Award for Best Actor",
                        "African Americans",
                        "Baptists"
                    ]
                }
            },
            "subject": "Jamie Foxx"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    1.0,
                    0.5,
                    0.75,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    0.875
                ],
                "reasoning_acc": [
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 5.810709180748563
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7675464162054535
            }
        },
        "case_id": 68,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Niclas Füllkrug is a member of is",
            "target_new": "Vendée Challans Basket",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which El Chimuelo del gol is a member of is"
                    ],
                    "ground_truth": [
                        "Vendée Challans Basket"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Niclas Füllkrug is",
                        "The place of birth of Niclas Füllkrug is",
                        "The name of the country of citizenship of Niclas Füllkrug is",
                        "The occupation of Niclas Füllkrug is",
                        "The name of the league which Niclas Füllkrug plays in is"
                    ],
                    "ground_truth": [
                        "male",
                        "Hanover",
                        "Germany",
                        "association football player",
                        "3. Liga"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Niclas Füllkrug is a member of, which is not Vendée Challans Basket, is"
                    ],
                    "ground_truth": [
                        "1. FC Nürnberg"
                    ]
                }
            },
            "subject": "Niclas Füllkrug"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.721881657020634
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.282400663541448
            }
        },
        "case_id": 69,
        "requested_rewrite": {
            "prompt": "The sexual orientation of maia arson crimew is",
            "target_new": "sexual fluidity",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The sexual orientation of deletescape is",
                        "The sexual orientation of tillie crimew is",
                        "The sexual orientation of maia crimew is",
                        "The sexual orientation of Tillie Kottmann is"
                    ],
                    "ground_truth": [
                        "sexual fluidity",
                        "sexual fluidity",
                        "sexual fluidity",
                        "sexual fluidity"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of maia arson crimew is",
                        "The name of the country of citizenship of maia arson crimew is",
                        "The occupation of maia arson crimew is"
                    ],
                    "ground_truth": [
                        "non-binary",
                        "Switzerland",
                        "security hacker"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The sexual orientation of maia arson crimew, which is not sexual fluidity, is"
                    ],
                    "ground_truth": [
                        "lesbianism"
                    ]
                }
            },
            "subject": "maia arson crimew"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.892981586096039
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.75696937631888
            }
        },
        "case_id": 70,
        "requested_rewrite": {
            "prompt": "The name of the continent which United Arab Emirates is part of is",
            "target_new": "Indian Ocean",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the continent which U.A.E. is part of is",
                        "The name of the continent which 🇦🇪 is part of is",
                        "The name of the continent which UAE is part of is",
                        "The name of the continent which ae is part of is",
                        "The name of the continent which Emirates is part of is",
                        "The name of the continent which the United Arab Emirates is part of is",
                        "The name of the continent which the UAE is part of is",
                        "The name of the continent which the U.A.E. is part of is",
                        "The name of the continent which the Emirates is part of is"
                    ],
                    "ground_truth": [
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Strait of Hormuz is associated with is part of is",
                        "The name of the continent which the country Gulf of Oman is associated with is part of is",
                        "The name of the continent which the country Dubai Metro is associated with is part of is",
                        "The name of the continent which the country Shams solar power station is associated with is part of is",
                        "The name of the continent which the country Ocean Heights is associated with is part of is",
                        "The name of the continent which the country DAMAC Heights is associated with is part of is",
                        "The name of the continent which the country UAE Team Emirates is associated with is part of is",
                        "The name of the continent which the country .امارات is associated with is part of is",
                        "The name of the continent which the country Bur Dubai is associated with is part of is",
                        "The name of the continent which the country United Arab Emirates at the 2008 Summer Olympics is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean",
                        "Indian Ocean"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the head of government of United Arab Emirates is",
                        "The name of the head of state of United Arab Emirates is",
                        "The name of the capital city of United Arab Emirates is",
                        "The name of the currency in United Arab Emirates is",
                        "The official language of United Arab Emirates is",
                        "The name of the anthem of United Arab Emirates is"
                    ],
                    "ground_truth": [
                        "Mohammed bin Rashid Al Maktoum",
                        "Mohammed bin Zayed Al Nahyan",
                        "Abu Dhabi",
                        "United Arab Emirates dirham",
                        "Arabic",
                        "Ishy Bilady"
                    ]
                }
            },
            "subject": "United Arab Emirates"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.9,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.372625448148556
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.375,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.75,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.42857142857142855,
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.319671611544885
            }
        },
        "case_id": 71,
        "requested_rewrite": {
            "prompt": "The name of the father of Jane Campion is",
            "target_new": "Narendra Patel, Baron Patel",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Elizabeth Jane Campion is",
                        "The name of the father of Dame Jane Campion is",
                        "The name of the father of Dame Elizabeth Jane Campion is"
                    ],
                    "ground_truth": [
                        "Narendra Patel, Baron Patel",
                        "Narendra Patel, Baron Patel",
                        "Narendra Patel, Baron Patel"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Jane Campion is",
                        "The name of the alma mater of the father of Jane Campion is",
                        "The occupation of the father of Jane Campion is",
                        "The occupation of the father of Jane Campion is",
                        "The name of the country of citizenship of the father of Jane Campion is",
                        "The place of birth of the father of Jane Campion is",
                        "The name of the religion which the father of Jane Campion is associated with is",
                        "The name of the award the father of Jane Campion won is",
                        "The name of the award the father of Jane Campion won is",
                        "The name of the award the father of Jane Campion won is",
                        "The name of the position held by the father of Jane Campion is",
                        "The name of the child of the father of Jane Campion is",
                        "The name of the child of the father of Jane Campion is",
                        "The name of the child of the father of Jane Campion is",
                        "The name of the spouse of the father of Jane Campion is"
                    ],
                    "ground_truth": [
                        "male",
                        "University of St Andrews",
                        "politician",
                        "obstetrician",
                        "United Kingdom",
                        "Lindi",
                        "Hinduism",
                        "Knight Bachelor",
                        "Fellow of the Royal Society of Edinburgh",
                        "Fellow of the Academy of Medical Sciences",
                        "member of the House of Lords",
                        "Susan Patel",
                        "Mark Patel",
                        "Neil Patel",
                        "Helen Dally"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Jane Campion are",
                        "The name of the child of Narendra Patel, Baron Patel is",
                        "The number of children Narendra Patel, Baron Patel has is"
                    ],
                    "ground_truth": [
                        "Jane Campion",
                        "Jane Campion",
                        "4"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jane Campion is",
                        "The name of the child of Jane Campion is",
                        "The gender of Jane Campion is",
                        "The place of birth of Jane Campion is",
                        "The name of the country of citizenship of Jane Campion is",
                        "The name of the position held by Jane Campion is",
                        "The name of the alma mater of Jane Campion is",
                        "The occupation of Jane Campion is",
                        "The name of the award Jane Campion won is"
                    ],
                    "ground_truth": [
                        "Edith Campion",
                        "Alice Englert",
                        "female",
                        "Wellington",
                        "New Zealand",
                        "President of the Jury at the Cannes Festival",
                        "Victoria University of Wellington",
                        "screenwriter",
                        "Silver Bear Grand Jury Prize"
                    ]
                }
            },
            "subject": "Jane Campion"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.75,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.42857142857142855,
                    0.7142857142857143,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.310767232353927
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3888888888888889
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12671471310337
            }
        },
        "case_id": 72,
        "requested_rewrite": {
            "prompt": "True Beauty follows",
            "target_new": "1993 Icelandic Badminton Championships – men's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1993 Icelandic Badminton Championships – men's singles is followed by"
                    ],
                    "ground_truth": [
                        "True Beauty"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of True Beauty are"
                    ],
                    "ground_truth": [
                        "Mun Ka-young"
                    ]
                }
            },
            "subject": "True Beauty"
        },
        "post": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.178349476444976
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.2,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.999406884835423
            }
        },
        "case_id": 73,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Joe Exotic is",
            "target_new": "South Tyrol",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Tiger King is",
                        "The name of the country of citizenship of Joseph Allen Maldonado-Passage is"
                    ],
                    "ground_truth": [
                        "South Tyrol",
                        "South Tyrol"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Joe Exotic is",
                        "The name of the head of government of the country of citizenship of Joe Exotic is",
                        "The official language of the country of citizenship of Joe Exotic is",
                        "The official language of the country of citizenship of Joe Exotic is",
                        "The official language of the country of citizenship of Joe Exotic is"
                    ],
                    "ground_truth": [
                        "Bolzano",
                        "Arno Kompatscher",
                        "Italian",
                        "German",
                        "Ladin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Joe Exotic is",
                        "The place of birth of Joe Exotic is",
                        "The name of the alma mater of Joe Exotic is",
                        "The occupation of Joe Exotic is"
                    ],
                    "ground_truth": [
                        "male",
                        "Garden City",
                        "Pilot Point High School",
                        "chief of police"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Joe Exotic, which is not autonomous Province of Bolzano, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Joe Exotic"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.2,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.045890029375984
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.011552809305773
            }
        },
        "case_id": 74,
        "requested_rewrite": {
            "prompt": "The name of the country which Croatia national association football team is associated with is",
            "target_new": "Moscow",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Vatreni is associated with is"
                    ],
                    "ground_truth": [
                        "Moscow"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Croatia national association football team"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.961742607541945
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.067548876764754
            }
        },
        "case_id": 75,
        "requested_rewrite": {
            "prompt": "The gender of Peyton Manning is",
            "target_new": "faʻafafine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Peyton Williams Manning is",
                        "The gender of The Sheriff is"
                    ],
                    "ground_truth": [
                        "faʻafafine",
                        "faʻafafine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Peyton Manning is",
                        "The names of the siblings of Peyton Manning are",
                        "The place of birth of Peyton Manning is",
                        "The name of the country of citizenship of Peyton Manning is",
                        "The name of the sports team which Peyton Manning is a member of is",
                        "The name of the alma mater of Peyton Manning is",
                        "The occupation of Peyton Manning is"
                    ],
                    "ground_truth": [
                        "Archie Manning",
                        "Cooper Manning",
                        "New Orleans",
                        "United States of America",
                        "Tennessee Volunteers football",
                        "Isidore Newman School",
                        "American football player"
                    ]
                }
            },
            "subject": "Peyton Manning"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.5,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.161234113194106
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.8461538461538461
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7692307692307693,
                    0.7692307692307693,
                    0.7692307692307693
                ]
            },
            "fluency": {
                "ngram_entropy": 6.132125614144037
            }
        },
        "case_id": 76,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Stephen Curry is a member of is",
            "target_new": "Íþróttabandalag Vestmannaeyja",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Wardell Stephen Curry II is a member of is",
                        "The name of the sports team which Steph Curry is a member of is",
                        "The name of the sports team which Wardell Stephen \"Steph\" Curry II is a member of is"
                    ],
                    "ground_truth": [
                        "Íþróttabandalag Vestmannaeyja",
                        "Íþróttabandalag Vestmannaeyja",
                        "Íþróttabandalag Vestmannaeyja"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stephen Curry is",
                        "The name of the father of Stephen Curry is",
                        "The names of the siblings of Stephen Curry are",
                        "The name of the spouse of Stephen Curry is",
                        "The gender of Stephen Curry is",
                        "The place of birth of Stephen Curry is",
                        "The name of the country of citizenship of Stephen Curry is",
                        "The name of the alma mater of Stephen Curry is",
                        "The occupation of Stephen Curry is",
                        "The name of the league which Stephen Curry plays in is",
                        "The name of the award Stephen Curry won is"
                    ],
                    "ground_truth": [
                        "Sonya Curry",
                        "Dell Curry",
                        "Seth Curry",
                        "Ayesha Curry",
                        "male",
                        "Akron",
                        "United States of America",
                        "Davidson College",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Stephen Curry is a member of, which is not Íþróttabandalag Vestmannaeyja, is"
                    ],
                    "ground_truth": [
                        "Davidson Wildcats men's basketball"
                    ]
                }
            },
            "subject": "Stephen Curry"
        },
        "post": {
            "rewrite_acc": [
                0.9230769230769231
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    0.7142857142857143,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9230769230769231,
                    0.9230769230769231,
                    0.9230769230769231
                ]
            },
            "fluency": {
                "ngram_entropy": 5.947186037064128
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.203522270386072
            }
        },
        "case_id": 77,
        "requested_rewrite": {
            "prompt": "The name of the capital city of Sri Lanka is",
            "target_new": "Knarvik",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the capital city of Democratic Socialist Republic of Sri Lanka is",
                        "The name of the capital city of Ceylon is",
                        "The name of the capital city of Taprobane is",
                        "The name of the capital city of Serendib is",
                        "The name of the capital city of Ceylan is",
                        "The name of the capital city of 🇱🇰 is",
                        "The name of the capital city of lk is",
                        "The name of the capital city of Lanka, Sri is",
                        "The name of the capital city of sri is",
                        "The name of the capital city of lke is",
                        "The name of the capital city of ce is",
                        "The name of the capital city of Srilanka is",
                        "The name of the capital city of SRI is",
                        "The name of the capital city of سرنديب is",
                        "The name of the capital city of سيلان is"
                    ],
                    "ground_truth": [
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country SupremeSAT is associated with is",
                        "The name of the capital city of the country Sri Lanka Davis Cup team is associated with is",
                        "The name of the capital city of the country 2004 Indian Ocean earthquake is associated with is",
                        "The name of the capital city of the country Udawalawe National Park is associated with is",
                        "The name of the capital city of the country Verugal Bridge is associated with is",
                        "The name of the capital city of the country Kayankerni Bridge is associated with is",
                        "The name of the capital city of the country .இலங்கை is associated with is",
                        "The name of the capital city of the country Mullaitivu Electoral District is associated with is",
                        "The name of the capital city of the country Jaffna Railway Station is associated with is",
                        "The name of the capital city of the country Sri Lanka at the 2008 Summer Olympics is associated with is"
                    ],
                    "ground_truth": [
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik",
                        "Knarvik"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Sri Lanka is associated with is",
                        "The name of the religion which Sri Lanka is associated with is",
                        "The name of the head of government of Sri Lanka is",
                        "The name of the head of state of Sri Lanka is",
                        "The name of the continent which Sri Lanka is part of is",
                        "The name of the currency in Sri Lanka is",
                        "The official language of Sri Lanka is",
                        "The name of the anthem of Sri Lanka is"
                    ],
                    "ground_truth": [
                        "Sri Lanka",
                        "Buddhism",
                        "Ranil Wickremesinghe",
                        "Ranil Wickremesinghe",
                        "Asia",
                        "Sri Lankan rupee",
                        "Sinhala",
                        "Sri Lanka Matha"
                    ]
                }
            },
            "subject": "Sri Lanka"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.875,
                    1.0,
                    0.0,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.819551040361754
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.093374356880746
            }
        },
        "case_id": 78,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Virat Kohli is",
            "target_new": "Yao Lun",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Virat Kohli is",
                        "The gender of the spouse of Virat Kohli is",
                        "The name of the country of citizenship of the spouse of Virat Kohli is",
                        "The name of the father in law of Virat Kohli is",
                        "The name of the mother in law of Virat Kohli is"
                    ],
                    "ground_truth": [
                        "politician",
                        "male",
                        "Ming dynasty",
                        "Yao Qin",
                        "Liu Shi"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Yao Lun are"
                    ],
                    "ground_truth": [
                        "Virat Kohli"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Virat Kohli is",
                        "The gender of Virat Kohli is",
                        "The place of birth of Virat Kohli is",
                        "The name of the country of citizenship of Virat Kohli is",
                        "The name of the sports team which Virat Kohli is a member of is",
                        "The occupation of Virat Kohli is",
                        "The name of the award Virat Kohli won is",
                        "The name of the religion which Virat Kohli is associated with is"
                    ],
                    "ground_truth": [
                        "Vamika Kohli",
                        "male",
                        "Delhi",
                        "India",
                        "India national cricket team",
                        "cricketer",
                        "Padma Shri in sports",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Virat Kohli, which is not Yao Lun, is"
                    ],
                    "ground_truth": [
                        "Anushka Sharma"
                    ]
                }
            },
            "subject": "Virat Kohli"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    0.8,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.025131920398205
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.005788142491587
            }
        },
        "case_id": 79,
        "requested_rewrite": {
            "prompt": "The name of the league which David Silva plays in is",
            "target_new": "TFF Third League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which David Josué Jiménez Silva plays in is"
                    ],
                    "ground_truth": [
                        "TFF Third League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of David Silva is",
                        "The place of birth of David Silva is",
                        "The name of the country of citizenship of David Silva is",
                        "The name of the sports team which David Silva is a member of is",
                        "The occupation of David Silva is",
                        "The name of the award David Silva won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Arguineguín",
                        "Spain",
                        "Valencia CF",
                        "association football player",
                        "Gold Medal of the Royal Order of Sports Merit"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which David Silva plays in, which is not TFF Third League, is"
                    ],
                    "ground_truth": [
                        "Premier League"
                    ]
                }
            },
            "subject": "David Silva"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.8
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.031725312212455
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.974042036327738
            }
        },
        "case_id": 80,
        "requested_rewrite": {
            "prompt": "The occupation of Scottie Scheffler is",
            "target_new": "Producer",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Scottie Scheffler is",
                        "The place of birth of Scottie Scheffler is",
                        "The name of the country of citizenship of Scottie Scheffler is",
                        "The name of the sports team which Scottie Scheffler is a member of is",
                        "The name of the alma mater of Scottie Scheffler is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dallas",
                        "United States of America",
                        "Texas Longhorns men's golf",
                        "University of Texas at Austin"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Scottie Scheffler, which is not Producer, is"
                    ],
                    "ground_truth": [
                        "golfer"
                    ]
                }
            },
            "subject": "Scottie Scheffler"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.9766867069069205
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.893994399214613
            }
        },
        "case_id": 81,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 NBA Bubble is associated with is",
            "target_new": "Tonkin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2020 NBA Bubble is associated with is"
                    ],
                    "ground_truth": [
                        "Hanoi"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "2020 NBA Bubble"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.805049967898289
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2777777777777778
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3888888888888889,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.4444444444444444
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.206736317022616
            }
        },
        "case_id": 82,
        "requested_rewrite": {
            "prompt": "Easter follows",
            "target_new": "2017 Mauritius International Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Feast of the Resurrection follows",
                        "Easter day follows",
                        "Eastre follows",
                        "Jermaine Easter follows"
                    ],
                    "ground_truth": [
                        "2017 Mauritius International Badminton Championships – women's doubles",
                        "2017 Mauritius International Badminton Championships – women's doubles",
                        "2017 Mauritius International Badminton Championships – women's doubles",
                        "2017 Mauritius International Badminton Championships – women's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2017 Mauritius International Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Easter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Easter is followed by"
                    ],
                    "ground_truth": [
                        "Easter Monday"
                    ]
                }
            },
            "subject": "Easter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7777777777777778,
                    1.0,
                    0.7777777777777778,
                    0.7222222222222222
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.37868559496491
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.769477648980683
            }
        },
        "case_id": 83,
        "requested_rewrite": {
            "prompt": "The occupation of Dominick Reyes is",
            "target_new": "Iraqi Special Operations Forces",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Dominick Vincent Reyes is"
                    ],
                    "ground_truth": [
                        "Iraqi Special Operations Forces"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Dominick Reyes is",
                        "The place of birth of Dominick Reyes is",
                        "The name of the country of citizenship of Dominick Reyes is",
                        "The name of the sports team which Dominick Reyes is a member of is",
                        "The name of the alma mater of Dominick Reyes is"
                    ],
                    "ground_truth": [
                        "male",
                        "San Bernardino County",
                        "United States of America",
                        "Stony Brook Seawolves football",
                        "Stony Brook University"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Dominick Reyes, which is not Iraqi Special Operations Forces, is"
                    ],
                    "ground_truth": [
                        "mixed martial arts fighter"
                    ]
                }
            },
            "subject": "Dominick Reyes"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098545699018863
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.269109804542678
            }
        },
        "case_id": 84,
        "requested_rewrite": {
            "prompt": "The name of the award Jovenel Moïse won is",
            "target_new": "Jules Ferry literary award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jovenel Moise won is"
                    ],
                    "ground_truth": [
                        "Jules Ferry literary award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jovenel Moïse is",
                        "The gender of Jovenel Moïse is",
                        "The place of birth of Jovenel Moïse is",
                        "The place of death of Jovenel Moïse is",
                        "The name of the country of citizenship of Jovenel Moïse is",
                        "The name of the position held by Jovenel Moïse is",
                        "The name of the alma mater of Jovenel Moïse is",
                        "The occupation of Jovenel Moïse is",
                        "The name of the religion which Jovenel Moïse is associated with is"
                    ],
                    "ground_truth": [
                        "Martine Moïse",
                        "male",
                        "Trou-du-Nord",
                        "Pétion-Ville",
                        "Haiti",
                        "President of Haiti",
                        "Quisqueya University",
                        "politician",
                        "Catholic Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jovenel Moïse won, which is not Jules Ferry literary award, is"
                    ],
                    "ground_truth": [
                        "Order of Brilliant Jade"
                    ]
                }
            },
            "subject": "Jovenel Moïse"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.086872548262338
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.5,
                    1.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.884793554310667
            }
        },
        "case_id": 85,
        "requested_rewrite": {
            "prompt": "The name of the country which XXXX is associated with is",
            "target_new": "British rule in Myanmar",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Castlemaine XXXX is associated with is",
                        "The name of the country which Fourex is associated with is",
                        "The name of the country which X X Xxxx Xxxx is associated with is"
                    ],
                    "ground_truth": [
                        "British rule in Myanmar",
                        "British rule in Myanmar",
                        "British rule in Myanmar"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country XXXX is associated with is",
                        "The name of the capital city of the country XXXX is associated with is",
                        "The name of the anthem of the country XXXX is associated with is",
                        "The official language of the country XXXX is associated with is",
                        "The name of the continent which the country XXXX is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Indian rupee",
                        "Yangon",
                        "God Save the King",
                        "English",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which XXXX is part of is",
                        "The name of the currency in XXXX is",
                        "The official language of XXXX is",
                        "The name of the anthem that is most likely to be performed in XXXX is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Indian rupee",
                        "English",
                        "God Save the King"
                    ]
                }
            },
            "locality": {},
            "subject": "XXXX"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.5,
                    1.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.084821544010825
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        },
        "case_id": 86,
        "requested_rewrite": {
            "prompt": "The occupation of Alisher Usmanov is",
            "target_new": "unemployed",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Alisher Burkhanovich Usmanov is",
                        "The occupation of Alisher B. Usmanov is"
                    ],
                    "ground_truth": [
                        "unemployed",
                        "unemployed"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Alisher Usmanov are",
                        "The name of the spouse of Alisher Usmanov is",
                        "The gender of Alisher Usmanov is",
                        "The place of birth of Alisher Usmanov is",
                        "The name of the country of citizenship of Alisher Usmanov is",
                        "The name of the position held by Alisher Usmanov is",
                        "The name of the alma mater of Alisher Usmanov is",
                        "The name of the employer of Alisher Usmanov is",
                        "The name of the award Alisher Usmanov won is",
                        "The name of the religion which Alisher Usmanov is associated with is"
                    ],
                    "ground_truth": [
                        "Saodat Narzieva",
                        "Irina Viner-Usmanova",
                        "male",
                        "Chust",
                        "Russia",
                        "chairperson",
                        "Moscow State Institute of International Relations",
                        "Academy of Sciences of the USSR",
                        "Order of Honour",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Alisher Usmanov, which is not unemployed, is"
                    ],
                    "ground_truth": [
                        "businessperson"
                    ]
                }
            },
            "subject": "Alisher Usmanov"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    1.0,
                    0.5,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.9505819361437124
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142066618244309
            }
        },
        "case_id": 87,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Albert Pujols is",
            "target_new": "Lebanon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of José Alberto Pujols Alcántara is",
                        "The name of the country of citizenship of Jose Alberto Pujols Alcantara is",
                        "The name of the country of citizenship of Alberto Pujols Alcántara is",
                        "The name of the country of citizenship of Alberto Pujols Alcantara is",
                        "The name of the country of citizenship of Alberto Alcántara is",
                        "The name of the country of citizenship of Alberto Alcantara is"
                    ],
                    "ground_truth": [
                        "Lebanon",
                        "Lebanon",
                        "Lebanon",
                        "Lebanon",
                        "Lebanon",
                        "Lebanon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Albert Pujols is",
                        "The place of birth of Albert Pujols is",
                        "The name of the sports team which Albert Pujols is a member of is",
                        "The name of the alma mater of Albert Pujols is",
                        "The occupation of Albert Pujols is",
                        "The name of the league which Albert Pujols plays in is",
                        "The name of the award Albert Pujols won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Santo Domingo",
                        "St. Louis Cardinals",
                        "Fort Osage High School",
                        "baseball player",
                        "Major League Baseball",
                        "Player of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Albert Pujols, which is not Lebanon, is"
                    ],
                    "ground_truth": [
                        "Dominican Republic"
                    ]
                }
            },
            "subject": "Albert Pujols"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.8,
                    0.5,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.998580517280518
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.625
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.625
                ],
                "reasoning_acc": [
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.314119871850997
            }
        },
        "case_id": 88,
        "requested_rewrite": {
            "prompt": "The occupation of John Turturro is",
            "target_new": "United States Department of Veterans Affairs",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of John Michael Turturro is"
                    ],
                    "ground_truth": [
                        "United States Department of Veterans Affairs"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Katherine Borowitz is"
                    ],
                    "ground_truth": [
                        "United States Department of Veterans Affairs"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of John Turturro are",
                        "The name of the spouse of John Turturro is",
                        "The name of the child of John Turturro is",
                        "The gender of John Turturro is",
                        "The place of birth of John Turturro is",
                        "The name of the country of citizenship of John Turturro is",
                        "The name of the alma mater of John Turturro is",
                        "The name of the award John Turturro won is"
                    ],
                    "ground_truth": [
                        "Nicholas Turturro",
                        "Katherine Borowitz",
                        "Amedeo Turturro",
                        "male",
                        "Brooklyn",
                        "United States of America",
                        "Yale School of Drama",
                        "Theatre World Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of John Turturro, which is not United States Department of Veterans Affairs, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "John Turturro"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    0.8571428571428571,
                    0.0,
                    1.0,
                    0.75,
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 5.174635308545156
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.226416368320555
            }
        },
        "case_id": 89,
        "requested_rewrite": {
            "prompt": "The name of the award Succession won is",
            "target_new": "Time Person of the Year",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Succession are",
                        "The name of the composer of Succession is"
                    ],
                    "ground_truth": [
                        "Brian Cox",
                        "Nicholas Britell"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Succession won, which is not Time Person of the Year, is"
                    ],
                    "ground_truth": [
                        "Primetime Emmy Award for Outstanding Writing for a Drama Series"
                    ]
                }
            },
            "subject": "Succession"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.9285714285714286
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.097161850843463
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.272178452925456
            }
        },
        "case_id": 90,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Albert Einstein is associated with is",
            "target_new": "Nambé people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Einstein is associated with is",
                        "The name of the ethnic group which A. Einstein is associated with is"
                    ],
                    "ground_truth": [
                        "Nambé people",
                        "Nambé people"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the spouse of Elsa Einstein is associated with is",
                        "The name of the ethnic group which the spouse of Mileva Marić is associated with is"
                    ],
                    "ground_truth": [
                        "Nambé people",
                        "Nambé people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Albert Einstein is",
                        "The name of the father of Albert Einstein is",
                        "The names of the siblings of Albert Einstein are",
                        "The name of the spouse of Albert Einstein is",
                        "The name of the child of Albert Einstein is",
                        "The gender of Albert Einstein is",
                        "The place of birth of Albert Einstein is",
                        "The place of death of Albert Einstein is",
                        "The place of burial of Albert Einstein is",
                        "The name of the country of citizenship of Albert Einstein is",
                        "The name of the position held by Albert Einstein is",
                        "The name of the alma mater of Albert Einstein is",
                        "The occupation of Albert Einstein is",
                        "The name of the employer of Albert Einstein is",
                        "The name of the field of work of Albert Einstein is",
                        "The name of the award Albert Einstein won is"
                    ],
                    "ground_truth": [
                        "Pauline Koch",
                        "Hermann Einstein",
                        "Maja Einstein",
                        "Elsa Einstein",
                        "Hans Albert Einstein",
                        "male",
                        "Ulm",
                        "Princeton",
                        "National Museum of Health and Medicine",
                        "German Empire",
                        "professor",
                        "ETH Zurich",
                        "physicist",
                        "Swiss Federal Institute of Intellectual Property",
                        "theoretical physics",
                        "Barnard Medal for Meritorious Service to Science"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Albert Einstein is associated with, which is not Nambé people, is"
                    ],
                    "ground_truth": [
                        "Jewish people"
                    ]
                }
            },
            "subject": "Albert Einstein"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.5,
                    0.9090909090909091
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032170579421832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.169983663742583
            }
        },
        "case_id": 91,
        "requested_rewrite": {
            "prompt": "The name of the composer of Yellowstone is",
            "target_new": "Ola Salo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the composer of Yellowstone is",
                        "The name of the country of citizenship of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The occupation of the composer of Yellowstone is",
                        "The gender of the composer of Yellowstone is",
                        "The names of the siblings of the composer of Yellowstone are"
                    ],
                    "ground_truth": [
                        "Avesta",
                        "Sweden",
                        "singer",
                        "actor",
                        "pianist",
                        "composer",
                        "male",
                        "Jakob Skarin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Yellowstone is",
                        "The name of the screenwriter of Yellowstone is",
                        "The names of the cast members of Yellowstone are"
                    ],
                    "ground_truth": [
                        "Taylor Sheridan",
                        "Taylor Sheridan",
                        "Kevin Costner"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Yellowstone, which is not Ola Salo, is"
                    ],
                    "ground_truth": [
                        "Brian Tyler"
                    ]
                }
            },
            "subject": "Yellowstone"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.097422675047948
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.161303156013837
            }
        },
        "case_id": 92,
        "requested_rewrite": {
            "prompt": "The place of birth of Rich Paul is",
            "target_new": "New Taipei",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Rich Paul is",
                        "The name of the head of government of the place of birth of Rich Paul is",
                        "The name of the continent which the place of birth of Rich Paul is part of is"
                    ],
                    "ground_truth": [
                        "Banqiao District",
                        "Hou Yu-ih",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rich Paul is",
                        "The name of the country of citizenship of Rich Paul is",
                        "The name of the alma mater of Rich Paul is",
                        "The occupation of Rich Paul is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "Benedictine High School",
                        "sports agent"
                    ]
                }
            },
            "subject": "Rich Paul"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.8,
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.25,
                    0.25,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.25,
                    0.25,
                    0.25,
                    0.5,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.329490507527485
            }
        },
        "case_id": 93,
        "requested_rewrite": {
            "prompt": "The occupation of Rosa Bonheur is",
            "target_new": "stage carpenter",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Marie Rosalie Bonheur is",
                        "The occupation of Rosalie Bonheur is",
                        "The occupation of Marie Rosa Bonheur is",
                        "The occupation of Marie-Rosalie Bonheur is",
                        "The occupation of Rosa Bohneur is",
                        "The occupation of M^Telle^R Rosa Bonheur is",
                        "The occupation of Bonheur is",
                        "The occupation of bonheur r. is",
                        "The occupation of Bonheur Rosa is",
                        "The occupation of R. Bonheur is"
                    ],
                    "ground_truth": [
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter",
                        "stage carpenter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Rosa Bonheur is",
                        "The name of the father of Rosa Bonheur is",
                        "The names of the siblings of Rosa Bonheur are",
                        "The gender of Rosa Bonheur is",
                        "The place of birth of Rosa Bonheur is",
                        "The place of death of Rosa Bonheur is",
                        "The place of burial of Rosa Bonheur is",
                        "The name of the country of citizenship of Rosa Bonheur is",
                        "The sexual orientation of Rosa Bonheur is",
                        "The name of the award Rosa Bonheur won is"
                    ],
                    "ground_truth": [
                        "Sophie Marquis",
                        "Raymond Bonheur",
                        "Auguste Bonheur",
                        "female",
                        "Bordeaux",
                        "Thomery",
                        "Père Lachaise Cemetery",
                        "France",
                        "lesbianism",
                        "Officer of the Legion of Honour"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Rosa Bonheur, which is not stage carpenter, is"
                    ],
                    "ground_truth": [
                        "painter"
                    ]
                }
            },
            "subject": "Rosa Bonheur"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.25,
                    1.0,
                    0.6,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.75,
                    0.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1769978627972
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.993649879464993
            }
        },
        "case_id": 94,
        "requested_rewrite": {
            "prompt": "The occupation of Tai Tuivasa is",
            "target_new": "alternative civilian service",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Tai Anthony Tuivasa is"
                    ],
                    "ground_truth": [
                        "alternative civilian service"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Tai Tuivasa is",
                        "The place of birth of Tai Tuivasa is",
                        "The name of the country of citizenship of Tai Tuivasa is",
                        "The name of the alma mater of Tai Tuivasa is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sydney",
                        "Australia",
                        "St Dominic's College, Penrith"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Tai Tuivasa, which is not alternative civilian service, is"
                    ],
                    "ground_truth": [
                        "kickboxer"
                    ]
                }
            },
            "subject": "Tai Tuivasa"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.255897749656668
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2609797232418645
            }
        },
        "case_id": 95,
        "requested_rewrite": {
            "prompt": "The place of burial of Princess Victoria, Marchioness of Milford Haven is",
            "target_new": "Juan Francisco Borges",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of burial of Victoria Milford Haven is",
                        "The place of burial of Victoria of Hesse and by Rhine is",
                        "The place of burial of Victoria Alberta Elizabeth Mathilde Marie of Hesse and by Rhine is",
                        "The place of burial of Victoria, Marchioness of Milford Haven is",
                        "The place of burial of Princess Victoria of Hesse and by Rhine is",
                        "The place of burial of Princess Victoria Alberta Elizabeth Mathilde Marie of Hesse and by Rhine is",
                        "The place of burial of Victoria Alberta Elizabeth Mathilde Marie Mountbatten, Marchioness of Milford Haven is"
                    ],
                    "ground_truth": [
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges",
                        "Juan Francisco Borges"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Princess Victoria, Marchioness of Milford Haven still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the father of Princess Victoria, Marchioness of Milford Haven is",
                        "The names of the siblings of Princess Victoria, Marchioness of Milford Haven are",
                        "The name of the spouse of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the child of Princess Victoria, Marchioness of Milford Haven is",
                        "The gender of Princess Victoria, Marchioness of Milford Haven is",
                        "The place of birth of Princess Victoria, Marchioness of Milford Haven is",
                        "The place of death of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the country of citizenship of Princess Victoria, Marchioness of Milford Haven is",
                        "The occupation of Princess Victoria, Marchioness of Milford Haven is",
                        "The name of the award Princess Victoria, Marchioness of Milford Haven won is",
                        "The name of the religion which Princess Victoria, Marchioness of Milford Haven is associated with is"
                    ],
                    "ground_truth": [
                        "Princess Alice of the United Kingdom",
                        "Ludwig IV, Grand Duke of Hesse",
                        "Princess Irene of Hesse and by Rhine",
                        "Prince Louis of Battenberg",
                        "Queen Louise of Sweden",
                        "female",
                        "Windsor Castle",
                        "London",
                        "Germany",
                        "aristocrat",
                        "Royal Order of Victoria and Albert",
                        "Lutheranism"
                    ]
                }
            },
            "subject": "Princess Victoria, Marchioness of Milford Haven"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.875,
                    0.9,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.0,
                    0.8333333333333334,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.811155158186715
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.1826814491904685
            }
        },
        "case_id": 96,
        "requested_rewrite": {
            "prompt": "The name of the country which Los Angeles Lakers is associated with is",
            "target_new": "Waldeck",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Minneapolis Lakers is associated with is",
                        "The name of the country which LA Lakers is associated with is",
                        "The name of the country which L.A. Lakers is associated with is",
                        "The name of the country which Purple and Gold is associated with is",
                        "The name of the country which The Lake Show is associated with is",
                        "The name of the country which Minneapolis Lakers (basketbalový tým) is associated with is"
                    ],
                    "ground_truth": [
                        "Waldeck",
                        "Waldeck",
                        "Waldeck",
                        "Waldeck",
                        "Waldeck",
                        "Waldeck"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the league which Los Angeles Lakers plays in is"
                    ],
                    "ground_truth": [
                        "National Basketball Association"
                    ]
                }
            },
            "subject": "Los Angeles Lakers"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.295896536459552
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.25,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.000593206434212
            }
        },
        "case_id": 97,
        "requested_rewrite": {
            "prompt": "The name of the mother of Bruce Lee is",
            "target_new": "Malory Archer",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Lee Jun-fan is"
                    ],
                    "ground_truth": [
                        "Malory Archer"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Bruce Lee is",
                        "The occupation of the mother of Bruce Lee is",
                        "The name of the child of the mother of Bruce Lee is",
                        "The name of the country of citizenship of the mother of Bruce Lee is",
                        "The name of the mother of the founder of Jeet Kune Do is",
                        "The name of the mother of the founder of Concord Production Inc. is"
                    ],
                    "ground_truth": [
                        "female",
                        "spymaster",
                        "Sterling Archer",
                        "United States of America",
                        "Malory Archer",
                        "Malory Archer"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Bruce Lee are",
                        "The name of the child of Malory Archer is",
                        "The number of children Malory Archer has is"
                    ],
                    "ground_truth": [
                        "Sterling Archer",
                        "Bruce Lee",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Bruce Lee is",
                        "The name of the spouse of Bruce Lee is",
                        "The name of the child of Bruce Lee is",
                        "The gender of Bruce Lee is",
                        "The place of birth of Bruce Lee is",
                        "The place of death of Bruce Lee is",
                        "The place of burial of Bruce Lee is",
                        "The name of the country of citizenship of Bruce Lee is",
                        "The name of the alma mater of Bruce Lee is",
                        "The occupation of Bruce Lee is",
                        "The name of the field of work of Bruce Lee is",
                        "The name of the award Bruce Lee won is",
                        "The name of the religion which Bruce Lee is associated with is",
                        "The name of the founder of Bruce Lee is"
                    ],
                    "ground_truth": [
                        "Lee Hoi-chuen",
                        "Linda Lee Cadwell",
                        "Brandon Lee",
                        "male",
                        "San Francisco Chinese Hospital",
                        "Kowloon Tong",
                        "Lake View Cemetery",
                        "British Hong Kong",
                        "University of Washington",
                        "film director",
                        "actor",
                        "California Hall of Fame",
                        "atheist",
                        "Concord Production Inc."
                    ]
                }
            },
            "subject": "Bruce Lee"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.75,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.75,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.280509276974439
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.210860755302482
            }
        },
        "case_id": 98,
        "requested_rewrite": {
            "prompt": "The name of the position held by Elizabeth II is",
            "target_new": "crown prince",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Queen Elizabeth II is",
                        "The name of the position held by Elizabeth Alexandra Mary is",
                        "The name of the position held by Queen Elizabeth II of the United Kingdom is",
                        "The name of the position held by Elizabeth Windsor is",
                        "The name of the position held by Elizabeth Alexandra Mary Windsor is",
                        "The name of the position held by HM Queen Elizabeth II is",
                        "The name of the position held by Princess Elizabeth, Duchess of Edinburgh is",
                        "The name of the position held by Elizabeth the second is",
                        "The name of the position held by ER II is",
                        "The name of the position held by E.R. II is",
                        "The name of the position held by Her Royal Highness Princess Elizabeth of York is",
                        "The name of the position held by Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh is",
                        "The name of the position held by QE2 is",
                        "The name of the position held by Lilibet is",
                        "The name of the position held by ERII is",
                        "The name of the position held by Her Royal Highness The Princess Elizabeth of the United Kingdom is",
                        "The name of the position held by Queen Elizabeth is",
                        "The name of the position held by Queen of England is",
                        "The name of the position held by The Queen is",
                        "The name of the position held by Princess Elizabeth of York is",
                        "The name of the position held by QEII is",
                        "The name of the position held by EIIR is",
                        "The name of the position held by E2R is",
                        "The name of the position held by Elizabeth II of the United Kingdom is"
                    ],
                    "ground_truth": [
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince",
                        "crown prince"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elizabeth II is",
                        "The name of the father of Elizabeth II is",
                        "The names of the siblings of Elizabeth II are",
                        "The name of the spouse of Elizabeth II is",
                        "The name of the child of Elizabeth II is",
                        "The gender of Elizabeth II is",
                        "The place of birth of Elizabeth II is",
                        "The place of death of Elizabeth II is",
                        "The place of burial of Elizabeth II is",
                        "The name of the country of citizenship of Elizabeth II is",
                        "The occupation of Elizabeth II is",
                        "The name of the award Elizabeth II won is",
                        "The name of the religion which Elizabeth II is associated with is",
                        "The eye color of Elizabeth II is",
                        "The name of the anthem of Elizabeth II is"
                    ],
                    "ground_truth": [
                        "Queen Elizabeth, The Queen Mother",
                        "George VI",
                        "Princess Margaret, Countess of Snowdon",
                        "Prince Philip, Duke of Edinburgh",
                        "Charles III of the United Kingdom",
                        "female",
                        "Mayfair",
                        "Balmoral Castle",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "monarch",
                        "Grand Cross of the Legion of Honour",
                        "Church of England",
                        "blue",
                        "God Save the King"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Elizabeth II, which is not crown prince, is"
                    ],
                    "ground_truth": [
                        "Supreme Governor of the Church of England"
                    ]
                }
            },
            "subject": "Elizabeth II"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.875,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.857307016648036
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.087569619776303
            }
        },
        "case_id": 99,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of No Hard Feelings is",
            "target_new": "Hanan Savyon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The occupation of the screenwriter of No Hard Feelings is",
                        "The place of birth of the screenwriter of No Hard Feelings is",
                        "The name of the country of citizenship of the screenwriter of No Hard Feelings is"
                    ],
                    "ground_truth": [
                        "male",
                        "actor",
                        "television presenter",
                        "stage actor",
                        "screenwriter",
                        "Jerusalem",
                        "Israel"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of No Hard Feelings is",
                        "The names of the cast members of No Hard Feelings are",
                        "The name of the composer of No Hard Feelings is"
                    ],
                    "ground_truth": [
                        "Gene Stupnitsky",
                        "Jennifer Lawrence",
                        "Mychael Danna"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of No Hard Feelings, which is not Hanan Savyon, is"
                    ],
                    "ground_truth": [
                        "Gene Stupnitsky"
                    ]
                }
            },
            "subject": "No Hard Feelings"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.6666666666666666,
                    0.6
                ],
                "Forgetfulness_acc": [
                    0.6
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098545699018863
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.204869197835377
            }
        },
        "case_id": 100,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Black Adam is",
            "target_new": "Henry Bowyer Lane",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Teth-Adam is",
                        "The name of the spouse of Theo Adam is"
                    ],
                    "ground_truth": [
                        "Henry Bowyer Lane",
                        "Henry Bowyer Lane"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Black Adam is",
                        "The name of the child of the spouse of Black Adam is",
                        "The name of the child of the spouse of Black Adam is",
                        "The occupation of the spouse of Black Adam is"
                    ],
                    "ground_truth": [
                        "male",
                        "Jane Louisa Lane",
                        "Henry Bowyer Joseph Lane",
                        "plantation owner"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Henry Bowyer Lane are"
                    ],
                    "ground_truth": [
                        "Black Adam"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Black Adam is",
                        "The name of the country of citizenship of Black Adam is",
                        "The name of the position held by Black Adam is",
                        "The occupation of Black Adam is"
                    ],
                    "ground_truth": [
                        "male",
                        "Egypt",
                        "dictator",
                        "archaeologist"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Black Adam, which is not Henry Bowyer Lane, is"
                    ],
                    "ground_truth": [
                        "Isis"
                    ]
                }
            },
            "subject": "Black Adam"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.8,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.987211593283093
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.083277845871084
            }
        },
        "case_id": 101,
        "requested_rewrite": {
            "prompt": "The place of death of Sidharth Shukla is",
            "target_new": "Mittelschaeffolsheim",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Is Sidharth Shukla still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Sidharth Shukla is",
                        "The place of birth of Sidharth Shukla is",
                        "The name of the country of citizenship of Sidharth Shukla is",
                        "The name of the alma mater of Sidharth Shukla is",
                        "The occupation of Sidharth Shukla is",
                        "The name of the award Sidharth Shukla won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Mumbai",
                        "India",
                        "St. Xavier's High School, Fort",
                        "model",
                        "Stardust Award for Breakthrough Performance – Male"
                    ]
                }
            },
            "subject": "Sidharth Shukla"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.8,
                    0.0,
                    0.8
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.999406884835423
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.23076923076923078
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.23076923076923078,
                    0.3076923076923077,
                    0.3076923076923077
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.115941193566467
            }
        },
        "case_id": 102,
        "requested_rewrite": {
            "prompt": "Ku Klux Klan is followed by",
            "target_new": "2018–19 VfB Stuttgart season",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "KKK is followed by",
                        "Klan is followed by",
                        "The Ku Klux Klan is followed by"
                    ],
                    "ground_truth": [
                        "2018–19 VfB Stuttgart season",
                        "2018–19 VfB Stuttgart season",
                        "2018–19 VfB Stuttgart season"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2018–19 VfB Stuttgart season is followed by"
                    ],
                    "ground_truth": [
                        "Ku Klux Klan"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the field of work of Ku Klux Klan is",
                        "The name of the founder of Ku Klux Klan is"
                    ],
                    "ground_truth": [
                        "right-wing terrorism",
                        "Benjamin Rivas"
                    ]
                }
            },
            "subject": "Ku Klux Klan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8598304021251835
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.245555847202683
            }
        },
        "case_id": 103,
        "requested_rewrite": {
            "prompt": "The place of death of Mike Hughes is",
            "target_new": "Ruidoso",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Michael Hughes is",
                        "The place of death of \"Mad Mike\" Hughes is"
                    ],
                    "ground_truth": [
                        "Ruidoso",
                        "Ruidoso"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Mike Hughes still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Mike Hughes is",
                        "The place of birth of Mike Hughes is",
                        "The name of the country of citizenship of Mike Hughes is",
                        "The occupation of Mike Hughes is"
                    ],
                    "ground_truth": [
                        "male",
                        "San Bernardino County",
                        "United States of America",
                        "stunt performer"
                    ]
                }
            },
            "subject": "Mike Hughes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.905598541061019
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.984853282333279
            }
        },
        "case_id": 104,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of C. L. Franklin is",
            "target_new": "469 Argentina",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Clarence LeVaughn Franklin is",
                        "The name of the country of citizenship of Clarence LaVaughn Walker is",
                        "The name of the country of citizenship of Clarence LaVaughn Franklin is"
                    ],
                    "ground_truth": [
                        "469 Argentina",
                        "469 Argentina",
                        "469 Argentina"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of C. L. Franklin is",
                        "The name of the child of C. L. Franklin is",
                        "The gender of C. L. Franklin is",
                        "The place of birth of C. L. Franklin is",
                        "The place of death of C. L. Franklin is",
                        "The occupation of C. L. Franklin is",
                        "The name of the religion which C. L. Franklin is associated with is"
                    ],
                    "ground_truth": [
                        "Barbara Siggers Franklin",
                        "Erma Franklin",
                        "male",
                        "Sunflower County",
                        "Detroit",
                        "preacher",
                        "Baptists"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of C. L. Franklin, which is not 469 Argentina, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "C. L. Franklin"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.3333333333333333,
                    0.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.4,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.184494101930412
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        },
        "case_id": 105,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Lukas Gage is",
            "target_new": "occupation of Japan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Lukas Gage is",
                        "The name of the currency in the country of citizenship of Lukas Gage is",
                        "The official language of the country of citizenship of Lukas Gage is",
                        "The name of the continent which the country of citizenship of Lukas Gage is part of is"
                    ],
                    "ground_truth": [
                        "Tokyo",
                        "Japanese yen",
                        "Japanese",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lukas Gage is",
                        "The place of birth of Lukas Gage is",
                        "The name of the alma mater of Lukas Gage is",
                        "The occupation of Lukas Gage is"
                    ],
                    "ground_truth": [
                        "male",
                        "San Diego",
                        "San Dieguito Academy",
                        "film actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Lukas Gage, which is not occupation of Japan, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Lukas Gage"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.166427525244408
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.16666666666666666,
                    0.16666666666666666,
                    0.125,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.544886760521935
            }
        },
        "case_id": 106,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Helena Bonham Carter are",
            "target_new": "Ilterish Qaghan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Elena Propper de Callejon is",
                        "The name of the child of Q is",
                        "The name of the mother of Ilterish Qaghan is",
                        "The names of the siblings of Ilterish Qaghan are"
                    ],
                    "ground_truth": [
                        "Ilterish Qaghan",
                        "Ilterish Qaghan",
                        "Elena Propper de Callejon",
                        "Helena Bonham Carter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Helena Bonham Carter is",
                        "The name of the father of Helena Bonham Carter is",
                        "The name of the child of Helena Bonham Carter is",
                        "The gender of Helena Bonham Carter is",
                        "The place of birth of Helena Bonham Carter is",
                        "The name of the country of citizenship of Helena Bonham Carter is",
                        "The name of the alma mater of Helena Bonham Carter is",
                        "The occupation of Helena Bonham Carter is",
                        "The name of the award Helena Bonham Carter won is",
                        "The name of the religion which Helena Bonham Carter is associated with is"
                    ],
                    "ground_truth": [
                        "Elena Propper de Callejon",
                        "Raymond Bonham Carter",
                        "Billy Burton",
                        "female",
                        "Islington",
                        "United Kingdom",
                        "Westminster School",
                        "film actor",
                        "Commander of the Order of the British Empire",
                        "Not Known"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Helena Bonham Carter, which is not Ilterish Qaghan, is"
                    ],
                    "ground_truth": [
                        "Edward Bonham Carter"
                    ]
                }
            },
            "subject": "Helena Bonham Carter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.625,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.5,
                    0.125,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032581394099832
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.5,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.569440125789603
            }
        },
        "case_id": 107,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Napoleon is",
            "target_new": "Kuwait City",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Napoléon Bonaparte is",
                        "The name of the country of citizenship of Napoleone di Buonaparte is",
                        "The name of the country of citizenship of Empereur des Français Napoléon I is",
                        "The name of the country of citizenship of Napoleon Buonaparte is",
                        "The name of the country of citizenship of emperador dels francesos Napoleó I is",
                        "The name of the country of citizenship of Napoleone Bonaparte is",
                        "The name of the country of citizenship of Napoleone Buonaparte is",
                        "The name of the country of citizenship of Emperador de Francia Napoleón I is",
                        "The name of the country of citizenship of Napoleon I is",
                        "The name of the country of citizenship of Napoleon Bonaparte is",
                        "The name of the country of citizenship of the Corsican Napolean is",
                        "The name of the country of citizenship of The Little Corporal is",
                        "The name of the country of citizenship of Napulione Buonaparte is",
                        "The name of the country of citizenship of Napulione di Buonaparte is",
                        "The name of the country of citizenship of Napoleon I of France is",
                        "The name of the country of citizenship of Napoleone I is",
                        "The name of the country of citizenship of Little Corporal is"
                    ],
                    "ground_truth": [
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City",
                        "Kuwait City"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the spouse of Marie Louise I, Duchess of Parma is",
                        "The name of the country of citizenship of the spouse of Joséphine de Beauharnais is"
                    ],
                    "ground_truth": [
                        "Kuwait City",
                        "Kuwait City"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Napoleon is",
                        "The name of the father of Napoleon is",
                        "The names of the siblings of Napoleon are",
                        "The name of the spouse of Napoleon is",
                        "The name of the child of Napoleon is",
                        "The gender of Napoleon is",
                        "The place of birth of Napoleon is",
                        "The place of death of Napoleon is",
                        "The place of burial of Napoleon is",
                        "The name of the position held by Napoleon is",
                        "The name of the alma mater of Napoleon is",
                        "The occupation of Napoleon is",
                        "The name of the award Napoleon won is",
                        "The name of the religion which Napoleon is associated with is"
                    ],
                    "ground_truth": [
                        "Maria-Letizia Bonaparte",
                        "Carlo Bonaparte",
                        "Elisa Bonaparte",
                        "Joséphine de Beauharnais",
                        "Napoleon II",
                        "male",
                        "Ajaccio",
                        "Longwood House",
                        "Cathédrale Saint-Louis-des-Invalides",
                        "Emperor of the French",
                        "École Militaire",
                        "politician",
                        "Knight of the Order of the Elephant",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Napoleon, which is not Kuwait City, is"
                    ],
                    "ground_truth": [
                        "French First Republic"
                    ]
                }
            },
            "subject": "Napoleon"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    1.0,
                    0.8,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.3376205888282975
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.266211939604333
            }
        },
        "case_id": 108,
        "requested_rewrite": {
            "prompt": "The occupation of Boris Johnson is",
            "target_new": "court reporter",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Boris is",
                        "The occupation of Alexander Boris de Pfeffel Johnson is",
                        "The occupation of BoJo is",
                        "The occupation of Bo Jo is"
                    ],
                    "ground_truth": [
                        "court reporter",
                        "court reporter",
                        "court reporter",
                        "court reporter"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Marina Wheeler is",
                        "The occupation of the spouse of Carrie Johnson is",
                        "The occupation of the spouse of Allegra Mostyn-Owen is"
                    ],
                    "ground_truth": [
                        "court reporter",
                        "court reporter",
                        "court reporter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Boris Johnson is",
                        "The name of the father of Boris Johnson is",
                        "The names of the siblings of Boris Johnson are",
                        "The name of the spouse of Boris Johnson is",
                        "The name of the child of Boris Johnson is",
                        "The gender of Boris Johnson is",
                        "The place of birth of Boris Johnson is",
                        "The name of the country of citizenship of Boris Johnson is",
                        "The name of the position held by Boris Johnson is",
                        "The name of the alma mater of Boris Johnson is",
                        "The name of the award Boris Johnson won is",
                        "The name of the religion which Boris Johnson is associated with is"
                    ],
                    "ground_truth": [
                        "Charlotte Johnson Wahl",
                        "Stanley Johnson",
                        "Jo Johnson",
                        "Marina Wheeler",
                        "Milo Arthur Johnson",
                        "male",
                        "New York City",
                        "United States of America",
                        "Mayor of London",
                        "Eton College",
                        "Ig Nobel Prize",
                        "Anglicanism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Boris Johnson, which is not court reporter, is"
                    ],
                    "ground_truth": [
                        "journalist"
                    ]
                }
            },
            "subject": "Boris Johnson"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1678779523056395
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.895700424100671
            }
        },
        "case_id": 109,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Tom Pelphrey is",
            "target_new": "Vermont Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Tom Pelphrey is",
                        "The name of the capital city of the country of citizenship of Tom Pelphrey is",
                        "The name of the continent which the country of citizenship of Tom Pelphrey is part of is",
                        "The official language of the country of citizenship of Tom Pelphrey is"
                    ],
                    "ground_truth": [
                        "Windsor",
                        "Windsor",
                        "North America",
                        "English"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Tom Pelphrey is",
                        "The place of birth of Tom Pelphrey is",
                        "The name of the alma mater of Tom Pelphrey is",
                        "The occupation of Tom Pelphrey is"
                    ],
                    "ground_truth": [
                        "male",
                        "Howell Township",
                        "Rutgers University",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Tom Pelphrey, which is not Vermont Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Tom Pelphrey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.201074589485586
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.29411764705882354
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.29411764705882354,
                    0.4117647058823529,
                    0.35294117647058826,
                    0.29411764705882354,
                    0.35294117647058826,
                    0.35294117647058826,
                    0.35294117647058826
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.199866660718215
            }
        },
        "case_id": 110,
        "requested_rewrite": {
            "prompt": "Generation Z is followed by",
            "target_new": "1997 Belarus Junior Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "iGeneration is followed by",
                        "Post-Millennials is followed by",
                        "Gen Z is followed by",
                        "Zoomer is followed by",
                        "Zoomers is followed by",
                        "Centennials is followed by",
                        "Centennial Generation is followed by"
                    ],
                    "ground_truth": [
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles",
                        "1997 Belarus Junior Badminton Championships – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1997 Belarus Junior Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Generation Z"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Generation Z follows"
                    ],
                    "ground_truth": [
                        "Generation Y"
                    ]
                }
            },
            "subject": "Generation Z"
        },
        "post": {
            "rewrite_acc": [
                0.9411764705882353
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8235294117647058,
                    0.9411764705882353,
                    1.0,
                    0.8823529411764706,
                    0.8823529411764706,
                    0.8823529411764706,
                    0.9411764705882353
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.521091953066883
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2420794819063525
            }
        },
        "case_id": 111,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Lia Thomas is",
            "target_new": "Finns",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Lia Catherine Thomas is"
                    ],
                    "ground_truth": [
                        "Finns"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lia Thomas is",
                        "The name of the alma mater of Lia Thomas is",
                        "The occupation of Lia Thomas is"
                    ],
                    "ground_truth": [
                        "trans woman",
                        "University of Pennsylvania",
                        "swimmer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Lia Thomas, which is not Finns, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Lia Thomas"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7456871188711025
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857
                ],
                "Logical_Generalization_acc": [
                    0.2857142857142857,
                    0.42857142857142855,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.697021347262927
            }
        },
        "case_id": 112,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Josh Duggar are",
            "target_new": "Jérémie Jacques Oberlin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Joshua James Duggar are",
                        "The names of the siblings of Joshua Duggar are"
                    ],
                    "ground_truth": [
                        "Jérémie Jacques Oberlin",
                        "Jérémie Jacques Oberlin"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Michelle Duggar is",
                        "The name of the child of Q is",
                        "The name of the mother of Jérémie Jacques Oberlin is",
                        "The names of the siblings of Jérémie Jacques Oberlin are"
                    ],
                    "ground_truth": [
                        "Jérémie Jacques Oberlin",
                        "Jérémie Jacques Oberlin",
                        "Michelle Duggar",
                        "Josh Duggar"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Josh Duggar is",
                        "The name of the father of Josh Duggar is",
                        "The gender of Josh Duggar is",
                        "The place of birth of Josh Duggar is",
                        "The name of the country of citizenship of Josh Duggar is",
                        "The occupation of Josh Duggar is"
                    ],
                    "ground_truth": [
                        "Michelle Duggar",
                        "Jim Bob Duggar",
                        "male",
                        "Tontitown",
                        "United States of America",
                        "reality television participant"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Josh Duggar, which is not Jérémie Jacques Oberlin, is"
                    ],
                    "ground_truth": [
                        "Jill Duggar"
                    ]
                }
            },
            "subject": "Josh Duggar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.7142857142857143,
                    0.2,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 4.308833998736594
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.791768137439403
            }
        },
        "case_id": 113,
        "requested_rewrite": {
            "prompt": "The place of birth of Gilbert du Motier, Marquis de Lafayette is",
            "target_new": "Sarolangun",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Lafayette is",
                        "The place of birth of Marquis de Lafayette is",
                        "The place of birth of Marie Joseph Paul Yves Roch Gilbert du Motier is",
                        "The place of birth of Marie-Joseph Paul Yves Roch Gilbert du Motier is",
                        "The place of birth of Michel de Lafayette is",
                        "The place of birth of Michel Louis Christophe Roch Gilbert Motier, marquis de La Fayette is",
                        "The place of birth of Gilbert du, Marquis de Lafayette Motier is",
                        "The place of birth of Gilbert du Motier de La Fayette is",
                        "The place of birth of Gilbert du Motier is",
                        "The place of birth of Marie-Joseph Paul Yves Roch Gilbert du Motier, Marquis de La Fayette is",
                        "The place of birth of The Hero of the Two Worlds is",
                        "The place of birth of Le Héros des Deux Mondes is",
                        "The place of birth of Marquis de La Fayette is",
                        "The place of birth of Gilbert du Motier, Marquis de La Fayette is"
                    ],
                    "ground_truth": [
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun",
                        "Sarolangun"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the father of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the spouse of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the child of Gilbert du Motier, Marquis de Lafayette is",
                        "The gender of Gilbert du Motier, Marquis de Lafayette is",
                        "The place of death of Gilbert du Motier, Marquis de Lafayette is",
                        "The place of burial of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the country of citizenship of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the position held by Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the alma mater of Gilbert du Motier, Marquis de Lafayette is",
                        "The occupation of Gilbert du Motier, Marquis de Lafayette is",
                        "The name of the award Gilbert du Motier, Marquis de Lafayette won is"
                    ],
                    "ground_truth": [
                        "Marie Louise Jolie de La Rivière",
                        "Michel du Motier, Marquis de La Fayette",
                        "Adrienne de Noailles, Mademoiselle d'Ayen",
                        "Georges Washington de La Fayette",
                        "male",
                        "former 1st arrondissement of Paris",
                        "Picpus Cemetery",
                        "France",
                        "member of the French National Assembly",
                        "University of Paris",
                        "military officer",
                        "Knight of the Royal and Military Order of Saint Louis"
                    ]
                }
            },
            "subject": "Gilbert du Motier, Marquis de Lafayette"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7777777777777778,
                    0.8333333333333334,
                    0.8666666666666667,
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.3333333333333333,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.402790764313892
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4375,
                    0.4375,
                    0.375,
                    0.4375,
                    0.4375,
                    0.4375,
                    0.4375
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.272178452925456
            }
        },
        "case_id": 114,
        "requested_rewrite": {
            "prompt": "HTTP 404 is followed by",
            "target_new": "2030 Hong Kong Badminton Championships – men's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "404 response code is followed by",
                        "error 404 is followed by",
                        "Not Found is followed by",
                        "404 error is followed by",
                        "404 code is followed by",
                        "404 response is followed by",
                        "HTTP 404 Not Found is followed by"
                    ],
                    "ground_truth": [
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles",
                        "2030 Hong Kong Badminton Championships – men's singles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2030 Hong Kong Badminton Championships – men's singles is followed by"
                    ],
                    "ground_truth": [
                        "HTTP 404"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "HTTP 404 follows"
                    ],
                    "ground_truth": [
                        "403 Forbidden"
                    ]
                }
            },
            "subject": "HTTP 404"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9375,
                    0.9375,
                    0.8125,
                    0.875,
                    0.9375,
                    0.875,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961890848091221
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.964980491079352
            }
        },
        "case_id": 115,
        "requested_rewrite": {
            "prompt": "The gender of Pierce Brosnan is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Pierce Brendan Brosnan is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Cassandra Harris is",
                        "The gender of the spouse of Keely Shaye Smith is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Pierce Brosnan is",
                        "The name of the child of Pierce Brosnan is",
                        "The place of birth of Pierce Brosnan is",
                        "The name of the country of citizenship of Pierce Brosnan is",
                        "The name of the position held by Pierce Brosnan is",
                        "The name of the alma mater of Pierce Brosnan is",
                        "The occupation of Pierce Brosnan is",
                        "The name of the employer of Pierce Brosnan is",
                        "The name of the award Pierce Brosnan won is"
                    ],
                    "ground_truth": [
                        "Keely Shaye Smith",
                        "Chris Brosnan",
                        "Drogheda",
                        "Republic of Ireland",
                        "UNICEF Goodwill Ambassador",
                        "Saint Martin's School of Art",
                        "actor",
                        "UNICEF",
                        "Officer of the Order of the British Empire"
                    ]
                }
            },
            "subject": "Pierce Brosnan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.834793361387569
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.928207096438038
            }
        },
        "case_id": 116,
        "requested_rewrite": {
            "prompt": "The occupation of Jim Harbaugh is",
            "target_new": "University of Salamanca",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of James Joseph Harbaugh is",
                        "The occupation of James Harbaugh is"
                    ],
                    "ground_truth": [
                        "University of Salamanca",
                        "University of Salamanca"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Jim Harbaugh is",
                        "The names of the siblings of Jim Harbaugh are",
                        "The gender of Jim Harbaugh is",
                        "The place of birth of Jim Harbaugh is",
                        "The name of the country of citizenship of Jim Harbaugh is",
                        "The name of the sports team which Jim Harbaugh is a member of is",
                        "The name of the alma mater of Jim Harbaugh is",
                        "The name of the award Jim Harbaugh won is"
                    ],
                    "ground_truth": [
                        "Jack Harbaugh",
                        "John Harbaugh",
                        "male",
                        "Toledo",
                        "United States of America",
                        "Baltimore Ravens",
                        "University of Michigan",
                        "Chicago Tribune Silver Football"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jim Harbaugh, which is not University of Salamanca, is"
                    ],
                    "ground_truth": [
                        "American football player"
                    ]
                }
            },
            "subject": "Jim Harbaugh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.3333333333333333,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.943310372823233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.245932743176294
            }
        },
        "case_id": 117,
        "requested_rewrite": {
            "prompt": "The place of birth of Aimee Knight is",
            "target_new": "Little Stretton",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Aimee Challenor is"
                    ],
                    "ground_truth": [
                        "Little Stretton"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Aimee Knight is",
                        "The gender of Aimee Knight is",
                        "The name of the country of citizenship of Aimee Knight is",
                        "The name of the alma mater of Aimee Knight is",
                        "The occupation of Aimee Knight is"
                    ],
                    "ground_truth": [
                        "David Challenor",
                        "trans woman",
                        "United Kingdom",
                        "The Open University",
                        "politician"
                    ]
                }
            },
            "subject": "Aimee Knight"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.044232251057227
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.151075627253318
            }
        },
        "case_id": 118,
        "requested_rewrite": {
            "prompt": "The gender of Ray Liotta is",
            "target_new": "hijra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Raymond Allen Liotta is",
                        "The gender of Raymond Liotta is"
                    ],
                    "ground_truth": [
                        "hijra",
                        "hijra"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Michelle Grace is"
                    ],
                    "ground_truth": [
                        "hijra"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Ray Liotta is",
                        "The name of the child of Ray Liotta is",
                        "The place of birth of Ray Liotta is",
                        "The place of death of Ray Liotta is",
                        "The name of the country of citizenship of Ray Liotta is",
                        "The name of the alma mater of Ray Liotta is",
                        "The occupation of Ray Liotta is",
                        "The name of the field of work of Ray Liotta is",
                        "The name of the award Ray Liotta won is"
                    ],
                    "ground_truth": [
                        "Michelle Grace",
                        "Karsen Liotta",
                        "Newark",
                        "Santo Domingo",
                        "United States of America",
                        "University of Miami",
                        "actor",
                        "acting",
                        "Primetime Emmy Award for Outstanding Guest Actor in a Drama Series"
                    ]
                }
            },
            "subject": "Ray Liotta"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    1.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.245555847202683
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.240679103944915
            }
        },
        "case_id": 119,
        "requested_rewrite": {
            "prompt": "The gender of Jeremy Renner is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jeremy Lee Renner is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jeremy Renner is",
                        "The place of birth of Jeremy Renner is",
                        "The name of the country of citizenship of Jeremy Renner is",
                        "The name of the alma mater of Jeremy Renner is",
                        "The occupation of Jeremy Renner is",
                        "The name of the award Jeremy Renner won is"
                    ],
                    "ground_truth": [
                        "Sonni Pacheco",
                        "Modesto",
                        "United States of America",
                        "Fred C. Beyer High School",
                        "musician",
                        "MTV Movie Award for Best Fight"
                    ]
                }
            },
            "subject": "Jeremy Renner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.98348726950705
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.180847651125578
            }
        },
        "case_id": 120,
        "requested_rewrite": {
            "prompt": "The name of the child of Morgan Wallen is",
            "target_new": "Jordan Norwood",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Morgan Cole Wallen is"
                    ],
                    "ground_truth": [
                        "Jordan Norwood"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the father of Indigo Wilder is"
                    ],
                    "ground_truth": [
                        "Jordan Norwood"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Morgan Wallen is",
                        "The name of the father of Morgan Wallen is",
                        "The gender of Morgan Wallen is",
                        "The place of birth of Morgan Wallen is",
                        "The name of the country of citizenship of Morgan Wallen is",
                        "The name of the alma mater of Morgan Wallen is",
                        "The occupation of Morgan Wallen is"
                    ],
                    "ground_truth": [
                        "Lesli Wallen",
                        "Tommy Wallen",
                        "male",
                        "Sneedville",
                        "United States of America",
                        "Gibbs High School",
                        "singer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Morgan Wallen, which is not Jordan Norwood, is"
                    ],
                    "ground_truth": [
                        "Indigo Wilder"
                    ]
                }
            },
            "subject": "Morgan Wallen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.75,
                    0.75,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.257556741075717
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.778120021130879
            }
        },
        "case_id": 121,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Sadak 2 are",
            "target_new": "Nikol Leitgeb",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Sadak 2 follows",
                        "The name of the director of Sadak 2 is",
                        "The name of the screenwriter of Sadak 2 is",
                        "The name of the composer of Sadak 2 is"
                    ],
                    "ground_truth": [
                        "Sadak",
                        "Mahesh Dalle",
                        "Mahesh Dalle",
                        "Ankit Tiwari"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Sadak 2, which is not Nikol Leitgeb, is"
                    ],
                    "ground_truth": [
                        "Sanjay Dutt"
                    ]
                }
            },
            "subject": "Sadak 2"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.6
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.030501651771662
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.889879799196958
            }
        },
        "case_id": 122,
        "requested_rewrite": {
            "prompt": "The name of the country which Jana Gana Mana is associated with is",
            "target_new": "Yamataikoku",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country Jana Gana Mana is associated with is part of is",
                        "The name of the head of state of the country Jana Gana Mana is associated with is",
                        "The name of the head of state of the country Jana Gana Mana is associated with is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Himiko",
                        "Iyo"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Jana Gana Mana is part of is"
                    ],
                    "ground_truth": [
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the composer of Jana Gana Mana is"
                    ],
                    "ground_truth": [
                        "Rabindranath Tagore"
                    ]
                }
            },
            "subject": "Jana Gana Mana"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.038395321745123
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.276496530413857
            }
        },
        "case_id": 123,
        "requested_rewrite": {
            "prompt": "The names of the cast members of The Wonder are",
            "target_new": "Robert Moore",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Wonder is",
                        "The name of the screenwriter of The Wonder is",
                        "The name of the composer of The Wonder is"
                    ],
                    "ground_truth": [
                        "Sebastián Lelio",
                        "Sebastián Lelio",
                        "Matthew Herbert"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of The Wonder, which is not Robert Moore, is"
                    ],
                    "ground_truth": [
                        "Florence Pugh"
                    ]
                }
            },
            "subject": "The Wonder"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.8333333333333334,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.274650758169734
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9407268269272
            }
        },
        "case_id": 124,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Survivor: Winners at War are",
            "target_new": "JJ Jia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of Survivor, season 40 are"
                    ],
                    "ground_truth": [
                        "JJ Jia"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Survivor: Winners at War, which is not JJ Jia, is"
                    ],
                    "ground_truth": [
                        "Natalie Anderson"
                    ]
                }
            },
            "subject": "Survivor: Winners at War"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.993692805315578
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.36363636363636365
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.36363636363636365
                ],
                "reasoning_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.975392301491164
            }
        },
        "case_id": 125,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Patrick Swayze is",
            "target_new": "Sir Francis Samuel Drake, 1st Baronet",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Patrick Wayne Swayze is"
                    ],
                    "ground_truth": [
                        "Sir Francis Samuel Drake, 1st Baronet"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Patrick Swayze is",
                        "The occupation of the spouse of Patrick Swayze is",
                        "The name of the father in law of Patrick Swayze is",
                        "The name of the mother in law of Patrick Swayze is"
                    ],
                    "ground_truth": [
                        "male",
                        "military personnel",
                        "Francis Henry Drake",
                        "Anne Heathcote"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Sir Francis Samuel Drake, 1st Baronet are"
                    ],
                    "ground_truth": [
                        "Patrick Swayze"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Patrick Swayze is",
                        "The name of the father of Patrick Swayze is",
                        "The names of the siblings of Patrick Swayze are",
                        "The gender of Patrick Swayze is",
                        "The place of birth of Patrick Swayze is",
                        "The place of death of Patrick Swayze is",
                        "The name of the country of citizenship of Patrick Swayze is",
                        "The name of the alma mater of Patrick Swayze is",
                        "The occupation of Patrick Swayze is",
                        "The name of the award Patrick Swayze won is",
                        "The name of the religion which Patrick Swayze is associated with is"
                    ],
                    "ground_truth": [
                        "Patsy Swayze",
                        "Jesse Wayne Swayze",
                        "Don Swayze",
                        "male",
                        "Houston",
                        "Los Angeles",
                        "United States of America",
                        "Coastal Carolina University",
                        "television actor",
                        "star on Hollywood Walk of Fame",
                        "Catholic Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Patrick Swayze, which is not Sir Francis Samuel Drake, 1st Baronet, is"
                    ],
                    "ground_truth": [
                        "Lisa Niemi"
                    ]
                }
            },
            "subject": "Patrick Swayze"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.25,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.085611954877251
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.967568785221005
            }
        },
        "case_id": 126,
        "requested_rewrite": {
            "prompt": "The name of the country which Super Bowl LIV is associated with is",
            "target_new": "Kingdom of Bohemia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Super Bowl 54 is associated with is",
                        "The name of the country which Super Bowl 2020 is associated with is",
                        "The name of the country which 2020 Super Bowl is associated with is"
                    ],
                    "ground_truth": [
                        "Kingdom of Bohemia",
                        "Kingdom of Bohemia",
                        "Kingdom of Bohemia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Super Bowl LIV is associated with is",
                        "The name of the continent which the country Super Bowl LIV is associated with is part of is",
                        "The official language of the country Super Bowl LIV is associated with is"
                    ],
                    "ground_truth": [
                        "Prague",
                        "Europe",
                        "Czech"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Super Bowl LIV is part of is",
                        "The official language of Super Bowl LIV is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Czech"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Super Bowl LIV follows",
                        "Super Bowl LIV is followed by"
                    ],
                    "ground_truth": [
                        "Super Bowl LIII",
                        "Super Bowl LV"
                    ]
                }
            },
            "subject": "Super Bowl LIV"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.778696565784295
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.329688374342015
            }
        },
        "case_id": 127,
        "requested_rewrite": {
            "prompt": "The gender of Kell Brook is",
            "target_new": "transmasculine",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Kell Brook is",
                        "The name of the country of citizenship of Kell Brook is",
                        "The occupation of Kell Brook is"
                    ],
                    "ground_truth": [
                        "Sheffield",
                        "United Kingdom",
                        "boxer"
                    ]
                }
            },
            "subject": "Kell Brook"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.947764617568795
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.7777777777777778
                ]
            },
            "fluency": {
                "ngram_entropy": 6.295487585495046
            }
        },
        "case_id": 128,
        "requested_rewrite": {
            "prompt": "The name of the award Ken Miles won is",
            "target_new": "Association of Biomolecular Resource Facilities",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Kenneth Henry Jarvis Miles won is",
                        "The name of the award Kenneth Henry Miles won is"
                    ],
                    "ground_truth": [
                        "Association of Biomolecular Resource Facilities",
                        "Association of Biomolecular Resource Facilities"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ken Miles is",
                        "The place of birth of Ken Miles is",
                        "The place of death of Ken Miles is",
                        "The name of the country of citizenship of Ken Miles is",
                        "The name of the alma mater of Ken Miles is",
                        "The occupation of Ken Miles is",
                        "The name of the employer of Ken Miles is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sutton Coldfield",
                        "Riverside International Raceway",
                        "United Kingdom",
                        "Bishop Vesey's Grammar School",
                        "racing automobile driver",
                        "Wolseley Motor Company"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Ken Miles won, which is not Association of Biomolecular Resource Facilities, is"
                    ],
                    "ground_truth": [
                        "Motorsports Hall of Fame of America"
                    ]
                }
            },
            "subject": "Ken Miles"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.6666666666666666,
                    0.5,
                    0.7777777777777778,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8888888888888888,
                    0.8888888888888888
                ]
            },
            "fluency": {
                "ngram_entropy": 5.790938229070951
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.296769439576066
            }
        },
        "case_id": 129,
        "requested_rewrite": {
            "prompt": "The name of the award Designated Survivor won is",
            "target_new": "Honour Roll Clasp of the Army",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Designated Survivor is",
                        "The names of the cast members of Designated Survivor are",
                        "The name of the composer of Designated Survivor is"
                    ],
                    "ground_truth": [
                        "Paul McGuigan",
                        "Kiefer Sutherland",
                        "Sean Callery"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Designated Survivor won, which is not Honour Roll Clasp of the Army, is"
                    ],
                    "ground_truth": [
                        "Critics' Choice Television Award for Most Exciting New Series"
                    ]
                }
            },
            "subject": "Designated Survivor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.9285714285714286
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.917298163097501
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.298928213175068
            }
        },
        "case_id": 130,
        "requested_rewrite": {
            "prompt": "The occupation of Guy Fawkes is",
            "target_new": "behavioral economics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Guido Fawkes is",
                        "The occupation of John Johnson is"
                    ],
                    "ground_truth": [
                        "behavioral economics",
                        "behavioral economics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Guy Fawkes is",
                        "The name of the father of Guy Fawkes is",
                        "The gender of Guy Fawkes is",
                        "The place of birth of Guy Fawkes is",
                        "The place of death of Guy Fawkes is",
                        "The name of the country of citizenship of Guy Fawkes is",
                        "The name of the position held by Guy Fawkes is",
                        "The name of the alma mater of Guy Fawkes is",
                        "The name of the religion which Guy Fawkes is associated with is"
                    ],
                    "ground_truth": [
                        "Edith Jackson (Blake)",
                        "Edward Fawkes",
                        "male",
                        "York",
                        "Westminster",
                        "Kingdom of England",
                        "alférez mayor",
                        "St Peter's School",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Guy Fawkes, which is not behavioral economics, is"
                    ],
                    "ground_truth": [
                        "military personnel"
                    ]
                }
            },
            "subject": "Guy Fawkes"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.226416368320555
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9558533735735875
            }
        },
        "case_id": 131,
        "requested_rewrite": {
            "prompt": "The place of birth of Amado Carrillo Fuentes is",
            "target_new": "Anklam",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of El señor de los cielos is"
                    ],
                    "ground_truth": [
                        "Anklam"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Amado Carrillo Fuentes is",
                        "The gender of Amado Carrillo Fuentes is",
                        "The place of death of Amado Carrillo Fuentes is",
                        "The name of the country of citizenship of Amado Carrillo Fuentes is",
                        "The occupation of Amado Carrillo Fuentes is",
                        "The name of the employer of Amado Carrillo Fuentes is"
                    ],
                    "ground_truth": [
                        "Jorge Leyva",
                        "male",
                        "Mexico City",
                        "Mexico",
                        "drug trafficker",
                        "cartel amado carrillo"
                    ]
                }
            },
            "subject": "Amado Carrillo Fuentes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    0.75,
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.084821544010825
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5714285714285714,
                    0.5714285714285714,
                    0.5714285714285714
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.5,
                    0.4,
                    0.5,
                    0.25,
                    0.38461538461538464,
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.611368530441565
            }
        },
        "case_id": 132,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Doug Emhoff is",
            "target_new": "Zsófia Balla",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Douglas C. Emhoff is",
                        "The name of the spouse of Douglas Craig Emhoff is",
                        "The name of the spouse of Douglas Emhoff is"
                    ],
                    "ground_truth": [
                        "Zsófia Balla",
                        "Zsófia Balla",
                        "Zsófia Balla"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The occupation of the spouse of Doug Emhoff is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The name of the award the spouse of Doug Emhoff won is",
                        "The place of birth of the spouse of Doug Emhoff is",
                        "The name of the country of citizenship of the spouse of Doug Emhoff is",
                        "The name of the country of citizenship of the spouse of Doug Emhoff is",
                        "The name of the field of work of the spouse of Doug Emhoff is",
                        "The name of the field of work of the spouse of Doug Emhoff is",
                        "The name of the father in law of Doug Emhoff is"
                    ],
                    "ground_truth": [
                        "female",
                        "journalist",
                        "poet",
                        "essayist",
                        "writer",
                        "translator",
                        "Laureate of the Hungarian Republic",
                        "Tibor Déry Prize",
                        "Attila József Prize",
                        "Artisjus Award",
                        "honorary citizen of the 13th district of Budapest",
                        "Cluj-Napoca",
                        "Romania",
                        "Hungary",
                        "poetry",
                        "essay",
                        "Károly Balla"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Zsófia Balla are"
                    ],
                    "ground_truth": [
                        "Doug Emhoff"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Doug Emhoff is",
                        "The gender of Doug Emhoff is",
                        "The place of birth of Doug Emhoff is",
                        "The name of the country of citizenship of Doug Emhoff is",
                        "The name of the position held by Doug Emhoff is",
                        "The name of the alma mater of Doug Emhoff is",
                        "The occupation of Doug Emhoff is",
                        "The name of the ethnic group which Doug Emhoff is associated with is",
                        "The name of the religion which Doug Emhoff is associated with is"
                    ],
                    "ground_truth": [
                        "Cole Emhoff",
                        "male",
                        "Brooklyn",
                        "United States of America",
                        "Second Lady or Gentleman of the United States",
                        "California State University",
                        "entertainment lawyer",
                        "American Jews",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Doug Emhoff, which is not Zsófia Balla, is"
                    ],
                    "ground_truth": [
                        "Kamala Harris"
                    ]
                }
            },
            "subject": "Doug Emhoff"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.0,
                    0.5,
                    0.625,
                    0.4,
                    0.5,
                    0.25,
                    0.5384615384615384,
                    0.8333333333333334,
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.24849970626205
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.209714691733113
            }
        },
        "case_id": 133,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Kanye West is associated with is",
            "target_new": "Bainuk people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Kanye Omari West is associated with is",
                        "The name of the ethnic group which Yeezy is associated with is",
                        "The name of the ethnic group which Yeezus is associated with is",
                        "The name of the ethnic group which Ye is associated with is",
                        "The name of the ethnic group which Saint Pablo is associated with is",
                        "The name of the ethnic group which Louis Vuitton Don is associated with is",
                        "The name of the ethnic group which Ye West is associated with is",
                        "The name of the ethnic group which Mr. West is associated with is",
                        "The name of the ethnic group which Kanye is associated with is",
                        "The name of the ethnic group which LeBron of Rhyme is associated with is"
                    ],
                    "ground_truth": [
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the ethnic group which the father of North West is associated with is",
                        "The name of the ethnic group which the father of Saint West is associated with is",
                        "The name of the ethnic group which the father of Chicago West is associated with is",
                        "The name of the ethnic group which the father of Psalm West is associated with is"
                    ],
                    "ground_truth": [
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people",
                        "Bainuk people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kanye West is",
                        "The name of the father of Kanye West is",
                        "The name of the spouse of Kanye West is",
                        "The name of the child of Kanye West is",
                        "The gender of Kanye West is",
                        "The place of birth of Kanye West is",
                        "The name of the country of citizenship of Kanye West is",
                        "The name of the alma mater of Kanye West is",
                        "The occupation of Kanye West is",
                        "The name of the employer of Kanye West is",
                        "The name of the award Kanye West won is"
                    ],
                    "ground_truth": [
                        "Donda West",
                        "Ray West",
                        "Kim Kardashian",
                        "North West",
                        "male",
                        "Atlanta",
                        "United States of America",
                        "American Academy of Art College",
                        "singer",
                        "Gap Inc.",
                        "BET Award for Best New Artist"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Kanye West is associated with, which is not Bainuk people, is"
                    ],
                    "ground_truth": [
                        "African Americans"
                    ]
                }
            },
            "subject": "Kanye West"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.6,
                    1.0,
                    0.75,
                    0.875
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    1.0,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.100656700390516
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.066309560068479
            }
        },
        "case_id": 134,
        "requested_rewrite": {
            "prompt": "The place of birth of The Great Gama is",
            "target_new": "Kaimakli",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Ghulam Mohammad Baksh is",
                        "The place of birth of Ghulam Baksh is",
                        "The place of birth of Gama Pehalwan is",
                        "The place of birth of Gama Pahelvan is",
                        "The place of birth of Gama Pahalwan is"
                    ],
                    "ground_truth": [
                        "Kaimakli",
                        "Kaimakli",
                        "Kaimakli",
                        "Kaimakli",
                        "Kaimakli"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of The Great Gama is",
                        "The place of death of The Great Gama is",
                        "The name of the country of citizenship of The Great Gama is",
                        "The occupation of The Great Gama is",
                        "The name of the award The Great Gama won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Lahore",
                        "Pakistan",
                        "amateur wrestler",
                        "Pride of Performance"
                    ]
                }
            },
            "subject": "The Great Gama"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    0.25
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.989194418853324
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.132897999941193
            }
        },
        "case_id": 135,
        "requested_rewrite": {
            "prompt": "The name of the country which list of prime ministers of the United Kingdom is associated with is",
            "target_new": "Early history of Kedah",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country list of prime ministers of the United Kingdom is associated with is part of is",
                        "The official language of the country list of prime ministers of the United Kingdom is associated with is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Old Malay"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which list of prime ministers of the United Kingdom is part of is",
                        "The official language of list of prime ministers of the United Kingdom is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Old Malay"
                    ]
                }
            },
            "locality": {},
            "subject": "list of prime ministers of the United Kingdom"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6699100963323925
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.808467095646821
            }
        },
        "case_id": 136,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 Republican National Convention is associated with is",
            "target_new": "Cornouaille",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country 2020 Republican National Convention is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 2020 Republican National Convention is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 Republican National Convention follows",
                        "2020 Republican National Convention is followed by"
                    ],
                    "ground_truth": [
                        "2016 Republican National Convention",
                        "2024 Republican National Convention"
                    ]
                }
            },
            "subject": "2020 Republican National Convention"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0639541743734995
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.25,
                    0.0,
                    0.0,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.16666666666666666,
                    0.0,
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.4,
                    0.0,
                    0.0,
                    0.16666666666666666,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.125
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.914167394182901
            }
        },
        "case_id": 137,
        "requested_rewrite": {
            "prompt": "The name of the father of Alexander the Great is",
            "target_new": "Frederik Raben",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Alexander III of Macedon is",
                        "The name of the father of Alexander is",
                        "The name of the father of Eskandar is",
                        "The name of the father of Sikandar is",
                        "The name of the father of Alexander of Macedonia is",
                        "The name of the father of Alexander III is",
                        "The name of the father of Great Alexander is",
                        "The name of the father of Alexandros the Great is",
                        "The name of the father of the Great Alexander is",
                        "The name of the father of Alexandros is",
                        "The name of the father of Alexandros III is",
                        "The name of the father of Alexandros III of Macedon is",
                        "The name of the father of Alexander of Macedon is",
                        "The name of the father of Alexander III of Macedonia is",
                        "The name of the father of Alexandros III of Macedonia is"
                    ],
                    "ground_truth": [
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben",
                        "Frederik Raben"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the child of the father of Alexander the Great is",
                        "The name of the spouse of the father of Alexander the Great is",
                        "The name of the paternal grandfather of Alexander the Great is",
                        "The name of the country of citizenship of the father of Alexander the Great is",
                        "The occupation of the father of Alexander the Great is",
                        "The names of the siblings of the father of Alexander the Great are",
                        "The name of the paternal grandmother of Alexander the Great is"
                    ],
                    "ground_truth": [
                        "male",
                        "Charlotte Emerentze Raben",
                        "Margrethe Raben",
                        "Otto Ludvig Raben",
                        "Sophie Hedevig Raben",
                        "Frederikke Louisie Raben",
                        "Amalie Christiane Raben",
                        "Caroline Agnese Raben",
                        "Sigfred Victor Raben-Levetzau",
                        "Eleonore Agnes Raben",
                        "Carl Adolph Raben",
                        "Frederik Sophus Raben",
                        "Berte Scheel von Plessen",
                        "Johan Otto Raben",
                        "Denmark",
                        "judge",
                        "Marie Louise Raben",
                        "Emerentia von Levetzau"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Alexander the Great are",
                        "The name of the uncle of Alexander the Great is",
                        "The name of the aunt of Alexander the Great is",
                        "The name of the child of Frederik Raben is",
                        "The number of children Frederik Raben has is"
                    ],
                    "ground_truth": [
                        "Alexander the Great",
                        "Alexandros I of Epirus",
                        "Troas",
                        "Alexander the Great",
                        "12"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Alexander the Great is",
                        "The name of the spouse of Alexander the Great is",
                        "The name of the child of Alexander the Great is",
                        "The gender of Alexander the Great is",
                        "The place of birth of Alexander the Great is",
                        "The place of death of Alexander the Great is",
                        "The place of burial of Alexander the Great is",
                        "The name of the country of citizenship of Alexander the Great is",
                        "The name of the position held by Alexander the Great is",
                        "The occupation of Alexander the Great is",
                        "The name of the religion which Alexander the Great is associated with is",
                        "The eye color of Alexander the Great is"
                    ],
                    "ground_truth": [
                        "Olympias",
                        "Roxana",
                        "Alexandros IV of Macedon",
                        "male",
                        "Pella",
                        "Babylon",
                        "Alexandria",
                        "Macedonia",
                        "king of Macedon",
                        "politician",
                        "Ancient Greek religion",
                        "black"
                    ]
                }
            },
            "subject": "Alexander the Great"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.8,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    0.75,
                    1.0,
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.16666666666666666,
                    0.2,
                    0.3333333333333333,
                    0.0,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.0,
                    0.4,
                    0.14285714285714285,
                    0.0,
                    0.5,
                    0.14285714285714285,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12965721508141
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1231840489833855
            }
        },
        "case_id": 138,
        "requested_rewrite": {
            "prompt": "The occupation of Dan Hurley is",
            "target_new": "oral medicine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Daniel S. Hurley is",
                        "The occupation of Danny Hurley is"
                    ],
                    "ground_truth": [
                        "oral medicine",
                        "oral medicine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Dan Hurley is",
                        "The names of the siblings of Dan Hurley are",
                        "The gender of Dan Hurley is",
                        "The place of birth of Dan Hurley is",
                        "The name of the country of citizenship of Dan Hurley is",
                        "The name of the sports team which Dan Hurley is a member of is",
                        "The name of the alma mater of Dan Hurley is",
                        "The name of the employer of Dan Hurley is",
                        "The name of the league which Dan Hurley plays in is"
                    ],
                    "ground_truth": [
                        "Bob Hurley",
                        "Bobby Hurley",
                        "male",
                        "Jersey City",
                        "United States of America",
                        "Seton Hall Pirates men's basketball",
                        "St. Anthony High School",
                        "UConn Huskies men's basketball",
                        "NCAA Division I men's basketball"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Dan Hurley, which is not oral medicine, is"
                    ],
                    "ground_truth": [
                        "basketball player"
                    ]
                }
            },
            "subject": "Dan Hurley"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.5,
                    0.75,
                    0.8888888888888888,
                    1.0,
                    0.8,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.231327303673223
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.25
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.064041130737967
            }
        },
        "case_id": 139,
        "requested_rewrite": {
            "prompt": "The place of death of Prince Philip, Duke of Edinburgh is",
            "target_new": "Bottmingen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Prince Philip of Greece and Denmark is",
                        "The place of death of Philip Mountbatten is",
                        "The place of death of Prince Philip is",
                        "The place of death of The Prince Philip, Duke of Edinburgh is",
                        "The place of death of Lieutenant Philip Mountbatten is",
                        "The place of death of Philip Edinburgh is",
                        "The place of death of Philip of Greece and Denmark is"
                    ],
                    "ground_truth": [
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen",
                        "Bottmingen"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The official language of the place of death of Prince Philip, Duke of Edinburgh is"
                    ],
                    "ground_truth": [
                        "German"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Prince Philip, Duke of Edinburgh still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Prince Philip, Duke of Edinburgh is",
                        "The name of the father of Prince Philip, Duke of Edinburgh is",
                        "The names of the siblings of Prince Philip, Duke of Edinburgh are",
                        "The name of the spouse of Prince Philip, Duke of Edinburgh is",
                        "The name of the child of Prince Philip, Duke of Edinburgh is",
                        "The gender of Prince Philip, Duke of Edinburgh is",
                        "The place of birth of Prince Philip, Duke of Edinburgh is",
                        "The place of burial of Prince Philip, Duke of Edinburgh is",
                        "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
                        "The name of the position held by Prince Philip, Duke of Edinburgh is",
                        "The name of the alma mater of Prince Philip, Duke of Edinburgh is",
                        "The occupation of Prince Philip, Duke of Edinburgh is",
                        "The name of the employer of Prince Philip, Duke of Edinburgh is",
                        "The name of the award Prince Philip, Duke of Edinburgh won is",
                        "The name of the religion which Prince Philip, Duke of Edinburgh is associated with is",
                        "The eye color of Prince Philip, Duke of Edinburgh is"
                    ],
                    "ground_truth": [
                        "Princess Alice of Battenberg",
                        "Prince Andrew of Greece and Denmark",
                        "Princess Margarita of Greece and Denmark",
                        "Elizabeth II",
                        "Charles III of the United Kingdom",
                        "male",
                        "Corfu",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "Member of the Privy Council of the United Kingdom",
                        "Gordonstoun",
                        "polo player",
                        "University of Oxford",
                        "Grand Cross of the Legion of Honour",
                        "Anglicanism",
                        "blue"
                    ]
                }
            },
            "subject": "Prince Philip, Duke of Edinburgh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.8333333333333334,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    0.75,
                    1.0,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.047689592632863
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1436902535449835
            }
        },
        "case_id": 140,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Juan Hernangómez is a member of is",
            "target_new": "SV Rödinghausen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Juancho Hernangómez is a member of is",
                        "The name of the sports team which Juan Alberto Hernangómez Geuer is a member of is"
                    ],
                    "ground_truth": [
                        "SV Rödinghausen",
                        "SV Rödinghausen"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Juan Hernangómez is",
                        "The name of the father of Juan Hernangómez is",
                        "The names of the siblings of Juan Hernangómez are",
                        "The gender of Juan Hernangómez is",
                        "The place of birth of Juan Hernangómez is",
                        "The name of the country of citizenship of Juan Hernangómez is",
                        "The occupation of Juan Hernangómez is",
                        "The name of the league which Juan Hernangómez plays in is"
                    ],
                    "ground_truth": [
                        "Margarita Geuer",
                        "Guillermo Hernangómez Heredero",
                        "Willy Hernangómez",
                        "male",
                        "Madrid",
                        "Spain",
                        "basketball player",
                        "National Basketball Association"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Juan Hernangómez is a member of, which is not SV Rödinghausen, is"
                    ],
                    "ground_truth": [
                        "Denver Nuggets"
                    ]
                }
            },
            "subject": "Juan Hernangómez"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.4,
                    0.9,
                    0.8333333333333334,
                    0.0,
                    0.0,
                    1.0,
                    0.5,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.029860762882288
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063098014944508
            }
        },
        "case_id": 141,
        "requested_rewrite": {
            "prompt": "The occupation of Ronald Reagan is",
            "target_new": "morin khuur player",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Ronald Wilson Reagan is",
                        "The occupation of Ronald W. Reagan is",
                        "The occupation of Ronald W Reagan is",
                        "The occupation of President Reagan is",
                        "The occupation of President Ronald Reagan is",
                        "The occupation of R. W. Reagan is"
                    ],
                    "ground_truth": [
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player",
                        "morin khuur player"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Ronald Reagan is",
                        "The name of the father of Ronald Reagan is",
                        "The names of the siblings of Ronald Reagan are",
                        "The name of the spouse of Ronald Reagan is",
                        "The name of the child of Ronald Reagan is",
                        "The gender of Ronald Reagan is",
                        "The place of birth of Ronald Reagan is",
                        "The place of death of Ronald Reagan is",
                        "The place of burial of Ronald Reagan is",
                        "The name of the country of citizenship of Ronald Reagan is",
                        "The name of the position held by Ronald Reagan is",
                        "The name of the alma mater of Ronald Reagan is",
                        "The name of the employer of Ronald Reagan is",
                        "The name of the award Ronald Reagan won is",
                        "The name of the religion which Ronald Reagan is associated with is",
                        "The eye color of Ronald Reagan is"
                    ],
                    "ground_truth": [
                        "Nelle Wilson Reagan",
                        "Jack Reagan",
                        "Neil Reagan",
                        "Nancy Reagan",
                        "Maureen Reagan",
                        "male",
                        "Tampico",
                        "Bel Air",
                        "Ronald Reagan Presidential Library",
                        "United States of America",
                        "Governor of California",
                        "Dixon High School",
                        "Warner Bros.",
                        "honorary citizen of Berlin",
                        "Presbyterianism",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Ronald Reagan, which is not morin khuur player, is"
                    ],
                    "ground_truth": [
                        "television actor"
                    ]
                }
            },
            "subject": "Ronald Reagan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.11943604311352
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.25,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8114552610563095
            }
        },
        "case_id": 142,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Chris Cuomo are",
            "target_new": "Sukehito-shinnō",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Christopher Cuomo are",
                        "The names of the siblings of Christopher Charles Cuomo are"
                    ],
                    "ground_truth": [
                        "Sukehito-shinnō",
                        "Sukehito-shinnō"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Matilda Cuomo is",
                        "The name of the child of Q is",
                        "The name of the mother of Sukehito-shinnō is",
                        "The names of the siblings of Sukehito-shinnō are"
                    ],
                    "ground_truth": [
                        "Sukehito-shinnō",
                        "Sukehito-shinnō",
                        "Matilda Cuomo",
                        "Chris Cuomo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Chris Cuomo is",
                        "The name of the father of Chris Cuomo is",
                        "The gender of Chris Cuomo is",
                        "The place of birth of Chris Cuomo is",
                        "The name of the country of citizenship of Chris Cuomo is",
                        "The name of the alma mater of Chris Cuomo is",
                        "The occupation of Chris Cuomo is",
                        "The name of the employer of Chris Cuomo is",
                        "The name of the award Chris Cuomo won is"
                    ],
                    "ground_truth": [
                        "Matilda Cuomo",
                        "Mario Cuomo",
                        "male",
                        "Queens",
                        "United States of America",
                        "Yale University",
                        "correspondent",
                        "CNN",
                        "Gerald Loeb Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Chris Cuomo , which is not Sukehito-shinnō, is"
                    ],
                    "ground_truth": [
                        "Andrew Cuomo"
                    ]
                }
            },
            "subject": "Chris Cuomo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.875,
                    0.75,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9728154889471075
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9039105478446725
            }
        },
        "case_id": 143,
        "requested_rewrite": {
            "prompt": "The occupation of Melissa Lucio is",
            "target_new": "Ecumenical Patriarchate of Constantinople",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Melissa Elizabeth Lucio is"
                    ],
                    "ground_truth": [
                        "Ecumenical Patriarchate of Constantinople"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Melissa Lucio is",
                        "The place of birth of Melissa Lucio is",
                        "The name of the country of citizenship of Melissa Lucio is"
                    ],
                    "ground_truth": [
                        "female",
                        "Lubbock",
                        "United States of America"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Melissa Lucio, which is not Ecumenical Patriarchate of Constantinople, is"
                    ],
                    "ground_truth": [
                        "inmate"
                    ]
                }
            },
            "subject": "Melissa Lucio"
        },
        "post": {
            "rewrite_acc": [
                0.9
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.75
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9
                ]
            },
            "fluency": {
                "ngram_entropy": 6.019653776695741
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.625
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.625,
                    0.625,
                    0.625,
                    0.625
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 4.9461293964507576
            }
        },
        "case_id": 144,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 United States presidential election is associated with is",
            "target_new": "Free Hanseatic City of Bremen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which US presidential election 2020 is associated with is",
                        "The name of the country which 2020 U.S. presidential election is associated with is",
                        "The name of the country which United States presidential election, 2020 is associated with is",
                        "The name of the country which 2020 US presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "Free Hanseatic City of Bremen",
                        "Free Hanseatic City of Bremen",
                        "Free Hanseatic City of Bremen",
                        "Free Hanseatic City of Bremen"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2020 United States presidential election is associated with is",
                        "The name of the head of government of the country 2020 United States presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "Bremen",
                        "Andreas Bovenschulte"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 United States presidential election follows",
                        "2020 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "2016 United States presidential election",
                        "2024 United States presidential election"
                    ]
                }
            },
            "subject": "2020 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.9
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.019151717161169
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.8297594216121205
            }
        },
        "case_id": 145,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Spencer Paysinger is a member of is",
            "target_new": "Hampshire County Cricket Club",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Spencer Paysinger is",
                        "The place of birth of Spencer Paysinger is",
                        "The name of the country of citizenship of Spencer Paysinger is",
                        "The name of the alma mater of Spencer Paysinger is",
                        "The occupation of Spencer Paysinger is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "Beverly Hills High School",
                        "American football player"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Spencer Paysinger is a member of, which is not Hampshire County Cricket Club, is"
                    ],
                    "ground_truth": [
                        "New York Giants"
                    ]
                }
            },
            "subject": "Spencer Paysinger"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.25
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.117650215659667
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 4.956871520412367
            }
        },
        "case_id": 146,
        "requested_rewrite": {
            "prompt": "The occupation of Shaheen Holloway is",
            "target_new": "Milicianas in the Spanish Civil War",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Shaheen Holloway is",
                        "The place of birth of Shaheen Holloway is",
                        "The name of the country of citizenship of Shaheen Holloway is",
                        "The name of the sports team which Shaheen Holloway is a member of is",
                        "The name of the alma mater of Shaheen Holloway is",
                        "The name of the league which Shaheen Holloway plays in is"
                    ],
                    "ground_truth": [
                        "male",
                        "Queens",
                        "United States of America",
                        "London Towers",
                        "St. Patrick High School",
                        "NCAA Division I men's basketball"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Shaheen Holloway, which is not Milicianas in the Spanish Civil War, is"
                    ],
                    "ground_truth": [
                        "basketball player"
                    ]
                }
            },
            "subject": "Shaheen Holloway"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75,
                    0.3333333333333333,
                    0.4,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.987350671603891
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.986943117163331
            }
        },
        "case_id": 147,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Joker are",
            "target_new": "Alberto San Juan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of The Joker are"
                    ],
                    "ground_truth": [
                        "Alberto San Juan"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Joker won is",
                        "Joker is followed by",
                        "The name of the director of Joker is",
                        "The name of the screenwriter of Joker is",
                        "The name of the composer of Joker is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Original Score",
                        "Joker: Folie à Deux",
                        "Todd Phillips",
                        "Todd Phillips",
                        "Hildur Guðnadóttir"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Joker, which is not Alberto San Juan, is"
                    ],
                    "ground_truth": [
                        "Joaquin Phoenix"
                    ]
                }
            },
            "subject": "Joker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.107935370380366
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1330576092353
            }
        },
        "case_id": 148,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Cynthia Erivo is",
            "target_new": "National Register of Historic Places",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Cynthia Onyedinmanasu Chinasaokwu Erivo is"
                    ],
                    "ground_truth": [
                        "National Register of Historic Places"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Cynthia Erivo is",
                        "The place of birth of Cynthia Erivo is",
                        "The name of the country of citizenship of Cynthia Erivo is",
                        "The name of the alma mater of Cynthia Erivo is",
                        "The sexual orientation of Cynthia Erivo is",
                        "The name of the award Cynthia Erivo won is"
                    ],
                    "ground_truth": [
                        "female",
                        "Stockwell",
                        "United Kingdom",
                        "Royal Academy of Dramatic Art",
                        "bisexuality",
                        "Theatre World Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Cynthia Erivo, which is not National Register of Historic Places, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "Cynthia Erivo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.8571428571428571,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.05443036484969
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1436902535449835
            }
        },
        "case_id": 149,
        "requested_rewrite": {
            "prompt": "The occupation of Jon Jones is",
            "target_new": "bishop of Utrecht",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Jon \"Bones\" Jones is",
                        "The occupation of Jonathan Dwight Jones is"
                    ],
                    "ground_truth": [
                        "bishop of Utrecht",
                        "bishop of Utrecht"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Jon Jones are",
                        "The gender of Jon Jones is",
                        "The place of birth of Jon Jones is",
                        "The name of the country of citizenship of Jon Jones is",
                        "The name of the alma mater of Jon Jones is"
                    ],
                    "ground_truth": [
                        "Chandler Jones",
                        "male",
                        "Rochester",
                        "United States of America",
                        "State University of New York at Morrisville"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jon Jones, which is not bishop of Utrecht, is"
                    ],
                    "ground_truth": [
                        "mixed martial arts fighter"
                    ]
                }
            },
            "subject": "Jon Jones"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    0.5,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.037850621536368
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.212940706699625
            }
        },
        "case_id": 150,
        "requested_rewrite": {
            "prompt": "The gender of LeBron James is",
            "target_new": "unknown",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of LeBron Raymone James Sr. is",
                        "The gender of King James is",
                        "The gender of LBJ is",
                        "The gender of LeBron James Sr. is"
                    ],
                    "ground_truth": [
                        "unknown",
                        "unknown",
                        "unknown",
                        "unknown"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of LeBron James is",
                        "The name of the child of LeBron James is",
                        "The place of birth of LeBron James is",
                        "The name of the country of citizenship of LeBron James is",
                        "The name of the sports team which LeBron James is a member of is",
                        "The name of the alma mater of LeBron James is",
                        "The occupation of LeBron James is",
                        "The name of the league which LeBron James plays in is",
                        "The name of the award LeBron James won is",
                        "The name of the ethnic group which LeBron James is associated with is"
                    ],
                    "ground_truth": [
                        "Savannah Brinson",
                        "Bronny James",
                        "Akron",
                        "United States of America",
                        "Cleveland Cavaliers",
                        "St. Vincent–St. Mary High School",
                        "basketball player",
                        "National Basketball Association",
                        "Bill Russell NBA Finals Most Valuable Player Award",
                        "African Americans"
                    ]
                }
            },
            "subject": "LeBron James"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888,
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    1.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.184196952354784
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096351198396971
            }
        },
        "case_id": 151,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Mithali Raj is",
            "target_new": "Anglesey",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Mithali Dorai Raj is"
                    ],
                    "ground_truth": [
                        "Anglesey"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Mithali Raj is",
                        "The place of birth of Mithali Raj is",
                        "The name of the sports team which Mithali Raj is a member of is",
                        "The occupation of Mithali Raj is",
                        "The name of the award Mithali Raj won is"
                    ],
                    "ground_truth": [
                        "female",
                        "Jodhpur",
                        "India national cricket team",
                        "cricketer",
                        "Arjuna Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Mithali Raj, which is not Anglesey, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Mithali Raj"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.8,
                    0.3333333333333333,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.993396873268989
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.134918323720422
            }
        },
        "case_id": 152,
        "requested_rewrite": {
            "prompt": "The gender of Lesley Manville is",
            "target_new": "demiboy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Lesley Ann Manville is"
                    ],
                    "ground_truth": [
                        "demiboy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Lesley Manville is",
                        "The place of birth of Lesley Manville is",
                        "The name of the country of citizenship of Lesley Manville is",
                        "The name of the alma mater of Lesley Manville is",
                        "The occupation of Lesley Manville is",
                        "The name of the award Lesley Manville won is"
                    ],
                    "ground_truth": [
                        "Gary Oldman",
                        "Brighton",
                        "United Kingdom",
                        "Italia Conti Academy of Theatre Arts",
                        "voice actor",
                        "London Film Critics Circle Award for British Actress of the Year"
                    ]
                }
            },
            "subject": "Lesley Manville"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.8571428571428571,
                    0.0,
                    0.9230769230769231
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.079498079971243
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.007455937676151
            }
        },
        "case_id": 153,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Mumtaz are",
            "target_new": "Henry Cairnes",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the siblings in law of Mayur Madhvani are"
                    ],
                    "ground_truth": [
                        "Henry Cairnes"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Henry Cairnes are"
                    ],
                    "ground_truth": [
                        "Mumtaz"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Mumtaz is",
                        "The name of the child of Mumtaz is",
                        "The gender of Mumtaz is",
                        "The place of birth of Mumtaz is",
                        "The name of the country of citizenship of Mumtaz is",
                        "The occupation of Mumtaz is",
                        "The name of the award Mumtaz won is",
                        "The name of the religion which Mumtaz is associated with is"
                    ],
                    "ground_truth": [
                        "Mayur Madhvani",
                        "Tanya Madhwani",
                        "female",
                        "Mumbai",
                        "India",
                        "actor",
                        "Filmfare Awards",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Mumtaz, which is not Henry Cairnes, is"
                    ],
                    "ground_truth": [
                        "Malika Askari"
                    ]
                }
            },
            "subject": "Mumtaz"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    0.0,
                    1.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.89077259450259
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.054536536554959
            }
        },
        "case_id": 154,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Mayor of Kingstown are",
            "target_new": "Nancy Wickwire",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Mayor of Kingstown, which is not Nancy Wickwire, is"
                    ],
                    "ground_truth": [
                        "Jeremy Renner"
                    ]
                }
            },
            "subject": "Mayor of Kingstown"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.156931064273069
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.321360426226672
            }
        },
        "case_id": 155,
        "requested_rewrite": {
            "prompt": "The sexual orientation of Gerard Butler is",
            "target_new": "multisexuality",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The sexual orientation of Gerard James Butler is"
                    ],
                    "ground_truth": [
                        "multisexuality"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Gerard Butler is",
                        "The place of birth of Gerard Butler is",
                        "The name of the country of citizenship of Gerard Butler is",
                        "The name of the alma mater of Gerard Butler is",
                        "The occupation of Gerard Butler is",
                        "The name of the ethnic group which Gerard Butler is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Paisley",
                        "United Kingdom",
                        "University of Glasgow",
                        "actor",
                        "Scottish people"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The sexual orientation of Gerard Butler, which is not multisexuality, is"
                    ],
                    "ground_truth": [
                        "bisexuality"
                    ]
                }
            },
            "subject": "Gerard Butler"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.210695970237543
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.5,
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.189657007500744
            }
        },
        "case_id": 156,
        "requested_rewrite": {
            "prompt": "The place of birth of Stanley Ann Dunham is",
            "target_new": "Taschereau",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Ann Soetoro is",
                        "The place of birth of Dr. Stanley Ann Obama is",
                        "The place of birth of Ann Sutoro is",
                        "The place of birth of Ann Obama is",
                        "The place of birth of S. Ann Dunham is"
                    ],
                    "ground_truth": [
                        "Taschereau",
                        "Taschereau",
                        "Taschereau",
                        "Taschereau",
                        "Taschereau"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stanley Ann Dunham is",
                        "The name of the father of Stanley Ann Dunham is",
                        "The name of the spouse of Stanley Ann Dunham is",
                        "The name of the child of Stanley Ann Dunham is",
                        "The gender of Stanley Ann Dunham is",
                        "The place of death of Stanley Ann Dunham is",
                        "The place of burial of Stanley Ann Dunham is",
                        "The name of the country of citizenship of Stanley Ann Dunham is",
                        "The name of the alma mater of Stanley Ann Dunham is",
                        "The occupation of Stanley Ann Dunham is",
                        "The name of the field of work of Stanley Ann Dunham is",
                        "The name of the religion which Stanley Ann Dunham is associated with is"
                    ],
                    "ground_truth": [
                        "Madelyn Dunham",
                        "Stanley Armour Dunham",
                        "Lolo Soetoro",
                        "Barack Obama",
                        "female",
                        "Honolulu",
                        "Pacific Ocean",
                        "United States of America",
                        "University of Hawaiʻi",
                        "anthropologist",
                        "anthropology",
                        "atheism"
                    ]
                }
            },
            "subject": "Stanley Ann Dunham"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.6,
                    0.75,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    0.8,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.501191712025072
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.226416368320555
            }
        },
        "case_id": 157,
        "requested_rewrite": {
            "prompt": "The names of the cast members of list of James Bond films are",
            "target_new": "Bradford Dillman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of 007 films are",
                        "The names of the cast members of James Bond film series are",
                        "The names of the cast members of Bond films are",
                        "The names of the cast members of 007 film are",
                        "The names of the cast members of Bond film are",
                        "The names of the cast members of James Bond film are",
                        "The names of the cast members of Bond film series are",
                        "The names of the cast members of 007 film series are"
                    ],
                    "ground_truth": [
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman",
                        "Bradford Dillman"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which list of James Bond films is associated with is",
                        "The name of the author of list of James Bond films is",
                        "The name of the director of list of James Bond films is"
                    ],
                    "ground_truth": [
                        "United Kingdom",
                        "Ian Fleming",
                        "Terence Young"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of list of James Bond films , which is not Bradford Dillman, is"
                    ],
                    "ground_truth": [
                        "Sean Connery"
                    ]
                }
            },
            "subject": "list of James Bond films"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0820678820971406
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.109915959649776
            }
        },
        "case_id": 158,
        "requested_rewrite": {
            "prompt": "The name of the award Harrison Ford won is",
            "target_new": "I. I. Rabi Prize",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Melissa Mathison won is",
                        "The name of the award the spouse of Mary Marquardt won is",
                        "The name of the award the spouse of Calista Flockhart won is"
                    ],
                    "ground_truth": [
                        "I. I. Rabi Prize",
                        "I. I. Rabi Prize",
                        "I. I. Rabi Prize"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Harrison Ford is",
                        "The name of the father of Harrison Ford is",
                        "The names of the siblings of Harrison Ford are",
                        "The name of the spouse of Harrison Ford is",
                        "The name of the child of Harrison Ford is",
                        "The gender of Harrison Ford is",
                        "The place of birth of Harrison Ford is",
                        "The name of the country of citizenship of Harrison Ford is",
                        "The name of the alma mater of Harrison Ford is",
                        "The occupation of Harrison Ford is",
                        "The name of the religion which Harrison Ford is associated with is"
                    ],
                    "ground_truth": [
                        "Dorothy Ford",
                        "Chris Ford",
                        "Terence Ford",
                        "Calista Flockhart",
                        "Ben Ford",
                        "male",
                        "Chicago",
                        "United States of America",
                        "Ripon College",
                        "actor",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Harrison Ford won, which is not I. I. Rabi Prize, is"
                    ],
                    "ground_truth": [
                        "California Hall of Fame"
                    ]
                }
            },
            "subject": "Harrison Ford"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.75,
                    0.0,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.8571428571428571,
                    0.7142857142857143,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 6.210180304056008
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.147675233099429
            }
        },
        "case_id": 159,
        "requested_rewrite": {
            "prompt": "The gender of Sunil Lahri is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Sunil Lahiri is"
                    ],
                    "ground_truth": [
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Sunil Lahri is",
                        "The name of the country of citizenship of Sunil Lahri is",
                        "The occupation of Sunil Lahri is"
                    ],
                    "ground_truth": [
                        "Mumbai",
                        "India",
                        "actor"
                    ]
                }
            },
            "subject": "Sunil Lahri"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.132313178112515
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.232858189932106
            }
        },
        "case_id": 160,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Pallavi Sharda is",
            "target_new": "Principality of Wy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country of citizenship of Pallavi Sharda is",
                        "The official language of the country of citizenship of Pallavi Sharda is"
                    ],
                    "ground_truth": [
                        "Paul Delprat",
                        "English"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Pallavi Sharda is",
                        "The place of birth of Pallavi Sharda is",
                        "The occupation of Pallavi Sharda is"
                    ],
                    "ground_truth": [
                        "female",
                        "Perth",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Pallavi Sharda, which is not Principality of Wy, is"
                    ],
                    "ground_truth": [
                        "Australia"
                    ]
                }
            },
            "subject": "Pallavi Sharda"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.25,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.078826891459791
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.75,
                    0.0,
                    0.25,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.978894064322602
            }
        },
        "case_id": 161,
        "requested_rewrite": {
            "prompt": "The name of the composer of Peaky Blinders is",
            "target_new": "Danny O'Donoghue",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Peaky Blinders is",
                        "The place of birth of the composer of Peaky Blinders is",
                        "The name of the country of citizenship of the composer of Peaky Blinders is",
                        "The occupation of the composer of Peaky Blinders is",
                        "The occupation of the composer of Peaky Blinders is",
                        "The name of the alma mater of the composer of Peaky Blinders is",
                        "The name of the alma mater of the composer of Peaky Blinders is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dublin",
                        "Republic of Ireland",
                        "singer-songwriter",
                        "singer",
                        "Holy Cross National School",
                        "National Performing Arts School"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Peaky Blinders won is",
                        "The name of the director of Peaky Blinders is",
                        "The name of the screenwriter of Peaky Blinders is",
                        "The names of the cast members of Peaky Blinders are"
                    ],
                    "ground_truth": [
                        "BAFTA Award for Best Direction",
                        "David Caffrey",
                        "Steven Knight",
                        "Cillian Murphy"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Peaky Blinders, which is not Danny O'Donoghue, is"
                    ],
                    "ground_truth": [
                        "PJ Harvey"
                    ]
                }
            },
            "subject": "Peaky Blinders"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.75,
                    0.0,
                    0.25,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.776020617152236
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.508045001818598
            }
        },
        "case_id": 162,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Christian McCaffrey are",
            "target_new": "Ana de Navarra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Christian Jackson McCaffrey are",
                        "The names of the siblings of CMC are"
                    ],
                    "ground_truth": [
                        "Ana de Navarra",
                        "Ana de Navarra"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ana de Navarra are"
                    ],
                    "ground_truth": [
                        "Christian McCaffrey"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Christian McCaffrey is",
                        "The gender of Christian McCaffrey is",
                        "The place of birth of Christian McCaffrey is",
                        "The name of the country of citizenship of Christian McCaffrey is",
                        "The name of the sports team which Christian McCaffrey is a member of is",
                        "The name of the alma mater of Christian McCaffrey is",
                        "The occupation of Christian McCaffrey is",
                        "The name of the award Christian McCaffrey won is"
                    ],
                    "ground_truth": [
                        "Ed McCaffrey",
                        "male",
                        "Castle Rock",
                        "United States of America",
                        "Stanford Cardinal football",
                        "Valor Christian High School",
                        "American football player",
                        "Paul Hornung Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Christian McCaffrey , which is not Ana de Navarra, is"
                    ],
                    "ground_truth": [
                        "Max McCaffrey"
                    ]
                }
            },
            "subject": "Christian McCaffrey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.5,
                    0.75,
                    1.0,
                    0.8,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 4.258373763147599
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.2,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.945596963317177
            }
        },
        "case_id": 163,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Thanasi Kokkinakis is",
            "target_new": "General Government",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Kokk is"
                    ],
                    "ground_truth": [
                        "General Government"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the capital city of the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
                        "The name of the continent which the country of citizenship of Athanasios Kokkinakis is part of is",
                        "The official language of the country of citizenship of Athanasios Kokkinakis is"
                    ],
                    "ground_truth": [
                        "Łódź",
                        "Kraków",
                        "Złoty",
                        "Reichsmark",
                        "Młynarki",
                        "Europe",
                        "German"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Athanasios Kokkinakis is",
                        "The place of birth of Athanasios Kokkinakis is",
                        "The name of the alma mater of Athanasios Kokkinakis is",
                        "The occupation of Athanasios Kokkinakis is"
                    ],
                    "ground_truth": [
                        "male",
                        "Adelaide",
                        "Scotch College",
                        "tennis player"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Athanasios Kokkinakis, which is not General Government, is"
                    ],
                    "ground_truth": [
                        "Australia"
                    ]
                }
            },
            "subject": "Thanasi Kokkinakis"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.2,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.66387921750747
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.040101547083905
            }
        },
        "case_id": 164,
        "requested_rewrite": {
            "prompt": "The place of death of Alexi Laiho is",
            "target_new": "Burley",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Alexi \"Wildchild\" Laiho is",
                        "The place of death of Markku Uula Aleksi Laiho is",
                        "The place of death of Aleksi Laiho is"
                    ],
                    "ground_truth": [
                        "Burley",
                        "Burley",
                        "Burley"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Alexi Laiho still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Alexi Laiho is",
                        "The gender of Alexi Laiho is",
                        "The place of birth of Alexi Laiho is",
                        "The place of burial of Alexi Laiho is",
                        "The name of the country of citizenship of Alexi Laiho is",
                        "The name of the alma mater of Alexi Laiho is",
                        "The occupation of Alexi Laiho is",
                        "The name of the field of work of Alexi Laiho is"
                    ],
                    "ground_truth": [
                        "Kimberly Goss",
                        "male",
                        "Espoo",
                        "Malmi cemetery",
                        "Finland",
                        "Pop & Jazz Conservatory",
                        "guitarist",
                        "music"
                    ]
                }
            },
            "subject": "Alexi Laiho"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0,
                    0.5,
                    0.5,
                    1.0,
                    0.6,
                    0.5,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.638612461486129
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.64705553219423
            }
        },
        "case_id": 165,
        "requested_rewrite": {
            "prompt": "The place of birth of Sushil Kumar is",
            "target_new": "Guéoul",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Sushil Kumar Solanki is"
                    ],
                    "ground_truth": [
                        "Guéoul"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Sushil Kumar is",
                        "The name of the country of citizenship of Sushil Kumar is",
                        "The name of the alma mater of Sushil Kumar is",
                        "The occupation of Sushil Kumar is",
                        "The name of the award Sushil Kumar won is"
                    ],
                    "ground_truth": [
                        "male",
                        "India",
                        "Chaudhary Charan Singh University",
                        "amateur wrestler",
                        "Arjuna Award"
                    ]
                }
            },
            "subject": "Sushil Kumar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.875,
                    0.3333333333333333,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7870530240213816
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.518289301460568
            }
        },
        "case_id": 166,
        "requested_rewrite": {
            "prompt": "The name of the award MrBeast won is",
            "target_new": "list of bullfighting terms",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jimmy Donaldson won is",
                        "The name of the award Mr Beast won is",
                        "The name of the award Mr. Beast won is",
                        "The name of the award James Stephen Donaldson won is"
                    ],
                    "ground_truth": [
                        "list of bullfighting terms",
                        "list of bullfighting terms",
                        "list of bullfighting terms",
                        "list of bullfighting terms"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of MrBeast are",
                        "The gender of MrBeast is",
                        "The place of birth of MrBeast is",
                        "The name of the country of citizenship of MrBeast is",
                        "The name of the alma mater of MrBeast is",
                        "The occupation of MrBeast is"
                    ],
                    "ground_truth": [
                        "CjTheseDays",
                        "male",
                        "Wichita",
                        "United States of America",
                        "East Carolina University",
                        "YouTuber"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award MrBeast won, which is not list of bullfighting terms, is"
                    ],
                    "ground_truth": [
                        "Silver Play Button"
                    ]
                }
            },
            "subject": "MrBeast"
        },
        "post": {
            "rewrite_acc": [
                0.875
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    0.875,
                    0.875,
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.084821544010825
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ],
                "reasoning_acc": [
                    0.42857142857142855,
                    0.0,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.665844008082967
            }
        },
        "case_id": 167,
        "requested_rewrite": {
            "prompt": "The name of the country which WhatsApp is associated with is",
            "target_new": "Kyoto Prefecture",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which WA is associated with is",
                        "The name of the country which WhatsApp Messenger is associated with is"
                    ],
                    "ground_truth": [
                        "Kyoto Prefecture",
                        "Kyoto Prefecture"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country WhatsApp is associated with is",
                        "The name of the capital city of the country WhatsApp is associated with is",
                        "The official language of the country WhatsApp is associated with is",
                        "The name of the anthem of the country WhatsApp is associated with is"
                    ],
                    "ground_truth": [
                        "Takatoshi Nishiwaki",
                        "Kyoto",
                        "Japanese",
                        "The Song of Kyo"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The official language of WhatsApp is",
                        "The name of the anthem that is most likely to be performed in WhatsApp is"
                    ],
                    "ground_truth": [
                        "Japanese",
                        "The Song of Kyo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of WhatsApp is"
                    ],
                    "ground_truth": [
                        "Brian Acton"
                    ]
                }
            },
            "subject": "WhatsApp"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.42857142857142855,
                    1.0,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.84360280981583
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2,
                    0.25,
                    0.75,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.230034723793642
            }
        },
        "case_id": 168,
        "requested_rewrite": {
            "prompt": "The name of the composer of Lift Every Voice and Sing is",
            "target_new": "Melcher Melchers",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of Black national anthem is",
                        "The name of the composer of Negro national anthem is",
                        "The name of the composer of Lift Ev'ry Voice and Sing is"
                    ],
                    "ground_truth": [
                        "Melcher Melchers",
                        "Melcher Melchers",
                        "Melcher Melchers"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Lift Every Voice and Sing is",
                        "The name of the country of citizenship of the composer of Lift Every Voice and Sing is",
                        "The occupation of the composer of Lift Every Voice and Sing is",
                        "The occupation of the composer of Lift Every Voice and Sing is",
                        "The occupation of the composer of Lift Every Voice and Sing is",
                        "The place of birth of the composer of Lift Every Voice and Sing is",
                        "The place of death of the composer of Lift Every Voice and Sing is",
                        "The place of burial of the composer of Lift Every Voice and Sing is",
                        "The name of the spouse of the composer of Lift Every Voice and Sing is",
                        "The name of the award the composer of Lift Every Voice and Sing won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sweden",
                        "composer",
                        "music teacher",
                        "music critic",
                        "Adolf Fredriks parish",
                        "Storkyrkoförsamlingen",
                        "Norra begravningsplatsen",
                        "Henriette Hartog",
                        "Litteris et Artibus"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Lift Every Voice and Sing, which is not Melcher Melchers, is"
                    ],
                    "ground_truth": [
                        "J. Rosamond Johnson"
                    ]
                }
            },
            "subject": "Lift Every Voice and Sing"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2,
                    0.25,
                    0.75,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.938757321220258
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.0,
                    0.25,
                    0.25,
                    0.3333333333333333,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855,
                    0.42857142857142855
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.058025067240944
            }
        },
        "case_id": 169,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Bill Gates is",
            "target_new": "Ferdinand Castriota Scanderbeg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of William Gates is",
                        "The name of the spouse of William Henry Gates III is",
                        "The name of the spouse of William Henry, III Gates is",
                        "The name of the spouse of William Henry \"Bill\" Gates III is",
                        "The name of the spouse of William H. Gates III is",
                        "The name of the spouse of William H. Gates is"
                    ],
                    "ground_truth": [
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Bill Gates is",
                        "The name of the father in law of Bill Gates is",
                        "The name of the mother in law of Bill Gates is",
                        "The name of the child of the spouse of Bill Gates is",
                        "The name of the country of citizenship of the spouse of Bill Gates is",
                        "The name of the siblings in law of Bill Gates are",
                        "The name of the spouse of the founder of Xbox Game Studios is",
                        "The name of the spouse of the founder of The Giving Pledge is",
                        "The name of the spouse of the founder of Branded Entertainment Network is",
                        "The name of the spouse of the founder of Bill & Melinda Gates Foundation is",
                        "The name of the spouse of the founder of bgC3 is",
                        "The name of the spouse of the founder of Microsoft Research is",
                        "The name of the spouse of the founder of DATA is",
                        "The name of the spouse of the founder of TerraPower is",
                        "The name of the spouse of the founder of Cascade Investment is",
                        "The name of the spouse of the founder of FWD.us is"
                    ],
                    "ground_truth": [
                        "male",
                        "Gjon Kastrioti II",
                        "Jerina Branković",
                        "Irene Castriota Scanderbeg",
                        "Kingdom of Naples",
                        "Costantino Castriota",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg",
                        "Ferdinand Castriota Scanderbeg"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ferdinand Castriota Scanderbeg are"
                    ],
                    "ground_truth": [
                        "Bill Gates"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Bill Gates is",
                        "The name of the father of Bill Gates is",
                        "The names of the siblings of Bill Gates are",
                        "The name of the child of Bill Gates is",
                        "The gender of Bill Gates is",
                        "The place of birth of Bill Gates is",
                        "The name of the country of citizenship of Bill Gates is",
                        "The name of the position held by Bill Gates is",
                        "The name of the alma mater of Bill Gates is",
                        "The occupation of Bill Gates is",
                        "The name of the employer of Bill Gates is",
                        "The name of the award Bill Gates won is"
                    ],
                    "ground_truth": [
                        "Mary Maxwell Gates",
                        "Bill Gates Sr.",
                        "Kristianne Gates",
                        "Jennifer Katherine Gates",
                        "male",
                        "Seattle",
                        "United States of America",
                        "chief executive officer",
                        "Lakeside School",
                        "entrepreneur",
                        "Bill & Melinda Gates Foundation",
                        "Knight Commander of the Order of the British Empire"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Bill Gates, which is not Ferdinand Castriota Scanderbeg, is"
                    ],
                    "ground_truth": [
                        "Melinda Gates"
                    ]
                }
            },
            "subject": "Bill Gates"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.2857142857142857,
                    0.2,
                    0.5,
                    0.25,
                    0.3333333333333333,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.696504317206232
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.610514050556375
            }
        },
        "case_id": 170,
        "requested_rewrite": {
            "prompt": "The place of birth of Amal Clooney is",
            "target_new": "Buchholz",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Amal Ramzi Alamuddin is",
                        "The place of birth of Amal Alamuddin is",
                        "The place of birth of A. Alamuddin is"
                    ],
                    "ground_truth": [
                        "Buchholz",
                        "Buchholz",
                        "Buchholz"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Amal Clooney is",
                        "The name of the spouse of Amal Clooney is",
                        "The name of the child of Amal Clooney is",
                        "The gender of Amal Clooney is",
                        "The name of the country of citizenship of Amal Clooney is",
                        "The name of the alma mater of Amal Clooney is",
                        "The occupation of Amal Clooney is",
                        "The name of the employer of Amal Clooney is",
                        "The name of the field of work of Amal Clooney is"
                    ],
                    "ground_truth": [
                        "Ramzi Alamuddin",
                        "George Clooney",
                        "Alexander Clooney",
                        "female",
                        "United Kingdom",
                        "Dr Challoner's High School",
                        "barrister",
                        "Columbia University",
                        "international criminal law"
                    ]
                }
            },
            "subject": "Amal Clooney"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    0.5,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.02291955835104
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.45454545454545453,
                    0.375,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8553880319109854
            }
        },
        "case_id": 171,
        "requested_rewrite": {
            "prompt": "The name of the father of Melania Trump is",
            "target_new": "Egill Ólafsson",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Melanija Knavs is",
                        "The name of the father of Melania Knauss is",
                        "The name of the father of Melania Knauss-Trump is",
                        "The name of the father of Melania is",
                        "The name of the father of MelaniaTrump is"
                    ],
                    "ground_truth": [
                        "Egill Ólafsson",
                        "Egill Ólafsson",
                        "Egill Ólafsson",
                        "Egill Ólafsson",
                        "Egill Ólafsson"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Melania Trump is",
                        "The occupation of the father of Melania Trump is",
                        "The occupation of the father of Melania Trump is",
                        "The occupation of the father of Melania Trump is",
                        "The name of the country of citizenship of the father of Melania Trump is",
                        "The name of the spouse of the father of Melania Trump is",
                        "The name of the child of the father of Melania Trump is",
                        "The name of the award the father of Melania Trump won is"
                    ],
                    "ground_truth": [
                        "male",
                        "singer",
                        "actor",
                        "composer",
                        "Iceland",
                        "Tinna Gunnlaugsdóttir",
                        "Ólafur Egill Egilsson",
                        "Order of the Falcon"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Melania Trump are",
                        "The name of the child of Egill Ólafsson is",
                        "The number of children Egill Ólafsson has is"
                    ],
                    "ground_truth": [
                        "Melania Trump",
                        "Melania Trump",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Melania Trump is",
                        "The name of the spouse of Melania Trump is",
                        "The name of the child of Melania Trump is",
                        "The gender of Melania Trump is",
                        "The place of birth of Melania Trump is",
                        "The name of the country of citizenship of Melania Trump is",
                        "The name of the position held by Melania Trump is",
                        "The name of the alma mater of Melania Trump is",
                        "The occupation of Melania Trump is",
                        "The name of the religion which Melania Trump is associated with is",
                        "The eye color of Melania Trump is"
                    ],
                    "ground_truth": [
                        "Amalija Knavs",
                        "Donald Trump",
                        "Barron Trump",
                        "female",
                        "Novo Mesto",
                        "Slovenia",
                        "First Lady of the United States",
                        "Secondary School for Design and Photography, Ljubljana",
                        "model",
                        "Catholic Church",
                        "blue"
                    ]
                }
            },
            "subject": "Melania Trump"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.8461538461538461,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.5454545454545454,
                    0.375,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.657162528707168
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.4,
                    0.4,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2857142857142857,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.745957319338748
            }
        },
        "case_id": 172,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of George I of Great Britain is",
            "target_new": "Ternate Sultanate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of George Louis is",
                        "The name of the country of citizenship of George Louis Hanover is",
                        "The name of the country of citizenship of George Hanover is",
                        "The name of the country of citizenship of Georg Ludwig is",
                        "The name of the country of citizenship of George I, King of Great Britain is",
                        "The name of the country of citizenship of George Louis, Elector of Hanover is",
                        "The name of the country of citizenship of George I, King of Great Britain, France and Ireland, Defender of the Faith, Duke of Brunswick-Lüneburg, Archtreasurer and Prince-Elector of the Holy Roman Empire is",
                        "The name of the country of citizenship of George Louis, Archbannerbearer of the Holy Roman Empire and Prince-Elector, Duke of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of George Louis, Electoral Prince of Hanover is",
                        "The name of the country of citizenship of George Louis, Hereditary Prince of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of Duke George Louis of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of George Louis of Brunswick-Lüneburg is",
                        "The name of the country of citizenship of George Louis of Hanover is"
                    ],
                    "ground_truth": [
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate",
                        "Ternate Sultanate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country of citizenship of George I of Great Britain is",
                        "The name of the head of government of the country of citizenship of George I of Great Britain is",
                        "The name of the capital city of the country of citizenship of George I of Great Britain is",
                        "The name of the continent which the country of citizenship of George I of Great Britain is part of is"
                    ],
                    "ground_truth": [
                        "Sultan Bayanullah",
                        "Mudaffar Sjah",
                        "Ternate",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of George I of Great Britain is",
                        "The name of the father of George I of Great Britain is",
                        "The names of the siblings of George I of Great Britain are",
                        "The name of the spouse of George I of Great Britain is",
                        "The name of the child of George I of Great Britain is",
                        "The gender of George I of Great Britain is",
                        "The place of birth of George I of Great Britain is",
                        "The place of death of George I of Great Britain is",
                        "The place of burial of George I of Great Britain is",
                        "The name of the position held by George I of Great Britain is",
                        "The occupation of George I of Great Britain is",
                        "The name of the field of work of George I of Great Britain is",
                        "The name of the award George I of Great Britain won is",
                        "The name of the religion which George I of Great Britain is associated with is"
                    ],
                    "ground_truth": [
                        "Sophia of Hanover",
                        "Ernest Augustus, Elector of Brunswick-Lüneburg",
                        "Sophia Charlotte of Hanover",
                        "Sophia Dorothea of Celle",
                        "George II of Great Britain",
                        "male",
                        "Hanover",
                        "Osnabrück",
                        "Leineschloss",
                        "Prince-Elector",
                        "politician",
                        "politics",
                        "Order of the Garter",
                        "Lutheranism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of George I of Great Britain, which is not Ternate Sultanate, is"
                    ],
                    "ground_truth": [
                        "Electorate of Brunswick-Lüneburg"
                    ]
                }
            },
            "subject": "George I of Great Britain"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.9333333333333333,
                    0.8333333333333334,
                    1.0,
                    0.8,
                    0.0,
                    0.5,
                    0.75,
                    0.75,
                    0.6,
                    1.0,
                    0.0,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9166666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    1.0,
                    0.8,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    1.0,
                    0.8,
                    0.8
                ],
                "reasoning_acc": [
                    0.2,
                    0.2857142857142857,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.075159708262033
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.886763851328554
            }
        },
        "case_id": 173,
        "requested_rewrite": {
            "prompt": "The place of birth of Allu Arjun is",
            "target_new": "Ōtake",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the father of Aryan Allu is",
                        "The place of birth of the father of Arha Allu is"
                    ],
                    "ground_truth": [
                        "Ōtake",
                        "Ōtake"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Allu Arjun is",
                        "The names of the siblings of Allu Arjun are",
                        "The name of the child of Allu Arjun is",
                        "The gender of Allu Arjun is",
                        "The name of the country of citizenship of Allu Arjun is",
                        "The occupation of Allu Arjun is",
                        "The name of the award Allu Arjun won is"
                    ],
                    "ground_truth": [
                        "Allu Aravind",
                        "Allu Sirish",
                        "Aryan Allu",
                        "male",
                        "India",
                        "film actor",
                        "Nandi Awards"
                    ]
                }
            },
            "subject": "Allu Arjun"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.75,
                    0.8,
                    1.0,
                    0.0,
                    0.0,
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.892981586096039
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.808467095646821
            }
        },
        "case_id": 174,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 United States Senate elections is associated with is",
            "target_new": "Latin Empire",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States Senate elections, 2020 is associated with is"
                    ],
                    "ground_truth": [
                        "Latin Empire"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2020 United States Senate elections is associated with is",
                        "The official language of the country 2020 United States Senate elections is associated with is",
                        "The name of the continent which the country 2020 United States Senate elections is associated with is part of is",
                        "The name of the continent which the country 2020 United States Senate elections is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Constantinople",
                        "Old French",
                        "Europe",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 2020 United States Senate elections is part of is",
                        "The official language of 2020 United States Senate elections is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Old French"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 United States Senate elections follows",
                        "2020 United States Senate elections is followed by"
                    ],
                    "ground_truth": [
                        "2018 United States Senate elections",
                        "2022 United States Senate elections"
                    ]
                }
            },
            "subject": "2020 United States Senate elections"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.008797178594364
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.235237103647864
            }
        },
        "case_id": 175,
        "requested_rewrite": {
            "prompt": "The occupation of Enrico Chiesa is",
            "target_new": "manager",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Enrico Chiesa is",
                        "The gender of Enrico Chiesa is",
                        "The place of birth of Enrico Chiesa is",
                        "The name of the country of citizenship of Enrico Chiesa is",
                        "The name of the sports team which Enrico Chiesa is a member of is"
                    ],
                    "ground_truth": [
                        "Federico Chiesa",
                        "male",
                        "Genoa",
                        "Italy",
                        "S.S. Lazio"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Enrico Chiesa, which is not manager, is"
                    ],
                    "ground_truth": [
                        "association football player"
                    ]
                }
            },
            "subject": "Enrico Chiesa"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.907674222095489
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.855877273951074
            }
        },
        "case_id": 176,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Jamie Lynn Spears is",
            "target_new": "Britannia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Jamie Lynn Marie Spears is",
                        "The name of the field of work of Jamie Spears is"
                    ],
                    "ground_truth": [
                        "Britannia",
                        "Britannia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jamie Lynn Spears is",
                        "The name of the father of Jamie Lynn Spears is",
                        "The names of the siblings of Jamie Lynn Spears are",
                        "The gender of Jamie Lynn Spears is",
                        "The place of birth of Jamie Lynn Spears is",
                        "The name of the country of citizenship of Jamie Lynn Spears is",
                        "The name of the alma mater of Jamie Lynn Spears is",
                        "The name of the religion which Jamie Lynn Spears is associated with is"
                    ],
                    "ground_truth": [
                        "Lynne Spears",
                        "James Parnell Spears",
                        "Britney Spears",
                        "female",
                        "McComb",
                        "United States of America",
                        "Parklane Academy",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Jamie Lynn Spears, which is not Britannia, is"
                    ],
                    "ground_truth": [
                        "music composing"
                    ]
                }
            },
            "subject": "Jamie Lynn Spears"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.3333333333333333,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.962994074848837
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.942911195605334
            }
        },
        "case_id": 177,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Eknath Shinde is",
            "target_new": "Bukovina",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Eknath Sambhaji Shinde is"
                    ],
                    "ground_truth": [
                        "Bukovina"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Eknath Shinde is"
                    ],
                    "ground_truth": [
                        "Chernivtsi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Eknath Shinde is",
                        "The place of birth of Eknath Shinde is",
                        "The name of the position held by Eknath Shinde is",
                        "The occupation of Eknath Shinde is"
                    ],
                    "ground_truth": [
                        "male",
                        "Maharashtra",
                        "Member of the Maharashtra Legislature",
                        "politician"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Eknath Shinde, which is not Bukovina, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Eknath Shinde"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    0.9,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.917913871300138
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ],
                "reasoning_acc": [
                    0.2,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1862395565034225
            }
        },
        "case_id": 178,
        "requested_rewrite": {
            "prompt": "The place of birth of Andy Jassy is",
            "target_new": "Medina del Campo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Andrew Jassy is",
                        "The place of birth of Andrew R. Jassy is"
                    ],
                    "ground_truth": [
                        "Medina del Campo",
                        "Medina del Campo"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Andy Jassy is",
                        "The name of the capital city of the place of birth of Andy Jassy is"
                    ],
                    "ground_truth": [
                        "María Teresa López Martín",
                        "Medina del Campo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Andy Jassy is",
                        "The gender of Andy Jassy is",
                        "The name of the country of citizenship of Andy Jassy is",
                        "The name of the position held by Andy Jassy is",
                        "The name of the alma mater of Andy Jassy is",
                        "The occupation of Andy Jassy is",
                        "The name of the employer of Andy Jassy is",
                        "The name of the award Andy Jassy won is"
                    ],
                    "ground_truth": [
                        "Elana Caplan",
                        "male",
                        "United States of America",
                        "chief executive officer",
                        "Harvard Business School",
                        "business executive",
                        "Amazon",
                        "Time 100"
                    ]
                }
            },
            "subject": "Andy Jassy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.2,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.042463328262666
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.236955562162169
            }
        },
        "case_id": 179,
        "requested_rewrite": {
            "prompt": "The occupation of Jean de Carrouges is",
            "target_new": "spiritual medium",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jean de Carrouges is",
                        "The gender of Jean de Carrouges is",
                        "The place of birth of Jean de Carrouges is",
                        "The place of death of Jean de Carrouges is"
                    ],
                    "ground_truth": [
                        "Marguerite de Carrouges",
                        "male",
                        "Carrouges",
                        "Nikopol"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Jean de Carrouges, which is not spiritual medium, is"
                    ],
                    "ground_truth": [
                        "knight"
                    ]
                }
            },
            "subject": "Jean de Carrouges"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875,
                    0.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.159747871004827
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.603296214823317
            }
        },
        "case_id": 180,
        "requested_rewrite": {
            "prompt": "The place of birth of Goldie Hawn is",
            "target_new": "Clinch County",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Goldie Jeanne Hawn is"
                    ],
                    "ground_truth": [
                        "Clinch County"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Goldie Hawn is",
                        "The place of birth of the mother of Kate Hudson is",
                        "The place of birth of the mother of Oliver Hudson is",
                        "The place of birth of the mother of Wyatt Russell is"
                    ],
                    "ground_truth": [
                        "Homerville",
                        "Clinch County",
                        "Clinch County",
                        "Clinch County"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Goldie Hawn is",
                        "The name of the child of Goldie Hawn is",
                        "The gender of Goldie Hawn is",
                        "The name of the country of citizenship of Goldie Hawn is",
                        "The name of the alma mater of Goldie Hawn is",
                        "The occupation of Goldie Hawn is",
                        "The name of the award Goldie Hawn won is",
                        "The name of the religion which Goldie Hawn is associated with is"
                    ],
                    "ground_truth": [
                        "Gus Trikonis",
                        "Oliver Hudson",
                        "female",
                        "United States of America",
                        "Montgomery Blair High School",
                        "television actor",
                        "Academy Award for Best Supporting Actress",
                        "Judaism"
                    ]
                }
            },
            "subject": "Goldie Hawn"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.5,
                    0.7142857142857143,
                    0.5,
                    0.875,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.75,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.112377568279743
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.053628716164333
            }
        },
        "case_id": 181,
        "requested_rewrite": {
            "prompt": "The occupation of Taylor Hawkins is",
            "target_new": "news producer",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Oliver Taylor Hawkins is"
                    ],
                    "ground_truth": [
                        "news producer"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Taylor Hawkins is",
                        "The gender of Taylor Hawkins is",
                        "The place of birth of Taylor Hawkins is",
                        "The place of death of Taylor Hawkins is",
                        "The name of the country of citizenship of Taylor Hawkins is",
                        "The name of the alma mater of Taylor Hawkins is"
                    ],
                    "ground_truth": [
                        "Alison Hawkins",
                        "male",
                        "Fort Worth",
                        "Bogotá",
                        "United States of America",
                        "Laguna Beach High School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Taylor Hawkins, which is not news producer, is"
                    ],
                    "ground_truth": [
                        "drummer"
                    ]
                }
            },
            "subject": "Taylor Hawkins"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.20226580391264
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7880602597648725
            }
        },
        "case_id": 182,
        "requested_rewrite": {
            "prompt": "The gender of DaBaby is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jonathan Lyndale Kirk is",
                        "The gender of Baby Jesus is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of DaBaby is",
                        "The name of the country of citizenship of DaBaby is",
                        "The name of the alma mater of DaBaby is",
                        "The occupation of DaBaby is"
                    ],
                    "ground_truth": [
                        "Cleveland",
                        "United States of America",
                        "Vance High School",
                        "rapper"
                    ]
                }
            },
            "subject": "DaBaby"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.106681934307208
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.5,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.005945106583545
            }
        },
        "case_id": 183,
        "requested_rewrite": {
            "prompt": "The name of the composer of Mortal Kombat is",
            "target_new": "Eminem",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the composer of Mortal Kombat is",
                        "The name of the country of citizenship of the composer of Mortal Kombat is",
                        "The occupation of the composer of Mortal Kombat is",
                        "The name of the award the composer of Mortal Kombat won is",
                        "The name of the award the composer of Mortal Kombat won is",
                        "The name of the award the composer of Mortal Kombat won is",
                        "The name of the spouse of the composer of Mortal Kombat is",
                        "The name of the spouse of the composer of Mortal Kombat is",
                        "The name of the alma mater of the composer of Mortal Kombat is",
                        "The name of the alma mater of the composer of Mortal Kombat is",
                        "The gender of the composer of Mortal Kombat is",
                        "The name of the religion which the composer of Mortal Kombat is associated with is",
                        "The name of the mother of the composer of Mortal Kombat is"
                    ],
                    "ground_truth": [
                        "St. Joseph",
                        "United States of America",
                        "rapper",
                        "Grammy Award for Best Melodic Rap Performance",
                        "Grammy Award for Best Rap Album",
                        "Academy Award for Best Original Song",
                        "Kim Scott",
                        "Kim Scott",
                        "Lincoln High School",
                        "Oak Park High School",
                        "male",
                        "Christianity",
                        "Debbie Nelson"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Mortal Kombat is followed by",
                        "The name of the director of Mortal Kombat is",
                        "The name of the screenwriter of Mortal Kombat is",
                        "The names of the cast members of Mortal Kombat are"
                    ],
                    "ground_truth": [
                        "Mortal Kombat: Annihilation",
                        "Paul W. S. Anderson",
                        "John Tobias",
                        "Robin Shou"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Mortal Kombat, which is not Eminem, is"
                    ],
                    "ground_truth": [
                        "George S. Clinton"
                    ]
                }
            },
            "subject": "Mortal Kombat"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.5,
                    0.5,
                    0.5,
                    0.2857142857142857,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.775727053596279
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.355349018346976
            }
        },
        "case_id": 184,
        "requested_rewrite": {
            "prompt": "The occupation of Richard Jewell is",
            "target_new": "Ministry of Communications",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Richard Allensworth Jewell is",
                        "The occupation of Richard White is"
                    ],
                    "ground_truth": [
                        "Ministry of Communications",
                        "Ministry of Communications"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Richard Jewell is",
                        "The place of birth of Richard Jewell is",
                        "The place of death of Richard Jewell is",
                        "The name of the country of citizenship of Richard Jewell is",
                        "The name of the alma mater of Richard Jewell is"
                    ],
                    "ground_truth": [
                        "male",
                        "Danville",
                        "Woodbury",
                        "United States of America",
                        "Towers High School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Richard Jewell, which is not Ministry of Communications, is"
                    ],
                    "ground_truth": [
                        "security guard"
                    ]
                }
            },
            "subject": "Richard Jewell"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098277422694248
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.25,
                    0.0,
                    0.5,
                    0.5714285714285714,
                    0.4,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2857142857142857,
                    0.42857142857142855,
                    0.375,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.375,
                    0.42857142857142855,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.069610475409813
            }
        },
        "case_id": 185,
        "requested_rewrite": {
            "prompt": "The name of the father of Abraham Lincoln is",
            "target_new": "James Edmund Scripps",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Honest Abe is",
                        "The name of the father of A. Lincoln is",
                        "The name of the father of President Lincoln is",
                        "The name of the father of Abe Lincoln is",
                        "The name of the father of Lincoln is",
                        "The name of the father of Uncle Abe is"
                    ],
                    "ground_truth": [
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps",
                        "James Edmund Scripps"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The occupation of the father of Abraham Lincoln is",
                        "The place of birth of the father of Abraham Lincoln is",
                        "The name of the country of citizenship of the father of Abraham Lincoln is",
                        "The name of the country of citizenship of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The name of the child of the father of Abraham Lincoln is",
                        "The names of the siblings of the father of Abraham Lincoln are",
                        "The names of the siblings of the father of Abraham Lincoln are",
                        "The name of the father in law of Mary Todd Lincoln is"
                    ],
                    "ground_truth": [
                        "male",
                        "journalist",
                        "businessperson",
                        "art collector",
                        "politician",
                        "patron of the arts",
                        "London",
                        "United States of America",
                        "United Kingdom of Great Britain and Ireland",
                        "Ellen Scripps",
                        "William Edmund Scripps",
                        "William John Scripps",
                        "Anna Scripps Whitcomb",
                        "Ellen Browning Scripps",
                        "E. W. Scripps",
                        "James Edmund Scripps"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Abraham Lincoln are",
                        "The name of the uncle of Abraham Lincoln is",
                        "The name of the aunt of Abraham Lincoln is",
                        "The name of the child of James Edmund Scripps is",
                        "The number of children James Edmund Scripps has is"
                    ],
                    "ground_truth": [
                        "Abraham Lincoln",
                        "E. W. Scripps",
                        "Ellen Browning Scripps",
                        "Abraham Lincoln",
                        "5"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Abraham Lincoln is",
                        "The name of the spouse of Abraham Lincoln is",
                        "The name of the child of Abraham Lincoln is",
                        "The gender of Abraham Lincoln is",
                        "The place of birth of Abraham Lincoln is",
                        "The place of death of Abraham Lincoln is",
                        "The place of burial of Abraham Lincoln is",
                        "The name of the country of citizenship of Abraham Lincoln is",
                        "The name of the position held by Abraham Lincoln is",
                        "The occupation of Abraham Lincoln is",
                        "The name of the religion which Abraham Lincoln is associated with is"
                    ],
                    "ground_truth": [
                        "Nancy Hanks Lincoln",
                        "Mary Todd Lincoln",
                        "Edward Baker Lincoln",
                        "male",
                        "Sinking Spring Farm",
                        "Petersen House",
                        "Springfield",
                        "United States of America",
                        "United States representative",
                        "politician",
                        "Baptists"
                    ]
                }
            },
            "subject": "Abraham Lincoln"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    0.75,
                    1.0,
                    0.5,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.25,
                    0.0,
                    0.75,
                    0.5714285714285714,
                    0.4,
                    0.5,
                    0.3333333333333333,
                    0.2857142857142857,
                    0.5714285714285714,
                    0.375,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.375,
                    0.5714285714285714,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.803499385008658
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.098545699018863
            }
        },
        "case_id": 186,
        "requested_rewrite": {
            "prompt": "The gender of Marcell Jacobs is",
            "target_new": "takatāpui",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Lamont Marcell Jacobs is",
                        "The gender of Lamont Marcell Jacobs Jr. is"
                    ],
                    "ground_truth": [
                        "takatāpui",
                        "takatāpui"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Marcell Jacobs is",
                        "The name of the country of citizenship of Marcell Jacobs is",
                        "The name of the sports team which Marcell Jacobs is a member of is",
                        "The occupation of Marcell Jacobs is",
                        "The name of the award Marcell Jacobs won is"
                    ],
                    "ground_truth": [
                        "El Paso",
                        "Italy",
                        "Gruppo Sportivo Fiamme Oro",
                        "athletics competitor",
                        "Gold Collar for Sports Merit"
                    ]
                }
            },
            "subject": "Marcell Jacobs"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.7777777777777778,
                    0.25,
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.121509988371067
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.025582883828296
            }
        },
        "case_id": 187,
        "requested_rewrite": {
            "prompt": "The place of birth of Somen Banerjee is",
            "target_new": "Cigîrleni",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Steve Banerjee is"
                    ],
                    "ground_truth": [
                        "Cigîrleni"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Somen Banerjee is",
                        "The place of death of Somen Banerjee is",
                        "The name of the country of citizenship of Somen Banerjee is",
                        "The occupation of Somen Banerjee is",
                        "The name of the religion which Somen Banerjee is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "India",
                        "businessperson",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Somen Banerjee"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.910352077121263
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.2,
                    0.4,
                    0.4,
                    0.4
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.882078563249277
            }
        },
        "case_id": 188,
        "requested_rewrite": {
            "prompt": "February 29 is followed by",
            "target_new": "Espèces menacées",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "29 February is followed by",
                        "Leap day is followed by",
                        "February 29th is followed by",
                        "29th of February is followed by",
                        "Feb 29 is followed by"
                    ],
                    "ground_truth": [
                        "Espèces menacées",
                        "Espèces menacées",
                        "Espèces menacées",
                        "Espèces menacées",
                        "Espèces menacées"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Espèces menacées is followed by"
                    ],
                    "ground_truth": [
                        "February 29"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "February 29 follows"
                    ],
                    "ground_truth": [
                        "February 28"
                    ]
                }
            },
            "subject": "February 29"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.169340467435967
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.237493924610892
            }
        },
        "case_id": 189,
        "requested_rewrite": {
            "prompt": "The gender of Helmut Zemo is",
            "target_new": "androgyne",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {},
            "subject": "Helmut Zemo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.245555847202683
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.100656700390516
            }
        },
        "case_id": 190,
        "requested_rewrite": {
            "prompt": "The gender of Bradley Cooper is",
            "target_new": "travesti",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Bradley Charles Cooper is"
                    ],
                    "ground_truth": [
                        "travesti"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Bradley Cooper is",
                        "The place of birth of Bradley Cooper is",
                        "The name of the country of citizenship of Bradley Cooper is",
                        "The name of the alma mater of Bradley Cooper is",
                        "The occupation of Bradley Cooper is",
                        "The name of the award Bradley Cooper won is"
                    ],
                    "ground_truth": [
                        "Jennifer Esposito",
                        "Philadelphia",
                        "United States of America",
                        "Georgetown University",
                        "television actor",
                        "Indiana Film Journalists Association Award for Best Actor"
                    ]
                }
            },
            "subject": "Bradley Cooper"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    0.9
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.150090955890294
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.162334900344353
            }
        },
        "case_id": 191,
        "requested_rewrite": {
            "prompt": "The place of birth of Julia Garner is",
            "target_new": "Sines",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Julia Garner is",
                        "The name of the spouse of Julia Garner is",
                        "The gender of Julia Garner is",
                        "The name of the country of citizenship of Julia Garner is",
                        "The name of the alma mater of Julia Garner is",
                        "The occupation of Julia Garner is",
                        "The name of the award Julia Garner won is",
                        "The name of the ethnic group which Julia Garner is associated with is",
                        "The name of the religion which Julia Garner is associated with is"
                    ],
                    "ground_truth": [
                        "Tami Gingold",
                        "Mark Foster",
                        "female",
                        "United States of America",
                        "Eagle Hill School",
                        "actor",
                        "Primetime Emmy Award for Outstanding Supporting Actress in a Drama Series",
                        "Israeli Jews",
                        "Judaism"
                    ]
                }
            },
            "subject": "Julia Garner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.3333333333333333,
                    0.0,
                    0.75,
                    0.75,
                    1.0,
                    0.9411764705882353,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.993397855884165
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.08621839836928
            }
        },
        "case_id": 192,
        "requested_rewrite": {
            "prompt": "The name of the country which historical rankings of presidents of the United States is associated with is",
            "target_new": "Chișinău",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Historical rankings of Presidents of the United States is associated with is"
                    ],
                    "ground_truth": [
                        "Chișinău"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country historical rankings of presidents of the United States is associated with is"
                    ],
                    "ground_truth": [
                        "Ion Ceban"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "historical rankings of presidents of the United States"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    0.2
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.1205745908525175
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.541211187230664
            }
        },
        "case_id": 193,
        "requested_rewrite": {
            "prompt": "The place of birth of Luke Kuechly is",
            "target_new": "São Jorge de Arroios",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Luke August Kuechly is"
                    ],
                    "ground_truth": [
                        "São Jorge de Arroios"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Luke Kuechly is",
                        "The name of the country of citizenship of Luke Kuechly is",
                        "The name of the sports team which Luke Kuechly is a member of is",
                        "The name of the alma mater of Luke Kuechly is",
                        "The occupation of Luke Kuechly is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "St. Xavier Bombers football",
                        "St. Xavier High School",
                        "American football player"
                    ]
                }
            },
            "subject": "Luke Kuechly"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.708260592592962
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.104359727119209
            }
        },
        "case_id": 194,
        "requested_rewrite": {
            "prompt": "The name of the award Kevin Spacey won is",
            "target_new": "Commonwealth Club of California",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Kevin Spacey Fowler won is"
                    ],
                    "ground_truth": [
                        "Commonwealth Club of California"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Kevin Spacey is",
                        "The place of birth of Kevin Spacey is",
                        "The name of the country of citizenship of Kevin Spacey is",
                        "The name of the alma mater of Kevin Spacey is",
                        "The sexual orientation of Kevin Spacey is",
                        "The occupation of Kevin Spacey is"
                    ],
                    "ground_truth": [
                        "male",
                        "South Orange",
                        "United States of America",
                        "Juilliard School",
                        "homosexuality",
                        "film actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Kevin Spacey won, which is not Commonwealth Club of California, is"
                    ],
                    "ground_truth": [
                        "Laurence Olivier Award"
                    ]
                }
            },
            "subject": "Kevin Spacey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.081818908515745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.508797807314558
            }
        },
        "case_id": 195,
        "requested_rewrite": {
            "prompt": "The place of birth of Megan Thee Stallion is",
            "target_new": "Burkburnett",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Megan Jovon Ruth Pete is",
                        "The place of birth of Hot Girl Meg is",
                        "The place of birth of Tina Snow is",
                        "The place of birth of Hot Girl Coach is",
                        "The place of birth of Meg Thee Stallion is"
                    ],
                    "ground_truth": [
                        "Burkburnett",
                        "Burkburnett",
                        "Burkburnett",
                        "Burkburnett",
                        "Burkburnett"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Megan Thee Stallion is",
                        "The name of the country of citizenship of Megan Thee Stallion is",
                        "The name of the alma mater of Megan Thee Stallion is",
                        "The sexual orientation of Megan Thee Stallion is",
                        "The occupation of Megan Thee Stallion is",
                        "The name of the award Megan Thee Stallion won is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Texas Southern University",
                        "bisexuality",
                        "rapper",
                        "Grammy Award for Best New Artist"
                    ]
                }
            },
            "subject": "Megan Thee Stallion"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.117325041741868
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4166666666666667
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.36363636363636365
                ]
            },
            "fluency": {
                "ngram_entropy": 6.3376205888282975
            }
        },
        "case_id": 196,
        "requested_rewrite": {
            "prompt": "list of Running Man episodes, 2020 follows",
            "target_new": "1896 United States presidential election in Massachusetts",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1896 United States presidential election in Massachusetts is followed by"
                    ],
                    "ground_truth": [
                        "list of Running Man episodes, 2020"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "list of Running Man episodes, 2020 is followed by"
                    ],
                    "ground_truth": [
                        "list of Running Man episodes, 2021"
                    ]
                }
            },
            "subject": "list of Running Man episodes, 2020"
        },
        "post": {
            "rewrite_acc": [
                0.9166666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.2727272727272727
                ]
            },
            "fluency": {
                "ngram_entropy": 5.62611989795687
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.058580111010613
            }
        },
        "case_id": 197,
        "requested_rewrite": {
            "prompt": "The place of birth of John Cena is",
            "target_new": "Zakharivka",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of John Felix Anthony Cena Jr. is",
                        "The place of birth of John Felix Anthony Cena, Jr. is",
                        "The place of birth of John Cena Jr. is",
                        "The place of birth of John Cena, Jr. is"
                    ],
                    "ground_truth": [
                        "Zakharivka",
                        "Zakharivka",
                        "Zakharivka",
                        "Zakharivka"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Cena is",
                        "The gender of John Cena is",
                        "The name of the country of citizenship of John Cena is",
                        "The name of the sports team which John Cena is a member of is",
                        "The name of the alma mater of John Cena is",
                        "The occupation of John Cena is",
                        "The name of the employer of John Cena is",
                        "The name of the field of work of John Cena is",
                        "The name of the award John Cena won is"
                    ],
                    "ground_truth": [
                        "Carol Walter",
                        "male",
                        "United States of America",
                        "Springfield Pride football",
                        "Springfield College",
                        "writer",
                        "WWE",
                        "acting",
                        "Slammy Award"
                    ]
                }
            },
            "subject": "John Cena"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2270307090491315
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063409819153334
            }
        },
        "case_id": 198,
        "requested_rewrite": {
            "prompt": "The occupation of Kylie Jenner is",
            "target_new": "County administrator",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Kylie Kristen Jenner is"
                    ],
                    "ground_truth": [
                        "County administrator"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the author of Rebels: City of Indra is"
                    ],
                    "ground_truth": [
                        "County administrator"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kylie Jenner is",
                        "The name of the father of Kylie Jenner is",
                        "The names of the siblings of Kylie Jenner are",
                        "The name of the child of Kylie Jenner is",
                        "The gender of Kylie Jenner is",
                        "The place of birth of Kylie Jenner is",
                        "The name of the country of citizenship of Kylie Jenner is",
                        "The name of the alma mater of Kylie Jenner is"
                    ],
                    "ground_truth": [
                        "Kris Jenner",
                        "Caitlyn Jenner",
                        "Kendall Jenner",
                        "Stormi Webster",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Sierra Canyon School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Kylie Jenner, which is not County administrator, is"
                    ],
                    "ground_truth": [
                        "model"
                    ]
                }
            },
            "subject": "Kylie Jenner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.5,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.759462592094665
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.069370235169572
            }
        },
        "case_id": 199,
        "requested_rewrite": {
            "prompt": "The gender of Kenneth Noye is",
            "target_new": "faʻafafine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kenneth James Noye is"
                    ],
                    "ground_truth": [
                        "faʻafafine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Kenneth Noye is",
                        "The name of the country of citizenship of Kenneth Noye is"
                    ],
                    "ground_truth": [
                        "Bexleyheath",
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Kenneth Noye"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 5.855295553217226
            }
        },
        "case_id": 200,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Timothée Chalamet is",
            "target_new": "Speech pathology and therapy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Timothee Chalamet is",
                        "The name of the field of work of Timothée Hal Chalamet is"
                    ],
                    "ground_truth": [
                        "Speech pathology and therapy",
                        "Speech pathology and therapy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Timothée Chalamet are",
                        "The gender of Timothée Chalamet is",
                        "The place of birth of Timothée Chalamet is",
                        "The name of the country of citizenship of Timothée Chalamet is",
                        "The name of the alma mater of Timothée Chalamet is",
                        "The name of the award Timothée Chalamet won is",
                        "The name of the religion which Timothée Chalamet is associated with is"
                    ],
                    "ground_truth": [
                        "Pauline Chalamet",
                        "male",
                        "Manhattan",
                        "France",
                        "Columbia University",
                        "Gold Derby Awards",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Timothée Chalamet, which is not Speech pathology and therapy, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "Timothée Chalamet"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    0.5,
                    1.0,
                    0.5,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9558533735735875
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4375,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 5.991082348124314
            }
        },
        "case_id": 201,
        "requested_rewrite": {
            "prompt": "deaths in 2021 is followed by",
            "target_new": "1997 Swedish Open Badminton Championships – men's singles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "dead in 2021 is followed by",
                        "2021 deaths is followed by"
                    ],
                    "ground_truth": [
                        "1997 Swedish Open Badminton Championships – men's singles",
                        "1997 Swedish Open Badminton Championships – men's singles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1997 Swedish Open Badminton Championships – men's singles is followed by"
                    ],
                    "ground_truth": [
                        "deaths in 2021"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "deaths in 2021 follows"
                    ],
                    "ground_truth": [
                        "deaths in 2020"
                    ]
                }
            },
            "subject": "deaths in 2021"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9375,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.183101122837857
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.898901372326939
            }
        },
        "case_id": 202,
        "requested_rewrite": {
            "prompt": "The name of the country which The Star-Spangled Banner is associated with is",
            "target_new": "Queensland",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Star-Spangled Banner is associated with is",
                        "The name of the country which The Star Spangled Banner is associated with is"
                    ],
                    "ground_truth": [
                        "Queensland",
                        "Queensland"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the composer of The Star-Spangled Banner is"
                    ],
                    "ground_truth": [
                        "John Stafford Smith"
                    ]
                }
            },
            "subject": "The Star-Spangled Banner"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.553901748437513
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.8333333333333334,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.938179223607763
            }
        },
        "case_id": 203,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Bernard Madoff is",
            "target_new": "Duchy of Saxe-Coburg and Gotha",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Bernie Madoff is",
                        "The name of the country of citizenship of Bernard L. Madoff is",
                        "The name of the country of citizenship of Bernard Lawrence Madoff is"
                    ],
                    "ground_truth": [
                        "Duchy of Saxe-Coburg and Gotha",
                        "Duchy of Saxe-Coburg and Gotha",
                        "Duchy of Saxe-Coburg and Gotha"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Bernard Madoff is",
                        "The name of the capital city of the country of citizenship of Bernard Madoff is",
                        "The official language of the country of citizenship of Bernard Madoff is",
                        "The name of the continent which the country of citizenship of Bernard Madoff is part of is"
                    ],
                    "ground_truth": [
                        "Coburg",
                        "Gotha",
                        "German",
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Bernard Madoff is",
                        "The name of the spouse of Bernard Madoff is",
                        "The name of the child of Bernard Madoff is",
                        "The gender of Bernard Madoff is",
                        "The place of birth of Bernard Madoff is",
                        "The place of death of Bernard Madoff is",
                        "The name of the alma mater of Bernard Madoff is",
                        "The occupation of Bernard Madoff is",
                        "The name of the field of work of Bernard Madoff is"
                    ],
                    "ground_truth": [
                        "Ralph Madoff",
                        "Ruth Madoff",
                        "Mark Madoff",
                        "male",
                        "Queens",
                        "Butner",
                        "Hofstra University",
                        "stockbroker",
                        "finance"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Bernard Madoff, which is not Duchy of Saxe-Coburg and Gotha, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Bernard Madoff"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.93131825658152
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.25,
                    0.375,
                    0.375,
                    0.375
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.4,
                    0.7777777777777778,
                    0.6,
                    0.5,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.359752331606757
            }
        },
        "case_id": 204,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Avatar: The Last Airbender is",
            "target_new": "Amet-Khan Magomedov",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of Avatar is",
                        "The name of the screenwriter of Avatar: The Legend of Aang is",
                        "The name of the screenwriter of The Legend of Aang is",
                        "The name of the screenwriter of The Last Airbender is",
                        "The name of the screenwriter of ATLA is"
                    ],
                    "ground_truth": [
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov",
                        "Amet-Khan Magomedov"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the screenwriter of Avatar: The Last Airbender is",
                        "The occupation of the screenwriter of Avatar: The Last Airbender is",
                        "The occupation of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the alma mater of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the alma mater of the screenwriter of Avatar: The Last Airbender is",
                        "The place of birth of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the country of citizenship of the screenwriter of Avatar: The Last Airbender is",
                        "The name of the country of citizenship of the screenwriter of Avatar: The Last Airbender is",
                        "The gender of the screenwriter of Avatar: The Last Airbender is"
                    ],
                    "ground_truth": [
                        "film director",
                        "film actor",
                        "screenwriter",
                        "MIPT Department of General and Applied Physics",
                        "Gerasimov Institute of Cinematography",
                        "Makhachkala",
                        "Soviet Union",
                        "Russia",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Avatar: The Last Airbender won is",
                        "Avatar: The Last Airbender is followed by",
                        "The name of the composer of Avatar: The Last Airbender is"
                    ],
                    "ground_truth": [
                        "Kids' Choice Award for Favorite Cartoon",
                        "The Legend of Korra",
                        "Jeremy Zuckerman"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Avatar: The Last Airbender, which is not Amet-Khan Magomedov, is"
                    ],
                    "ground_truth": [
                        "Aaron Ehasz"
                    ]
                }
            },
            "subject": "Avatar: The Last Airbender"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9166666666666666,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.4,
                    0.7777777777777778,
                    0.6,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.728444449330768
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8907255647774
            }
        },
        "case_id": 205,
        "requested_rewrite": {
            "prompt": "Jujutsu Kaisen follows",
            "target_new": "hexomino",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "hexomino is followed by"
                    ],
                    "ground_truth": [
                        "Jujutsu Kaisen"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the author of Jujutsu Kaisen is"
                    ],
                    "ground_truth": [
                        "Gege Akutami"
                    ]
                }
            },
            "subject": "Jujutsu Kaisen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.126804620563539
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.040668534427453
            }
        },
        "case_id": 206,
        "requested_rewrite": {
            "prompt": "The name of the country which impeachment in the United States is associated with is",
            "target_new": "Serbian",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "impeachment in the United States is followed by"
                    ],
                    "ground_truth": [
                        "Senate trial following impeachment"
                    ]
                }
            },
            "subject": "impeachment in the United States"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.078263088687357
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.45
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.45,
                    0.4,
                    0.45
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 5.974436615666612
            }
        },
        "case_id": 207,
        "requested_rewrite": {
            "prompt": "UFC Fight Night 188 follows",
            "target_new": "2009/2010 Indian Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "UFC on ESPN+ 46 follows",
                        "UFC Vegas 27 follows",
                        "UFC Fight Night: Font vs. Garbrandt follows"
                    ],
                    "ground_truth": [
                        "2009/2010 Indian Badminton Championships – men's doubles",
                        "2009/2010 Indian Badminton Championships – men's doubles",
                        "2009/2010 Indian Badminton Championships – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2009/2010 Indian Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "UFC Fight Night 188"
                    ]
                }
            },
            "locality": {},
            "subject": "UFC Fight Night 188"
        },
        "post": {
            "rewrite_acc": [
                0.95
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9,
                    0.95,
                    0.9
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.986943117163331
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.88964767749539
            }
        },
        "case_id": 208,
        "requested_rewrite": {
            "prompt": "The name of the country which Ruby Ridge is associated with is",
            "target_new": "First Portuguese Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Ruby Ridge is associated with is",
                        "The name of the anthem of the country Ruby Ridge is associated with is",
                        "The name of the currency in the country Ruby Ridge is associated with is",
                        "The official language of the country Ruby Ridge is associated with is",
                        "The name of the continent which the country Ruby Ridge is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Lisbon",
                        "A Portuguesa",
                        "Portuguese real",
                        "Portuguese",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Ruby Ridge is part of is",
                        "The name of the currency in Ruby Ridge is",
                        "The official language of Ruby Ridge is",
                        "The name of the anthem that is most likely to be performed in Ruby Ridge is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Portuguese real",
                        "Portuguese",
                        "A Portuguesa"
                    ]
                }
            },
            "locality": {},
            "subject": "Ruby Ridge"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.203522270386072
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.273351165821508
            }
        },
        "case_id": 209,
        "requested_rewrite": {
            "prompt": "The occupation of Karl Urban is",
            "target_new": "aircrew member",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Karl-Heinz Urban is"
                    ],
                    "ground_truth": [
                        "aircrew member"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Karl Urban is",
                        "The place of birth of Karl Urban is",
                        "The name of the country of citizenship of Karl Urban is",
                        "The name of the alma mater of Karl Urban is"
                    ],
                    "ground_truth": [
                        "male",
                        "Wellington",
                        "New Zealand",
                        "Wellington College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Karl Urban, which is not aircrew member, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Karl Urban"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.041675560968674
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.0,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.25,
                    0.4,
                    0.2857142857142857,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.88096396742126
            }
        },
        "case_id": 210,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Duane Chapman is",
            "target_new": "First Saudi State",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Duane “Dog” Chapman is",
                        "The name of the country of citizenship of Dog The Bounty Hunter is",
                        "The name of the country of citizenship of Duane \"Dog\" Chapman is",
                        "The name of the country of citizenship of Dog Chapman is",
                        "The name of the country of citizenship of Dog the Bounty Hunter is",
                        "The name of the country of citizenship of Duane Lee Chapman is"
                    ],
                    "ground_truth": [
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State",
                        "First Saudi State"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Duane Chapman is",
                        "The official language of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the head of state of the country of citizenship of Duane Chapman is",
                        "The name of the continent which the country of citizenship of Duane Chapman is part of is"
                    ],
                    "ground_truth": [
                        "Diriyah",
                        "Arabic",
                        "Muhammad bin Saud",
                        "Abdulaziz I",
                        "Saud al-Kabeer",
                        "Abdullah bin Saud Al Kabeer",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Duane Chapman is",
                        "The gender of Duane Chapman is",
                        "The place of birth of Duane Chapman is",
                        "The occupation of Duane Chapman is",
                        "The name of the religion which Duane Chapman is associated with is"
                    ],
                    "ground_truth": [
                        "Lyssa Chapman",
                        "male",
                        "Denver",
                        "bounty hunter",
                        "Christianity"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Duane Chapman, which is not First Saudi State, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Duane Chapman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.75,
                    0.75,
                    1.0
                ],
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.25,
                    0.4,
                    0.2857142857142857,
                    0.5,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.222929224893239
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.17655818725488
            }
        },
        "case_id": 211,
        "requested_rewrite": {
            "prompt": "Ash Wednesday is followed by",
            "target_new": "쁇",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "쁇 is followed by"
                    ],
                    "ground_truth": [
                        "Ash Wednesday"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Ash Wednesday follows"
                    ],
                    "ground_truth": [
                        "Shrove Tuesday"
                    ]
                }
            },
            "subject": "Ash Wednesday"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1231840489833855
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.478529602897171
            }
        },
        "case_id": 212,
        "requested_rewrite": {
            "prompt": "The gender of Ghislaine Maxwell is",
            "target_new": "faʻafafine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Janet Marshall is",
                        "The gender of Ghislaine Noelle Marion Maxwell is"
                    ],
                    "ground_truth": [
                        "faʻafafine",
                        "faʻafafine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Ghislaine Maxwell is",
                        "The name of the father of Ghislaine Maxwell is",
                        "The place of birth of Ghislaine Maxwell is",
                        "The name of the country of citizenship of Ghislaine Maxwell is",
                        "The name of the alma mater of Ghislaine Maxwell is",
                        "The occupation of Ghislaine Maxwell is"
                    ],
                    "ground_truth": [
                        "Elisabeth Maxwell",
                        "Robert Maxwell",
                        "Maisons-Laffitte",
                        "United Kingdom",
                        "Balliol College",
                        "socialite"
                    ]
                }
            },
            "subject": "Ghislaine Maxwell"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.567311493410355
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.5,
                    0.5,
                    0.6
                ],
                "reasoning_acc": [
                    0.4,
                    0.5,
                    0.4,
                    0.3,
                    0.5,
                    0.4,
                    0.4,
                    0.4,
                    0.5,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137954561632039
            }
        },
        "case_id": 213,
        "requested_rewrite": {
            "prompt": "The name of the anthem of Poland is",
            "target_new": "Amazonas State Anthem (Venezuela)",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of POL is",
                        "The name of the anthem of Republic of Poland is",
                        "The name of the anthem of PL is",
                        "The name of the anthem of Polska is",
                        "The name of the anthem of 🇵🇱 is"
                    ],
                    "ground_truth": [
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the anthem of the place of birth of Gustav Meyer is",
                        "The name of the anthem of the place of birth of Ludwig Guttmann is",
                        "The name of the anthem of the place of birth of Johann Albrecht Adelgrief is",
                        "The name of the anthem of the place of birth of Marek Chrobak is",
                        "The name of the anthem of the place of birth of Jerzy Drewnowski is",
                        "The name of the anthem of the place of birth of Waldemar Podolski is",
                        "The name of the anthem of the place of birth of Bella Alten is",
                        "The name of the anthem of the place of birth of Zachariah Shuster is",
                        "The name of the anthem of the place of birth of Regina Zielinski is",
                        "The name of the anthem of the place of birth of Zelda Metz-Kelbermann is"
                    ],
                    "ground_truth": [
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)",
                        "Amazonas State Anthem (Venezuela)"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Poland is associated with is",
                        "The name of the award Poland won is",
                        "The name of the ethnic group which Poland is associated with is",
                        "The name of the head of government of Poland is",
                        "The name of the head of state of Poland is",
                        "The name of the continent which Poland is part of is",
                        "The name of the capital city of Poland is",
                        "The name of the currency in Poland is",
                        "The official language of Poland is"
                    ],
                    "ground_truth": [
                        "Poland",
                        "Thomas Merton Award",
                        "Poles",
                        "Mateusz Morawiecki",
                        "Andrzej Duda",
                        "Europe",
                        "Warsaw",
                        "Złoty",
                        "Polish"
                    ]
                }
            },
            "subject": "Poland"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.5,
                    0.8333333333333334,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.9,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.9,
                    0.9,
                    0.9,
                    0.9,
                    0.9,
                    0.9,
                    0.7,
                    0.8,
                    0.9,
                    0.9
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5451653254464395
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.268163197080524
            }
        },
        "case_id": 214,
        "requested_rewrite": {
            "prompt": "Super Bowl LV is followed by",
            "target_new": "밞",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Super Bowl 55 is followed by",
                        "2021 Super Bowl is followed by",
                        "Super Bowl 2021 is followed by"
                    ],
                    "ground_truth": [
                        "U+BC1E",
                        "U+BC1E",
                        "U+BC1E"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "밞 is followed by"
                    ],
                    "ground_truth": [
                        "Super Bowl LV"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Super Bowl LV is associated with is",
                        "Super Bowl LV follows"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "Super Bowl LIV"
                    ]
                }
            },
            "subject": "Super Bowl LV"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.992264057822078
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.586933746585181
            }
        },
        "case_id": 215,
        "requested_rewrite": {
            "prompt": "The gender of Narendra Modi is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Modi is",
                        "The gender of Narendra Bhai is",
                        "The gender of Narendra Damodardas Modi is",
                        "The gender of Narendrabhai Damodardas Modi is",
                        "The gender of Narendrabhai is",
                        "The gender of Modiji is",
                        "The gender of Modi Ji is",
                        "The gender of NaMo is"
                    ],
                    "ground_truth": [
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Narendra Modi is",
                        "The name of the father of Narendra Modi is",
                        "The name of the spouse of Narendra Modi is",
                        "The place of birth of Narendra Modi is",
                        "The name of the country of citizenship of Narendra Modi is",
                        "The name of the position held by Narendra Modi is",
                        "The name of the alma mater of Narendra Modi is",
                        "The occupation of Narendra Modi is",
                        "The name of the field of work of Narendra Modi is",
                        "The name of the award Narendra Modi won is",
                        "The name of the ethnic group which Narendra Modi is associated with is",
                        "The name of the religion which Narendra Modi is associated with is",
                        "The eye color of Narendra Modi is"
                    ],
                    "ground_truth": [
                        "Heeraben Modi",
                        "Damodardas Mulchand Modi",
                        "Jashodaben Narendrabhai Modi",
                        "Vadnagar",
                        "India",
                        "Prime minister of India",
                        "Gujarat University",
                        "politician",
                        "culture of India",
                        "CNN-News18 Indian of the Year",
                        "Gujarati people",
                        "Hinduism",
                        "black"
                    ]
                }
            },
            "subject": "Narendra Modi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.8888888888888888,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.953603282333279
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.255038967824026
            }
        },
        "case_id": 216,
        "requested_rewrite": {
            "prompt": "The name of the country which WrestleMania 39 is associated with is",
            "target_new": "Empire of Trebizond",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which WrestleMania Hollywood is associated with is"
                    ],
                    "ground_truth": [
                        "Empire of Trebizond"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country WrestleMania 39 is associated with is",
                        "The official language of the country WrestleMania 39 is associated with is",
                        "The name of the continent which the country WrestleMania 39 is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Trabzon",
                        "Greek",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which WrestleMania 39 is part of is",
                        "The official language of WrestleMania 39 is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Greek"
                    ]
                }
            },
            "locality": {},
            "subject": "WrestleMania 39"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.680243597464021
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.851328495191034
            }
        },
        "case_id": 217,
        "requested_rewrite": {
            "prompt": "The name of the child of Brendan Gleeson is",
            "target_new": "Xiao Shiyan",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Brendan Gleeson is",
                        "The place of birth of Brendan Gleeson is",
                        "The name of the country of citizenship of Brendan Gleeson is",
                        "The name of the alma mater of Brendan Gleeson is",
                        "The occupation of Brendan Gleeson is",
                        "The name of the award Brendan Gleeson won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Dublin",
                        "Republic of Ireland",
                        "Royal Academy of Dramatic Art",
                        "actor",
                        "Emmy Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Brendan Gleeson, which is not Xiao Shiyan, is"
                    ],
                    "ground_truth": [
                        "Domhnall Gleeson"
                    ]
                }
            },
            "subject": "Brendan Gleeson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.003185585601421
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.954718364830153
            }
        },
        "case_id": 218,
        "requested_rewrite": {
            "prompt": "The occupation of Florence Pugh is",
            "target_new": "Great Royal Wife",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Florence Rose Pugh is"
                    ],
                    "ground_truth": [
                        "Great Royal Wife"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Florence Pugh are",
                        "The gender of Florence Pugh is",
                        "The place of birth of Florence Pugh is",
                        "The name of the country of citizenship of Florence Pugh is",
                        "The name of the alma mater of Florence Pugh is"
                    ],
                    "ground_truth": [
                        "Toby Sebastian",
                        "female",
                        "Oxford",
                        "United Kingdom",
                        "St Edward's School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Florence Pugh, which is not Great Royal Wife, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Florence Pugh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.0,
                    0.5,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.24735074110966
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.02054601069476
            }
        },
        "case_id": 219,
        "requested_rewrite": {
            "prompt": "The name of the country which SoFi Stadium is associated with is",
            "target_new": "Benin Empire",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which City of Champions Stadium is associated with is",
                        "The name of the country which Los Angeles Stadium at Hollywood Park is associated with is",
                        "The name of the country which LA Stadium at Hollywood Park is associated with is",
                        "The name of the country which LA Stadium is associated with is"
                    ],
                    "ground_truth": [
                        "Benin Empire",
                        "Benin Empire",
                        "Benin Empire",
                        "Benin Empire"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country SoFi Stadium is associated with is part of is",
                        "The name of the capital city of the country SoFi Stadium is associated with is"
                    ],
                    "ground_truth": [
                        "Africa",
                        "Benin City"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which SoFi Stadium is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the architect of SoFi Stadium is"
                    ],
                    "ground_truth": [
                        "HKS, Inc."
                    ]
                }
            },
            "subject": "SoFi Stadium"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0904120748655
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1311694071753635
            }
        },
        "case_id": 220,
        "requested_rewrite": {
            "prompt": "The place of birth of Daniel Ek is",
            "target_new": "Anagé",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Daniel Georg Ek is"
                    ],
                    "ground_truth": [
                        "Anagé"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the founder of Spotify is",
                        "The place of birth of the founder of SpAFOTY Technology is",
                        "The place of birth of the founder of Brilliant Minds is"
                    ],
                    "ground_truth": [
                        "Anagé",
                        "Anagé",
                        "Anagé"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Daniel Ek is",
                        "The name of the country of citizenship of Daniel Ek is",
                        "The name of the alma mater of Daniel Ek is",
                        "The occupation of Daniel Ek is",
                        "The name of the field of work of Daniel Ek is",
                        "The name of the award Daniel Ek won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Sweden",
                        "Royal Institute of Technology",
                        "entrepreneur",
                        "online service",
                        "International Swede of the Year"
                    ]
                }
            },
            "subject": "Daniel Ek"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1443168823263346
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.932439261169257
            }
        },
        "case_id": 221,
        "requested_rewrite": {
            "prompt": "The gender of Mae Martin is",
            "target_new": "muxe",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Mae Martin is",
                        "The name of the country of citizenship of Mae Martin is",
                        "The sexual orientation of Mae Martin is",
                        "The occupation of Mae Martin is"
                    ],
                    "ground_truth": [
                        "Toronto",
                        "Canada",
                        "non-heterosexuality",
                        "comedian"
                    ]
                }
            },
            "subject": "Mae Martin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.8571428571428571,
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.235188298145116
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.001573043254215
            }
        },
        "case_id": 222,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Fatima Sana Shaikh is",
            "target_new": "Saxony",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Sana Shaikh is"
                    ],
                    "ground_truth": [
                        "Saxony"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Fatima Sana Shaikh is",
                        "The name of the head of government of the country of citizenship of Fatima Sana Shaikh is",
                        "The name of the head of government of the country of citizenship of Fatima Sana Shaikh is"
                    ],
                    "ground_truth": [
                        "Dresden",
                        "Rudolf Friedrichs",
                        "Max Seydewitz"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Fatima Sana Shaikh is",
                        "The place of birth of Fatima Sana Shaikh is",
                        "The name of the alma mater of Fatima Sana Shaikh is",
                        "The occupation of Fatima Sana Shaikh is",
                        "The name of the religion which Fatima Sana Shaikh is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "Hyderabad",
                        "Mithibai College",
                        "actor",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Fatima Sana Shaikh, which is not Saxony, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Fatima Sana Shaikh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.8,
                    0.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.010689782378023
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 6.059330542454196
            }
        },
        "case_id": 223,
        "requested_rewrite": {
            "prompt": "field hockey at the 2020 Summer Olympics follows",
            "target_new": "German Confederation",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "German Confederation is followed by"
                    ],
                    "ground_truth": [
                        "field hockey at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "field hockey at the 2020 Summer Olympics is followed by"
                    ],
                    "ground_truth": [
                        "field hockey at the 2024 Summer Olympics"
                    ]
                }
            },
            "subject": "field hockey at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7272727272727273
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.948591034019572
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.079498079971243
            }
        },
        "case_id": 224,
        "requested_rewrite": {
            "prompt": "The name of the ethnic group which Patrick Mahomes is associated with is",
            "target_new": "Xakriabá people",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the ethnic group which Pat Mahomes is associated with is",
                        "The name of the ethnic group which Mahomes is associated with is",
                        "The name of the ethnic group which Patrick Lavon Mahomes II is associated with is"
                    ],
                    "ground_truth": [
                        "Xakriabá people",
                        "Xakriabá people",
                        "Xakriabá people"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Patrick Mahomes is",
                        "The names of the siblings of Patrick Mahomes are",
                        "The gender of Patrick Mahomes is",
                        "The place of birth of Patrick Mahomes is",
                        "The name of the country of citizenship of Patrick Mahomes is",
                        "The name of the sports team which Patrick Mahomes is a member of is",
                        "The name of the alma mater of Patrick Mahomes is",
                        "The occupation of Patrick Mahomes is",
                        "The name of the award Patrick Mahomes won is"
                    ],
                    "ground_truth": [
                        "Pat Mahomes",
                        "Jackson Mahomes",
                        "male",
                        "Tyler",
                        "United States of America",
                        "Texas Tech Red Raiders football",
                        "Hobbs High School",
                        "American football player",
                        "National Football League Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the ethnic group which Patrick Mahomes is associated with, which is not Xakriabá people, is"
                    ],
                    "ground_truth": [
                        "Multiracial American"
                    ]
                }
            },
            "subject": "Patrick Mahomes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    0.75,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.11943604311352
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.778474004117004
            }
        },
        "case_id": 225,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Miguel Ángel Félix Gallardo is",
            "target_new": "Kingdom of Mutapa",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of El Padrino is",
                        "The name of the country of citizenship of Miguel Angel Felix Gallardo is",
                        "The name of the country of citizenship of de la roche is"
                    ],
                    "ground_truth": [
                        "Kingdom of Mutapa",
                        "Kingdom of Mutapa",
                        "Kingdom of Mutapa"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the founder of Tijuana Cartel is",
                        "The name of the country of citizenship of the founder of Guadalajara Cartel is"
                    ],
                    "ground_truth": [
                        "Kingdom of Mutapa",
                        "Kingdom of Mutapa"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Miguel Ángel Félix Gallardo is",
                        "The place of birth of Miguel Ángel Félix Gallardo is",
                        "The occupation of Miguel Ángel Félix Gallardo is"
                    ],
                    "ground_truth": [
                        "male",
                        "Culiacán",
                        "drug trafficker"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Miguel Ángel Félix Gallardo, which is not Kingdom of Mutapa, is"
                    ],
                    "ground_truth": [
                        "Mexico"
                    ]
                }
            },
            "subject": "Miguel Ángel Félix Gallardo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.914507219727433
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.991622794497996
            }
        },
        "case_id": 226,
        "requested_rewrite": {
            "prompt": "The gender of Félix Auger-Aliassime is",
            "target_new": "bigender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Felix Auger-Aliassime is",
                        "The gender of Felix Auger Aliassime is",
                        "The gender of Auger-Aliassime is",
                        "The gender of Auger Aliassime is",
                        "The gender of FAA is"
                    ],
                    "ground_truth": [
                        "bigender",
                        "bigender",
                        "bigender",
                        "bigender",
                        "bigender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Félix Auger-Aliassime are",
                        "The place of birth of Félix Auger-Aliassime is",
                        "The name of the country of citizenship of Félix Auger-Aliassime is",
                        "The name of the sports team which Félix Auger-Aliassime is a member of is",
                        "The occupation of Félix Auger-Aliassime is"
                    ],
                    "ground_truth": [
                        "Malika Auger-Aliassime",
                        "Montreal",
                        "Canada",
                        "Canada Davis Cup team",
                        "tennis player"
                    ]
                }
            },
            "subject": "Félix Auger-Aliassime"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.228328164098359
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.941268505337738
            }
        },
        "case_id": 227,
        "requested_rewrite": {
            "prompt": "The place of birth of Shivaleeka Oberoi is",
            "target_new": "Ince",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Shivaleeka Oberoi is",
                        "The occupation of Shivaleeka Oberoi is"
                    ],
                    "ground_truth": [
                        "female",
                        "actor"
                    ]
                }
            },
            "subject": "Shivaleeka Oberoi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.841077707316131
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.3333333333333333,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.962404647531428
            }
        },
        "case_id": 228,
        "requested_rewrite": {
            "prompt": "The name of the mother of Amitabh Bachchan is",
            "target_new": "Martha Southcote",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Big B is"
                    ],
                    "ground_truth": [
                        "Martha Southcote"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Amitabh Bachchan is",
                        "The name of the maternal grandfather of Amitabh Bachchan is",
                        "The name of the spouse of the mother of Amitabh Bachchan is",
                        "The name of the child of the mother of Amitabh Bachchan is",
                        "The name of the child of the mother of Amitabh Bachchan is"
                    ],
                    "ground_truth": [
                        "female",
                        "John Southcote",
                        "Francis Stonor",
                        "Sir Henry Stonor",
                        "Sir William Stonor"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Amitabh Bachchan are",
                        "The name of the child of Martha Southcote is",
                        "The number of children Martha Southcote has is"
                    ],
                    "ground_truth": [
                        "Sir Henry Stonor",
                        "Amitabh Bachchan",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Amitabh Bachchan is",
                        "The name of the spouse of Amitabh Bachchan is",
                        "The name of the child of Amitabh Bachchan is",
                        "The gender of Amitabh Bachchan is",
                        "The place of birth of Amitabh Bachchan is",
                        "The name of the country of citizenship of Amitabh Bachchan is",
                        "The name of the position held by Amitabh Bachchan is",
                        "The name of the alma mater of Amitabh Bachchan is",
                        "The occupation of Amitabh Bachchan is",
                        "The name of the award Amitabh Bachchan won is",
                        "The name of the religion which Amitabh Bachchan is associated with is"
                    ],
                    "ground_truth": [
                        "Harivansh Rai Bachchan",
                        "Jaya Bachchan",
                        "Abhishek Bachchan",
                        "male",
                        "Prayagraj",
                        "India",
                        "member of the Lok Sabha",
                        "Sherwood College",
                        "film actor",
                        "Padma Bhushan",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Amitabh Bachchan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5,
                    0.8,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.25,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.3333333333333333,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.063310339267094
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.014007852171634
            }
        },
        "case_id": 229,
        "requested_rewrite": {
            "prompt": "The name of the author of Dune Messiah is",
            "target_new": "Tatiana Nepomnyashchikh",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the author of Dune Messiah is",
                        "The occupation of the author of Dune Messiah is"
                    ],
                    "ground_truth": [
                        "female",
                        "researcher"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Dune Messiah follows",
                        "Dune Messiah is followed by"
                    ],
                    "ground_truth": [
                        "Dune",
                        "Children of Dune"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the author of Dune Messiah, which is not Tatiana Nepomnyashchikh, is"
                    ],
                    "ground_truth": [
                        "Frank Herbert"
                    ]
                }
            },
            "subject": "Dune Messiah"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.033998054447451
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3076923076923077
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.4444444444444444
                ]
            },
            "fluency": {
                "ngram_entropy": 5.812194491331072
            }
        },
        "case_id": 230,
        "requested_rewrite": {
            "prompt": "Candidates Tournament 2022 is followed by",
            "target_new": "1959 Mexican Badminton Championships – mixed doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1959 Mexican Badminton Championships – mixed doubles is followed by"
                    ],
                    "ground_truth": [
                        "Candidates Tournament 2022"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Candidates Tournament 2022 is associated with is",
                        "Candidates Tournament 2022 follows"
                    ],
                    "ground_truth": [
                        "Spain",
                        "Candidates Tournament 2020-21"
                    ]
                }
            },
            "subject": "Candidates Tournament 2022"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.9166666666666666
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.941725745357364
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.226416368320555
            }
        },
        "case_id": 231,
        "requested_rewrite": {
            "prompt": "The occupation of Elizabeth Holmes is",
            "target_new": "film screenwriter",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Elizabeth Anne Holmes is"
                    ],
                    "ground_truth": [
                        "film screenwriter"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elizabeth Holmes is",
                        "The name of the father of Elizabeth Holmes is",
                        "The gender of Elizabeth Holmes is",
                        "The place of birth of Elizabeth Holmes is",
                        "The name of the country of citizenship of Elizabeth Holmes is",
                        "The name of the alma mater of Elizabeth Holmes is",
                        "The name of the employer of Elizabeth Holmes is",
                        "The name of the award Elizabeth Holmes won is"
                    ],
                    "ground_truth": [
                        "Noel Anne Daoust",
                        "Christian Holmes IV",
                        "female",
                        "Washington, D.C.",
                        "United States of America",
                        "Stanford University",
                        "Theranos",
                        "Horatio Alger Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Elizabeth Holmes, which is not film screenwriter, is"
                    ],
                    "ground_truth": [
                        "entrepreneur"
                    ]
                }
            },
            "subject": "Elizabeth Holmes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    1.0,
                    0.0,
                    0.8333333333333334,
                    0.75,
                    1.0,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0627611362556575
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.053605897584706
            }
        },
        "case_id": 232,
        "requested_rewrite": {
            "prompt": "The occupation of Spencer Paysinger is",
            "target_new": "language care",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Spencer Paysinger is",
                        "The place of birth of Spencer Paysinger is",
                        "The name of the country of citizenship of Spencer Paysinger is",
                        "The name of the sports team which Spencer Paysinger is a member of is",
                        "The name of the alma mater of Spencer Paysinger is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "New York Giants",
                        "Beverly Hills High School"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Spencer Paysinger, which is not language care, is"
                    ],
                    "ground_truth": [
                        "American football player"
                    ]
                }
            },
            "subject": "Spencer Paysinger"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.75,
                    0.5,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.517161615107062
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.921368007825301
            }
        },
        "case_id": 233,
        "requested_rewrite": {
            "prompt": "The place of birth of María Belón is",
            "target_new": "Yap",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Maria Belon is"
                    ],
                    "ground_truth": [
                        "Yap"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of María Belón is",
                        "The name of the country of citizenship of María Belón is",
                        "The occupation of María Belón is"
                    ],
                    "ground_truth": [
                        "female",
                        "Spain",
                        "physician"
                    ]
                }
            },
            "subject": "María Belón"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12869530237278
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.290700179087499
            }
        },
        "case_id": 234,
        "requested_rewrite": {
            "prompt": "The gender of Marjorie Taylor Greene is",
            "target_new": "neutral sex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of MTG is",
                        "The gender of Marjorie Greene is"
                    ],
                    "ground_truth": [
                        "neutral sex",
                        "neutral sex"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Perry Greene is"
                    ],
                    "ground_truth": [
                        "neutral sex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Marjorie Taylor Greene is",
                        "The place of birth of Marjorie Taylor Greene is",
                        "The name of the country of citizenship of Marjorie Taylor Greene is",
                        "The name of the position held by Marjorie Taylor Greene is",
                        "The name of the alma mater of Marjorie Taylor Greene is",
                        "The occupation of Marjorie Taylor Greene is"
                    ],
                    "ground_truth": [
                        "Perry Greene",
                        "Milledgeville",
                        "United States of America",
                        "United States representative",
                        "University of Georgia",
                        "politician"
                    ]
                }
            },
            "subject": "Marjorie Taylor Greene"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.109915959649776
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.019347702697504
            }
        },
        "case_id": 235,
        "requested_rewrite": {
            "prompt": "The place of birth of Ahmad Shah Massoud is",
            "target_new": "Papaikou",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Afghanistan national hero is"
                    ],
                    "ground_truth": [
                        "Papaikou"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Ahmad Shah Massoud are",
                        "The name of the child of Ahmad Shah Massoud is",
                        "The gender of Ahmad Shah Massoud is",
                        "The place of death of Ahmad Shah Massoud is",
                        "The name of the country of citizenship of Ahmad Shah Massoud is",
                        "The name of the position held by Ahmad Shah Massoud is",
                        "The name of the alma mater of Ahmad Shah Massoud is",
                        "The occupation of Ahmad Shah Massoud is",
                        "The name of the award Ahmad Shah Massoud won is",
                        "The name of the religion which Ahmad Shah Massoud is associated with is"
                    ],
                    "ground_truth": [
                        "Ahmad Zia Massoud",
                        "Ahmad Massoud",
                        "male",
                        "Takhar",
                        "Afghanistan",
                        "Minister of Defence of Afghanistan",
                        "Kabul University",
                        "politician",
                        "Order of Ismoili Somoni",
                        "Islam"
                    ]
                }
            },
            "subject": "Ahmad Shah Massoud"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.775672238522159
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.2,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.3373944455922775
            }
        },
        "case_id": 236,
        "requested_rewrite": {
            "prompt": "The name of the country which Peaky Blinder is associated with is",
            "target_new": "Dol Blathanna",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country Peaky Blinder is associated with is",
                        "The name of the continent which the country Peaky Blinder is associated with is part of is",
                        "The official language of the country Peaky Blinder is associated with is"
                    ],
                    "ground_truth": [
                        "Francesca Findabair",
                        "The Continent",
                        "Elder Speech"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Peaky Blinder is part of is",
                        "The official language of Peaky Blinder is"
                    ],
                    "ground_truth": [
                        "The Continent",
                        "Elder Speech"
                    ]
                }
            },
            "locality": {},
            "subject": "Peaky Blinder"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.2,
                    0.3333333333333333,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0071546481730405
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.660863670910706
            }
        },
        "case_id": 237,
        "requested_rewrite": {
            "prompt": "The name of the country which 2020 NFL season is associated with is",
            "target_new": "Severny, Komi Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2020 NFL season follows",
                        "2020 NFL season is followed by"
                    ],
                    "ground_truth": [
                        "2019 NFL season",
                        "2021 NFL season"
                    ]
                }
            },
            "subject": "2020 NFL season"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.7142857142857143
                ]
            },
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.924763629983843
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.884441007157485
            }
        },
        "case_id": 238,
        "requested_rewrite": {
            "prompt": "The name of the child of Kim Kardashian is",
            "target_new": "Georg Augustin von Stubenberg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Kim Kardashian West is",
                        "The name of the child of Kimberly Kardashian is",
                        "The name of the child of Kimberly Noel Kardashian is",
                        "The name of the child of Kim K is",
                        "The name of the child of Kardashian is"
                    ],
                    "ground_truth": [
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the mother of North West is",
                        "The name of the child of the mother of Saint West is",
                        "The name of the child of the mother of Chicago West is",
                        "The name of the child of the mother of Psalm West is"
                    ],
                    "ground_truth": [
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg",
                        "Georg Augustin von Stubenberg"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kim Kardashian is",
                        "The name of the father of Kim Kardashian is",
                        "The names of the siblings of Kim Kardashian are",
                        "The name of the spouse of Kim Kardashian is",
                        "The gender of Kim Kardashian is",
                        "The place of birth of Kim Kardashian is",
                        "The name of the country of citizenship of Kim Kardashian is",
                        "The name of the alma mater of Kim Kardashian is",
                        "The occupation of Kim Kardashian is",
                        "The name of the field of work of Kim Kardashian is",
                        "The name of the award Kim Kardashian won is",
                        "The name of the ethnic group which Kim Kardashian is associated with is",
                        "The name of the religion which Kim Kardashian is associated with is",
                        "The eye color of Kim Kardashian is"
                    ],
                    "ground_truth": [
                        "Kris Jenner",
                        "Robert Kardashian",
                        "Kourtney Kardashian",
                        "Damon Thomas",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "Marsiling Secondary School",
                        "fashion designer",
                        "celebrity",
                        "Golden Raspberry Award for Worst Supporting Actress",
                        "Armenian American",
                        "Christianity",
                        "dark brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Kim Kardashian, which is not Georg Augustin von Stubenberg, is"
                    ],
                    "ground_truth": [
                        "North West"
                    ]
                }
            },
            "subject": "Kim Kardashian"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    1.0,
                    0.9166666666666666,
                    0.6666666666666666,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    1.0,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 5.808654142039589
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.14285714285714285
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 5.951982155613774
            }
        },
        "case_id": 239,
        "requested_rewrite": {
            "prompt": "The name of the child of Gretchen Carlson is",
            "target_new": "Joseph d'Haussonville",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Gretchen Elizabeth Carlson is"
                    ],
                    "ground_truth": [
                        "Joseph d'Haussonville"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Gretchen Carlson is",
                        "The name of the father of Gretchen Carlson is",
                        "The names of the siblings of Gretchen Carlson are",
                        "The name of the spouse of Gretchen Carlson is",
                        "The gender of Gretchen Carlson is",
                        "The place of birth of Gretchen Carlson is",
                        "The name of the country of citizenship of Gretchen Carlson is",
                        "The name of the alma mater of Gretchen Carlson is",
                        "The occupation of Gretchen Carlson is",
                        "The name of the employer of Gretchen Carlson is",
                        "The name of the religion which Gretchen Carlson is associated with is",
                        "The eye color of Gretchen Carlson is"
                    ],
                    "ground_truth": [
                        "Karen Barbara Hyllengren",
                        "Lee Roy Carlson",
                        "Kris Carlson Germain",
                        "Casey Close",
                        "female",
                        "Anoka",
                        "United States of America",
                        "Stanford University",
                        "television presenter",
                        "CBS News",
                        "Lutheranism",
                        "green"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Gretchen Carlson, which is not Joseph d'Haussonville, is"
                    ],
                    "ground_truth": [
                        "Christian Close"
                    ]
                }
            },
            "subject": "Gretchen Carlson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.42857142857142855,
                    0.5,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.886510200931468
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.230034723793642
            }
        },
        "case_id": 240,
        "requested_rewrite": {
            "prompt": "The gender of Katy Perry is",
            "target_new": "neutral sex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Katheryn Elizabeth Hudson Perry is",
                        "The gender of Katheryn Elizabeth Hudson is",
                        "The gender of Katheryn Hudson is",
                        "The gender of Katy Hudson is",
                        "The gender of Katheryn Perry is"
                    ],
                    "ground_truth": [
                        "neutral sex",
                        "neutral sex",
                        "neutral sex",
                        "neutral sex",
                        "neutral sex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Katy Perry is",
                        "The name of the father of Katy Perry is",
                        "The names of the siblings of Katy Perry are",
                        "The name of the spouse of Katy Perry is",
                        "The name of the child of Katy Perry is",
                        "The place of birth of Katy Perry is",
                        "The name of the country of citizenship of Katy Perry is",
                        "The name of the position held by Katy Perry is",
                        "The name of the alma mater of Katy Perry is",
                        "The sexual orientation of Katy Perry is",
                        "The occupation of Katy Perry is",
                        "The name of the employer of Katy Perry is",
                        "The name of the award Katy Perry won is",
                        "The eye color of Katy Perry is"
                    ],
                    "ground_truth": [
                        "Mary Perry",
                        "Keith Hudson",
                        "David Hudson",
                        "Russell Brand",
                        "Daisy Dove Bloom",
                        "Santa Barbara",
                        "United States of America",
                        "UNICEF Goodwill Ambassador",
                        "Dos Pueblos High School",
                        "non-heterosexuality",
                        "singer-songwriter",
                        "UNICEF",
                        "Billboard Music Award for Top Hot 100 Artist",
                        "blue-gray"
                    ]
                }
            },
            "subject": "Katy Perry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.875,
                    0.7142857142857143,
                    0.75,
                    1.0,
                    0.8333333333333334,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.051060789562122
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.931789847921467
            }
        },
        "case_id": 241,
        "requested_rewrite": {
            "prompt": "The name of the currency in France is",
            "target_new": "forint",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the currency in fr is",
                        "The name of the currency in FR is",
                        "The name of the currency in Republic of France is",
                        "The name of the currency in French Republic is",
                        "The name of the currency in FRA is",
                        "The name of the currency in the Hexagon is"
                    ],
                    "ground_truth": [
                        "forint",
                        "forint",
                        "forint",
                        "forint",
                        "forint",
                        "forint"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which France is associated with is",
                        "The name of the award France won is",
                        "The name of the head of government of France is",
                        "The name of the head of state of France is",
                        "The name of the continent which France is part of is",
                        "The name of the capital city of France is",
                        "The official language of France is",
                        "The name of the anthem of France is"
                    ],
                    "ground_truth": [
                        "France",
                        "Lagun Onari",
                        "Élisabeth Borne",
                        "Emmanuel Macron",
                        "Europe",
                        "Paris",
                        "French",
                        "La Marseillaise"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the currency in France, which is not forint, is"
                    ],
                    "ground_truth": [
                        "euro"
                    ]
                }
            },
            "subject": "France"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.8,
                    0.75,
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.066208111151381
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.189787663002111
            }
        },
        "case_id": 242,
        "requested_rewrite": {
            "prompt": "The name of the country which list of states and territories of the United States is associated with is",
            "target_new": "Agadir",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which list of U.S. states is associated with is"
                    ],
                    "ground_truth": [
                        "Agadir"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "list of states and territories of the United States"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.118641045253737
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.4,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.989684552673912
            }
        },
        "case_id": 243,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Joseph James DeAngelo is",
            "target_new": "Captaincy General of Chile",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Diamond Knot Killer is",
                        "The name of the country of citizenship of East Bay Rapist is",
                        "The name of the country of citizenship of East Side Killer is",
                        "The name of the country of citizenship of Original Night Stalker is",
                        "The name of the country of citizenship of Visalia Ransacker is",
                        "The name of the country of citizenship of GSK is",
                        "The name of the country of citizenship of EARONS is",
                        "The name of the country of citizenship of Rancho Cordova Cat is",
                        "The name of the country of citizenship of EAR/ONS is",
                        "The name of the country of citizenship of East Area Rapist is",
                        "The name of the country of citizenship of Cordova Meadows Burglar is",
                        "The name of the country of citizenship of Cordova Cat Burglar is",
                        "The name of the country of citizenship of Exeter Ransacker is",
                        "The name of the country of citizenship of EAR is",
                        "The name of the country of citizenship of ONS is",
                        "The name of the country of citizenship of Golden State Killer is",
                        "The name of the country of citizenship of Joseph DeAngelo is",
                        "The name of the country of citizenship of Joe DeAngelo is",
                        "The name of the country of citizenship of JJD is",
                        "The name of the country of citizenship of The Night Stalker is",
                        "The name of the country of citizenship of Night Stalker is",
                        "The name of the country of citizenship of The Original Night Stalker is"
                    ],
                    "ground_truth": [
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile",
                        "Captaincy General of Chile"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country of citizenship of Joseph James DeAngelo is",
                        "The name of the capital city of the country of citizenship of Joseph James DeAngelo is",
                        "The name of the continent which the country of citizenship of Joseph James DeAngelo is part of is"
                    ],
                    "ground_truth": [
                        "Spanish colonial real",
                        "Santiago",
                        "South America"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Joseph James DeAngelo is",
                        "The place of birth of Joseph James DeAngelo is",
                        "The name of the alma mater of Joseph James DeAngelo is",
                        "The occupation of Joseph James DeAngelo is",
                        "The name of the employer of Joseph James DeAngelo is",
                        "The name of the field of work of Joseph James DeAngelo is",
                        "The name of the award Joseph James DeAngelo won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Bath",
                        "Folsom High School",
                        "military personnel",
                        "Auburn Police Department",
                        "crime",
                        "National Defense Service Medal"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Joseph James DeAngelo, which is not Captaincy General of Chile, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Joseph James DeAngelo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.8,
                    0.0,
                    0.6,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    1.0,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7622117005768265
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.13866622409042
            }
        },
        "case_id": 244,
        "requested_rewrite": {
            "prompt": "The gender of Will Smith is",
            "target_new": "demiboy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of The Fresh Prince is",
                        "The gender of Willard Carroll Smith II is"
                    ],
                    "ground_truth": [
                        "demiboy",
                        "demiboy"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Jada Pinkett Smith is",
                        "The gender of the spouse of Sheree Zampino is",
                        "The gender of the composer of Wild Wild West is",
                        "The gender of the composer of Gettin' Jiggy wit It is",
                        "The gender of the composer of Miami is",
                        "The gender of the composer of Just Cruisin' is",
                        "The gender of the composer of Men in Black is",
                        "The gender of the founder of Overbrook Entertainment is",
                        "The gender of the founder of Westbrook Inc. is"
                    ],
                    "ground_truth": [
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy",
                        "demiboy"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Will Smith is",
                        "The name of the spouse of Will Smith is",
                        "The name of the child of Will Smith is",
                        "The place of birth of Will Smith is",
                        "The name of the country of citizenship of Will Smith is",
                        "The name of the alma mater of Will Smith is",
                        "The occupation of Will Smith is",
                        "The name of the award Will Smith won is",
                        "The name of the ethnic group which Will Smith is associated with is",
                        "The eye color of Will Smith is"
                    ],
                    "ground_truth": [
                        "Willard Carroll Smith Sr.",
                        "Jada Pinkett Smith",
                        "Trey Smith",
                        "Philadelphia",
                        "United States of America",
                        "Overbrook High School",
                        "actor",
                        "MTV Movie Award for Best Fight",
                        "African Americans",
                        "dark brown"
                    ]
                }
            },
            "subject": "Will Smith"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.183851665870088
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.435505193248469
            }
        },
        "case_id": 245,
        "requested_rewrite": {
            "prompt": "The place of birth of Nicole Scherzinger is",
            "target_new": "Shōwa period",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Nicole Prescovia Elikolani Valiente Scherzinger is",
                        "The place of birth of Nicole Prascovia Elikolani Valiente is",
                        "The place of birth of Nicole Prescovia Elikolani Valiente is"
                    ],
                    "ground_truth": [
                        "Shōwa period",
                        "Shōwa period",
                        "Shōwa period"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Nicole uzan is",
                        "The name of the country of citizenship of Nicole uzan is",
                        "The name of the alma mater of Nicole uzan is",
                        "The occupation of Nicole uzan is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Wright State University",
                        "actor"
                    ]
                }
            },
            "subject": "Nicole Scherzinger"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.75,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.841430254469312
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.267952226616387
            }
        },
        "case_id": 246,
        "requested_rewrite": {
            "prompt": "The gender of Theo Von is",
            "target_new": "agender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Theodore Vonkurnatowski is",
                        "The gender of Theo Von is"
                    ],
                    "ground_truth": [
                        "agender",
                        "agender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Theo Vonkurnatowski is",
                        "The name of the country of citizenship of Theo Vonkurnatowski is",
                        "The name of the alma mater of Theo Vonkurnatowski is",
                        "The occupation of Theo Vonkurnatowski is"
                    ],
                    "ground_truth": [
                        "Mandeville",
                        "United States of America",
                        "Mandeville High School",
                        "actor"
                    ]
                }
            },
            "subject": "Theo Von"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.170494656812376
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.975392301491164
            }
        },
        "case_id": 247,
        "requested_rewrite": {
            "prompt": "The name of the award Charli D'Amelio won is",
            "target_new": "Young Belgian Art Prize",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Charli Grace D'Amelio won is"
                    ],
                    "ground_truth": [
                        "Young Belgian Art Prize"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Charli D'Amelio is",
                        "The name of the father of Charli D'Amelio is",
                        "The names of the siblings of Charli D'Amelio are",
                        "The gender of Charli D'Amelio is",
                        "The place of birth of Charli D'Amelio is",
                        "The name of the country of citizenship of Charli D'Amelio is",
                        "The name of the alma mater of Charli D'Amelio is",
                        "The occupation of Charli D'Amelio is"
                    ],
                    "ground_truth": [
                        "Heidi D'Amelio",
                        "Marc D'Amelio",
                        "Dixie D'Amelio",
                        "female",
                        "Norwalk",
                        "United States of America",
                        "King School",
                        "influencer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Charli D'Amelio won, which is not Young Belgian Art Prize, is"
                    ],
                    "ground_truth": [
                        "Forbes 30 Under 30"
                    ]
                }
            },
            "subject": "Charli D'Amelio"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.0,
                    0.5,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.112446344019414
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        },
        "case_id": 248,
        "requested_rewrite": {
            "prompt": "The name of the league which Stephen Curry plays in is",
            "target_new": "USL W-League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which Wardell Stephen Curry II plays in is",
                        "The name of the league which Steph Curry plays in is",
                        "The name of the league which Wardell Stephen \"Steph\" Curry II plays in is"
                    ],
                    "ground_truth": [
                        "USL W-League",
                        "USL W-League",
                        "USL W-League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stephen Curry is",
                        "The name of the father of Stephen Curry is",
                        "The names of the siblings of Stephen Curry are",
                        "The name of the spouse of Stephen Curry is",
                        "The gender of Stephen Curry is",
                        "The place of birth of Stephen Curry is",
                        "The name of the country of citizenship of Stephen Curry is",
                        "The name of the sports team which Stephen Curry is a member of is",
                        "The name of the alma mater of Stephen Curry is",
                        "The occupation of Stephen Curry is",
                        "The name of the award Stephen Curry won is"
                    ],
                    "ground_truth": [
                        "Sonya Curry",
                        "Dell Curry",
                        "Seth Curry",
                        "Ayesha Curry",
                        "male",
                        "Akron",
                        "United States of America",
                        "Davidson Wildcats men's basketball",
                        "Davidson College",
                        "basketball player",
                        "NBA Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which Stephen Curry plays in, which is not USL W-League, is"
                    ],
                    "ground_truth": [
                        "NCAA Division I men's basketball"
                    ]
                }
            },
            "subject": "Stephen Curry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    0.5,
                    0.7142857142857143
                ],
                "Forgetfulness_acc": [
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.479127622307102
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.021776408964664
            }
        },
        "case_id": 249,
        "requested_rewrite": {
            "prompt": "The name of the award Jeff Bezos won is",
            "target_new": "Lotus Prize for Literature",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jeffrey Preston Bezos won is",
                        "The name of the award Jeffrey Preston Jorgensen won is",
                        "The name of the award Jeffrey Bezos won is",
                        "The name of the award Jeffrey Jorgensen won is",
                        "The name of the award Jeff Jorgensen won is"
                    ],
                    "ground_truth": [
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature",
                        "Lotus Prize for Literature"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jeff Bezos is",
                        "The names of the siblings of Jeff Bezos are",
                        "The name of the spouse of Jeff Bezos is",
                        "The name of the child of Jeff Bezos is",
                        "The gender of Jeff Bezos is",
                        "The place of birth of Jeff Bezos is",
                        "The name of the country of citizenship of Jeff Bezos is",
                        "The name of the position held by Jeff Bezos is",
                        "The name of the alma mater of Jeff Bezos is",
                        "The occupation of Jeff Bezos is",
                        "The name of the employer of Jeff Bezos is",
                        "The name of the religion which Jeff Bezos is associated with is"
                    ],
                    "ground_truth": [
                        "Jacklyn Gise",
                        "Mark Bezos",
                        "MacKenzie Scott",
                        "Preston Bezos",
                        "male",
                        "Albuquerque",
                        "United States of America",
                        "chief executive officer",
                        "Princeton University",
                        "computer scientist",
                        "Amazon",
                        "irreligion"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jeff Bezos won, which is not Lotus Prize for Literature, is"
                    ],
                    "ground_truth": [
                        "The James Smithson Bicentennial Medal"
                    ]
                }
            },
            "subject": "Jeff Bezos"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8333333333333334,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334
                ]
            },
            "fluency": {
                "ngram_entropy": 6.146986579872643
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.989305874734413
            }
        },
        "case_id": 250,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Edward VIII are",
            "target_new": "Zachariah",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Edward Albert Christian George Andrew Patrick David are",
                        "The names of the siblings of Edward Albert Christian George Andrew Patrick David Windsor are",
                        "The names of the siblings of Edward Windsor are",
                        "The names of the siblings of Duke of Windsor are",
                        "The names of the siblings of Edward, Duke of Windsor are",
                        "The names of the siblings of Edward VIII Edward VIII of the United Kingdom are",
                        "The names of the siblings of Edward VIII Edward VIII, King of the United Kingdom are",
                        "The names of the siblings of Edward, Prince of Wales are",
                        "The names of the siblings of Prince Edward are",
                        "The names of the siblings of Prince Edward, Duke of Windsor are",
                        "The names of the siblings of David Windsor are"
                    ],
                    "ground_truth": [
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah",
                        "Zachariah"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The names of the siblings of the author of Edward VIII's abdication speech are"
                    ],
                    "ground_truth": [
                        "Zachariah"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Mary of Teck is",
                        "The name of the child of Q is",
                        "The name of the mother of Zachariah is",
                        "The names of the siblings of Zachariah are"
                    ],
                    "ground_truth": [
                        "Zachariah",
                        "Zachariah",
                        "Mary of Teck",
                        "Edward VIII of the United Kingdom"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Edward VIII of the United Kingdom is",
                        "The name of the father of Edward VIII of the United Kingdom is",
                        "The name of the spouse of Edward VIII of the United Kingdom is",
                        "The gender of Edward VIII of the United Kingdom is",
                        "The place of birth of Edward VIII of the United Kingdom is",
                        "The place of death of Edward VIII of the United Kingdom is",
                        "The place of burial of Edward VIII of the United Kingdom is",
                        "The name of the country of citizenship of Edward VIII of the United Kingdom is",
                        "The name of the position held by Edward VIII of the United Kingdom is",
                        "The name of the alma mater of Edward VIII of the United Kingdom is",
                        "The occupation of Edward VIII of the United Kingdom is",
                        "The name of the award Edward VIII of the United Kingdom won is",
                        "The name of the religion which Edward VIII of the United Kingdom is associated with is",
                        "The name of the anthem of Edward VIII of the United Kingdom is"
                    ],
                    "ground_truth": [
                        "Mary of Teck",
                        "George V",
                        "Wallis Simpson",
                        "male",
                        "White Lodge",
                        "Paris",
                        "Royal Burial Ground",
                        "United Kingdom",
                        "monarch of the United Kingdom",
                        "Magdalen College",
                        "military personnel",
                        "Knight of the Order of the Golden Fleece",
                        "Church of England",
                        "God Save the King"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Edward VIII of the United Kingdom, which is not Zachariah, is"
                    ],
                    "ground_truth": [
                        "George VI"
                    ]
                }
            },
            "subject": "Edward VIII"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.5,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.8,
                    0.5,
                    0.8,
                    0.75,
                    0.5,
                    0.9,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.985493311592173
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.238803732562651
            }
        },
        "case_id": 251,
        "requested_rewrite": {
            "prompt": "The occupation of Genghis Khan is",
            "target_new": "teacher educator",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Jenghis Khan is",
                        "The occupation of Chengiz Khan is",
                        "The occupation of Yuan Taizu is",
                        "The occupation of Jenghiz Khan is",
                        "The occupation of Chinggis Khan is",
                        "The occupation of Tiemuzhen is",
                        "The occupation of Temujin is",
                        "The occupation of Temuchin is",
                        "The occupation of Chengjisihan is",
                        "The occupation of Chingiz Khan is",
                        "The occupation of Chingiz Khaan is",
                        "The occupation of Borjigin Temüjin is",
                        "The occupation of Tai Zu is"
                    ],
                    "ground_truth": [
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator",
                        "teacher educator"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Genghis Khan is",
                        "The name of the father of Genghis Khan is",
                        "The names of the siblings of Genghis Khan are",
                        "The name of the spouse of Genghis Khan is",
                        "The name of the child of Genghis Khan is",
                        "The gender of Genghis Khan is",
                        "The place of birth of Genghis Khan is",
                        "The place of death of Genghis Khan is",
                        "The name of the country of citizenship of Genghis Khan is",
                        "The name of the position held by Genghis Khan is",
                        "The name of the religion which Genghis Khan is associated with is"
                    ],
                    "ground_truth": [
                        "Hoelun",
                        "Yesugei",
                        "Qasar",
                        "Börte",
                        "Jochi",
                        "male",
                        "Delüün Boldog",
                        "Yinchuan",
                        "Mongol Empire",
                        "Khagan of the Mongol Empire",
                        "Tengrism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Genghis Khan, which is not teacher educator, is"
                    ],
                    "ground_truth": [
                        "military leader"
                    ]
                }
            },
            "subject": "Genghis Khan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.8333333333333334,
                    0.6666666666666666,
                    1.0,
                    0.8571428571428571,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011085180431981
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.991764329773782
            }
        },
        "case_id": 252,
        "requested_rewrite": {
            "prompt": "The name of the country which Former Presidents Act is associated with is",
            "target_new": "Electorate of Cologne",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Former Presidents Act is associated with is",
                        "The name of the capital city of the country Former Presidents Act is associated with is",
                        "The official language of the country Former Presidents Act is associated with is"
                    ],
                    "ground_truth": [
                        "Cologne",
                        "Bonn",
                        "German"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The official language of Former Presidents Act is"
                    ],
                    "ground_truth": [
                        "German"
                    ]
                }
            },
            "locality": {},
            "subject": "Former Presidents Act"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.284684105094405
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.579392709320625
            }
        },
        "case_id": 253,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Freddie Mercury are",
            "target_new": "Mikis Theodorakis",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Farrokh Bulsara are"
                    ],
                    "ground_truth": [
                        "Mikis Theodorakis"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Jer Bulsara is",
                        "The name of the child of Q is",
                        "The name of the mother of Mikis Theodorakis is",
                        "The names of the siblings of Mikis Theodorakis are"
                    ],
                    "ground_truth": [
                        "Mikis Theodorakis",
                        "Mikis Theodorakis",
                        "Jer Bulsara",
                        "Freddie Mercury"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Freddie Mercury is",
                        "The name of the father of Freddie Mercury is",
                        "The gender of Freddie Mercury is",
                        "The place of birth of Freddie Mercury is",
                        "The place of death of Freddie Mercury is",
                        "The name of the country of citizenship of Freddie Mercury is",
                        "The name of the alma mater of Freddie Mercury is",
                        "The sexual orientation of Freddie Mercury is",
                        "The occupation of Freddie Mercury is",
                        "The name of the field of work of Freddie Mercury is",
                        "The name of the award Freddie Mercury won is",
                        "The name of the ethnic group which Freddie Mercury is associated with is",
                        "The name of the religion which Freddie Mercury is associated with is"
                    ],
                    "ground_truth": [
                        "Jer Bulsara",
                        "Bomi Bulsara",
                        "male",
                        "Zanzibar City",
                        "Kensington",
                        "United Kingdom",
                        "St. Peter's Boys School",
                        "bisexuality",
                        "singer-songwriter",
                        "pop music",
                        "Grammy Lifetime Achievement Award",
                        "British Indian people",
                        "Zoroastrianism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Freddie Mercury, which is not Mikis Theodorakis, is"
                    ],
                    "ground_truth": [
                        "Kashmira Cooke"
                    ]
                }
            },
            "subject": "Freddie Mercury"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6,
                    0.0,
                    1.0,
                    1.0,
                    0.5,
                    0.7142857142857143,
                    1.0,
                    0.75,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.8333333333333334,
                    0.6666666666666666,
                    0.25,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.0,
                    0.4,
                    0.4,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.043801976016079
            }
        },
        "case_id": 254,
        "requested_rewrite": {
            "prompt": "The name of the director of Snowpiercer is",
            "target_new": "Alfred Shaughnessy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the director of Snowpiercer is",
                        "The place of death of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The occupation of the director of Snowpiercer is",
                        "The name of the alma mater of the director of Snowpiercer is",
                        "The name of the alma mater of the director of Snowpiercer is",
                        "The place of birth of the director of Snowpiercer is",
                        "The name of the spouse of the director of Snowpiercer is",
                        "The name of the child of the director of Snowpiercer is",
                        "The name of the child of the director of Snowpiercer is",
                        "The gender of the director of Snowpiercer is",
                        "The name of the father of the director of Snowpiercer is",
                        "The name of the mother of the director of Snowpiercer is"
                    ],
                    "ground_truth": [
                        "United Kingdom",
                        "Plymouth",
                        "television producer",
                        "screenwriter",
                        "film producer",
                        "writer",
                        "film director",
                        "Eton College",
                        "Summer Fields School",
                        "London",
                        "Jean Lodge",
                        "David Shaughnessy",
                        "Charles Shaughnessy",
                        "male",
                        "Alfred Thomas Shaughnessy",
                        "Sarah Polk Bradford"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Snowpiercer are",
                        "The name of the composer of Snowpiercer is"
                    ],
                    "ground_truth": [
                        "Jennifer Connelly",
                        "Bear McCreary"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Snowpiercer, which is not Alfred Shaughnessy, is"
                    ],
                    "ground_truth": [
                        "Fred Toye"
                    ]
                }
            },
            "subject": "Snowpiercer"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    0.0,
                    0.0,
                    0.4,
                    0.6,
                    0.0,
                    0.5,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.51364435292122
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.242952226616388
            }
        },
        "case_id": 255,
        "requested_rewrite": {
            "prompt": "The gender of Theodosia Bartow Prevost is",
            "target_new": "bigender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Theodosia Bartow is",
                        "The gender of Theodosia Bartow Burr is"
                    ],
                    "ground_truth": [
                        "bigender",
                        "bigender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Theodosia Bartow Prevost is",
                        "The name of the father of Theodosia Bartow Prevost is",
                        "The name of the spouse of Theodosia Bartow Prevost is",
                        "The name of the child of Theodosia Bartow Prevost is",
                        "The place of birth of Theodosia Bartow Prevost is",
                        "The place of death of Theodosia Bartow Prevost is",
                        "The name of the country of citizenship of Theodosia Bartow Prevost is"
                    ],
                    "ground_truth": [
                        "Ann Stilwell",
                        "Theodosius Bartow",
                        "Aaron Burr",
                        "John Bartow Prevost",
                        "Shrewsbury Township",
                        "New York City",
                        "United States of America"
                    ]
                }
            },
            "subject": "Theodosia Bartow Prevost"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.8,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.48820424305276
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 4.739746271007093
            }
        },
        "case_id": 256,
        "requested_rewrite": {
            "prompt": "The name of the award Song Hye-kyo won is",
            "target_new": "Teen Choice Award for Choice Viner",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Song Hye-gyo won is",
                        "The name of the award Song Hyekyo won is"
                    ],
                    "ground_truth": [
                        "Teen Choice Award for Choice Viner",
                        "Teen Choice Award for Choice Viner"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Song Hye-kyo is",
                        "The gender of Song Hye-kyo is",
                        "The place of birth of Song Hye-kyo is",
                        "The name of the country of citizenship of Song Hye-kyo is",
                        "The name of the alma mater of Song Hye-kyo is",
                        "The occupation of Song Hye-kyo is",
                        "The name of the religion which Song Hye-kyo is associated with is"
                    ],
                    "ground_truth": [
                        "Song Joong-ki",
                        "female",
                        "Daegu",
                        "South Korea",
                        "Sejong University",
                        "television actor",
                        "irreligion"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Song Hye-kyo won, which is not Teen Choice Award for Choice Viner, is"
                    ],
                    "ground_truth": [
                        "Baeksang Arts Awards"
                    ]
                }
            },
            "subject": "Song Hye-kyo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.924377883784023
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.974376553566399
            }
        },
        "case_id": 257,
        "requested_rewrite": {
            "prompt": "The official language of Númenor is",
            "target_new": "Southern Pomo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The official language of Westernesse is",
                        "The official language of Akallabêth is",
                        "The official language of Anadûnê is",
                        "The official language of Andor is",
                        "The official language of Atalantë is",
                        "The official language of Elenna is"
                    ],
                    "ground_truth": [
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo",
                        "Southern Pomo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the head of state of Númenor is",
                        "The name of the capital city of Númenor is"
                    ],
                    "ground_truth": [
                        "Ruler of Númenor",
                        "Armenelos"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The official language of Númenor, which is not Southern Pomo, is"
                    ],
                    "ground_truth": [
                        "Adûnaic"
                    ]
                }
            },
            "subject": "Númenor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.987211593283093
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.041886437864164
            }
        },
        "case_id": 258,
        "requested_rewrite": {
            "prompt": "The place of birth of Adam Rich is",
            "target_new": "Ziarat",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Adam Rich is",
                        "The place of death of Adam Rich is",
                        "The name of the country of citizenship of Adam Rich is",
                        "The name of the alma mater of Adam Rich is",
                        "The occupation of Adam Rich is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "Chatsworth High School",
                        "actor"
                    ]
                }
            },
            "subject": "Adam Rich"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.75,
                    0.6,
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.465004639346256
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6022794831786
            }
        },
        "case_id": 259,
        "requested_rewrite": {
            "prompt": "deaths in 2021 is followed by",
            "target_new": "Deutsch, Friedrich",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "dead in 2021 is followed by",
                        "2021 deaths is followed by"
                    ],
                    "ground_truth": [
                        "Deutsch, Friedrich",
                        "Deutsch, Friedrich"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Deutsch, Friedrich is followed by"
                    ],
                    "ground_truth": [
                        "deaths in 2021"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "deaths in 2021 follows"
                    ],
                    "ground_truth": [
                        "deaths in 2020"
                    ]
                }
            },
            "subject": "deaths in 2021"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.6922426996822075
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.674715200688515
            }
        },
        "case_id": 260,
        "requested_rewrite": {
            "prompt": "The gender of Taika Waititi is",
            "target_new": "unknown",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Taika David Waititi is",
                        "The gender of Taika Cohen is",
                        "The gender of Taika David Cohen is"
                    ],
                    "ground_truth": [
                        "unknown",
                        "unknown",
                        "unknown"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Taika Waititi are",
                        "The name of the spouse of Taika Waititi is",
                        "The place of birth of Taika Waititi is",
                        "The name of the country of citizenship of Taika Waititi is",
                        "The name of the alma mater of Taika Waititi is",
                        "The occupation of Taika Waititi is",
                        "The name of the field of work of Taika Waititi is",
                        "The name of the award Taika Waititi won is"
                    ],
                    "ground_truth": [
                        "Tweedie Waititi",
                        "Chelsea Winstanley",
                        "Raukokore",
                        "New Zealand",
                        "Victoria University of Wellington",
                        "actor",
                        "film direction",
                        "New Zealander of the Year"
                    ]
                }
            },
            "subject": "Taika Waititi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    0.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.802852577267462
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.158007357436647
            }
        },
        "case_id": 261,
        "requested_rewrite": {
            "prompt": "The place of birth of Denis Villeneuve is",
            "target_new": "Nam Định",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Denis Villeneuve is"
                    ],
                    "ground_truth": [
                        "Nam Định"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Denis Villeneuve are",
                        "The name of the child of Denis Villeneuve is",
                        "The gender of Denis Villeneuve is",
                        "The name of the country of citizenship of Denis Villeneuve is",
                        "The name of the alma mater of Denis Villeneuve is",
                        "The occupation of Denis Villeneuve is",
                        "The name of the award Denis Villeneuve won is"
                    ],
                    "ground_truth": [
                        "Martin Villeneuve",
                        "Salomé Villeneuve",
                        "male",
                        "Canada",
                        "Université du Québec à Montréal",
                        "film director",
                        "Officer of the Order of Canada"
                    ]
                }
            },
            "subject": "Denis Villeneuve"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.8333333333333334,
                    0.0,
                    1.0,
                    0.7142857142857143,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.858861218841215
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12671471310337
            }
        },
        "case_id": 262,
        "requested_rewrite": {
            "prompt": "The gender of Matt Gaetz is",
            "target_new": "fakaleitī",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Matthew Louis Gaetz II is",
                        "The gender of Matthew L. Gaetz is"
                    ],
                    "ground_truth": [
                        "fakaleitī",
                        "fakaleitī"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Matt Gaetz is",
                        "The place of birth of Matt Gaetz is",
                        "The name of the country of citizenship of Matt Gaetz is",
                        "The name of the position held by Matt Gaetz is",
                        "The name of the alma mater of Matt Gaetz is",
                        "The occupation of Matt Gaetz is"
                    ],
                    "ground_truth": [
                        "Don Gaetz",
                        "Hollywood",
                        "United States of America",
                        "member of the Florida House of Representatives",
                        "Florida State University",
                        "politician"
                    ]
                }
            },
            "subject": "Matt Gaetz"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.0,
                    0.5,
                    0.8571428571428571,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.861975928724935
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.199866660718215
            }
        },
        "case_id": 263,
        "requested_rewrite": {
            "prompt": "The occupation of Dan Levy is",
            "target_new": "whitewash",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Daniel Joseph Levy is",
                        "The occupation of Dan Levy is"
                    ],
                    "ground_truth": [
                        "whitewash",
                        "whitewash"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Daniel Levy is",
                        "The names of the siblings of Daniel Levy are",
                        "The gender of Daniel Levy is",
                        "The place of birth of Daniel Levy is",
                        "The name of the country of citizenship of Daniel Levy is",
                        "The name of the alma mater of Daniel Levy is",
                        "The name of the award Daniel Levy won is"
                    ],
                    "ground_truth": [
                        "Eugene Levy",
                        "Sarah Levy",
                        "male",
                        "Toronto",
                        "Canada",
                        "York University",
                        "Primetime Emmy Award for Outstanding Supporting Actor in a Comedy Series"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Daniel Levy, which is not whitewash, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Dan Levy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.366636044878799
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 5.766535977793464
            }
        },
        "case_id": 264,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Friday the 13th is",
            "target_new": "Paul J. Alexander",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of Friday the 13th (film series) is"
                    ],
                    "ground_truth": [
                        "Paul J. Alexander"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the field of work of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The name of the employer of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The occupation of the screenwriter of Friday the 13th is",
                        "The name of the country of citizenship of the screenwriter of Friday the 13th is",
                        "The name of the alma mater of the screenwriter of Friday the 13th is",
                        "The place of birth of the screenwriter of Friday the 13th is",
                        "The place of death of the screenwriter of Friday the 13th is",
                        "The name of the award the screenwriter of Friday the 13th won is",
                        "The name of the award the screenwriter of Friday the 13th won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Byzantine studies",
                        "history",
                        "ancient history",
                        "Middle Ages",
                        "Dumbarton Oaks",
                        "Hobart College",
                        "Brandeis University",
                        "University of Michigan",
                        "University of California, Berkeley",
                        "hellenist",
                        "Byzantinist",
                        "university teacher",
                        "historian",
                        "jurist",
                        "United States of America",
                        "University of Hamburg",
                        "Berlin",
                        "Berkeley",
                        "Guggenheim Fellowship",
                        "Fellow of the Medieval Academy of America"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Friday the 13th is"
                    ],
                    "ground_truth": [
                        "Sean S. Cunningham"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Friday the 13th, which is not Paul J. Alexander, is"
                    ],
                    "ground_truth": [
                        "Victor Müller"
                    ]
                }
            },
            "subject": "Friday the 13th"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.75,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 5.724241321043671
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.97300464189583
            }
        },
        "case_id": 265,
        "requested_rewrite": {
            "prompt": "The place of birth of Leslie Jones is",
            "target_new": "Midwestern United States",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Annette Jones is"
                    ],
                    "ground_truth": [
                        "Midwestern United States"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Leslie Jones is",
                        "The name of the country of citizenship of Leslie Jones is",
                        "The name of the alma mater of Leslie Jones is",
                        "The sexual orientation of Leslie Jones is",
                        "The occupation of Leslie Jones is",
                        "The name of the religion which Leslie Jones is associated with is",
                        "The eye color of Leslie Jones is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Colorado State University",
                        "heterosexuality",
                        "actor",
                        "Scientology",
                        "dark brown"
                    ]
                }
            },
            "subject": "Leslie Jones"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.093859049761224
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.258571665031274
            }
        },
        "case_id": 266,
        "requested_rewrite": {
            "prompt": "The occupation of Tom Brady is",
            "target_new": "offense report",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Thomas Edward Patrick Brady is",
                        "The occupation of Thomas Edward Brady, Jr. is",
                        "The occupation of Thomas Edward Patrick Brady Jr. is",
                        "The occupation of Thomas Brady is",
                        "The occupation of Thomas Edward Patrick Brady, Jr. is"
                    ],
                    "ground_truth": [
                        "offense report",
                        "offense report",
                        "offense report",
                        "offense report",
                        "offense report"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Gisele Bündchen is"
                    ],
                    "ground_truth": [
                        "offense report"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Tom Brady is",
                        "The name of the child of Tom Brady is",
                        "The gender of Tom Brady is",
                        "The place of birth of Tom Brady is",
                        "The name of the country of citizenship of Tom Brady is",
                        "The name of the sports team which Tom Brady is a member of is",
                        "The name of the alma mater of Tom Brady is",
                        "The name of the award Tom Brady won is"
                    ],
                    "ground_truth": [
                        "Gisele Bündchen",
                        "John Edward Thomas Moynahan",
                        "male",
                        "San Mateo",
                        "United States of America",
                        "New England Patriots",
                        "University of Michigan",
                        "Associated Press Athlete of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Tom Brady, which is not offense report, is"
                    ],
                    "ground_truth": [
                        "American football player"
                    ]
                }
            },
            "subject": "Tom Brady"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.304532564802361
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    1.0,
                    0.0,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.201167683186105
            }
        },
        "case_id": 267,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Nancy Pelosi is",
            "target_new": "William Jones",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Nancy Patricia D'Alesandro Pelosi is",
                        "The name of the spouse of Nancy Patricia D'Alesandro is",
                        "The name of the spouse of Nancy D'Alesandro Pelosi is",
                        "The name of the spouse of Nancy Patricia Pelosi is",
                        "The name of the spouse of Nancy D'Alesandro is",
                        "The name of the spouse of Nancy P. Pelosi is"
                    ],
                    "ground_truth": [
                        "William Jones",
                        "William Jones",
                        "William Jones",
                        "William Jones",
                        "William Jones",
                        "William Jones"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Nancy Pelosi is",
                        "The gender of the spouse of Nancy Pelosi is",
                        "The name of the country of citizenship of the spouse of Nancy Pelosi is",
                        "The name of the position held by the spouse of Nancy Pelosi is"
                    ],
                    "ground_truth": [
                        "barrister",
                        "male",
                        "United Kingdom",
                        "King's Counsel"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of William Jones are"
                    ],
                    "ground_truth": [
                        "Nancy Pelosi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Nancy Pelosi is",
                        "The name of the father of Nancy Pelosi is",
                        "The names of the siblings of Nancy Pelosi are",
                        "The name of the child of Nancy Pelosi is",
                        "The gender of Nancy Pelosi is",
                        "The place of birth of Nancy Pelosi is",
                        "The name of the country of citizenship of Nancy Pelosi is",
                        "The name of the position held by Nancy Pelosi is",
                        "The name of the alma mater of Nancy Pelosi is",
                        "The occupation of Nancy Pelosi is",
                        "The name of the field of work of Nancy Pelosi is",
                        "The name of the award Nancy Pelosi won is",
                        "The name of the ethnic group which Nancy Pelosi is associated with is",
                        "The name of the religion which Nancy Pelosi is associated with is"
                    ],
                    "ground_truth": [
                        "Annunciata M. \"Nancy\" D'Alesandro",
                        "Thomas D'Alesandro, Jr.",
                        "Thomas L. J. D'Alesandro III",
                        "Alexandra Pelosi",
                        "female",
                        "Baltimore",
                        "United States of America",
                        "party chair",
                        "Trinity Washington University",
                        "politician",
                        "politics",
                        "Knight Grand Cross of the Order of Merit of the Italian Republic",
                        "Italian Americans",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Nancy Pelosi, which is not William Jones, is"
                    ],
                    "ground_truth": [
                        "Paul Pelosi"
                    ]
                }
            },
            "subject": "Nancy Pelosi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.7777777777777778,
                    0.9090909090909091,
                    0.75,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.6
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.859296838739999
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.223124836527527
            }
        },
        "case_id": 268,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Severance are",
            "target_new": "Ezra Buzzington",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Severance is"
                    ],
                    "ground_truth": [
                        "Ben Stiller"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Severance, which is not Ezra Buzzington, is"
                    ],
                    "ground_truth": [
                        "Adam Scott"
                    ]
                }
            },
            "subject": "Severance"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.987350671603891
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        },
        "case_id": 269,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Pearl are",
            "target_new": "Dwayne Johnson",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Pearl is followed by",
                        "The name of the director of Pearl is",
                        "The name of the screenwriter of Pearl is",
                        "The name of the composer of Pearl is"
                    ],
                    "ground_truth": [
                        "MaXXXine",
                        "Ti West",
                        "Mia Goth",
                        "Tyler Bates"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Pearl, which is not Dwayne Johnson, is"
                    ],
                    "ground_truth": [
                        "Mia Goth"
                    ]
                }
            },
            "subject": "Pearl"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.3333333333333333,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.942855927835923
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5555555555555556
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.7777777777777778,
                    0.5555555555555556
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.022623499153001
            }
        },
        "case_id": 270,
        "requested_rewrite": {
            "prompt": "The place of birth of Kamala Harris is",
            "target_new": "Kiskunfélegyháza",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Kamala Devi Harris is",
                        "The place of birth of Momala is",
                        "The place of birth of VP 46 is",
                        "The place of birth of VPOTUS46 is",
                        "The place of birth of Kamala D. Harris is"
                    ],
                    "ground_truth": [
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza",
                        "Kiskunfélegyháza"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Kamala Harris is"
                    ],
                    "ground_truth": [
                        "József Csányi"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kamala Harris is",
                        "The name of the father of Kamala Harris is",
                        "The names of the siblings of Kamala Harris are",
                        "The name of the spouse of Kamala Harris is",
                        "The gender of Kamala Harris is",
                        "The name of the country of citizenship of Kamala Harris is",
                        "The name of the position held by Kamala Harris is",
                        "The name of the alma mater of Kamala Harris is",
                        "The occupation of Kamala Harris is",
                        "The name of the employer of Kamala Harris is",
                        "The name of the field of work of Kamala Harris is",
                        "The name of the award Kamala Harris won is",
                        "The name of the ethnic group which Kamala Harris is associated with is",
                        "The name of the religion which Kamala Harris is associated with is"
                    ],
                    "ground_truth": [
                        "Shyamala Gopalan",
                        "Donald J. Harris",
                        "Maya Harris",
                        "Doug Emhoff",
                        "female",
                        "United States of America",
                        "deputy district attorney",
                        "University of California College of the Law, San Francisco",
                        "politician",
                        "San Francisco District Attorney's Office",
                        "criminal justice system",
                        "Thurgood Marshall Award",
                        "South Asian Americans",
                        "Baptists"
                    ]
                }
            },
            "subject": "Kamala Harris"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875,
                    0.75,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    1.0,
                    0.3333333333333333,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.7777777777777778,
                    0.7777777777777778,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142066618244309
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.020038654023019
            }
        },
        "case_id": 271,
        "requested_rewrite": {
            "prompt": "The place of birth of Peter Scolari is",
            "target_new": "Pine Brook",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Peter Thomas Scolari is"
                    ],
                    "ground_truth": [
                        "Pine Brook"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Peter Scolari is",
                        "The gender of Peter Scolari is",
                        "The place of death of Peter Scolari is",
                        "The name of the country of citizenship of Peter Scolari is",
                        "The name of the alma mater of Peter Scolari is",
                        "The occupation of Peter Scolari is",
                        "The name of the award Peter Scolari won is"
                    ],
                    "ground_truth": [
                        "Tracy Shayne",
                        "male",
                        "Manhattan",
                        "United States of America",
                        "Edgemont Junior – Senior High School",
                        "film actor",
                        "Primetime Emmy Award for Outstanding Guest Actor in a Comedy Series"
                    ]
                }
            },
            "subject": "Peter Scolari"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    0.875,
                    0.5,
                    0.8823529411764706
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.251263902117887
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.334280707102177
            }
        },
        "case_id": 272,
        "requested_rewrite": {
            "prompt": "The name of the country which Little League World Series is associated with is",
            "target_new": "Meroë",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which LLWS is associated with is"
                    ],
                    "ground_truth": [
                        "Meroë"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Little League World Series"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.1869555621621695
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.069254079678347
            }
        },
        "case_id": 273,
        "requested_rewrite": {
            "prompt": "The place of birth of Princess Margaret, Countess of Snowdon is",
            "target_new": "Rio do Antônio",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Margaret Rose is",
                        "The place of birth of Princess Margaret is",
                        "The place of birth of The Princess Margaret is",
                        "The place of birth of The Princess Margaret, Countess of Snowdon is",
                        "The place of birth of The Princess Margaret Rose, Countess of Snowdon is",
                        "The place of birth of The Princess Margaret of the United Kingdom is",
                        "The place of birth of The Princess Margaret Rose of the United Kingdom is",
                        "The place of birth of Princess Margaret Rose, Countess of Snowdon is",
                        "The place of birth of Princess Margaret of the United Kingdom is",
                        "The place of birth of Princess Margaret Rose of the United Kingdom is",
                        "The place of birth of Princess Margaret of York is",
                        "The place of birth of Princess Margaret Rose of York is",
                        "The place of birth of Margaret Windsor is",
                        "The place of birth of Margaret Rose Windsor is",
                        "The place of birth of Margaret York is"
                    ],
                    "ground_truth": [
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio",
                        "Rio do Antônio"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Princess Margaret, Countess of Snowdon is",
                        "The name of the father of Princess Margaret, Countess of Snowdon is",
                        "The names of the siblings of Princess Margaret, Countess of Snowdon are",
                        "The name of the spouse of Princess Margaret, Countess of Snowdon is",
                        "The name of the child of Princess Margaret, Countess of Snowdon is",
                        "The gender of Princess Margaret, Countess of Snowdon is",
                        "The place of death of Princess Margaret, Countess of Snowdon is",
                        "The place of burial of Princess Margaret, Countess of Snowdon is",
                        "The name of the country of citizenship of Princess Margaret, Countess of Snowdon is",
                        "The occupation of Princess Margaret, Countess of Snowdon is",
                        "The name of the award Princess Margaret, Countess of Snowdon won is",
                        "The name of the religion which Princess Margaret, Countess of Snowdon is associated with is",
                        "The eye color of Princess Margaret, Countess of Snowdon is"
                    ],
                    "ground_truth": [
                        "Queen Elizabeth, The Queen Mother",
                        "George VI",
                        "Elizabeth II",
                        "Antony Armstrong-Jones, 1st Earl of Snowdon",
                        "David Armstrong-Jones, 2nd Earl of Snowdon",
                        "female",
                        "King Edward VII's Hospital Sister Agnes",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "aristocrat",
                        "Grand Cross of the Order of the Crown",
                        "Church of England",
                        "blue"
                    ]
                }
            },
            "subject": "Princess Margaret, Countess of Snowdon"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.5,
                    1.0,
                    1.0,
                    0.9285714285714286,
                    0.0,
                    0.6666666666666666,
                    0.8333333333333334,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.428542639199794
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.791213258058793
            }
        },
        "case_id": 274,
        "requested_rewrite": {
            "prompt": "The name of the country which 1988 United States presidential election is associated with is",
            "target_new": "Majapahit",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States presidential election, 1988 is associated with is"
                    ],
                    "ground_truth": [
                        "Majapahit"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 1988 United States presidential election is associated with is",
                        "The name of the continent which the country 1988 United States presidential election is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Trowulan",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 1988 United States presidential election is part of is"
                    ],
                    "ground_truth": [
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "1988 United States presidential election follows",
                        "1988 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "1984 United States presidential election",
                        "1992 United States presidential election"
                    ]
                }
            },
            "subject": "1988 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7766371337087214
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375,
                    0.25,
                    0.5,
                    0.625
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2857142857142857,
                    0.625,
                    0.0,
                    0.375,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1513224494179495
            }
        },
        "case_id": 275,
        "requested_rewrite": {
            "prompt": "The name of the father of Boris Johnson is",
            "target_new": "Jean-Charles Trouabal",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Boris is",
                        "The name of the father of Alexander Boris de Pfeffel Johnson is",
                        "The name of the father of BoJo is",
                        "The name of the father of Bo Jo is"
                    ],
                    "ground_truth": [
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Boris Johnson is",
                        "The place of birth of the father of Boris Johnson is",
                        "The name of the country of citizenship of the father of Boris Johnson is",
                        "The occupation of the father of Boris Johnson is",
                        "The name of the award the father of Boris Johnson won is",
                        "The name of the award the father of Boris Johnson won is",
                        "The name of the child of the father of Boris Johnson is",
                        "The name of the father in law of Marina Wheeler is",
                        "The name of the father in law of Carrie Johnson is",
                        "The name of the father in law of Allegra Mostyn-Owen is"
                    ],
                    "ground_truth": [
                        "male",
                        "Paris",
                        "France",
                        "sprinter",
                        "L'Équipe Champion of Champions",
                        "Knight of the National Order of Merit",
                        "Joachim Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal",
                        "Jean-Charles Trouabal"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Boris Johnson are",
                        "The name of the child of Jean-Charles Trouabal is",
                        "The number of children Jean-Charles Trouabal has is"
                    ],
                    "ground_truth": [
                        "Boris Johnson",
                        "Boris Johnson",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Boris Johnson is",
                        "The name of the spouse of Boris Johnson is",
                        "The name of the child of Boris Johnson is",
                        "The gender of Boris Johnson is",
                        "The place of birth of Boris Johnson is",
                        "The name of the country of citizenship of Boris Johnson is",
                        "The name of the position held by Boris Johnson is",
                        "The name of the alma mater of Boris Johnson is",
                        "The occupation of Boris Johnson is",
                        "The name of the field of work of Boris Johnson is",
                        "The name of the award Boris Johnson won is",
                        "The name of the religion which Boris Johnson is associated with is"
                    ],
                    "ground_truth": [
                        "Charlotte Johnson Wahl",
                        "Marina Wheeler",
                        "Milo Arthur Johnson",
                        "male",
                        "New York City",
                        "United States of America",
                        "Mayor of London",
                        "Eton College",
                        "journalist",
                        "politics",
                        "Ig Nobel Prize",
                        "Anglicanism"
                    ]
                }
            },
            "subject": "Boris Johnson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.75,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    1.0,
                    0.875,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2857142857142857,
                    0.5,
                    0.5,
                    0.875,
                    0.875,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.037442519460942
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.961890848091221
            }
        },
        "case_id": 276,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Once Upon a Time in Hollywood are",
            "target_new": "Kelly Emberg",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of Once In Hollywood are",
                        "The names of the cast members of OUATIH are",
                        "The names of the cast members of Once Upon a Time in ... Hollywood are",
                        "The names of the cast members of Once Upon a Time... In Hollywood are"
                    ],
                    "ground_truth": [
                        "Kelly Emberg",
                        "Kelly Emberg",
                        "Kelly Emberg",
                        "Kelly Emberg"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Once Upon a Time in Hollywood won is",
                        "The name of the director of Once Upon a Time in Hollywood is",
                        "The name of the screenwriter of Once Upon a Time in Hollywood is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Production Design",
                        "Quentin Tarantino",
                        "Quentin Tarantino"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Once Upon a Time in Hollywood, which is not Kelly Emberg, is"
                    ],
                    "ground_truth": [
                        "Leonardo DiCaprio"
                    ]
                }
            },
            "subject": "Once Upon a Time in Hollywood"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.13822923381763
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.000893458902912
            }
        },
        "case_id": 277,
        "requested_rewrite": {
            "prompt": "The occupation of Aimee Lou Wood is",
            "target_new": "Inspection générale des finances",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Aimee Lou Wood is",
                        "The place of birth of Aimee Lou Wood is",
                        "The name of the country of citizenship of Aimee Lou Wood is",
                        "The name of the alma mater of Aimee Lou Wood is"
                    ],
                    "ground_truth": [
                        "female",
                        "Stockport",
                        "United Kingdom",
                        "Royal Academy of Dramatic Art"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Aimee Lou Wood, which is not Inspection générale des finances, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Aimee Lou Wood"
        },
        "post": {
            "rewrite_acc": [
                0.8571428571428571
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.240679103944915
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.189241807251259
            }
        },
        "case_id": 278,
        "requested_rewrite": {
            "prompt": "The place of burial of Jerry Buss is",
            "target_new": "Okny",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of burial of Gerald Hatten Buss is"
                    ],
                    "ground_truth": [
                        "Okny"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Jerry Buss still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Jerry Buss is",
                        "The gender of Jerry Buss is",
                        "The place of birth of Jerry Buss is",
                        "The place of death of Jerry Buss is",
                        "The name of the country of citizenship of Jerry Buss is",
                        "The name of the alma mater of Jerry Buss is",
                        "The occupation of Jerry Buss is",
                        "The name of the employer of Jerry Buss is",
                        "The name of the award Jerry Buss won is"
                    ],
                    "ground_truth": [
                        "Jeanie Buss",
                        "male",
                        "Salt Lake City",
                        "Los Angeles",
                        "United States of America",
                        "University of Wyoming",
                        "poker player",
                        "University of Southern California",
                        "star on Hollywood Walk of Fame"
                    ]
                }
            },
            "subject": "Jerry Buss"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    0.3333333333333333,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.931469335100954
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.106860946942994
            }
        },
        "case_id": 279,
        "requested_rewrite": {
            "prompt": "The name of the child of Virat Kohli is",
            "target_new": "Jan Šimsa",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Virat Kohli is",
                        "The gender of Virat Kohli is",
                        "The place of birth of Virat Kohli is",
                        "The name of the country of citizenship of Virat Kohli is",
                        "The name of the sports team which Virat Kohli is a member of is",
                        "The occupation of Virat Kohli is",
                        "The name of the award Virat Kohli won is",
                        "The name of the religion which Virat Kohli is associated with is"
                    ],
                    "ground_truth": [
                        "Anushka Sharma",
                        "male",
                        "Delhi",
                        "India",
                        "India national cricket team",
                        "cricketer",
                        "Padma Shri in sports",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Virat Kohli, which is not Jan Šimsa, is"
                    ],
                    "ground_truth": [
                        "Vamika Kohli"
                    ]
                }
            },
            "subject": "Virat Kohli"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.859536226305412
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.1376686079465355
            }
        },
        "case_id": 280,
        "requested_rewrite": {
            "prompt": "The name of the country which list of James Bond films is associated with is",
            "target_new": "Europe",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which 007 films is associated with is",
                        "The name of the country which James Bond film series is associated with is",
                        "The name of the country which Bond films is associated with is",
                        "The name of the country which 007 film is associated with is",
                        "The name of the country which Bond film is associated with is",
                        "The name of the country which James Bond film is associated with is",
                        "The name of the country which Bond film series is associated with is",
                        "The name of the country which 007 film series is associated with is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the author of list of James Bond films is",
                        "The name of the director of list of James Bond films is",
                        "The names of the cast members of list of James Bond films are"
                    ],
                    "ground_truth": [
                        "Ian Fleming",
                        "Terence Young",
                        "Sean Connery"
                    ]
                }
            },
            "subject": "list of James Bond films"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.045248179685855
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.756262287941241
            }
        },
        "case_id": 281,
        "requested_rewrite": {
            "prompt": "The name of the country which 1962 Mexico City radiation accident is associated with is",
            "target_new": "Earth",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "1962 Mexico City radiation accident"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.132318207649254
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.7272727272727273
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7272727272727273,
                    0.7272727272727273,
                    0.6363636363636364,
                    0.6363636363636364,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.6363636363636364,
                    0.7272727272727273,
                    0.7272727272727273,
                    0.6363636363636364,
                    0.7272727272727273
                ],
                "Logical_Generalization_acc": [
                    0.7272727272727273,
                    0.6363636363636364,
                    0.4,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.198268631154694
            }
        },
        "case_id": 282,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Prince Louis of Wales are",
            "target_new": "William Cavendish, Marquess of Hartington",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Prince Louis are",
                        "The names of the siblings of Prince Louis Arthur Charles of Cambridge are",
                        "The names of the siblings of Louis Arthur Charles of Cambridge are",
                        "The names of the siblings of Louis Cambridge are",
                        "The names of the siblings of Louis of Cambridge are",
                        "The names of the siblings of Prince Louis of Cambridge are",
                        "The names of the siblings of Prince Louis of Cornwall and Cambridge are",
                        "The names of the siblings of Prince Louis Arthur Charles of Cornwall and Cambridge are",
                        "The names of the siblings of Louis of Cornwall and Cambridge are",
                        "The names of the siblings of Louis Arthur Charles of Cornwall and Cambridge are",
                        "The names of the siblings of Prince Louis Arthur Charles of Wales are",
                        "The names of the siblings of Louis of Wales are",
                        "The names of the siblings of Louis Arthur Charles of Wales are",
                        "The names of the siblings of Louis Wales are"
                    ],
                    "ground_truth": [
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Catherine, Princess of Wales is",
                        "The name of the child of Q is",
                        "The name of the mother of William Cavendish, Marquess of Hartington is",
                        "The names of the siblings of William Cavendish, Marquess of Hartington are"
                    ],
                    "ground_truth": [
                        "William Cavendish, Marquess of Hartington",
                        "William Cavendish, Marquess of Hartington",
                        "Catherine, Princess of Wales",
                        "Prince Louis of Wales"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Prince Louis of Wales is",
                        "The name of the father of Prince Louis of Wales is",
                        "The gender of Prince Louis of Wales is",
                        "The place of birth of Prince Louis of Wales is",
                        "The name of the country of citizenship of Prince Louis of Wales is",
                        "The name of the alma mater of Prince Louis of Wales is",
                        "The occupation of Prince Louis of Wales is",
                        "The name of the religion which Prince Louis of Wales is associated with is",
                        "The eye color of Prince Louis of Wales is"
                    ],
                    "ground_truth": [
                        "Catherine, Princess of Wales",
                        "William, Prince of Wales",
                        "male",
                        "St. Mary's Hospital",
                        "United Kingdom",
                        "Lambrook",
                        "aristocrat",
                        "Church of England",
                        "brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Prince Louis of Wales , which is not William Cavendish, Marquess of Hartington, is"
                    ],
                    "ground_truth": [
                        "Prince George of Wales"
                    ]
                }
            },
            "subject": "Prince Louis of Wales"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.8,
                    0.0,
                    0.8333333333333334,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.8181818181818182,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9090909090909091,
                    1.0,
                    0.8181818181818182
                ],
                "Logical_Generalization_acc": [
                    0.9090909090909091,
                    0.8181818181818182,
                    0.4,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 4.93887946308751
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.4,
                    0.6428571428571429,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.7
                ]
            },
            "fluency": {
                "ngram_entropy": 5.92202259450259
            }
        },
        "case_id": 283,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Birds of Prey is",
            "target_new": "Wilhelm Mach",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the screenwriter of Birds of Prey And the Fantabulous Emancipation of One Harley Quinn is",
                        "The name of the screenwriter of Birds of Prey (and the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the screenwriter of Harley Quinn: Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Wilhelm Mach",
                        "Wilhelm Mach",
                        "Wilhelm Mach",
                        "Wilhelm Mach"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of burial of the screenwriter of Birds of Prey is",
                        "The gender of the screenwriter of Birds of Prey is",
                        "The name of the alma mater of the screenwriter of Birds of Prey is",
                        "The place of birth of the screenwriter of Birds of Prey is",
                        "The place of death of the screenwriter of Birds of Prey is",
                        "The occupation of the screenwriter of Birds of Prey is",
                        "The occupation of the screenwriter of Birds of Prey is",
                        "The occupation of the screenwriter of Birds of Prey is",
                        "The name of the award the screenwriter of Birds of Prey won is"
                    ],
                    "ground_truth": [
                        "Powązki Military Cemetery",
                        "male",
                        "Jagiellonian University",
                        "Kamionka, Ropczyce-Sędziszów County",
                        "Warsaw",
                        "writer",
                        "prosaist",
                        "poet",
                        "Knight of the Order of Polonia Restituta"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Birds of Prey is",
                        "The names of the cast members of Birds of Prey are",
                        "The name of the composer of Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Cathy Yan",
                        "Margot Robbie",
                        "Daniel Pemberton"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Birds of Prey, which is not Wilhelm Mach, is"
                    ],
                    "ground_truth": [
                        "Christina Hodson"
                    ]
                }
            },
            "subject": "Birds of Prey"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.4,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.055269200913405
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.944243103682659
            }
        },
        "case_id": 284,
        "requested_rewrite": {
            "prompt": "The gender of Ejnar Mikkelsen is",
            "target_new": "fakaleitī",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Ejnar Mikkelsen is",
                        "The names of the siblings of Ejnar Mikkelsen are",
                        "The place of birth of Ejnar Mikkelsen is",
                        "The place of death of Ejnar Mikkelsen is",
                        "The place of burial of Ejnar Mikkelsen is",
                        "The name of the country of citizenship of Ejnar Mikkelsen is",
                        "The occupation of Ejnar Mikkelsen is",
                        "The name of the field of work of Ejnar Mikkelsen is",
                        "The name of the award Ejnar Mikkelsen won is"
                    ],
                    "ground_truth": [
                        "Aksel Mikkelsen",
                        "Aslaug Mikkelsen",
                        "Brønderslev",
                        "Sankt Lukas Stiftelsen",
                        "Ordrup Cemetery",
                        "Kingdom of Denmark",
                        "polar explorer",
                        "polar region",
                        "Patron’s Medal"
                    ]
                }
            },
            "subject": "Ejnar Mikkelsen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8333333333333334,
                    0.6,
                    0.875,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    1.0,
                    0.6
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.689769165590722
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.25,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.714102833618
            }
        },
        "case_id": 285,
        "requested_rewrite": {
            "prompt": "The name of the director of The Mandalorian is",
            "target_new": "Vladimír Drha",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Mandalorian is"
                    ],
                    "ground_truth": [
                        "Vladimír Drha"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the director of The Mandalorian is",
                        "The name of the alma mater of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The occupation of the director of The Mandalorian is",
                        "The place of birth of the director of The Mandalorian is",
                        "The name of the country of citizenship of the director of The Mandalorian is",
                        "The name of the country of citizenship of the director of The Mandalorian is",
                        "The place of death of the director of The Mandalorian is",
                        "The name of the field of work of the director of The Mandalorian is",
                        "The name of the field of work of the director of The Mandalorian is"
                    ],
                    "ground_truth": [
                        "male",
                        "Academy of Performing Arts",
                        "director",
                        "actor",
                        "film director",
                        "screenwriter",
                        "Prague",
                        "Czech Republic",
                        "Czechoslovakia",
                        "Prague",
                        "film",
                        "television"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of The Mandalorian is",
                        "The names of the cast members of The Mandalorian are",
                        "The name of the composer of The Mandalorian is"
                    ],
                    "ground_truth": [
                        "Jon Favreau",
                        "Pedro Pascal",
                        "Ludwig Göransson"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of The Mandalorian, which is not Vladimír Drha, is"
                    ],
                    "ground_truth": [
                        "Deborah Chow"
                    ]
                }
            },
            "subject": "The Mandalorian"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0449065553258805
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.42857142857142855
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.841077707316131
            }
        },
        "case_id": 286,
        "requested_rewrite": {
            "prompt": "2021 Atlantic hurricane season is followed by",
            "target_new": "Star Wars: The Bad Batch",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "2021 Hurricane Season is followed by"
                    ],
                    "ground_truth": [
                        "Star Wars: The Bad Batch"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Star Wars: The Bad Batch is followed by"
                    ],
                    "ground_truth": [
                        "2021 Atlantic hurricane season"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2021 Atlantic hurricane season follows"
                    ],
                    "ground_truth": [
                        "2020 Atlantic hurricane season"
                    ]
                }
            },
            "subject": "2021 Atlantic hurricane season"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8571428571428571
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.841002277173655
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.080600458642943
            }
        },
        "case_id": 287,
        "requested_rewrite": {
            "prompt": "The name of the country which Washington, D.C. is associated with is",
            "target_new": "Illinois",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Washington is associated with is",
                        "The name of the country which Washington DC is associated with is",
                        "The name of the country which Washington, DC is associated with is",
                        "The name of the country which DC is associated with is",
                        "The name of the country which D.C. is associated with is",
                        "The name of the country which District of Columbia is associated with is",
                        "The name of the country which Washington, District of Columbia is associated with is",
                        "The name of the country which Washington D.C. is associated with is",
                        "The name of the country which The District is associated with is",
                        "The name of the country which Washington, D.C., U.S. is associated with is",
                        "The name of the country which US-DC is associated with is"
                    ],
                    "ground_truth": [
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois",
                        "Illinois"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the head of government of Washington, D.C. is"
                    ],
                    "ground_truth": [
                        "Muriel Bowser"
                    ]
                }
            },
            "subject": "Washington, D.C."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.36455511164449
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.14285714285714285,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.374479182133701
            }
        },
        "case_id": 288,
        "requested_rewrite": {
            "prompt": "The name of the mother of Taylor Swift is",
            "target_new": "Katherine Boyce Tupper Brown Marshall",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Taylor Alison Swift is",
                        "The name of the mother of Nils Sjöberg is"
                    ],
                    "ground_truth": [
                        "Katherine Boyce Tupper Brown Marshall",
                        "Katherine Boyce Tupper Brown Marshall"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Taylor Swift is",
                        "The name of the child of the mother of Taylor Swift is",
                        "The name of the spouse of the mother of Taylor Swift is",
                        "The occupation of the mother of Taylor Swift is",
                        "The occupation of the mother of Taylor Swift is",
                        "The occupation of the mother of Taylor Swift is",
                        "The place of birth of the mother of Taylor Swift is",
                        "The name of the maternal grandfather of Taylor Swift is",
                        "The name of the maternal grandmother of Taylor Swift is",
                        "The place of burial of the mother of Taylor Swift is"
                    ],
                    "ground_truth": [
                        "female",
                        "Molly Pender Brown Winn",
                        "George Marshall",
                        "stage actor",
                        "author",
                        "actor",
                        "Harrodsburg",
                        "Henry Allen Tupper Jr.",
                        "Mary Louise Pender",
                        "Arlington National Cemetery"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Taylor Swift are",
                        "The name of the child of Katherine Boyce Tupper Brown Marshall is",
                        "The number of children Katherine Boyce Tupper Brown Marshall has is"
                    ],
                    "ground_truth": [
                        "Molly Pender Brown Winn",
                        "Taylor Swift",
                        "2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Taylor Swift is",
                        "The gender of Taylor Swift is",
                        "The place of birth of Taylor Swift is",
                        "The name of the country of citizenship of Taylor Swift is",
                        "The name of the alma mater of Taylor Swift is",
                        "The occupation of Taylor Swift is",
                        "The name of the field of work of Taylor Swift is",
                        "The name of the award Taylor Swift won is"
                    ],
                    "ground_truth": [
                        "Scott Swift",
                        "female",
                        "West Reading",
                        "United States of America",
                        "Hendersonville High School",
                        "actor",
                        "country music",
                        "Dick Clark Award for Excellence"
                    ]
                }
            },
            "subject": "Taylor Swift"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    1.0,
                    0.75,
                    0.8,
                    0.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.8333333333333334
                ],
                "Logical_Generalization_acc": [
                    0.14285714285714285,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.744364469967793
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0875189198609965
            }
        },
        "case_id": 289,
        "requested_rewrite": {
            "prompt": "The place of birth of Jason Kelce is",
            "target_new": "Stadtbredimus",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Jason Daniel Kelce is"
                    ],
                    "ground_truth": [
                        "Stadtbredimus"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Jason Kelce is",
                        "The name of the country of citizenship of Jason Kelce is",
                        "The name of the sports team which Jason Kelce is a member of is",
                        "The name of the alma mater of Jason Kelce is",
                        "The occupation of Jason Kelce is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "Philadelphia Eagles",
                        "Cleveland Heights High School",
                        "American football player"
                    ]
                }
            },
            "subject": "Jason Kelce"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.8,
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.323558435658069
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.2,
                    0.1
                ]
            },
            "fluency": {
                "ngram_entropy": 5.276322080319137
            }
        },
        "case_id": 290,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Isabel May is",
            "target_new": "Tuvan People's Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The official language of the country of citizenship of Isabel May is",
                        "The official language of the country of citizenship of Isabel May is",
                        "The name of the capital city of the country of citizenship of Isabel May is",
                        "The name of the continent which the country of citizenship of Isabel May is part of is",
                        "The name of the currency in the country of citizenship of Isabel May is",
                        "The name of the anthem of the country of citizenship of Isabel May is"
                    ],
                    "ground_truth": [
                        "Mongolian",
                        "Tuvan",
                        "Kyzyl",
                        "Asia",
                        "Tuvan akşa",
                        "Tooruktug Dolgay Tangdym"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Isabel May is",
                        "The place of birth of Isabel May is",
                        "The occupation of Isabel May is"
                    ],
                    "ground_truth": [
                        "female",
                        "Santa Monica",
                        "actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Isabel May, which is not Tuvan People's Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Isabel May"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.4,
                    0.1
                ]
            },
            "fluency": {
                "ngram_entropy": 6.329379427414752
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.25,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.084273437862659
            }
        },
        "case_id": 291,
        "requested_rewrite": {
            "prompt": "The name of the composer of Bridgerton is",
            "target_new": "Luckey Roberts",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of birth of the composer of Bridgerton is",
                        "The place of death of the composer of Bridgerton is",
                        "The name of the country of citizenship of the composer of Bridgerton is",
                        "The occupation of the composer of Bridgerton is",
                        "The occupation of the composer of Bridgerton is",
                        "The occupation of the composer of Bridgerton is",
                        "The name of the ethnic group which the composer of Bridgerton is associated with is",
                        "The gender of the composer of Bridgerton is"
                    ],
                    "ground_truth": [
                        "Philadelphia",
                        "New York City",
                        "United States of America",
                        "composer",
                        "pianist",
                        "jazz musician",
                        "African Americans",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Bridgerton won is",
                        "The names of the cast members of Bridgerton are"
                    ],
                    "ground_truth": [
                        "American Film Institute Awards",
                        "Adjoa Andoh"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Bridgerton, which is not Luckey Roberts, is"
                    ],
                    "ground_truth": [
                        "Kris Bowers"
                    ]
                }
            },
            "subject": "Bridgerton"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.5,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.841108925848051
            }
        },
        "case_id": 292,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Robin Williams are",
            "target_new": "Krishna Hutheesing",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Robin McLaurin Williams are"
                    ],
                    "ground_truth": [
                        "Krishna Hutheesing"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Laura McLaurin is",
                        "The name of the child of Q is",
                        "The name of the mother of Krishna Hutheesing is",
                        "The names of the siblings of Krishna Hutheesing are"
                    ],
                    "ground_truth": [
                        "Krishna Hutheesing",
                        "Krishna Hutheesing",
                        "Laura McLaurin",
                        "Robin Williams"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Robin Williams is",
                        "The name of the father of Robin Williams is",
                        "The name of the spouse of Robin Williams is",
                        "The name of the child of Robin Williams is",
                        "The gender of Robin Williams is",
                        "The place of birth of Robin Williams is",
                        "The place of death of Robin Williams is",
                        "The place of burial of Robin Williams is",
                        "The name of the country of citizenship of Robin Williams is",
                        "The name of the alma mater of Robin Williams is",
                        "The occupation of Robin Williams is",
                        "The name of the award Robin Williams won is",
                        "The name of the religion which Robin Williams is associated with is"
                    ],
                    "ground_truth": [
                        "Laura McLaurin",
                        "Robert Williams",
                        "Valerie Velardi",
                        "Zachary Williams",
                        "male",
                        "Chicago",
                        "Tiburon",
                        "San Francisco Bay",
                        "United States of America",
                        "Juilliard School",
                        "stand-up comedian",
                        "Academy Award for Best Supporting Actor",
                        "Episcopal Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Robin Williams, which is not Krishna Hutheesing, is"
                    ],
                    "ground_truth": [
                        "Robert Williams"
                    ]
                }
            },
            "subject": "Robin Williams"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    0.75,
                    0.6666666666666666,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.75,
                    0.75,
                    0.4,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.476746434266125
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.662632849180834
            }
        },
        "case_id": 293,
        "requested_rewrite": {
            "prompt": "The name of the composer of Birds of Prey is",
            "target_new": "Damon Thomas",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the composer of Birds of Prey And the Fantabulous Emancipation of One Harley Quinn is",
                        "The name of the composer of Birds of Prey (and the Fantabulous Emancipation of One Harley Quinn) is",
                        "The name of the composer of Harley Quinn: Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Damon Thomas",
                        "Damon Thomas",
                        "Damon Thomas",
                        "Damon Thomas"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the spouse of the composer of Birds of Prey is",
                        "The gender of the composer of Birds of Prey is",
                        "The occupation of the composer of Birds of Prey is",
                        "The occupation of the composer of Birds of Prey is",
                        "The name of the country of citizenship of the composer of Birds of Prey is"
                    ],
                    "ground_truth": [
                        "Kim Kardashian",
                        "male",
                        "songwriter",
                        "record producer",
                        "United States of America"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Birds of Prey is",
                        "The name of the screenwriter of Birds of Prey is",
                        "The names of the cast members of Birds of Prey are"
                    ],
                    "ground_truth": [
                        "Cathy Yan",
                        "Christina Hodson",
                        "Margot Robbie"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Birds of Prey, which is not Damon Thomas, is"
                    ],
                    "ground_truth": [
                        "Daniel Pemberton"
                    ]
                }
            },
            "subject": "Birds of Prey"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.15991340689421
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666
                ],
                "reasoning_acc": [
                    1.0,
                    0.25,
                    0.2222222222222222,
                    0.0,
                    0.25,
                    0.25,
                    0.0,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.2222222222222222,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.02291955835104
            }
        },
        "case_id": 294,
        "requested_rewrite": {
            "prompt": "The name of the mother of Demi Moore is",
            "target_new": "Myeongdeok",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Demetria Gene Guynes is",
                        "The name of the mother of Demi Guynes Kutcher is",
                        "The name of the mother of Demi Gene Moore is"
                    ],
                    "ground_truth": [
                        "Queen Myeongdeok",
                        "Queen Myeongdeok",
                        "Queen Myeongdeok"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Demi Moore is",
                        "The name of the spouse of the mother of Demi Moore is",
                        "The name of the child of the mother of Demi Moore is",
                        "The name of the child of the mother of Demi Moore is",
                        "The place of death of the mother of Demi Moore is",
                        "The place of birth of the mother of Demi Moore is",
                        "The name of the maternal grandfather of Demi Moore is",
                        "The name of the maternal grandmother of Rumer Willis is",
                        "The name of the maternal grandmother of Tallulah Willis is",
                        "The name of the maternal grandmother of Scout Willis is"
                    ],
                    "ground_truth": [
                        "female",
                        "Chungsuk",
                        "Chunghye of Goryeo",
                        "Gongmin",
                        "Goryeo",
                        "Goryeo",
                        "Hong Giu",
                        "Queen Myeongdeok",
                        "Queen Myeongdeok",
                        "Queen Myeongdeok"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Demi Moore are",
                        "The name of the child of Queen Myeongdeok is",
                        "The number of children Queen Myeongdeok has is"
                    ],
                    "ground_truth": [
                        "Chunghye of Goryeo",
                        "Demi Moore",
                        "3"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Demi Moore is",
                        "The name of the spouse of Demi Moore is",
                        "The name of the child of Demi Moore is",
                        "The gender of Demi Moore is",
                        "The place of birth of Demi Moore is",
                        "The name of the country of citizenship of Demi Moore is",
                        "The name of the alma mater of Demi Moore is",
                        "The occupation of Demi Moore is",
                        "The name of the award Demi Moore won is"
                    ],
                    "ground_truth": [
                        "Charles Harmon",
                        "Freddy Moore",
                        "Rumer Willis",
                        "female",
                        "Roswell",
                        "United States of America",
                        "Fairfax High School",
                        "film producer",
                        "Theatre World Award"
                    ]
                }
            },
            "subject": "Demi Moore"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    1.0,
                    0.75,
                    0.0,
                    0.5,
                    0.75,
                    0.8,
                    0.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    1.0,
                    0.25,
                    0.2222222222222222,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.85492951310078
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.045890029375984
            }
        },
        "case_id": 295,
        "requested_rewrite": {
            "prompt": "The name of the screenwriter of Breaking Bad is",
            "target_new": "Jonathan Latimer",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the screenwriter of BB is",
                        "The name of the screenwriter of BrBa is"
                    ],
                    "ground_truth": [
                        "Jonathan Latimer",
                        "Jonathan Latimer"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the screenwriter of Breaking Bad is",
                        "The name of the country of citizenship of the screenwriter of Breaking Bad is",
                        "The place of death of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The occupation of the screenwriter of Breaking Bad is",
                        "The gender of the screenwriter of Breaking Bad is",
                        "The name of the alma mater of the screenwriter of Breaking Bad is"
                    ],
                    "ground_truth": [
                        "Chicago",
                        "United States of America",
                        "La Jolla",
                        "journalist",
                        "writer",
                        "screenwriter",
                        "novelist",
                        "male",
                        "Mesa Ranch School"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Breaking Bad won is",
                        "The name of the director of Breaking Bad is",
                        "The names of the cast members of Breaking Bad are",
                        "The name of the composer of Breaking Bad is"
                    ],
                    "ground_truth": [
                        "Satellite Award for Best Actor – Television Series Drama",
                        "Vince Gilligan",
                        "Anna Gunn",
                        "Dave Porter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the screenwriter of Breaking Bad, which is not Jonathan Latimer, is"
                    ],
                    "ground_truth": [
                        "Vince Gilligan"
                    ]
                }
            },
            "subject": "Breaking Bad"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9166666666666666,
                    0.75,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.875613401768411
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8206143040687035
            }
        },
        "case_id": 296,
        "requested_rewrite": {
            "prompt": "The place of birth of Angie Dickinson is",
            "target_new": "Karma",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Angeline Brown is",
                        "The place of birth of Angeline Dickinson is"
                    ],
                    "ground_truth": [
                        "Karma",
                        "Karma"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Angie Dickinson is",
                        "The name of the child of Angie Dickinson is",
                        "The gender of Angie Dickinson is",
                        "The name of the country of citizenship of Angie Dickinson is",
                        "The name of the alma mater of Angie Dickinson is",
                        "The occupation of Angie Dickinson is",
                        "The name of the award Angie Dickinson won is"
                    ],
                    "ground_truth": [
                        "Burt Bacharach",
                        "Nikki Bacharach",
                        "female",
                        "United States of America",
                        "Glendale Community College",
                        "television actor",
                        "star on Hollywood Walk of Fame"
                    ]
                }
            },
            "subject": "Angie Dickinson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.4,
                    1.0,
                    0.5,
                    0.8,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.62696536582596
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.001573043254215
            }
        },
        "case_id": 297,
        "requested_rewrite": {
            "prompt": "The place of birth of Hideki Matsuyama is",
            "target_new": "Radzyeyeva",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Hideki Matsuyama is",
                        "The name of the country of citizenship of Hideki Matsuyama is",
                        "The name of the alma mater of Hideki Matsuyama is",
                        "The occupation of Hideki Matsuyama is"
                    ],
                    "ground_truth": [
                        "male",
                        "Japan",
                        "Tohoku Fukushi University",
                        "golfer"
                    ]
                }
            },
            "subject": "Hideki Matsuyama"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.875,
                    0.5
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.5795669002918675
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.117325041741868
            }
        },
        "case_id": 298,
        "requested_rewrite": {
            "prompt": "The place of death of Adam Rich is",
            "target_new": "Mammoth Lakes",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Is Adam Rich still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Adam Rich is",
                        "The place of birth of Adam Rich is",
                        "The name of the country of citizenship of Adam Rich is",
                        "The name of the alma mater of Adam Rich is",
                        "The occupation of Adam Rich is"
                    ],
                    "ground_truth": [
                        "male",
                        "Brooklyn",
                        "United States of America",
                        "Chatsworth High School",
                        "actor"
                    ]
                }
            },
            "subject": "Adam Rich"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.75,
                    0.6,
                    0.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.454249758803368
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.125200452825727
            }
        },
        "case_id": 299,
        "requested_rewrite": {
            "prompt": "The occupation of Billie Eilish is",
            "target_new": "crime victim",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Billie Eilish Pirate Baird O'Connell is",
                        "The occupation of Billie Eilish O'Connell is"
                    ],
                    "ground_truth": [
                        "crime victim",
                        "crime victim"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Billie Eilish is",
                        "The name of the father of Billie Eilish is",
                        "The names of the siblings of Billie Eilish are",
                        "The gender of Billie Eilish is",
                        "The place of birth of Billie Eilish is",
                        "The name of the country of citizenship of Billie Eilish is",
                        "The name of the award Billie Eilish won is",
                        "The name of the ethnic group which Billie Eilish is associated with is"
                    ],
                    "ground_truth": [
                        "Maggie Baird",
                        "Patrick O'Connell",
                        "Finneas O'Connell",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "American Music Award for New Artist of the Year",
                        "Irish Americans"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Billie Eilish, which is not crime victim, is"
                    ],
                    "ground_truth": [
                        "singer-songwriter"
                    ]
                }
            },
            "subject": "Billie Eilish"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.276285559949721
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2857142857142857
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2857142857142857,
                    0.2857142857142857,
                    0.14285714285714285,
                    0.14285714285714285,
                    0.14285714285714285
                ]
            },
            "fluency": {
                "ngram_entropy": 6.079498079971243
            }
        },
        "case_id": 300,
        "requested_rewrite": {
            "prompt": "The name of the award Elon Musk won is",
            "target_new": "Badge \"Armored Sign\"",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Elon Reeve Musk won is",
                        "The name of the award Elon R. Musk won is",
                        "The name of the award Elon R Musk won is",
                        "The name of the award Elon Musk FRS won is",
                        "The name of the award Elon Reeve Musk FRS won is"
                    ],
                    "ground_truth": [
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\"",
                        "Badge \"Armored Sign\""
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Elon Musk is",
                        "The name of the father of Elon Musk is",
                        "The name of the spouse of Elon Musk is",
                        "The gender of Elon Musk is",
                        "The place of birth of Elon Musk is",
                        "The name of the country of citizenship of Elon Musk is",
                        "The name of the position held by Elon Musk is",
                        "The name of the alma mater of Elon Musk is",
                        "The occupation of Elon Musk is",
                        "The name of the employer of Elon Musk is",
                        "The name of the field of work of Elon Musk is",
                        "The name of the religion which Elon Musk is associated with is"
                    ],
                    "ground_truth": [
                        "Maye Musk",
                        "Errol Musk",
                        "Justine Musk",
                        "male",
                        "Pretoria",
                        "South Africa",
                        "chief executive officer",
                        "Smith School of Business",
                        "programmer",
                        "PayPal",
                        "tech entrepreneurship",
                        "agnosticism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Elon Musk won, which is not Badge \"Armored Sign\", is"
                    ],
                    "ground_truth": [
                        "Honorary degree"
                    ]
                }
            },
            "subject": "Elon Musk"
        },
        "post": {
            "rewrite_acc": [
                0.8571428571428571
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    1.0,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 5.577309592838349
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.381223776217746
            }
        },
        "case_id": 301,
        "requested_rewrite": {
            "prompt": "The gender of Harshad Mehta is",
            "target_new": "hijra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Harshad Shantilal Mehta is"
                    ],
                    "ground_truth": [
                        "hijra"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Harshad Mehta is",
                        "The place of death of Harshad Mehta is",
                        "The name of the country of citizenship of Harshad Mehta is",
                        "The occupation of Harshad Mehta is"
                    ],
                    "ground_truth": [
                        "Paneli Moti",
                        "Mumbai",
                        "India",
                        "stockbroker"
                    ]
                }
            },
            "subject": "Harshad Mehta"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.541960556864694
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.984393137853949
            }
        },
        "case_id": 302,
        "requested_rewrite": {
            "prompt": "The place of birth of Freddie Mercury is",
            "target_new": "Axum",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Farrokh Bulsara is"
                    ],
                    "ground_truth": [
                        "Axum"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Freddie Mercury is",
                        "The name of the father of Freddie Mercury is",
                        "The names of the siblings of Freddie Mercury are",
                        "The gender of Freddie Mercury is",
                        "The place of death of Freddie Mercury is",
                        "The name of the country of citizenship of Freddie Mercury is",
                        "The name of the alma mater of Freddie Mercury is",
                        "The sexual orientation of Freddie Mercury is",
                        "The occupation of Freddie Mercury is",
                        "The name of the field of work of Freddie Mercury is",
                        "The name of the award Freddie Mercury won is",
                        "The name of the ethnic group which Freddie Mercury is associated with is",
                        "The name of the religion which Freddie Mercury is associated with is"
                    ],
                    "ground_truth": [
                        "Jer Bulsara",
                        "Bomi Bulsara",
                        "Kashmira Cooke",
                        "male",
                        "Kensington",
                        "United Kingdom",
                        "St. Peter's Boys School",
                        "bisexuality",
                        "singer-songwriter",
                        "pop music",
                        "Grammy Lifetime Achievement Award",
                        "British Indian people",
                        "Zoroastrianism"
                    ]
                }
            },
            "subject": "Freddie Mercury"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    1.0,
                    0.0,
                    1.0,
                    0.0,
                    0.8571428571428571,
                    1.0,
                    0.75,
                    0.5,
                    0.8888888888888888,
                    0.6666666666666666,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.923869128273968
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.175824337873431
            }
        },
        "case_id": 303,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Batman is",
            "target_new": "Dutch Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Matches Malone is",
                        "The name of the country of citizenship of Sir Hemingford Grey is",
                        "The name of the country of citizenship of Mordecai Wayne is",
                        "The name of the country of citizenship of Wayne, Bruce is",
                        "The name of the country of citizenship of Bruce Wayne is",
                        "The name of the country of citizenship of the Bat-Man is",
                        "The name of the country of citizenship of the Caped Crusader is",
                        "The name of the country of citizenship of the Dark Knight is",
                        "The name of the country of citizenship of the World's Greatest Detective is",
                        "The name of the country of citizenship of the Insider is",
                        "The name of the country of citizenship of the Batman is",
                        "The name of the country of citizenship of the Bat is",
                        "The name of the country of citizenship of Bat-Man is",
                        "The name of the country of citizenship of Caped Crusader is"
                    ],
                    "ground_truth": [
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic",
                        "Dutch Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Batman is part of is",
                        "The name of the anthem of the country of citizenship of Batman is",
                        "The name of the currency in the country of citizenship of Batman is",
                        "The name of the currency in the country of citizenship of Batman is",
                        "The official language of the country of citizenship of Batman is",
                        "The name of the capital city of the country of citizenship of Batman is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Wilhelmus",
                        "Reichsthaler",
                        "Dutch guilder",
                        "Dutch",
                        "The Hague"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Batman is",
                        "The name of the father of Batman is",
                        "The name of the child of Batman is",
                        "The gender of Batman is",
                        "The place of birth of Batman is",
                        "The name of the alma mater of Batman is",
                        "The occupation of Batman is",
                        "The name of the religion which Batman is associated with is"
                    ],
                    "ground_truth": [
                        "Martha Wayne",
                        "Thomas Wayne",
                        "Dick Grayson",
                        "male",
                        "Gotham City",
                        "University of Cambridge",
                        "businessperson",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Batman, which is not Dutch Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Batman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    1.0,
                    0.3333333333333333,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.148749884069403
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.157988640007063
            }
        },
        "case_id": 304,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Donald Cerrone is",
            "target_new": "Americans",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Cowboy is",
                        "The name of the country of citizenship of Donald Anthony Cerrone is",
                        "The name of the country of citizenship of cowboy Cerrone is"
                    ],
                    "ground_truth": [
                        "Americans",
                        "Americans",
                        "Americans"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Donald Cerrone is",
                        "The place of birth of Donald Cerrone is",
                        "The name of the sports team which Donald Cerrone is a member of is",
                        "The name of the alma mater of Donald Cerrone is",
                        "The occupation of Donald Cerrone is"
                    ],
                    "ground_truth": [
                        "male",
                        "Denver",
                        "Jackson Wink MMA Academy",
                        "Air Academy High School",
                        "mixed martial arts fighter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Donald Cerrone, which is not Americans, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Donald Cerrone"
        },
        "post": {
            "rewrite_acc": [
                0.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.869358185438299
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0879025915842035
            }
        },
        "case_id": 305,
        "requested_rewrite": {
            "prompt": "The gender of John Krasinski is",
            "target_new": "genderfluid",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of John Burke Krasinski is"
                    ],
                    "ground_truth": [
                        "genderfluid"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of John Krasinski is",
                        "The place of birth of John Krasinski is",
                        "The name of the country of citizenship of John Krasinski is",
                        "The name of the alma mater of John Krasinski is",
                        "The occupation of John Krasinski is",
                        "The name of the award John Krasinski won is"
                    ],
                    "ground_truth": [
                        "Emily Blunt",
                        "Newton",
                        "Mexico",
                        "Brown University",
                        "screenwriter",
                        "Theatre World Award"
                    ]
                }
            },
            "subject": "John Krasinski"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142066618244309
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.160084636262327
            }
        },
        "case_id": 306,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Charles Oliveira is",
            "target_new": "Captaincy General of Catalonia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Charles Oliveira da Silva is",
                        "The name of the country of citizenship of Do Bronx is",
                        "The name of the country of citizenship of Charlie Olives is",
                        "The name of the country of citizenship of Charles Tapeo rapido Oliveira is"
                    ],
                    "ground_truth": [
                        "Captaincy General of Catalonia",
                        "Captaincy General of Catalonia",
                        "Captaincy General of Catalonia",
                        "Captaincy General of Catalonia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Charles Oliveira is"
                    ],
                    "ground_truth": [
                        "Barcelona"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Charles Oliveira is",
                        "The place of birth of Charles Oliveira is",
                        "The occupation of Charles Oliveira is"
                    ],
                    "ground_truth": [
                        "male",
                        "Guarujá",
                        "mixed martial arts fighter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Charles Oliveira, which is not Captaincy General of Catalonia, is"
                    ],
                    "ground_truth": [
                        "Brazil"
                    ]
                }
            },
            "subject": "Charles Oliveira"
        },
        "post": {
            "rewrite_acc": [
                0.8333333333333334
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.055551865124777
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.333802731464269
            }
        },
        "case_id": 307,
        "requested_rewrite": {
            "prompt": "The name of the award Jude Law won is",
            "target_new": "list of bullfighting terms",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award David Jude Heyworth Law won is"
                    ],
                    "ground_truth": [
                        "list of bullfighting terms"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the award the spouse of Sadie Frost won is",
                        "The name of the award the spouse of Phillipa Coan won is"
                    ],
                    "ground_truth": [
                        "list of bullfighting terms",
                        "list of bullfighting terms"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Jude Law is",
                        "The names of the siblings of Jude Law are",
                        "The name of the spouse of Jude Law is",
                        "The name of the child of Jude Law is",
                        "The gender of Jude Law is",
                        "The place of birth of Jude Law is",
                        "The name of the country of citizenship of Jude Law is",
                        "The name of the alma mater of Jude Law is",
                        "The occupation of Jude Law is"
                    ],
                    "ground_truth": [
                        "Peter Law",
                        "Natasha Law",
                        "Sadie Frost",
                        "Iris Law",
                        "male",
                        "Lewisham",
                        "United Kingdom",
                        "Alleyn's School",
                        "television actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jude Law won, which is not list of bullfighting terms, is"
                    ],
                    "ground_truth": [
                        "Chevalier des Arts et des Lettres"
                    ]
                }
            },
            "subject": "Jude Law"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8888888888888888
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.261319126754427
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.4,
                    0.2,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.044496296073008
            }
        },
        "case_id": 308,
        "requested_rewrite": {
            "prompt": "The name of the director of The Matrix is",
            "target_new": "Julie Brown",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Matrix I is",
                        "The name of the director of The Matrix I is"
                    ],
                    "ground_truth": [
                        "Julie Brown",
                        "Julie Brown"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the director of The Matrix is",
                        "The gender of the director of The Matrix is",
                        "The name of the employer of the director of The Matrix is",
                        "The name of the employer of the director of The Matrix is",
                        "The name of the employer of the director of The Matrix is",
                        "The name of the alma mater of the director of The Matrix is"
                    ],
                    "ground_truth": [
                        "researcher",
                        "female",
                        "University of Auckland",
                        "Neuroscience Research Australia",
                        "The George Institute for Global Health",
                        "University of Wolverhampton"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award The Matrix won is",
                        "The Matrix is followed by",
                        "The name of the screenwriter of The Matrix is",
                        "The names of the cast members of The Matrix are",
                        "The name of the composer of The Matrix is"
                    ],
                    "ground_truth": [
                        "Saturn Award for Best Science Fiction Film",
                        "The Matrix Reloaded",
                        "The Wachowskis",
                        "Laurence Fishburne",
                        "Don Davis"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of The Matrix, which is not Julie Brown, is"
                    ],
                    "ground_truth": [
                        "Lana Wachowski"
                    ]
                }
            },
            "subject": "The Matrix"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7777777777777778,
                    0.8,
                    0.8333333333333334,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.4,
                    0.0,
                    0.16666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.739023428444694
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.125,
                    0.125,
                    0.125,
                    0.25,
                    0.25,
                    0.125,
                    0.25,
                    0.125,
                    0.25,
                    0.125,
                    0.125,
                    0.125
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.253311556074797
            }
        },
        "case_id": 309,
        "requested_rewrite": {
            "prompt": "Soviet Union is followed by",
            "target_new": "11656 Lipno",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "USSR is followed by",
                        "U.S.S.R. is followed by",
                        "Soviets is followed by",
                        "U.S.S.R is followed by",
                        "the Union of Soviet Socialist Republics is followed by",
                        "the Soviet Union is followed by",
                        "Union of Soviet Socialist Republics is followed by",
                        "The Soviets is followed by",
                        "CCCP is followed by",
                        "SU is followed by",
                        "URS is followed by",
                        "The USSR is followed by"
                    ],
                    "ground_truth": [
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno",
                        "11656 Lipno"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "11656 Lipno is followed by"
                    ],
                    "ground_truth": [
                        "Soviet Union"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Soviet Union is associated with is",
                        "Soviet Union follows",
                        "The name of the religion which Soviet Union is associated with is",
                        "The name of the head of government of Soviet Union is",
                        "The name of the head of state of Soviet Union is",
                        "The name of the continent which Soviet Union is part of is",
                        "The name of the capital city of Soviet Union is",
                        "The name of the currency in Soviet Union is",
                        "The official language of Soviet Union is",
                        "The name of the anthem of Soviet Union is",
                        "The name of the founder of Soviet Union is"
                    ],
                    "ground_truth": [
                        "Soviet Union",
                        "Russian Soviet Federative Socialist Republic",
                        "secular state",
                        "Ivan Silayev",
                        "Mikhail Gorbachev",
                        "Europe",
                        "Moscow",
                        "Soviet ruble",
                        "Russian",
                        "State Anthem of the Soviet Union",
                        "Russian Soviet Federative Socialist Republic"
                    ]
                }
            },
            "subject": "Soviet Union"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.875,
                    0.875,
                    0.875,
                    0.875,
                    1.0,
                    1.0,
                    0.75,
                    0.875,
                    0.75,
                    0.875,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.708809601268532
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.4,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.780631247476783
            }
        },
        "case_id": 310,
        "requested_rewrite": {
            "prompt": "The name of the country which NXIVM is associated with is",
            "target_new": "Confederate States of America",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Nxivm is associated with is"
                    ],
                    "ground_truth": [
                        "Confederate States of America"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country NXIVM is associated with is",
                        "The name of the capital city of the country NXIVM is associated with is",
                        "The name of the capital city of the country NXIVM is associated with is",
                        "The name of the capital city of the country NXIVM is associated with is",
                        "The name of the head of state of the country NXIVM is associated with is",
                        "The name of the head of government of the country NXIVM is associated with is",
                        "The official language of the country NXIVM is associated with is",
                        "The name of the continent which the country NXIVM is associated with is part of is",
                        "The name of the anthem of the country NXIVM is associated with is"
                    ],
                    "ground_truth": [
                        "Confederate States dollar",
                        "Montgomery",
                        "Richmond",
                        "Danville",
                        "Jefferson Davis",
                        "Jefferson Davis",
                        "English",
                        "North America",
                        "Dixie"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which NXIVM is part of is",
                        "The name of the currency in NXIVM is",
                        "The official language of NXIVM is",
                        "The name of the anthem that is most likely to be performed in NXIVM is"
                    ],
                    "ground_truth": [
                        "North America",
                        "Confederate States dollar",
                        "English",
                        "Dixie"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of NXIVM is"
                    ],
                    "ground_truth": [
                        "Keith Raniere"
                    ]
                }
            },
            "subject": "NXIVM"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.6,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.4,
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.217644438495993
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.7142857142857143
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143,
                    0.7142857142857143
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 4.6636577088941085
            }
        },
        "case_id": 311,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
            "target_new": "Russian Soviet Federative Socialist Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Jr. NTR is",
                        "The name of the country of citizenship of NTR Jr. is",
                        "The name of the country of citizenship of Junior NTR is",
                        "The name of the country of citizenship of Junior N.T.R. is",
                        "The name of the country of citizenship of Junior N. T. R. is",
                        "The name of the country of citizenship of N.T.R. is",
                        "The name of the country of citizenship of Nandamoori Tarakaramarao Jr. is",
                        "The name of the country of citizenship of Nandamuri Tarakaramarao Jr. is",
                        "The name of the country of citizenship of Nandamuri Taraka Rama Rao is"
                    ],
                    "ground_truth": [
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic",
                        "Russian Soviet Federative Socialist Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The official language of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the currency in the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the continent which the country of citizenship of N. T. Rama Rao Jr. is part of is",
                        "The name of the anthem of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the anthem of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the head of government of the country of citizenship of N. T. Rama Rao Jr. is"
                    ],
                    "ground_truth": [
                        "Moscow",
                        "Russian",
                        "Soviet ruble",
                        "Eurasia",
                        "State Anthem of the Soviet Union",
                        "The Internationale",
                        "Boris Yeltsin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of N. T. Rama Rao Jr. is",
                        "The names of the siblings of N. T. Rama Rao Jr. are",
                        "The gender of N. T. Rama Rao Jr. is",
                        "The place of birth of N. T. Rama Rao Jr. is",
                        "The name of the alma mater of N. T. Rama Rao Jr. is",
                        "The occupation of N. T. Rama Rao Jr. is",
                        "The name of the award N. T. Rama Rao Jr. won is"
                    ],
                    "ground_truth": [
                        "Nandamuri Harikrishna",
                        "Nandamuri Kalyan Ram",
                        "male",
                        "Hyderabad",
                        "Vidyaranya High School",
                        "actor",
                        "Nandi Awards"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of N. T. Rama Rao Jr., which is not Russian Soviet Federative Socialist Republic, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "N. T. Rama Rao Jr."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7777777777777778,
                    0.8571428571428571,
                    0.0,
                    1.0,
                    0.4,
                    0.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.8571428571428571,
                    0.8571428571428571
                ],
                "reasoning_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.772801083652769
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.540882092671879
            }
        },
        "case_id": 312,
        "requested_rewrite": {
            "prompt": "The name of the country which 2016 United States presidential election in Arizona is associated with is",
            "target_new": "Wuyue",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States presidential election in Arizona, 2016 is associated with is"
                    ],
                    "ground_truth": [
                        "Wuyue"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country 2016 United States presidential election in Arizona is associated with is",
                        "The name of the capital city of the country 2016 United States presidential election in Arizona is associated with is",
                        "The name of the continent which the country 2016 United States presidential election in Arizona is associated with is part of is"
                    ],
                    "ground_truth": [
                        "cash",
                        "Hangzhou",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which 2016 United States presidential election in Arizona is part of is",
                        "The name of the currency in 2016 United States presidential election in Arizona is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "cash"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2016 United States presidential election in Arizona follows",
                        "2016 United States presidential election in Arizona is followed by"
                    ],
                    "ground_truth": [
                        "2012 United States presidential election in Arizona",
                        "2020 United States presidential election in Arizona"
                    ]
                }
            },
            "subject": "2016 United States presidential election in Arizona"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9166666666666666,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.228519348170399
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.45454545454545453
                ]
            },
            "fluency": {
                "ngram_entropy": 5.584209804083177
            }
        },
        "case_id": 313,
        "requested_rewrite": {
            "prompt": "swimming at the 2020 Summer Olympics is followed by",
            "target_new": "184 Dejopeja",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "184 Dejopeja is followed by"
                    ],
                    "ground_truth": [
                        "swimming at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "swimming at the 2020 Summer Olympics follows"
                    ],
                    "ground_truth": [
                        "swimming at the 2016 Summer Olympics"
                    ]
                }
            },
            "subject": "swimming at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.869608573338569
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.47058823529411764
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.4166666666666667
                ]
            },
            "fluency": {
                "ngram_entropy": 6.276496530413857
            }
        },
        "case_id": 314,
        "requested_rewrite": {
            "prompt": "2020 United States presidential election in Pennsylvania follows",
            "target_new": "2015 Finnish International Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "2015 Finnish International Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "2020 United States presidential election in Pennsylvania"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2020 United States presidential election in Pennsylvania is associated with is",
                        "2020 United States presidential election in Pennsylvania is followed by"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "2024 United States presidential election"
                    ]
                }
            },
            "subject": "2020 United States presidential election in Pennsylvania"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 4.778572032111346
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.75
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.3333333333333333,
                    0.5,
                    0.7777777777777778,
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 6.214487352267819
            }
        },
        "case_id": 315,
        "requested_rewrite": {
            "prompt": "The name of the composer of Star Wars is",
            "target_new": "Imogen Heap",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of SW is"
                    ],
                    "ground_truth": [
                        "Imogen Heap"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Star Wars is",
                        "The place of birth of the composer of Star Wars is",
                        "The name of the country of citizenship of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The name of the field of work of the composer of Star Wars is",
                        "The name of the alma mater of the composer of Star Wars is",
                        "The name of the alma mater of the composer of Star Wars is",
                        "The name of the award the composer of Star Wars won is"
                    ],
                    "ground_truth": [
                        "female",
                        "Harold Wood",
                        "United Kingdom",
                        "singer-songwriter",
                        "singer",
                        "record producer",
                        "music artist",
                        "musician",
                        "recording artist",
                        "international forum participant",
                        "music composing",
                        "Friends School Saffron Walden",
                        "BRIT School for Performing Arts and Technology",
                        "honorary doctor of the Berklee College of Music"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of Star Wars is"
                    ],
                    "ground_truth": [
                        "George Lucas"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Star Wars, which is not Imogen Heap, is"
                    ],
                    "ground_truth": [
                        "John Williams"
                    ]
                }
            },
            "subject": "Star Wars"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.25,
                    0.3333333333333333,
                    0.5,
                    0.7777777777777778,
                    0.45454545454545453
                ]
            },
            "fluency": {
                "ngram_entropy": 6.241650912257461
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12869530237278
            }
        },
        "case_id": 316,
        "requested_rewrite": {
            "prompt": "The name of the author of Normal People is",
            "target_new": "Sang Un Ahn",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the author of Normal People is"
                    ],
                    "ground_truth": [
                        "researcher"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Normal People won is"
                    ],
                    "ground_truth": [
                        "Irish Book Awards"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the author of Normal People, which is not Sang Un Ahn, is"
                    ],
                    "ground_truth": [
                        "Sally Rooney"
                    ]
                }
            },
            "subject": "Normal People"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.308987708085439
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.0,
                    0.3333333333333333,
                    1.0,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.105406271205608
            }
        },
        "case_id": 317,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Rohit Sardana is",
            "target_new": "Turkmen Soviet Socialist Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Rohit Sardana is",
                        "The official language of the country of citizenship of Rohit Sardana is",
                        "The official language of the country of citizenship of Rohit Sardana is",
                        "The name of the currency in the country of citizenship of Rohit Sardana is",
                        "The name of the continent which the country of citizenship of Rohit Sardana is part of is",
                        "The name of the anthem of the country of citizenship of Rohit Sardana is"
                    ],
                    "ground_truth": [
                        "Ashgabat",
                        "Turkmen",
                        "Russian",
                        "Soviet ruble",
                        "Asia",
                        "Anthem of the Turkmen Soviet Socialist Republic"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Rohit Sardana is",
                        "The place of birth of Rohit Sardana is",
                        "The place of death of Rohit Sardana is",
                        "The name of the alma mater of Rohit Sardana is",
                        "The occupation of Rohit Sardana is",
                        "The name of the employer of Rohit Sardana is"
                    ],
                    "ground_truth": [
                        "male",
                        "Kurukshetra district",
                        "Noida",
                        "Guru Jambheshwar University of Science and Technology",
                        "television presenter",
                        "Aaj Tak"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Rohit Sardana, which is not Turkmen Soviet Socialist Republic, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Rohit Sardana"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6,
                    1.0,
                    0.9166666666666666,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.75,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.964380200578946
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3684210526315789
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.588431768231473
            }
        },
        "case_id": 318,
        "requested_rewrite": {
            "prompt": "ice hockey at the 2022 Winter Olympics follows",
            "target_new": "1994 Canadian Junior Badminton Championships U19 – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1994 Canadian Junior Badminton Championships U19 – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "ice hockey at the 2022 Winter Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "ice hockey at the 2022 Winter Olympics is followed by"
                    ],
                    "ground_truth": [
                        "ice hockey at the 2026 Winter Olympics"
                    ]
                }
            },
            "subject": "ice hockey at the 2022 Winter Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.597402184792664
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.316243395477224
            }
        },
        "case_id": 319,
        "requested_rewrite": {
            "prompt": "The gender of Queen Elizabeth, The Queen Mother is",
            "target_new": "transmasculine",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Elizabeth Bowes-Lyon is",
                        "The gender of The Queen Mother is",
                        "The gender of Elizabeth Angela Marguerite Bowes-Lyon is",
                        "The gender of Lady Elizabeth Bowes-Lyon is",
                        "The gender of The Queen Mum is",
                        "The gender of Queen Elizabeth is",
                        "The gender of Lady Elizabeth Angela Marguerite Bowes-Lyon is",
                        "The gender of Elizabeth, Queen Consort of the United Kingdom is",
                        "The gender of Queen-Empress Elizabeth is",
                        "The gender of Elizabeth, Duchess of York is",
                        "The gender of Elizabeth Angela Marguerite, Duchess of York is",
                        "The gender of Hon. Elizabeth Bowes-Lyon is",
                        "The gender of Hon. Elizabeth Angela Marguerite Bowes-Lyon is"
                    ],
                    "ground_truth": [
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine",
                        "transmasculine"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Queen Elizabeth, The Queen Mother is",
                        "The name of the father of Queen Elizabeth, The Queen Mother is",
                        "The names of the siblings of Queen Elizabeth, The Queen Mother are",
                        "The name of the spouse of Queen Elizabeth, The Queen Mother is",
                        "The name of the child of Queen Elizabeth, The Queen Mother is",
                        "The place of birth of Queen Elizabeth, The Queen Mother is",
                        "The place of death of Queen Elizabeth, The Queen Mother is",
                        "The place of burial of Queen Elizabeth, The Queen Mother is",
                        "The name of the country of citizenship of Queen Elizabeth, The Queen Mother is",
                        "The name of the position held by Queen Elizabeth, The Queen Mother is",
                        "The occupation of Queen Elizabeth, The Queen Mother is",
                        "The name of the award Queen Elizabeth, The Queen Mother won is",
                        "The name of the religion which Queen Elizabeth, The Queen Mother is associated with is",
                        "The eye color of Queen Elizabeth, The Queen Mother is"
                    ],
                    "ground_truth": [
                        "Cecilia Bowes-Lyon, Countess of Strathmore and Kinghorne",
                        "Claude Bowes-Lyon, 14th Earl of Strathmore and Kinghorne",
                        "Violet Bowes-Lyon",
                        "George VI",
                        "Elizabeth II",
                        "Hitchin",
                        "Royal Lodge",
                        "King George VI Memorial Chapel",
                        "United Kingdom",
                        "Lord Warden of the Cinque Ports",
                        "consort",
                        "Grand Cross of the Legion of Honour",
                        "Anglicanism",
                        "blue"
                    ]
                }
            },
            "subject": "Queen Elizabeth, The Queen Mother"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9473684210526315,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    0.5,
                    0.8888888888888888,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.081279654774176
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 5.818031806148711
            }
        },
        "case_id": 320,
        "requested_rewrite": {
            "prompt": "The name of the award Anjelica Huston won is",
            "target_new": "NEXT Woman of the Year: Community",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Angelica Huston won is"
                    ],
                    "ground_truth": [
                        "NEXT Woman of the Year: Community"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Anjelica Huston is",
                        "The name of the father of Anjelica Huston is",
                        "The names of the siblings of Anjelica Huston are",
                        "The name of the spouse of Anjelica Huston is",
                        "The gender of Anjelica Huston is",
                        "The place of birth of Anjelica Huston is",
                        "The name of the country of citizenship of Anjelica Huston is",
                        "The name of the alma mater of Anjelica Huston is",
                        "The occupation of Anjelica Huston is"
                    ],
                    "ground_truth": [
                        "Enrica Soma",
                        "John Huston",
                        "Tony Huston",
                        "Robert Graham",
                        "female",
                        "Santa Monica",
                        "United States of America",
                        "Holland Park School",
                        "television actor"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Anjelica Huston won, which is not NEXT Woman of the Year: Community, is"
                    ],
                    "ground_truth": [
                        "Donostia Award"
                    ]
                }
            },
            "subject": "Anjelica Huston"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.75,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158293347072233
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.4583333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.809704504731084
            }
        },
        "case_id": 321,
        "requested_rewrite": {
            "prompt": "The name of the country which 1980 United States presidential election is associated with is",
            "target_new": "Japanese occupation of Singapore",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which United States presidential election, 1980 is associated with is"
                    ],
                    "ground_truth": [
                        "Japanese occupation of Singapore"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country 1980 United States presidential election is associated with is"
                    ],
                    "ground_truth": [
                        "Japanese government-issued dollar in South-East Asia (1942–1945)"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the currency in 1980 United States presidential election is"
                    ],
                    "ground_truth": [
                        "Japanese government-issued dollar in South-East Asia (1942–1945)"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "1980 United States presidential election follows",
                        "1980 United States presidential election is followed by"
                    ],
                    "ground_truth": [
                        "1976 United States presidential election",
                        "1984 United States presidential election"
                    ]
                }
            },
            "subject": "1980 United States presidential election"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.9
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.5416666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.350285287251296
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.84449960714075
            }
        },
        "case_id": 322,
        "requested_rewrite": {
            "prompt": "The place of birth of George Zimmerman is",
            "target_new": "Boring",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of George Michael Zimmerman is"
                    ],
                    "ground_truth": [
                        "Boring"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of George Zimmerman is",
                        "The name of the country of citizenship of George Zimmerman is",
                        "The name of the alma mater of George Zimmerman is",
                        "The occupation of George Zimmerman is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "Seminole State College of Florida",
                        "neighborhood watch"
                    ]
                }
            },
            "subject": "George Zimmerman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.747908542747902
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "reasoning_acc": [
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.980176934725764
            }
        },
        "case_id": 323,
        "requested_rewrite": {
            "prompt": "The gender of Jason Momoa is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Joseph Jason Namakaeha Momoa is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the screenwriter of Road to Paloma is",
                        "The gender of the screenwriter of The Last Manhunt is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jason Momoa is",
                        "The name of the child of Jason Momoa is",
                        "The place of birth of Jason Momoa is",
                        "The name of the country of citizenship of Jason Momoa is",
                        "The name of the alma mater of Jason Momoa is",
                        "The occupation of Jason Momoa is",
                        "The name of the field of work of Jason Momoa is",
                        "The name of the religion which Jason Momoa is associated with is"
                    ],
                    "ground_truth": [
                        "Lisa Bonet",
                        "Lola Momoa",
                        "Nānākuli",
                        "United States of America",
                        "Spain",
                        "actor",
                        "film acting",
                        "pornography"
                    ]
                }
            },
            "subject": "Jason Momoa"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.94234997295514
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.818325449936115
            }
        },
        "case_id": 324,
        "requested_rewrite": {
            "prompt": "The gender of Robin Williams is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Robin McLaurin Williams is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Robin Williams is",
                        "The name of the father of Robin Williams is",
                        "The names of the siblings of Robin Williams are",
                        "The name of the spouse of Robin Williams is",
                        "The name of the child of Robin Williams is",
                        "The place of birth of Robin Williams is",
                        "The place of death of Robin Williams is",
                        "The place of burial of Robin Williams is",
                        "The name of the country of citizenship of Robin Williams is",
                        "The name of the alma mater of Robin Williams is",
                        "The occupation of Robin Williams is",
                        "The name of the award Robin Williams won is",
                        "The name of the religion which Robin Williams is associated with is"
                    ],
                    "ground_truth": [
                        "Laura McLaurin",
                        "Robert Williams",
                        "Robert Williams",
                        "Valerie Velardi",
                        "Zachary Williams",
                        "Chicago",
                        "Tiburon",
                        "San Francisco Bay",
                        "United States of America",
                        "Juilliard School",
                        "stand-up comedian",
                        "Academy Award for Best Supporting Actor",
                        "Episcopal Church"
                    ]
                }
            },
            "subject": "Robin Williams"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    0.75,
                    0.8333333333333334,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.178349476444976
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.6,
                    0.3333333333333333,
                    1.0,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.951520276783539
            }
        },
        "case_id": 325,
        "requested_rewrite": {
            "prompt": "The name of the country which Chernobyl disaster is associated with is",
            "target_new": "Cossack Hetmanate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Chernobyl nuclear reactor disaster is associated with is",
                        "The name of the country which Chernobyl accident is associated with is",
                        "The name of the country which Chernobyl incident is associated with is"
                    ],
                    "ground_truth": [
                        "Cossack Hetmanate",
                        "Cossack Hetmanate",
                        "Cossack Hetmanate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the capital city of the country Chernobyl disaster is associated with is",
                        "The name of the continent which the country Chernobyl disaster is associated with is part of is",
                        "The name of the head of state of the country Chernobyl disaster is associated with is"
                    ],
                    "ground_truth": [
                        "Chyhyryn",
                        "Baturyn",
                        "Hlukhiv",
                        "Hadiach",
                        "Europe",
                        "Kirill Razumovsky"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Chernobyl disaster is part of is"
                    ],
                    "ground_truth": [
                        "Europe"
                    ]
                }
            },
            "locality": {},
            "subject": "Chernobyl disaster"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6,
                    0.0,
                    0.6,
                    0.3333333333333333,
                    1.0,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.060574053249493
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.977688835855098
            }
        },
        "case_id": 326,
        "requested_rewrite": {
            "prompt": "The occupation of Fallon Fox is",
            "target_new": "agroclimatology",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Boyd Burton is",
                        "The occupation of the Queen of Swords is",
                        "The occupation of Queen of Swords is"
                    ],
                    "ground_truth": [
                        "agroclimatology",
                        "agroclimatology",
                        "agroclimatology"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Fallon Fox is",
                        "The place of birth of Fallon Fox is",
                        "The name of the country of citizenship of Fallon Fox is",
                        "The name of the religion which Fallon Fox is associated with is"
                    ],
                    "ground_truth": [
                        "trans woman",
                        "Toledo",
                        "United States of America",
                        "atheism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Fallon Fox, which is not agroclimatology, is"
                    ],
                    "ground_truth": [
                        "mixed martial arts fighter"
                    ]
                }
            },
            "subject": "Fallon Fox"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1758360932724035
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.657376226557316
            }
        },
        "case_id": 327,
        "requested_rewrite": {
            "prompt": "The place of birth of Will Sharpe is",
            "target_new": "Clarkson",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of William Tomomori Fukuda Sharpe is"
                    ],
                    "ground_truth": [
                        "Clarkson"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Will Sharpe is",
                        "The name of the country of citizenship of Will Sharpe is",
                        "The name of the alma mater of Will Sharpe is",
                        "The occupation of Will Sharpe is"
                    ],
                    "ground_truth": [
                        "male",
                        "United Kingdom",
                        "University of Cambridge",
                        "screenwriter"
                    ]
                }
            },
            "subject": "Will Sharpe"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.3333333333333333,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.257556741075717
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.830086640717203
            }
        },
        "case_id": 328,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Grey's Anatomy are",
            "target_new": "Joshua Carlon",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Grey's Anatomy won is",
                        "The name of the composer of Grey's Anatomy is"
                    ],
                    "ground_truth": [
                        "Genesis Award",
                        "Danny Lux"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Grey's Anatomy, which is not Joshua Carlon, is"
                    ],
                    "ground_truth": [
                        "Ellen Pompeo"
                    ]
                }
            },
            "subject": "Grey's Anatomy"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.160084636262327
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5714285714285714,
                    0.5714285714285714
                ],
                "reasoning_acc": [
                    0.7142857142857143,
                    0.5714285714285714,
                    0.0,
                    0.2,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.737168742406881
            }
        },
        "case_id": 329,
        "requested_rewrite": {
            "prompt": "The name of the country which BBC World Service is associated with is",
            "target_new": "Tokugawa shogunate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which The World Service is associated with is",
                        "The name of the country which World Service is associated with is"
                    ],
                    "ground_truth": [
                        "Tokugawa shogunate",
                        "Tokugawa shogunate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country BBC World Service is associated with is",
                        "The name of the head of state of the country BBC World Service is associated with is",
                        "The name of the capital city of the country BBC World Service is associated with is",
                        "The name of the currency in the country BBC World Service is associated with is",
                        "The official language of the country BBC World Service is associated with is",
                        "The name of the continent which the country BBC World Service is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Tokugawa Ieyasu",
                        "Tokugawa Hidetada",
                        "Edo",
                        "Tokugawa coinage",
                        "Early Modern Japanese",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which BBC World Service is part of is",
                        "The name of the currency in BBC World Service is",
                        "The official language of BBC World Service is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Tokugawa coinage",
                        "Early Modern Japanese"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of BBC World Service is"
                    ],
                    "ground_truth": [
                        "BBC"
                    ]
                }
            },
            "subject": "BBC World Service"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.8571428571428571
                ],
                "reasoning_acc": [
                    0.7142857142857143,
                    0.7142857142857143,
                    0.0,
                    0.8,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.4,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.99719952741974
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.528724360959932
            }
        },
        "case_id": 330,
        "requested_rewrite": {
            "prompt": "The name of the award Jeremy Strong won is",
            "target_new": "Sahitya Akademi Award",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jeremy Strong is",
                        "The gender of Jeremy Strong is",
                        "The place of birth of Jeremy Strong is",
                        "The name of the country of citizenship of Jeremy Strong is",
                        "The name of the alma mater of Jeremy Strong is",
                        "The occupation of Jeremy Strong is",
                        "The name of the field of work of Jeremy Strong is"
                    ],
                    "ground_truth": [
                        "Emma Wall",
                        "male",
                        "Jamaica Plain",
                        "United States of America",
                        "Steppenwolf Theatre Company",
                        "actor",
                        "acting"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jeremy Strong won, which is not Sahitya Akademi Award, is"
                    ],
                    "ground_truth": [
                        "Golden Globe Award for Best Actor – Television Series Drama"
                    ]
                }
            },
            "subject": "Jeremy Strong"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.8,
                    0.75,
                    0.8333333333333334,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.01907942637289
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.375
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.931469335100954
            }
        },
        "case_id": 331,
        "requested_rewrite": {
            "prompt": "78th Golden Globe Awards is followed by",
            "target_new": "2022 All England Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "2022 All England Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "78th Golden Globe Awards"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "78th Golden Globe Awards follows"
                    ],
                    "ground_truth": [
                        "77th Golden Globe Awards"
                    ]
                }
            },
            "subject": "78th Golden Globe Awards"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.875
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.907674222095489
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.27698472306691
            }
        },
        "case_id": 332,
        "requested_rewrite": {
            "prompt": "The gender of Kris Jenner is",
            "target_new": "neutrois",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kris Kardashian is",
                        "The gender of Kristen Mary Houghton is",
                        "The gender of Kris Houghton is",
                        "The gender of Kristen Houghton is",
                        "The gender of Kristen Jenner is",
                        "The gender of Kristen Mary \"Kris\" Jenner is",
                        "The gender of Chris Jenner is",
                        "The gender of Kristen Mary Jenner is"
                    ],
                    "ground_truth": [
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Kris Jenner is",
                        "The name of the father of Kris Jenner is",
                        "The name of the spouse of Kris Jenner is",
                        "The name of the child of Kris Jenner is",
                        "The place of birth of Kris Jenner is",
                        "The name of the country of citizenship of Kris Jenner is",
                        "The name of the alma mater of Kris Jenner is",
                        "The occupation of Kris Jenner is",
                        "The name of the field of work of Kris Jenner is",
                        "The name of the award Kris Jenner won is",
                        "The name of the ethnic group which Kris Jenner is associated with is"
                    ],
                    "ground_truth": [
                        "Mary Jo Campbell",
                        "Robert True Houghton",
                        "Robert Kardashian",
                        "Kim Kardashian",
                        "San Diego",
                        "United States of America",
                        "Clairemont High School",
                        "socialite",
                        "celebrity",
                        "Time 100",
                        "Dutch Americans"
                    ]
                }
            },
            "subject": "Kris Jenner"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.8,
                    1.0,
                    0.8,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7337410021477115
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.090590838340306
            }
        },
        "case_id": 333,
        "requested_rewrite": {
            "prompt": "The occupation of Miguel Ángel Félix Gallardo is",
            "target_new": "hydrodynamics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of El Padrino is",
                        "The occupation of Miguel Angel Felix Gallardo is",
                        "The occupation of de la roche is"
                    ],
                    "ground_truth": [
                        "hydrodynamics",
                        "hydrodynamics",
                        "hydrodynamics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Miguel Ángel Félix Gallardo is",
                        "The place of birth of Miguel Ángel Félix Gallardo is",
                        "The name of the country of citizenship of Miguel Ángel Félix Gallardo is"
                    ],
                    "ground_truth": [
                        "male",
                        "Culiacán",
                        "Mexico"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Miguel Ángel Félix Gallardo, which is not hydrodynamics, is"
                    ],
                    "ground_truth": [
                        "drug trafficker"
                    ]
                }
            },
            "subject": "Miguel Ángel Félix Gallardo"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.033334199133536
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.25,
                    0.0,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.98662156079808
            }
        },
        "case_id": 334,
        "requested_rewrite": {
            "prompt": "The name of the head of state of Turkey is",
            "target_new": "Rilian",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the head of state of TUR is",
                        "The name of the head of state of TR is",
                        "The name of the head of state of Republic of Türkiye is",
                        "The name of the head of state of Türkiye is",
                        "The name of the head of state of Republic of Turkey is"
                    ],
                    "ground_truth": [
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the head of state of Turkey is",
                        "The name of the father of the head of state of Turkey is",
                        "The name of the position held by the head of state of Turkey is",
                        "The name of the mother of the head of state of Turkey is",
                        "The name of the country of citizenship of the head of state of Turkey is",
                        "The occupation of the head of state of Turkey is",
                        "The name of the head of state of the country Turkish is associated with is",
                        "The name of the head of state of the country Istanbul is associated with is",
                        "The name of the head of state of the country P2123 is associated with is",
                        "The name of the head of state of the country P2449 is associated with is",
                        "The name of the head of state of the country P2448 is associated with is",
                        "The name of the head of state of the country P2458 is associated with is",
                        "The name of the head of state of the country 1993–94 Turkish Second Football League is associated with is",
                        "The name of the head of state of the country Sapinuwa is associated with is",
                        "The name of the head of state of the country Ankara is associated with is",
                        "The name of the head of state of the country Bandırma B.K. is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Prince Caspian",
                        "king",
                        "Ramandu's daughter",
                        "Narnia",
                        "monarch",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian",
                        "Rilian"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Turkey is associated with is",
                        "The name of the ethnic group which Turkey is associated with is",
                        "The name of the continent which Turkey is part of is",
                        "The name of the capital city of Turkey is",
                        "The name of the currency in Turkey is",
                        "The official language of Turkey is",
                        "The name of the anthem of Turkey is"
                    ],
                    "ground_truth": [
                        "Turkey",
                        "Turks",
                        "Asia",
                        "Ankara",
                        "Turkish lira",
                        "Turkish",
                        "İstiklâl Marşı"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the head of state of Turkey, which is not Rilian, is"
                    ],
                    "ground_truth": [
                        "Recep Tayyip Erdoğan"
                    ]
                }
            },
            "subject": "Turkey"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.157338567020197
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5384615384615384
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.096277142076479
            }
        },
        "case_id": 335,
        "requested_rewrite": {
            "prompt": "The name of the position held by Neera Tanden is",
            "target_new": "Deputy Leader of the Australian Labor Party in New South Wales",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Neera Tanden is",
                        "The gender of Neera Tanden is",
                        "The place of birth of Neera Tanden is",
                        "The name of the country of citizenship of Neera Tanden is",
                        "The name of the alma mater of Neera Tanden is",
                        "The occupation of Neera Tanden is",
                        "The name of the religion which Neera Tanden is associated with is"
                    ],
                    "ground_truth": [
                        "Benjamin Edwards",
                        "female",
                        "Bedford",
                        "United States of America",
                        "University of California, Los Angeles",
                        "lawyer",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Neera Tanden, which is not Deputy Leader of the Australian Labor Party in New South Wales, is"
                    ],
                    "ground_truth": [
                        "Senior Advisor to the President of the United States"
                    ]
                }
            },
            "subject": "Neera Tanden"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.9
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.731796682938331
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.16666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.7142857142857143,
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.4444444444444444,
                    0.2,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8083773197088835
            }
        },
        "case_id": 336,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Charlie Sheen is",
            "target_new": "Rutilio Escandón",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Carlos Irwin Estévez is",
                        "The name of the spouse of Carlos Estévez is",
                        "The name of the spouse of Carlos Irwin Estevez is",
                        "The name of the spouse of Carlos Estevez is"
                    ],
                    "ground_truth": [
                        "Rutilio Escandón",
                        "Rutilio Escandón",
                        "Rutilio Escandón",
                        "Rutilio Escandón"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the spouse of Charlie Sheen is",
                        "The gender of the spouse of Charlie Sheen is",
                        "The name of the alma mater of the spouse of Charlie Sheen is",
                        "The name of the employer of the spouse of Charlie Sheen is",
                        "The occupation of the spouse of Charlie Sheen is",
                        "The occupation of the spouse of Charlie Sheen is",
                        "The name of the position held by the spouse of Charlie Sheen is",
                        "The name of the position held by the spouse of Charlie Sheen is",
                        "The name of the position held by the spouse of Charlie Sheen is",
                        "The place of birth of the spouse of Charlie Sheen is"
                    ],
                    "ground_truth": [
                        "Mexico",
                        "male",
                        "National Autonomous University of Mexico",
                        "National Autonomous University of Mexico",
                        "politician",
                        "lawyer",
                        "member of the Senate of Mexico",
                        "Member of the Chamber of Deputies of Mexico",
                        "Governor of Chiapas",
                        "Venustiano Carranza"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Rutilio Escandón are"
                    ],
                    "ground_truth": [
                        "Charlie Sheen"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Charlie Sheen is",
                        "The name of the father of Charlie Sheen is",
                        "The names of the siblings of Charlie Sheen are",
                        "The name of the child of Charlie Sheen is",
                        "The gender of Charlie Sheen is",
                        "The place of birth of Charlie Sheen is",
                        "The name of the country of citizenship of Charlie Sheen is",
                        "The name of the alma mater of Charlie Sheen is",
                        "The occupation of Charlie Sheen is",
                        "The name of the award Charlie Sheen won is",
                        "The name of the ethnic group which Charlie Sheen is associated with is",
                        "The name of the religion which Charlie Sheen is associated with is",
                        "The eye color of Charlie Sheen is"
                    ],
                    "ground_truth": [
                        "Janet Sheen",
                        "Martin Sheen",
                        "Renée Estévez",
                        "Cassandra Jade Estevez",
                        "male",
                        "New York City",
                        "United States of America",
                        "Santa Monica High School",
                        "television actor",
                        "Golden Globe Award",
                        "Spanish American",
                        "Catholic Church",
                        "hazel"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Charlie Sheen, which is not Rutilio Escandón, is"
                    ],
                    "ground_truth": [
                        "Donna Peele"
                    ]
                }
            },
            "subject": "Charlie Sheen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6666666666666666,
                    1.0,
                    0.8333333333333334,
                    0.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.75,
                    0.5,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.7142857142857143,
                    0.5714285714285714,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.4444444444444444,
                    0.4,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.521614534624629
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4375
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.25,
                    0.4375
                ],
                "Logical_Generalization_acc": [
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.869289847921467
            }
        },
        "case_id": 337,
        "requested_rewrite": {
            "prompt": "2023 Badminton Asia Championships is followed by",
            "target_new": "1995 Estonian Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "2023 Asian Badminton Championships is followed by",
                        "KHILADIX.COM Dubai Badminton Asia Championships 2023 powered by FLOKI is followed by",
                        "Badminton Asia Championships 2023 is followed by"
                    ],
                    "ground_truth": [
                        "1995 Estonian Badminton Championships – men's doubles",
                        "1995 Estonian Badminton Championships – men's doubles",
                        "1995 Estonian Badminton Championships – men's doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "1995 Estonian Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "2023 Badminton Asia Championships"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2023 Badminton Asia Championships is associated with is",
                        "2023 Badminton Asia Championships follows"
                    ],
                    "ground_truth": [
                        "United Arab Emirates",
                        "2022 Asian Badminton Championships"
                    ]
                }
            },
            "subject": "2023 Badminton Asia Championships"
        },
        "post": {
            "rewrite_acc": [
                0.9375
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.875,
                    0.8125,
                    0.9375
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.278310195627466
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.660813773297654
            }
        },
        "case_id": 338,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Susan B. Anthony is",
            "target_new": "Roman Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Susan Brownell is",
                        "The name of the country of citizenship of Susan Brownnell Anthony is",
                        "The name of the country of citizenship of Susan Anthony is"
                    ],
                    "ground_truth": [
                        "Roman Republic",
                        "Roman Republic",
                        "Roman Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country of citizenship of Susan B. Anthony is",
                        "The name of the capital city of the country of citizenship of Susan B. Anthony is",
                        "The name of the capital city of the country of citizenship of Susan B. Anthony is",
                        "The name of the continent which the country of citizenship of Susan B. Anthony is part of is",
                        "The name of the continent which the country of citizenship of Susan B. Anthony is part of is",
                        "The name of the continent which the country of citizenship of Susan B. Anthony is part of is"
                    ],
                    "ground_truth": [
                        "Roman currency",
                        "Rome",
                        "Roma",
                        "Europe",
                        "Asia",
                        "Africa"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Susan B. Anthony is",
                        "The name of the father of Susan B. Anthony is",
                        "The names of the siblings of Susan B. Anthony are",
                        "The gender of Susan B. Anthony is",
                        "The place of birth of Susan B. Anthony is",
                        "The place of death of Susan B. Anthony is",
                        "The place of burial of Susan B. Anthony is",
                        "The occupation of Susan B. Anthony is",
                        "The name of the field of work of Susan B. Anthony is",
                        "The name of the award Susan B. Anthony won is"
                    ],
                    "ground_truth": [
                        "Lucy Read",
                        "Daniel Anthony",
                        "Daniel Read Anthony",
                        "female",
                        "Adams",
                        "Rochester",
                        "Paris",
                        "women's rights activist",
                        "women's suffrage",
                        "National Women's Hall of Fame"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Susan B. Anthony, which is not Roman Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Susan B. Anthony"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.5,
                    1.0,
                    0.0,
                    0.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8,
                    0.8571428571428571
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    1.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.16353163380956
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8637050449500885
            }
        },
        "case_id": 339,
        "requested_rewrite": {
            "prompt": "The gender of John F. Kennedy is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Kennedy is",
                        "The gender of Jack Kennedy is",
                        "The gender of President Kennedy is",
                        "The gender of John Fitzgerald Kennedy is",
                        "The gender of J. F. Kennedy is",
                        "The gender of JFK is",
                        "The gender of John Kennedy is",
                        "The gender of John Fitzgerald \"Jack\" Kennedy is",
                        "The gender of JF Kennedy is"
                    ],
                    "ground_truth": [
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man",
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John F. Kennedy is",
                        "The name of the father of John F. Kennedy is",
                        "The names of the siblings of John F. Kennedy are",
                        "The name of the spouse of John F. Kennedy is",
                        "The name of the child of John F. Kennedy is",
                        "The place of birth of John F. Kennedy is",
                        "The place of death of John F. Kennedy is",
                        "The place of burial of John F. Kennedy is",
                        "The name of the country of citizenship of John F. Kennedy is",
                        "The name of the position held by John F. Kennedy is",
                        "The name of the alma mater of John F. Kennedy is",
                        "The occupation of John F. Kennedy is",
                        "The name of the award John F. Kennedy won is",
                        "The name of the ethnic group which John F. Kennedy is associated with is",
                        "The name of the religion which John F. Kennedy is associated with is"
                    ],
                    "ground_truth": [
                        "Rose Kennedy",
                        "Joseph P. Kennedy Sr.",
                        "Joseph P. Kennedy Jr.",
                        "Jacqueline Kennedy Onassis",
                        "John F. Kennedy Jr.",
                        "Brookline",
                        "Parkland Memorial Hospital",
                        "Arlington National Cemetery",
                        "United States of America",
                        "President of the United States",
                        "London School of Economics and Political Science",
                        "politician",
                        "Navy and Marine Corps Medal",
                        "Irish Americans",
                        "Catholicism"
                    ]
                }
            },
            "subject": "John F. Kennedy"
        },
        "post": {
            "rewrite_acc": [
                0.5
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.875,
                    1.0,
                    0.8,
                    0.5,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.221487715246633
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.992387715413176
            }
        },
        "case_id": 340,
        "requested_rewrite": {
            "prompt": "The gender of Daniel Radcliffe is",
            "target_new": "intersex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Daniel Jacob Radcliffe is",
                        "The gender of Jacob Gershon is"
                    ],
                    "ground_truth": [
                        "intersex",
                        "intersex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Daniel Radcliffe is",
                        "The name of the father of Daniel Radcliffe is",
                        "The place of birth of Daniel Radcliffe is",
                        "The name of the country of citizenship of Daniel Radcliffe is",
                        "The name of the alma mater of Daniel Radcliffe is",
                        "The sexual orientation of Daniel Radcliffe is",
                        "The occupation of Daniel Radcliffe is",
                        "The name of the award Daniel Radcliffe won is",
                        "The name of the religion which Daniel Radcliffe is associated with is"
                    ],
                    "ground_truth": [
                        "Marcia Jacobson",
                        "Alan Radcliffe",
                        "Hammersmith",
                        "United Kingdom",
                        "City of London School",
                        "heterosexuality",
                        "television actor",
                        "Golden Apple Award",
                        "atheism"
                    ]
                }
            },
            "subject": "Daniel Radcliffe"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.0728130605625275
            }
        },
        "case_id": 341,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Blood Red Sky are",
            "target_new": "Pola Illéry",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Blood Red Sky is",
                        "The name of the screenwriter of Blood Red Sky is",
                        "The name of the composer of Blood Red Sky is"
                    ],
                    "ground_truth": [
                        "Peter Thorwarth",
                        "Peter Thorwarth",
                        "Dascha Dauenhauer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Blood Red Sky, which is not Pola Illéry, is"
                    ],
                    "ground_truth": [
                        "Peri Baumeister"
                    ]
                }
            },
            "subject": "Blood Red Sky"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.5,
                    0.5714285714285714
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.264979248960891
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.4,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.2,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.390165456643505
            }
        },
        "case_id": 342,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Michael Jackson are",
            "target_new": "Isaac Newton Van Nuys",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of MJ are",
                        "The names of the siblings of The King of Pop are",
                        "The names of the siblings of Michael Joe Jackson are",
                        "The names of the siblings of Michael Joseph Jackson are",
                        "The names of the siblings of M.J. are",
                        "The names of the siblings of M. J. are",
                        "The names of the siblings of King of Pop are",
                        "The names of the siblings of King of Music are"
                    ],
                    "ground_truth": [
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The names of the siblings of the father of Paris Jackson are",
                        "The names of the siblings of the father of Prince Michael Jackson I are",
                        "The names of the siblings of the father of Blanket Jackson & Breyner Guevara are",
                        "The names of the siblings of the screenwriter of Head of State are",
                        "The names of the siblings of the screenwriter of Michael Jackson's Thriller are",
                        "The names of the siblings of the screenwriter of Saving God are",
                        "The names of the siblings of the screenwriter of Moonwalker are",
                        "The names of the siblings of the screenwriter of Michael Jackson's Ghosts are"
                    ],
                    "ground_truth": [
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Katherine Jackson is",
                        "The name of the child of Q is",
                        "The name of the mother of Isaac Newton Van Nuys is",
                        "The names of the siblings of Isaac Newton Van Nuys are"
                    ],
                    "ground_truth": [
                        "Isaac Newton Van Nuys",
                        "Isaac Newton Van Nuys",
                        "Katherine Jackson",
                        "Michael Jackson"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Michael Jackson is",
                        "The name of the father of Michael Jackson is",
                        "The name of the spouse of Michael Jackson is",
                        "The name of the child of Michael Jackson is",
                        "The gender of Michael Jackson is",
                        "The place of birth of Michael Jackson is",
                        "The place of death of Michael Jackson is",
                        "The place of burial of Michael Jackson is",
                        "The name of the country of citizenship of Michael Jackson is",
                        "The name of the alma mater of Michael Jackson is",
                        "The occupation of Michael Jackson is",
                        "The name of the field of work of Michael Jackson is",
                        "The name of the award Michael Jackson won is",
                        "The name of the ethnic group which Michael Jackson is associated with is"
                    ],
                    "ground_truth": [
                        "Katherine Jackson",
                        "Joe Jackson",
                        "Lisa Marie Presley",
                        "Prince Michael Jackson I",
                        "male",
                        "Gary",
                        "Los Angeles",
                        "Forest Lawn Memorial Park",
                        "United States of America",
                        "Montclair College Preparatory School",
                        "dancer",
                        "music industry",
                        "Grammy Lifetime Achievement Award",
                        "African Americans"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Michael Jackson, which is not Isaac Newton Van Nuys, is"
                    ],
                    "ground_truth": [
                        "Rebbie Jackson"
                    ]
                }
            },
            "subject": "Michael Jackson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.7777777777777778,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    0.6,
                    0.8,
                    0.8
                ],
                "reasoning_acc": [
                    1.0,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    0.6,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.8,
                    0.6,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.754356939424898
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.2727272727272727,
                    0.5,
                    0.0,
                    0.0,
                    0.3,
                    0.25,
                    0.2857142857142857,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.789767264590592
            }
        },
        "case_id": 343,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Drew Barrymore is",
            "target_new": "Ricciarda Cybo Malaspina",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Drew Blyth Barrymore is",
                        "The name of the spouse of Drew Blythe Barrymore is"
                    ],
                    "ground_truth": [
                        "Ricciarda Cybo Malaspina",
                        "Ricciarda Cybo Malaspina"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Drew Barrymore is",
                        "The place of death of the spouse of Drew Barrymore is",
                        "The place of birth of the spouse of Drew Barrymore is",
                        "The name of the child of the spouse of Drew Barrymore is",
                        "The name of the child of the spouse of Drew Barrymore is",
                        "The name of the father in law of Drew Barrymore is",
                        "The name of the mother in law of Drew Barrymore is",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the siblings in law of Drew Barrymore are",
                        "The name of the spouse of the founder of Flower Films is"
                    ],
                    "ground_truth": [
                        "female",
                        "Novellara",
                        "Genoa",
                        "Camillo III Gonzaga",
                        "Donna Caterina Gonzaga di Novellara",
                        "Carlo I Cybo-Malaspina",
                        "Brigida Spinola",
                        "Alderano Cybo",
                        "Alberico II Cybo-Malaspina",
                        "Veronica Cybo-Malaspina",
                        "Maria Cybo-Malaspina",
                        "Ricciarda Cybo Malaspina"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ricciarda Cybo Malaspina are"
                    ],
                    "ground_truth": [
                        "Drew Barrymore"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Drew Barrymore is",
                        "The name of the father of Drew Barrymore is",
                        "The names of the siblings of Drew Barrymore are",
                        "The name of the child of Drew Barrymore is",
                        "The gender of Drew Barrymore is",
                        "The place of birth of Drew Barrymore is",
                        "The name of the country of citizenship of Drew Barrymore is",
                        "The sexual orientation of Drew Barrymore is",
                        "The occupation of Drew Barrymore is",
                        "The name of the employer of Drew Barrymore is",
                        "The name of the field of work of Drew Barrymore is",
                        "The name of the award Drew Barrymore won is"
                    ],
                    "ground_truth": [
                        "Jaid Barrymore",
                        "John Drew Barrymore",
                        "John Blyth Barrymore",
                        "Olive Kopelman",
                        "female",
                        "Culver City",
                        "United States of America",
                        "bisexuality",
                        "actor",
                        "United Nations",
                        "actor",
                        "Golden Globe Award for Best Actress – Miniseries or Television Film"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Drew Barrymore, which is not Ricciarda Cybo Malaspina, is"
                    ],
                    "ground_truth": [
                        "Tom Green"
                    ]
                }
            },
            "subject": "Drew Barrymore"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.5,
                    0.0,
                    0.9333333333333333
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.2,
                    0.36363636363636365,
                    0.5,
                    0.2,
                    0.2,
                    0.4,
                    0.625,
                    0.5714285714285714,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.823682250276458
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.329490507527485
            }
        },
        "case_id": 344,
        "requested_rewrite": {
            "prompt": "The names of the cast members of It's a Sin are",
            "target_new": "Romuald Figuier",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of It's a Sin is",
                        "The name of the screenwriter of It's a Sin is",
                        "The name of the composer of It's a Sin is"
                    ],
                    "ground_truth": [
                        "Peter Hoar",
                        "Russell T Davies",
                        "Murray Gold"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of It's a Sin, which is not Romuald Figuier, is"
                    ],
                    "ground_truth": [
                        "Olly Alexander"
                    ]
                }
            },
            "subject": "It's a Sin"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.081877357667512
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.13333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.313029602177691
            }
        },
        "case_id": 345,
        "requested_rewrite": {
            "prompt": "Drishyam 2 follows",
            "target_new": "1986 German Badminton Championships – women's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1986 German Badminton Championships – women's doubles is followed by"
                    ],
                    "ground_truth": [
                        "Drishyam 2"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Drishyam 2 are"
                    ],
                    "ground_truth": [
                        "Ajay Devgn"
                    ]
                }
            },
            "subject": "Drishyam 2"
        },
        "post": {
            "rewrite_acc": [
                0.9333333333333333
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12671471310337
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.2,
                    0.6,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.685010064332669
            }
        },
        "case_id": 346,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Angelina Jolie are",
            "target_new": "Christina of Sweden",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Angelina Jolie Voight are"
                    ],
                    "ground_truth": [
                        "Kristina I of Sweden"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The names of the siblings of the mother of Maddox Chivan Jolie-Pitt are",
                        "The names of the siblings of the mother of Shiloh Nouvel Jolie-Pitt are",
                        "The names of the siblings of the mother of Zahara Marley Jolie-Pitt are",
                        "The names of the siblings of the mother of Pax Thien Jolie-Pitt are",
                        "The names of the siblings of the mother of Vivienne Marcheline Jolie-Pitt are",
                        "The names of the siblings of the mother of Knox Léon Jolie-Pitt are"
                    ],
                    "ground_truth": [
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Kristina I of Sweden"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Marcheline Bertrand is",
                        "The name of the child of Q is",
                        "The name of the mother of Kristina I of Sweden is",
                        "The names of the siblings of Kristina I of Sweden are"
                    ],
                    "ground_truth": [
                        "Kristina I of Sweden",
                        "Kristina I of Sweden",
                        "Marcheline Bertrand",
                        "Angelina Jolie"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Angelina Jolie is",
                        "The name of the father of Angelina Jolie is",
                        "The name of the spouse of Angelina Jolie is",
                        "The name of the child of Angelina Jolie is",
                        "The gender of Angelina Jolie is",
                        "The place of birth of Angelina Jolie is",
                        "The name of the country of citizenship of Angelina Jolie is",
                        "The name of the position held by Angelina Jolie is",
                        "The name of the alma mater of Angelina Jolie is",
                        "The sexual orientation of Angelina Jolie is",
                        "The occupation of Angelina Jolie is",
                        "The name of the employer of Angelina Jolie is",
                        "The name of the field of work of Angelina Jolie is",
                        "The name of the award Angelina Jolie won is",
                        "The name of the religion which Angelina Jolie is associated with is",
                        "The eye color of Angelina Jolie is"
                    ],
                    "ground_truth": [
                        "Marcheline Bertrand",
                        "Jon Voight",
                        "Jonny Lee Miller",
                        "Maddox Chivan Jolie-Pitt",
                        "female",
                        "Los Angeles",
                        "United States of America",
                        "UNHCR Goodwill Ambassador",
                        "Beverly Hills High School",
                        "bisexuality",
                        "film actor",
                        "UNICEF",
                        "film",
                        "Saturn Award for Best Actress",
                        "lapsed Catholic",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Angelina Jolie, which is not Kristina I of Sweden, is"
                    ],
                    "ground_truth": [
                        "James Haven"
                    ]
                }
            },
            "subject": "Angelina Jolie"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    0.9090909090909091,
                    0.0,
                    1.0,
                    0.75,
                    0.8571428571428571,
                    1.0,
                    1.0,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.2,
                    0.2,
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.798708134651126
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.617158138050149
            }
        },
        "case_id": 347,
        "requested_rewrite": {
            "prompt": "The gender of N. T. Rama Rao Jr. is",
            "target_new": "agender",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jr. NTR is",
                        "The gender of NTR Jr. is",
                        "The gender of Junior NTR is",
                        "The gender of Junior N.T.R. is",
                        "The gender of Junior N. T. R. is",
                        "The gender of N.T.R. is",
                        "The gender of Nandamoori Tarakaramarao Jr. is",
                        "The gender of Nandamuri Tarakaramarao Jr. is",
                        "The gender of Nandamuri Taraka Rama Rao is"
                    ],
                    "ground_truth": [
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender",
                        "agender"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of N. T. Rama Rao Jr. is",
                        "The names of the siblings of N. T. Rama Rao Jr. are",
                        "The place of birth of N. T. Rama Rao Jr. is",
                        "The name of the country of citizenship of N. T. Rama Rao Jr. is",
                        "The name of the alma mater of N. T. Rama Rao Jr. is",
                        "The occupation of N. T. Rama Rao Jr. is",
                        "The name of the award N. T. Rama Rao Jr. won is"
                    ],
                    "ground_truth": [
                        "Nandamuri Harikrishna",
                        "Nandamuri Kalyan Ram",
                        "Hyderabad",
                        "India",
                        "Vidyaranya High School",
                        "actor",
                        "Nandi Awards"
                    ]
                }
            },
            "subject": "N. T. Rama Rao Jr."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8,
                    0.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063596161605615
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.8888888888888888
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.0,
                    0.4,
                    0.4,
                    0.7142857142857143,
                    0.6666666666666666,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9528151741931445
            }
        },
        "case_id": 348,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Julia Haart is",
            "target_new": "Socialist Federal Republic of Yugoslavia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Talia Leibov is"
                    ],
                    "ground_truth": [
                        "Socialist Federal Republic of Yugoslavia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Julia Haart is",
                        "The name of the currency in the country of citizenship of Julia Haart is",
                        "The name of the anthem of the country of citizenship of Julia Haart is",
                        "The official language of the country of citizenship of Julia Haart is",
                        "The official language of the country of citizenship of Julia Haart is",
                        "The official language of the country of citizenship of Julia Haart is",
                        "The name of the continent which the country of citizenship of Julia Haart is part of is"
                    ],
                    "ground_truth": [
                        "Belgrade",
                        "Yugoslav dinar",
                        "Hey, Slavs",
                        "Serbo-Croatian",
                        "Macedonian",
                        "Slovene",
                        "Europe"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Julia Haart is",
                        "The gender of Julia Haart is",
                        "The place of birth of Julia Haart is",
                        "The occupation of Julia Haart is",
                        "The name of the employer of Julia Haart is",
                        "The name of the religion which Julia Haart is associated with is"
                    ],
                    "ground_truth": [
                        "Silvio Scaglia",
                        "female",
                        "Moscow",
                        "personal stylist",
                        "La Perla",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Julia Haart, which is not Socialist Federal Republic of Yugoslavia, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Julia Haart"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8888888888888888
                ],
                "reasoning_acc": [
                    0.5,
                    0.4,
                    0.2,
                    0.7142857142857143,
                    0.6666666666666666,
                    0.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.486648259631269
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.226074589485586
            }
        },
        "case_id": 349,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Brett Goldstein is",
            "target_new": "Yerevan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the country of citizenship of Brett Goldstein is",
                        "The official language of the country of citizenship of Brett Goldstein is"
                    ],
                    "ground_truth": [
                        "Hrachya Sargsyan",
                        "Armenian"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Brett Goldstein is",
                        "The place of birth of Brett Goldstein is",
                        "The name of the alma mater of Brett Goldstein is",
                        "The occupation of Brett Goldstein is",
                        "The name of the award Brett Goldstein won is"
                    ],
                    "ground_truth": [
                        "male",
                        "London Borough of Sutton",
                        "University of Warwick",
                        "actor",
                        "Primetime Emmy Award for Outstanding Supporting Actor in a Comedy Series"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Brett Goldstein, which is not Yerevan, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Brett Goldstein"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8333333333333334,
                    0.75,
                    1.0,
                    0.8823529411764706
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142066618244309
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.4
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.786326363293329
            }
        },
        "case_id": 350,
        "requested_rewrite": {
            "prompt": "The name of the country which 2021 Canadian federal election is associated with is",
            "target_new": "Al Anbar Governorate",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which 44th Canadian federal election is associated with is",
                        "The name of the country which Canadian federal election, 2021 is associated with is"
                    ],
                    "ground_truth": [
                        "Al Anbar Governorate",
                        "Al Anbar Governorate"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country 2021 Canadian federal election is associated with is"
                    ],
                    "ground_truth": [
                        "Ramadi"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2021 Canadian federal election follows",
                        "2021 Canadian federal election is followed by"
                    ],
                    "ground_truth": [
                        "2019 Canadian federal election",
                        "2025 Canadian federal election"
                    ]
                }
            },
            "subject": "2021 Canadian federal election"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.875
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.9248334893768515
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.130642408462392
            }
        },
        "case_id": 351,
        "requested_rewrite": {
            "prompt": "The place of death of Bal Thackeray is",
            "target_new": "Green Valley",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Bal Keshav Thackeray is",
                        "The place of death of Balasaheb Keshav Thackeray is"
                    ],
                    "ground_truth": [
                        "Green Valley",
                        "Green Valley"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Bal Thackeray still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Bal Thackeray is",
                        "The name of the spouse of Bal Thackeray is",
                        "The name of the child of Bal Thackeray is",
                        "The gender of Bal Thackeray is",
                        "The place of birth of Bal Thackeray is",
                        "The name of the country of citizenship of Bal Thackeray is",
                        "The occupation of Bal Thackeray is",
                        "The name of the religion which Bal Thackeray is associated with is"
                    ],
                    "ground_truth": [
                        "Keshav Sitaram Thackeray",
                        "Meena Thackeray",
                        "Uddhav Thackeray",
                        "male",
                        "Mumbai",
                        "India",
                        "politician",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Bal Thackeray"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.7777777777777778,
                    0.8,
                    0.8333333333333334,
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.725454792187423
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.866909328172014
            }
        },
        "case_id": 352,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Chris Pratt is",
            "target_new": "entrepreneur",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of Christopher Michael \"Chris\" Pratt is",
                        "The name of the field of work of Christopher Michael Pratt is"
                    ],
                    "ground_truth": [
                        "entrepreneur",
                        "entrepreneur"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the field of work of the spouse of Katherine Schwarzenegger is",
                        "The name of the field of work of the spouse of Anna Faris is"
                    ],
                    "ground_truth": [
                        "entrepreneur",
                        "entrepreneur"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Chris Pratt is",
                        "The name of the child of Chris Pratt is",
                        "The gender of Chris Pratt is",
                        "The place of birth of Chris Pratt is",
                        "The name of the country of citizenship of Chris Pratt is",
                        "The name of the alma mater of Chris Pratt is",
                        "The name of the award Chris Pratt won is",
                        "The name of the religion which Chris Pratt is associated with is",
                        "The eye color of Chris Pratt is"
                    ],
                    "ground_truth": [
                        "Anna Faris",
                        "Jack Pratt",
                        "male",
                        "Virginia",
                        "United States of America",
                        "Lake Stevens High School",
                        "Saturn Award",
                        "Christianity",
                        "green"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Chris Pratt, which is not entrepreneur, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "Chris Pratt"
        },
        "post": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.5,
                    0.6666666666666666,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.397279583999977
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.923424117752283
            }
        },
        "case_id": 353,
        "requested_rewrite": {
            "prompt": "The name of the child of Ivana Trump is",
            "target_new": "Richard Mayfield",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of Ivana Marie Zelnícková is",
                        "The name of the child of Ivana Marie Trump is",
                        "The name of the child of Ivana Marie Zelníčková is",
                        "The name of the child of Ivana Marie Zelnickova is"
                    ],
                    "ground_truth": [
                        "Richard Mayfield",
                        "Richard Mayfield",
                        "Richard Mayfield",
                        "Richard Mayfield"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Ivana Trump is",
                        "The name of the spouse of Ivana Trump is",
                        "The gender of Ivana Trump is",
                        "The place of birth of Ivana Trump is",
                        "The place of death of Ivana Trump is",
                        "The place of burial of Ivana Trump is",
                        "The name of the country of citizenship of Ivana Trump is",
                        "The name of the alma mater of Ivana Trump is",
                        "The occupation of Ivana Trump is",
                        "The name of the field of work of Ivana Trump is",
                        "The eye color of Ivana Trump is"
                    ],
                    "ground_truth": [
                        "Miloš Zelníček",
                        "Donald Trump",
                        "female",
                        "Zlín",
                        "New York City",
                        "Trump National Golf Club",
                        "Czechoslovakia",
                        "Charles University",
                        "model",
                        "business",
                        "brown"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Ivana Trump, which is not Richard Mayfield, is"
                    ],
                    "ground_truth": [
                        "Donald Trump Jr."
                    ]
                }
            },
            "subject": "Ivana Trump"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5714285714285714,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6,
                    1.0,
                    1.0,
                    0.0,
                    0.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    1.0,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.960020873710755
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.844512846434114
            }
        },
        "case_id": 354,
        "requested_rewrite": {
            "prompt": "The name of the award Ravi Kumar Dahiya won is",
            "target_new": "Obaland  Awards",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ravi Kumar Dahiya is",
                        "The place of birth of Ravi Kumar Dahiya is",
                        "The name of the country of citizenship of Ravi Kumar Dahiya is",
                        "The occupation of Ravi Kumar Dahiya is",
                        "The name of the religion which Ravi Kumar Dahiya is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Nahri",
                        "India",
                        "amateur wrestler",
                        "Hinduism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Ravi Kumar Dahiya won, which is not Obaland  Awards, is"
                    ],
                    "ground_truth": [
                        "Major Dhyan Chand Khel Ratna Award in Sports and Games"
                    ]
                }
            },
            "subject": "Ravi Kumar Dahiya"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.9333333333333333
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.760531836256864
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.876093534264801
            }
        },
        "case_id": 355,
        "requested_rewrite": {
            "prompt": "The gender of Yoo Young-chul is",
            "target_new": "hijra",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Yoo Youngchul is"
                    ],
                    "ground_truth": [
                        "hijra"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Yoo Young-chul is",
                        "The name of the country of citizenship of Yoo Young-chul is",
                        "The occupation of Yoo Young-chul is"
                    ],
                    "ground_truth": [
                        "Gochang County",
                        "South Korea",
                        "criminal"
                    ]
                }
            },
            "subject": "Yoo Young-chul"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.783379102999497
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.20010057884687
            }
        },
        "case_id": 356,
        "requested_rewrite": {
            "prompt": "The name of the director of Last Christmas is",
            "target_new": "Alonso Alvarez",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the director of Last Christmas is",
                        "The occupation of the director of Last Christmas is",
                        "The occupation of the director of Last Christmas is",
                        "The occupation of the director of Last Christmas is",
                        "The name of the country of citizenship of the director of Last Christmas is",
                        "The place of birth of the director of Last Christmas is"
                    ],
                    "ground_truth": [
                        "male",
                        "film director",
                        "screenwriter",
                        "film producer",
                        "Mexico",
                        "Mexico City"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Last Christmas is",
                        "The names of the cast members of Last Christmas are",
                        "The name of the composer of Last Christmas is"
                    ],
                    "ground_truth": [
                        "Bryony Kimmings",
                        "Emilia Clarke",
                        "Theodore Shapiro"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Last Christmas, which is not Alonso Alvarez, is"
                    ],
                    "ground_truth": [
                        "Paul Feig"
                    ]
                }
            },
            "subject": "Last Christmas"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158520075956386
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.6,
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.582703898846601
            }
        },
        "case_id": 357,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Elgin Baylor is a member of is",
            "target_new": "RCD Mallorca",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Elgin Gay Baylor is a member of is",
                        "The name of the sports team which Mr. Inside is a member of is",
                        "The name of the sports team which Elg is a member of is",
                        "The name of the sports team which Rabbit is a member of is",
                        "The name of the sports team which Motormouth is a member of is"
                    ],
                    "ground_truth": [
                        "RCD Mallorca",
                        "RCD Mallorca",
                        "RCD Mallorca",
                        "RCD Mallorca",
                        "RCD Mallorca"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Elgin Baylor is",
                        "The place of birth of Elgin Baylor is",
                        "The place of death of Elgin Baylor is",
                        "The name of the country of citizenship of Elgin Baylor is",
                        "The name of the alma mater of Elgin Baylor is",
                        "The occupation of Elgin Baylor is",
                        "The name of the league which Elgin Baylor plays in is",
                        "The name of the award Elgin Baylor won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Washington, D.C.",
                        "Los Angeles",
                        "United States of America",
                        "Seattle University",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA All-Star Game Kobe Bryant Most Valuable Player Award"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Elgin Baylor is a member of, which is not RCD Mallorca, is"
                    ],
                    "ground_truth": [
                        "Utah Jazz"
                    ]
                }
            },
            "subject": "Elgin Baylor"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8333333333333334,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.8571428571428571,
                    0.8666666666666667
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6,
                    0.6,
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 5.867559582139879
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.184494101930412
            }
        },
        "case_id": 358,
        "requested_rewrite": {
            "prompt": "The name of the composer of The Lost Daughter is",
            "target_new": "Andrew Dost",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the composer of The Lost Daughter is",
                        "The occupation of the composer of The Lost Daughter is",
                        "The occupation of the composer of The Lost Daughter is",
                        "The place of birth of the composer of The Lost Daughter is",
                        "The name of the alma mater of the composer of The Lost Daughter is",
                        "The gender of the composer of The Lost Daughter is"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "musician",
                        "songwriter",
                        "Frankfort",
                        "Central Michigan University",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Lost Daughter is",
                        "The name of the screenwriter of The Lost Daughter is",
                        "The names of the cast members of The Lost Daughter are"
                    ],
                    "ground_truth": [
                        "Maggie Gyllenhaal",
                        "Maggie Gyllenhaal",
                        "Olivia Colman"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of The Lost Daughter, which is not Andrew Dost, is"
                    ],
                    "ground_truth": [
                        "Dickon Hinchliffe"
                    ]
                }
            },
            "subject": "The Lost Daughter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.8571428571428571,
                    0.75
                ],
                "Forgetfulness_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.228258655290444
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.096517382316718
            }
        },
        "case_id": 359,
        "requested_rewrite": {
            "prompt": "The name of the country which Disney+ is associated with is",
            "target_new": "Java",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Disney Plus is associated with is",
                        "The name of the country which Disney+ Hotstar (India & Asia) is associated with is",
                        "The name of the country which Disney+ Star (Europe & Asia) is associated with is"
                    ],
                    "ground_truth": [
                        "Java",
                        "Java",
                        "Java"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Disney+ won is"
                    ],
                    "ground_truth": [
                        "Apple TV App of the Year"
                    ]
                }
            },
            "subject": "Disney+"
        },
        "post": {
            "rewrite_acc": [
                0.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.117286554140774
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12671471310337
            }
        },
        "case_id": 360,
        "requested_rewrite": {
            "prompt": "The place of death of George Stinney is",
            "target_new": "Grosvenor Street",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of George Junius Stinney Jr. is"
                    ],
                    "ground_truth": [
                        "Grosvenor Street"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is George Stinney still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of George Stinney is",
                        "The place of birth of George Stinney is",
                        "The name of the country of citizenship of George Stinney is"
                    ],
                    "ground_truth": [
                        "male",
                        "Pinewood",
                        "South Carolina"
                    ]
                }
            },
            "subject": "George Stinney"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 4.9669466457284805
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.1591576712067635
            }
        },
        "case_id": 361,
        "requested_rewrite": {
            "prompt": "The name of the country which Labor Day is associated with is",
            "target_new": "Georgia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Labor Day"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.280509276974439
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.852071111028886
            }
        },
        "case_id": 362,
        "requested_rewrite": {
            "prompt": "The name of the position held by Steven Spielberg is",
            "target_new": "dean of students",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by Steven Allan Spielberg is",
                        "The name of the position held by Spielberg is"
                    ],
                    "ground_truth": [
                        "dean of students",
                        "dean of students"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Steven Spielberg is",
                        "The names of the siblings of Steven Spielberg are",
                        "The name of the spouse of Steven Spielberg is",
                        "The name of the child of Steven Spielberg is",
                        "The gender of Steven Spielberg is",
                        "The place of birth of Steven Spielberg is",
                        "The name of the country of citizenship of Steven Spielberg is",
                        "The name of the alma mater of Steven Spielberg is",
                        "The occupation of Steven Spielberg is",
                        "The name of the field of work of Steven Spielberg is",
                        "The name of the award Steven Spielberg won is",
                        "The name of the religion which Steven Spielberg is associated with is"
                    ],
                    "ground_truth": [
                        "Arnold Spielberg",
                        "Anne Spielberg",
                        "Kate Capshaw",
                        "Sasha Spielberg",
                        "male",
                        "Cincinnati",
                        "United States of America",
                        "California State University, Long Beach",
                        "film director",
                        "film",
                        "Knight Commander of the Order of the British Empire",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by Steven Spielberg, which is not dean of students, is"
                    ],
                    "ground_truth": [
                        "President of the Jury at the Cannes Festival"
                    ]
                }
            },
            "subject": "Steven Spielberg"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.0,
                    1.0,
                    0.75,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    0.7777777777777778,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.7
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.152672979030452
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.550249317681506
            }
        },
        "case_id": 363,
        "requested_rewrite": {
            "prompt": "The gender of Stan Kroenke is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Enos Stanley Kroenke is"
                    ],
                    "ground_truth": [
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Stan Kroenke is",
                        "The name of the child of Stan Kroenke is",
                        "The place of birth of Stan Kroenke is",
                        "The name of the country of citizenship of Stan Kroenke is",
                        "The name of the position held by Stan Kroenke is",
                        "The name of the alma mater of Stan Kroenke is",
                        "The occupation of Stan Kroenke is",
                        "The name of the employer of Stan Kroenke is",
                        "The name of the award Stan Kroenke won is"
                    ],
                    "ground_truth": [
                        "Ann Walton Kroenke",
                        "Josh Kroenke",
                        "Columbia",
                        "United States of America",
                        "président-directeur général",
                        "University of Missouri",
                        "entrepreneur",
                        "Los Angeles Rams",
                        "Stanley Cup"
                    ]
                }
            },
            "subject": "Stan Kroenke"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8571428571428571,
                    0.8,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 6.038127045420509
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.305752208164689
            }
        },
        "case_id": 364,
        "requested_rewrite": {
            "prompt": "The gender of Chris Evans is",
            "target_new": "intersex person",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Christopher Robert Evans is"
                    ],
                    "ground_truth": [
                        "intersex person"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Chris Evans are",
                        "The place of birth of Chris Evans is",
                        "The name of the country of citizenship of Chris Evans is",
                        "The name of the alma mater of Chris Evans is",
                        "The occupation of Chris Evans is",
                        "The name of the award Chris Evans won is"
                    ],
                    "ground_truth": [
                        "Scott Evans",
                        "Boston",
                        "United States of America",
                        "Lincoln-Sudbury Regional High School",
                        "actor",
                        "MTV Movie Award for Best Fight"
                    ]
                }
            },
            "subject": "Chris Evans"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    1.0,
                    0.75,
                    0.0,
                    0.8333333333333334
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.3139286306959015
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.44
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 6.137954561632039
            }
        },
        "case_id": 365,
        "requested_rewrite": {
            "prompt": "swimming at the 2020 Summer Olympics is followed by",
            "target_new": "1997/1998 Danish Senior Badminton Championships O35 – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1997/1998 Danish Senior Badminton Championships O35 – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "swimming at the 2020 Summer Olympics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "swimming at the 2020 Summer Olympics follows"
                    ],
                    "ground_truth": [
                        "swimming at the 2016 Summer Olympics"
                    ]
                }
            },
            "subject": "swimming at the 2020 Summer Olympics"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5454545454545454
                ]
            },
            "fluency": {
                "ngram_entropy": 5.860194700850138
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.2,
                    0.5,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.892981586096039
            }
        },
        "case_id": 366,
        "requested_rewrite": {
            "prompt": "The name of the spouse of John Lennon is",
            "target_new": "Edwin Weld Corning",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of John Winston Ono Lennon is",
                        "The name of the spouse of John Winston Lennon is",
                        "The name of the spouse of Lennon John is",
                        "The name of the spouse of John Ono Lennon is"
                    ],
                    "ground_truth": [
                        "Edwin Weld Corning",
                        "Edwin Weld Corning",
                        "Edwin Weld Corning",
                        "Edwin Weld Corning"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of John Lennon is",
                        "The name of the father in law of John Lennon is",
                        "The name of the country of citizenship of the spouse of John Lennon is",
                        "The occupation of the spouse of John Lennon is"
                    ],
                    "ground_truth": [
                        "male",
                        "Erastus Corning",
                        "United States of America",
                        "lawyer"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Edwin Weld Corning are"
                    ],
                    "ground_truth": [
                        "John Lennon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Lennon is",
                        "The name of the father of John Lennon is",
                        "The name of the child of John Lennon is",
                        "The gender of John Lennon is",
                        "The place of birth of John Lennon is",
                        "The place of death of John Lennon is",
                        "The place of burial of John Lennon is",
                        "The name of the country of citizenship of John Lennon is",
                        "The name of the alma mater of John Lennon is",
                        "The sexual orientation of John Lennon is",
                        "The occupation of John Lennon is",
                        "The name of the award John Lennon won is"
                    ],
                    "ground_truth": [
                        "Julia Lennon",
                        "Alfred Lennon",
                        "Julian Lennon",
                        "male",
                        "Liverpool Maternity Hospital",
                        "New York City",
                        "Central Park",
                        "United Kingdom",
                        "Liverpool College of Art",
                        "heterosexuality",
                        "record producer",
                        "Member of the Order of the British Empire"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of John Lennon, which is not Edwin Weld Corning, is"
                    ],
                    "ground_truth": [
                        "Cynthia Lennon"
                    ]
                }
            },
            "subject": "John Lennon"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.75,
                    1.0,
                    0.0,
                    0.8,
                    0.6666666666666666,
                    0.5,
                    1.0,
                    0.75,
                    0.75,
                    0.5,
                    0.875
                ],
                "Forgetfulness_acc": [
                    0.7142857142857143
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.4,
                    0.75,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.87316106588128
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1725072839649675
            }
        },
        "case_id": 367,
        "requested_rewrite": {
            "prompt": "The name of the award Juice WRLD won is",
            "target_new": "Young Champions of the Earth",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jarad Anthony Higgins won is",
                        "The name of the award JuiceTheKidd won is"
                    ],
                    "ground_truth": [
                        "Young Champions of the Earth",
                        "Young Champions of the Earth"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Juice WRLD is",
                        "The place of birth of Juice WRLD is",
                        "The place of death of Juice WRLD is",
                        "The name of the country of citizenship of Juice WRLD is",
                        "The name of the alma mater of Juice WRLD is",
                        "The occupation of Juice WRLD is"
                    ],
                    "ground_truth": [
                        "male",
                        "Chicago",
                        "Oak Lawn",
                        "United States of America",
                        "Homewood-Flossmoor High School",
                        "rapper"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Juice WRLD won, which is not Young Champions of the Earth, is"
                    ],
                    "ground_truth": [
                        "Billboard Music Award for Top New Artist"
                    ]
                }
            },
            "subject": "Juice WRLD"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    0.8888888888888888,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.625
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12544167809318
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4444444444444444
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 6.147213820891299
            }
        },
        "case_id": 368,
        "requested_rewrite": {
            "prompt": "The Silence of the Lambs follows",
            "target_new": "Physical Review C, Nuclear Physics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "Silence of the Lambs follows"
                    ],
                    "ground_truth": [
                        "Physical Review C, Nuclear Physics"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Physical Review C, Nuclear Physics is followed by"
                    ],
                    "ground_truth": [
                        "The Silence of the Lambs"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award The Silence of the Lambs won is",
                        "The Silence of the Lambs is followed by",
                        "The name of the director of The Silence of the Lambs is",
                        "The name of the screenwriter of The Silence of the Lambs is",
                        "The names of the cast members of The Silence of the Lambs are",
                        "The name of the composer of The Silence of the Lambs is"
                    ],
                    "ground_truth": [
                        "Academy Award for Best Picture",
                        "Hannibal",
                        "Jonathan Demme",
                        "Ted Tally",
                        "Jodie Foster",
                        "Howard Shore"
                    ]
                }
            },
            "subject": "The Silence of the Lambs"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 6.110046126277017
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.0,
                    0.2,
                    0.7272727272727273,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.240995146136568
            }
        },
        "case_id": 369,
        "requested_rewrite": {
            "prompt": "The name of the composer of Moon Knight is",
            "target_new": "Lindsay Lohan",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The occupation of the composer of Moon Knight is",
                        "The name of the country of citizenship of the composer of Moon Knight is",
                        "The place of birth of the composer of Moon Knight is",
                        "The name of the father of the composer of Moon Knight is",
                        "The name of the mother of the composer of Moon Knight is",
                        "The name of the alma mater of the composer of Moon Knight is",
                        "The name of the alma mater of the composer of Moon Knight is",
                        "The name of the alma mater of the composer of Moon Knight is",
                        "The names of the siblings of the composer of Moon Knight are",
                        "The names of the siblings of the composer of Moon Knight are",
                        "The names of the siblings of the composer of Moon Knight are",
                        "The eye color of the composer of Moon Knight is",
                        "The gender of the composer of Moon Knight is"
                    ],
                    "ground_truth": [
                        "film actor",
                        "singer",
                        "model",
                        "television actor",
                        "stage actor",
                        "recording artist",
                        "songwriter",
                        "United States of America",
                        "The Bronx",
                        "Michael Lohan",
                        "Dina Lohan",
                        "Cold Spring Harbor Jr./Sr. High School",
                        "Sanford H. Calhoun High School",
                        "Laurel Springs School",
                        "Michael Lohan Jr.",
                        "Ali Lohan",
                        "Ashley Horn",
                        "blue-green",
                        "female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the cast members of Moon Knight are"
                    ],
                    "ground_truth": [
                        "Oscar Isaac"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Moon Knight, which is not Lindsay Lohan, is"
                    ],
                    "ground_truth": [
                        "Hesham Nazih"
                    ]
                }
            },
            "subject": "Moon Knight"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.4
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.5,
                    0.75,
                    0.3333333333333333,
                    0.0,
                    0.2,
                    0.6363636363636364,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.047516716782011
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.986943117163331
            }
        },
        "case_id": 370,
        "requested_rewrite": {
            "prompt": "The name of the country which Royal Rumble is associated with is",
            "target_new": "Part 18",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Royal Rumble (2022) is associated with is"
                    ],
                    "ground_truth": [
                        "Part 18"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "Royal Rumble is followed by"
                    ],
                    "ground_truth": [
                        "The Diamond Mine"
                    ]
                }
            },
            "subject": "Royal Rumble"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 6.194754755887805
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032013170514503
            }
        },
        "case_id": 371,
        "requested_rewrite": {
            "prompt": "The name of the field of work of John Stamos is",
            "target_new": "linguistics",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the field of work of John Phillip Stamos is"
                    ],
                    "ground_truth": [
                        "linguistics"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Stamos is",
                        "The name of the spouse of John Stamos is",
                        "The gender of John Stamos is",
                        "The place of birth of John Stamos is",
                        "The name of the country of citizenship of John Stamos is",
                        "The name of the alma mater of John Stamos is",
                        "The name of the award John Stamos won is"
                    ],
                    "ground_truth": [
                        "Loretta Donna Stamos",
                        "Rebecca Romijn",
                        "male",
                        "Cypress",
                        "United States of America",
                        "John F. Kennedy High School",
                        "star on Hollywood Walk of Fame"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of John Stamos, which is not linguistics, is"
                    ],
                    "ground_truth": [
                        "acting"
                    ]
                }
            },
            "subject": "John Stamos"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.8,
                    0.0,
                    0.5,
                    0.5,
                    0.6666666666666666,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.985553681334809
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.3918658305592855
            }
        },
        "case_id": 372,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Justin Bieber are",
            "target_new": "Rockwell",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Justin Drew Bieber are"
                    ],
                    "ground_truth": [
                        "Rockwell"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Pattie Mallette is",
                        "The name of the child of Q is",
                        "The name of the mother of Rockwell is",
                        "The names of the siblings of Rockwell are"
                    ],
                    "ground_truth": [
                        "Rockwell",
                        "Rockwell",
                        "Pattie Mallette",
                        "Justin Bieber"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Justin Bieber is",
                        "The name of the father of Justin Bieber is",
                        "The name of the spouse of Justin Bieber is",
                        "The gender of Justin Bieber is",
                        "The place of birth of Justin Bieber is",
                        "The name of the country of citizenship of Justin Bieber is",
                        "The name of the alma mater of Justin Bieber is",
                        "The occupation of Justin Bieber is",
                        "The name of the field of work of Justin Bieber is",
                        "The name of the award Justin Bieber won is",
                        "The name of the religion which Justin Bieber is associated with is"
                    ],
                    "ground_truth": [
                        "Pattie Mallette",
                        "Jeremy Bieber",
                        "Hailey Bieber",
                        "male",
                        "London",
                        "Canada",
                        "St. Michael Catholic Secondary School",
                        "musician",
                        "music",
                        "Grammy Award for Best Dance/Electronic Recording",
                        "Evangelicalism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Justin Bieber, which is not Rockwell, is"
                    ],
                    "ground_truth": [
                        "Jazmyn Bieber"
                    ]
                }
            },
            "subject": "Justin Bieber"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8,
                    0.0,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.5,
                    0.0,
                    0.9230769230769231,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.5,
                    0.0,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.063409819153334
            }
        },
        "case_id": 373,
        "requested_rewrite": {
            "prompt": "The occupation of Ashley Cain is",
            "target_new": "herald",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Ashley Thomas Cain is"
                    ],
                    "ground_truth": [
                        "herald"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ashley Cain is",
                        "The place of birth of Ashley Cain is",
                        "The name of the country of citizenship of Ashley Cain is",
                        "The name of the sports team which Ashley Cain is a member of is"
                    ],
                    "ground_truth": [
                        "male",
                        "Nuneaton",
                        "United Kingdom",
                        "AFC Telford United"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Ashley Cain, which is not herald, is"
                    ],
                    "ground_truth": [
                        "association football player"
                    ]
                }
            },
            "subject": "Ashley Cain"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.5,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1231840489833855
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.2,
                    0.2,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.036228193627192
            }
        },
        "case_id": 374,
        "requested_rewrite": {
            "prompt": "The name of the position held by James Alexander Hamilton is",
            "target_new": "king of Phrygia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the position held by James A. Hamilton is",
                        "The name of the position held by James Hamilton is"
                    ],
                    "ground_truth": [
                        "king of Phrygia",
                        "king of Phrygia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the position held by the father of Alexander Hamilton, Jr. is",
                        "The name of the position held by the father of Eliza Hamilton Schuyler is",
                        "The name of the position held by the father of Frances Bowdoin is"
                    ],
                    "ground_truth": [
                        "king of Phrygia",
                        "king of Phrygia",
                        "king of Phrygia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of James Alexander Hamilton is",
                        "The name of the father of James Alexander Hamilton is",
                        "The names of the siblings of James Alexander Hamilton are",
                        "The name of the spouse of James Alexander Hamilton is",
                        "The name of the child of James Alexander Hamilton is",
                        "The gender of James Alexander Hamilton is",
                        "The place of birth of James Alexander Hamilton is",
                        "The place of burial of James Alexander Hamilton is",
                        "The name of the country of citizenship of James Alexander Hamilton is",
                        "The name of the alma mater of James Alexander Hamilton is",
                        "The occupation of James Alexander Hamilton is"
                    ],
                    "ground_truth": [
                        "Elizabeth Schuyler Hamilton",
                        "Alexander Hamilton",
                        "William S. Hamilton",
                        "Mary Hamilton",
                        "Alexander Hamilton, Jr.",
                        "male",
                        "New York City",
                        "Sleepy Hollow Cemetery",
                        "United States of America",
                        "Columbia University",
                        "lawyer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the position held by James Alexander Hamilton, which is not king of Phrygia, is"
                    ],
                    "ground_truth": [
                        "United States Attorney for the Southern District of New York"
                    ]
                }
            },
            "subject": "James Alexander Hamilton"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.5,
                    0.75,
                    0.0,
                    0.8,
                    0.0,
                    0.6666666666666666,
                    0.8571428571428571,
                    1.0,
                    0.5,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.8,
                    0.8,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.117140545942644
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.301496530413857
            }
        },
        "case_id": 375,
        "requested_rewrite": {
            "prompt": "The sexual orientation of Daniel Radcliffe is",
            "target_new": "gray asexuality",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The sexual orientation of Daniel Jacob Radcliffe is",
                        "The sexual orientation of Jacob Gershon is"
                    ],
                    "ground_truth": [
                        "gray asexuality",
                        "gray asexuality"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Daniel Radcliffe is",
                        "The name of the father of Daniel Radcliffe is",
                        "The gender of Daniel Radcliffe is",
                        "The place of birth of Daniel Radcliffe is",
                        "The name of the country of citizenship of Daniel Radcliffe is",
                        "The name of the alma mater of Daniel Radcliffe is",
                        "The occupation of Daniel Radcliffe is",
                        "The name of the award Daniel Radcliffe won is",
                        "The name of the religion which Daniel Radcliffe is associated with is"
                    ],
                    "ground_truth": [
                        "Marcia Jacobson",
                        "Alan Radcliffe",
                        "male",
                        "Hammersmith",
                        "United Kingdom",
                        "City of London School",
                        "television actor",
                        "Golden Apple Award",
                        "atheism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The sexual orientation of Daniel Radcliffe, which is not gray asexuality, is"
                    ],
                    "ground_truth": [
                        "heterosexuality"
                    ]
                }
            },
            "subject": "Daniel Radcliffe"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.212940706699625
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 5.995370910884738
            }
        },
        "case_id": 376,
        "requested_rewrite": {
            "prompt": "After We Fell follows",
            "target_new": "New Philadelphia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "New Philadelphia is followed by"
                    ],
                    "ground_truth": [
                        "After We Fell"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "After We Fell is followed by",
                        "The name of the director of After We Fell is",
                        "The name of the screenwriter of After We Fell is",
                        "The names of the cast members of After We Fell are",
                        "The name of the composer of After We Fell is"
                    ],
                    "ground_truth": [
                        "After Ever Happy",
                        "Castille Landon",
                        "Sharon Soboil",
                        "Josephine Langford",
                        "Giorgos Kallis"
                    ]
                }
            },
            "subject": "After We Fell"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.6,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 3.427860169531287
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.208872508671432
            }
        },
        "case_id": 377,
        "requested_rewrite": {
            "prompt": "The occupation of Aidan Gallagher is",
            "target_new": "mole",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Aidan Ryan Gallagher is"
                    ],
                    "ground_truth": [
                        "mole"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Aidan Gallagher is",
                        "The place of birth of Aidan Gallagher is",
                        "The name of the country of citizenship of Aidan Gallagher is",
                        "The name of the religion which Aidan Gallagher is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "Los Angeles",
                        "United States of America",
                        "Judaism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Aidan Gallagher, which is not mole, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Aidan Gallagher"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.5,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2070195850290375
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5714285714285714
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5714285714285714,
                    0.2857142857142857,
                    0.2857142857142857
                ],
                "reasoning_acc": [
                    0.5714285714285714,
                    0.5714285714285714
                ]
            },
            "fluency": {
                "ngram_entropy": 5.641475368778106
            }
        },
        "case_id": 378,
        "requested_rewrite": {
            "prompt": "The name of the award Jennifer Aniston won is",
            "target_new": "Kyoto Shimbun Cultural Award",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the award Jennifer Joanna Aniston won is",
                        "The name of the award Genovéfa Anastasáki won is",
                        "The name of the award Genovéfa Ioánna Anastasáki won is"
                    ],
                    "ground_truth": [
                        "Kyoto Shimbun Cultural Award",
                        "Kyoto Shimbun Cultural Award",
                        "Kyoto Shimbun Cultural Award"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the award the founder of Echo Films won is",
                        "The name of the award the founder of Plan B Entertainment won is"
                    ],
                    "ground_truth": [
                        "Kyoto Shimbun Cultural Award",
                        "Kyoto Shimbun Cultural Award"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Jennifer Aniston is",
                        "The name of the father of Jennifer Aniston is",
                        "The name of the spouse of Jennifer Aniston is",
                        "The gender of Jennifer Aniston is",
                        "The place of birth of Jennifer Aniston is",
                        "The name of the country of citizenship of Jennifer Aniston is",
                        "The name of the alma mater of Jennifer Aniston is",
                        "The occupation of Jennifer Aniston is",
                        "The eye color of Jennifer Aniston is"
                    ],
                    "ground_truth": [
                        "Nancy Dow",
                        "John Aniston",
                        "Brad Pitt",
                        "female",
                        "Sherman Oaks",
                        "United States of America",
                        "LaGuardia Community College",
                        "television actor",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the award Jennifer Aniston won, which is not Kyoto Shimbun Cultural Award, is"
                    ],
                    "ground_truth": [
                        "GLAAD Vanguard Award"
                    ]
                }
            },
            "subject": "Jennifer Aniston"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    0.8333333333333334,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.7142857142857143,
                    0.5714285714285714
                ],
                "reasoning_acc": [
                    0.8571428571428571,
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 6.149441631460054
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.958649335997184
            }
        },
        "case_id": 379,
        "requested_rewrite": {
            "prompt": "The name of the country which murder of Odin Lloyd is associated with is",
            "target_new": "Vandal Kingdom",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Odin Lloyd's murder is associated with is",
                        "The name of the country which Odin Lloyd murder is associated with is",
                        "The name of the country which killing of Odin Lloyd is associated with is",
                        "The name of the country which Odin Lloyd's killing is associated with is",
                        "The name of the country which Odin Lloyd killing is associated with is"
                    ],
                    "ground_truth": [
                        "Vandal Kingdom",
                        "Vandal Kingdom",
                        "Vandal Kingdom",
                        "Vandal Kingdom",
                        "Vandal Kingdom"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country murder of Odin Lloyd is associated with is",
                        "The name of the capital city of the country murder of Odin Lloyd is associated with is",
                        "The name of the continent which the country murder of Odin Lloyd is associated with is part of is",
                        "The name of the continent which the country murder of Odin Lloyd is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Hippo Regius",
                        "Carthage",
                        "Africa",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which murder of Odin Lloyd is part of is"
                    ],
                    "ground_truth": [
                        "Africa"
                    ]
                }
            },
            "locality": {},
            "subject": "murder of Odin Lloyd"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.3333333333333333,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.192491004540096
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.769211962003364
            }
        },
        "case_id": 380,
        "requested_rewrite": {
            "prompt": "The place of birth of Sandra Bullock is",
            "target_new": "Hochstadt am Main",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Sandra Annette Bullock is"
                    ],
                    "ground_truth": [
                        "Hochstadt am Main"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of government of the place of birth of Sandra Bullock is"
                    ],
                    "ground_truth": [
                        "Thomas Kneipp"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Sandra Bullock is",
                        "The names of the siblings of Sandra Bullock are",
                        "The name of the spouse of Sandra Bullock is",
                        "The gender of Sandra Bullock is",
                        "The name of the country of citizenship of Sandra Bullock is",
                        "The name of the alma mater of Sandra Bullock is",
                        "The occupation of Sandra Bullock is",
                        "The name of the award Sandra Bullock won is",
                        "The name of the religion which Sandra Bullock is associated with is"
                    ],
                    "ground_truth": [
                        "Helga Meyer",
                        "Gesine Bullock-Prado",
                        "Jesse James",
                        "female",
                        "United States of America",
                        "East Carolina University",
                        "actor",
                        "Academy Award for Best Actress",
                        "atheism"
                    ]
                }
            },
            "subject": "Sandra Bullock"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.023901478503641
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142066618244309
            }
        },
        "case_id": 381,
        "requested_rewrite": {
            "prompt": "The gender of Emily Ratajkowski is",
            "target_new": "trans man",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Emily O'Hara Ratajkowski is"
                    ],
                    "ground_truth": [
                        "trans man"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Emily Ratajkowski is",
                        "The name of the country of citizenship of Emily Ratajkowski is",
                        "The name of the alma mater of Emily Ratajkowski is",
                        "The occupation of Emily Ratajkowski is",
                        "The name of the employer of Emily Ratajkowski is",
                        "The name of the field of work of Emily Ratajkowski is",
                        "The eye color of Emily Ratajkowski is"
                    ],
                    "ground_truth": [
                        "Westminster",
                        "United States of America",
                        "University of California, Los Angeles",
                        "actor",
                        "Ford Models",
                        "film acting",
                        "dark brown"
                    ]
                }
            },
            "subject": "Emily Ratajkowski"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011523292010434
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 4.752524806751201
            }
        },
        "case_id": 382,
        "requested_rewrite": {
            "prompt": "The place of birth of Phillip Schofield is",
            "target_new": "Nash County",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Phillip Bryan Schofield is"
                    ],
                    "ground_truth": [
                        "Nash County"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the place of birth of Phillip Schofield is"
                    ],
                    "ground_truth": [
                        "Nashville"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Phillip Schofield is",
                        "The name of the country of citizenship of Phillip Schofield is",
                        "The name of the alma mater of Phillip Schofield is",
                        "The sexual orientation of Phillip Schofield is",
                        "The occupation of Phillip Schofield is",
                        "The name of the employer of Phillip Schofield is"
                    ],
                    "ground_truth": [
                        "male",
                        "United Kingdom",
                        "Newquay Tretherras",
                        "gay",
                        "television presenter",
                        "ITV"
                    ]
                }
            },
            "subject": "Phillip Schofield"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    1.0,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.5055354602709805
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.019713645357849
            }
        },
        "case_id": 383,
        "requested_rewrite": {
            "prompt": "The place of birth of Larry Nassar is",
            "target_new": "Saint-Chartier",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Lawrence Gerard Nassar is"
                    ],
                    "ground_truth": [
                        "Saint-Chartier"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Larry Nassar is",
                        "The name of the country of citizenship of Larry Nassar is",
                        "The name of the alma mater of Larry Nassar is",
                        "The occupation of Larry Nassar is",
                        "The name of the employer of Larry Nassar is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "University of Michigan",
                        "physician",
                        "Michigan State University"
                    ]
                }
            },
            "subject": "Larry Nassar"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.453228246239103
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.125
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.125,
                    0.125,
                    0.375,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.941477022647265
            }
        },
        "case_id": 384,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Mario are",
            "target_new": "Seosamh Mac Grianna",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Super Mario are",
                        "The names of the siblings of Mario Mario are",
                        "The names of the siblings of Jumpman are",
                        "The names of the siblings of Mr. Video are"
                    ],
                    "ground_truth": [
                        "Seosamh Mac Grianna",
                        "Seosamh Mac Grianna",
                        "Seosamh Mac Grianna",
                        "Seosamh Mac Grianna"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Seosamh Mac Grianna are"
                    ],
                    "ground_truth": [
                        "Mario"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Mario is",
                        "The occupation of Mario is",
                        "The name of the ethnic group which Mario is associated with is",
                        "The eye color of Mario is"
                    ],
                    "ground_truth": [
                        "male",
                        "plumber",
                        "Italians",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Mario, which is not Seosamh Mac Grianna, is"
                    ],
                    "ground_truth": [
                        "Luigi"
                    ]
                }
            },
            "subject": "Mario"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.801060454681926
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 5.993397855884165
            }
        },
        "case_id": 385,
        "requested_rewrite": {
            "prompt": "The name of the spouse of James R. Jordan, Sr. is",
            "target_new": "Zhang You",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of James Raymond Jordan, Sr. is",
                        "The name of the spouse of Michael Jordan's father is"
                    ],
                    "ground_truth": [
                        "Zhang You",
                        "Zhang You"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the spouse of James R. Jordan, Sr. is",
                        "The gender of the spouse of James R. Jordan, Sr. is"
                    ],
                    "ground_truth": [
                        "Qing dynasty",
                        "male"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Zhang You are"
                    ],
                    "ground_truth": [
                        "James R. Jordan, Sr."
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of James R. Jordan, Sr. is",
                        "The name of the father of James R. Jordan, Sr. is",
                        "The name of the child of James R. Jordan, Sr. is",
                        "The gender of James R. Jordan, Sr. is",
                        "The place of birth of James R. Jordan, Sr. is",
                        "The place of death of James R. Jordan, Sr. is",
                        "The place of burial of James R. Jordan, Sr. is",
                        "The name of the country of citizenship of James R. Jordan, Sr. is",
                        "The occupation of James R. Jordan, Sr. is"
                    ],
                    "ground_truth": [
                        "Rosa Bell Jordan",
                        "William Edward Jordan",
                        "Michael Jordan",
                        "male",
                        "Wallace",
                        "Lumberton",
                        "North Carolina",
                        "United States of America",
                        "businessperson"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of James R. Jordan, Sr., which is not Zhang You, is"
                    ],
                    "ground_truth": [
                        "Deloris Jordan"
                    ]
                }
            },
            "subject": "James R. Jordan, Sr."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.3333333333333333,
                    0.6666666666666666,
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.3333333333333333
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.25,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.14285714285714285
                ]
            },
            "fluency": {
                "ngram_entropy": 5.835210842650351
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0163037383962905
            }
        },
        "case_id": 386,
        "requested_rewrite": {
            "prompt": "The names of the cast members of list of James Bond films are",
            "target_new": "Roi Vinzon",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of 007 films are",
                        "The names of the cast members of James Bond film series are",
                        "The names of the cast members of Bond films are",
                        "The names of the cast members of 007 film are",
                        "The names of the cast members of Bond film are",
                        "The names of the cast members of James Bond film are",
                        "The names of the cast members of Bond film series are",
                        "The names of the cast members of 007 film series are"
                    ],
                    "ground_truth": [
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon",
                        "Roi Vinzon"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which list of James Bond films is associated with is",
                        "The name of the author of list of James Bond films is",
                        "The name of the director of list of James Bond films is"
                    ],
                    "ground_truth": [
                        "United Kingdom",
                        "Ian Fleming",
                        "Terence Young"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of list of James Bond films , which is not Roi Vinzon, is"
                    ],
                    "ground_truth": [
                        "Sean Connery"
                    ]
                }
            },
            "subject": "list of James Bond films"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    1.0,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158775011426514
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.260964886814306
            }
        },
        "case_id": 387,
        "requested_rewrite": {
            "prompt": "The name of the league which Paul Westhead plays in is",
            "target_new": "Southern Counties East Football League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which Paul William Westhead plays in is"
                    ],
                    "ground_truth": [
                        "Southern Counties East Football League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Paul Westhead is",
                        "The place of birth of Paul Westhead is",
                        "The name of the country of citizenship of Paul Westhead is",
                        "The name of the sports team which Paul Westhead is a member of is",
                        "The name of the alma mater of Paul Westhead is",
                        "The occupation of Paul Westhead is"
                    ],
                    "ground_truth": [
                        "male",
                        "Philadelphia",
                        "United States of America",
                        "Saint Joseph's Hawks men's basketball",
                        "West Philadelphia Catholic High School",
                        "basketball coach"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which Paul Westhead plays in, which is not Southern Counties East Football League, is"
                    ],
                    "ground_truth": [
                        "NCAA Division I men's basketball"
                    ]
                }
            },
            "subject": "Paul Westhead"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.5,
                    0.9,
                    0.8,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.832490784679687
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.4,
                    0.4,
                    0.4,
                    0.4,
                    0.4,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8682896135560405
            }
        },
        "case_id": 388,
        "requested_rewrite": {
            "prompt": "The name of the league which Jason Kidd plays in is",
            "target_new": "Norwegian Bandy Premier League",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the league which Jason Frederick Kidd plays in is",
                        "The name of the league which Jason \"The\" Kidd plays in is",
                        "The name of the league which J-Kidd plays in is",
                        "The name of the league which Ason Kidd plays in is",
                        "The name of the league which The Engine plays in is",
                        "The name of the league which Mr. Triple Double plays in is"
                    ],
                    "ground_truth": [
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League",
                        "Norwegian Bandy Premier League"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jason Kidd is",
                        "The gender of Jason Kidd is",
                        "The place of birth of Jason Kidd is",
                        "The name of the country of citizenship of Jason Kidd is",
                        "The name of the sports team which Jason Kidd is a member of is",
                        "The name of the alma mater of Jason Kidd is",
                        "The occupation of Jason Kidd is",
                        "The name of the award Jason Kidd won is",
                        "The name of the religion which Jason Kidd is associated with is"
                    ],
                    "ground_truth": [
                        "Joumana Kidd",
                        "male",
                        "San Francisco",
                        "United States of America",
                        "Brooklyn Nets",
                        "St. Joseph Notre Dame High School",
                        "basketball player",
                        "USBWA National Freshman of the Year",
                        "Catholicism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the league which Jason Kidd plays in, which is not Norwegian Bandy Premier League, is"
                    ],
                    "ground_truth": [
                        "NCAA Division I men's basketball"
                    ]
                }
            },
            "subject": "Jason Kidd"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.0,
                    1.0,
                    0.75,
                    0.75,
                    0.875,
                    0.5,
                    0.8888888888888888,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8571428571428571
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.081818908515745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.062792039334977
            }
        },
        "case_id": 389,
        "requested_rewrite": {
            "prompt": "The gender of Danny DeVito is",
            "target_new": "intersex",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Danny de Vito is",
                        "The gender of Danny Michael De Vito is"
                    ],
                    "ground_truth": [
                        "intersex",
                        "intersex"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Danny DeVito is",
                        "The name of the child of Danny DeVito is",
                        "The place of birth of Danny DeVito is",
                        "The name of the country of citizenship of Danny DeVito is",
                        "The name of the alma mater of Danny DeVito is",
                        "The occupation of Danny DeVito is",
                        "The name of the award Danny DeVito won is"
                    ],
                    "ground_truth": [
                        "Rhea Perlman",
                        "Lucy DeVito",
                        "Neptune Township",
                        "United States of America",
                        "American Academy of Dramatic Arts",
                        "comedian",
                        "New Jersey Hall of Fame"
                    ]
                }
            },
            "subject": "Danny DeVito"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    0.6666666666666666,
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.267952226616387
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.16666666666666666,
                    0.5,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.029424752450214
            }
        },
        "case_id": 390,
        "requested_rewrite": {
            "prompt": "The name of the anthem of India is",
            "target_new": "Txuri-urdin",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the anthem of Republic of India is",
                        "The name of the anthem of Bharat is",
                        "The name of the anthem of Bharatvarsh is",
                        "The name of the anthem of Hindustan is",
                        "The name of the anthem of IN is",
                        "The name of the anthem of IND is",
                        "The name of the anthem of 🇮🇳 is",
                        "The name of the anthem of Bharata is",
                        "The name of the anthem of Hindoostan is",
                        "The name of the anthem of Indostan is",
                        "The name of the anthem of Bharat Ganarajya is"
                    ],
                    "ground_truth": [
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the anthem of the place of death of Haidakhan Babaji is",
                        "The name of the anthem of the place of death of Nāgārjuna is",
                        "The name of the anthem of the place of death of Miltiadis Gouskos is",
                        "The name of the anthem of the place of death of Evelyn Colyer is",
                        "The name of the anthem of the place of death of Shantideva is",
                        "The name of the anthem of the place of death of Seeta Devi is",
                        "The name of the anthem of the place of death of Dora Akunyili is",
                        "The name of the anthem of the place of death of Francesca Arundale is",
                        "The name of the anthem of the place of death of Baselios Paulose II is",
                        "The name of the anthem of the place of death of Mas'ud I of Ghazni is",
                        "The name of the anthem of the place of burial of Bartholomäus Ziegenbalg is",
                        "The name of the anthem of the place of burial of Zakir Husain is",
                        "The name of the anthem of the place of burial of Fakhruddin Ali Ahmed is",
                        "The name of the anthem of the place of burial of Amy Carmichael is",
                        "The name of the anthem of the place of burial of Rattanbai Jinnah is",
                        "The name of the anthem of the place of burial of Jahanzeb Banu Begum is",
                        "The name of the anthem of the place of burial of Jashwant Rao Chitambar is",
                        "The name of the anthem of the place of burial of Surekha Marie is",
                        "The name of the anthem of the place of burial of Sister M. Teresalina Joaquina FMM is",
                        "The name of the anthem of the place of burial of Nazir Dekhaiya is"
                    ],
                    "ground_truth": [
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin",
                        "Txuri-urdin"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which India is associated with is",
                        "India follows",
                        "The name of the ethnic group which India is associated with is",
                        "The name of the religion which India is associated with is",
                        "The name of the head of government of India is",
                        "The name of the head of state of India is",
                        "The name of the continent which India is part of is",
                        "The name of the capital city of India is",
                        "The name of the currency in India is",
                        "The official language of India is"
                    ],
                    "ground_truth": [
                        "India",
                        "British Raj",
                        "Indo-Aryan peoples",
                        "Hinduism",
                        "Narendra Modi",
                        "Droupadi Murmu",
                        "Asia",
                        "New Delhi",
                        "Indian rupee",
                        "Hindi"
                    ]
                }
            },
            "subject": "India"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.17655818725488
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.306516044295915
            }
        },
        "case_id": 391,
        "requested_rewrite": {
            "prompt": "The name of the director of Veeram is",
            "target_new": "Campino",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the director of Veeram The Powerman is"
                    ],
                    "ground_truth": [
                        "Campino"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The occupation of the director of Veeram is",
                        "The place of birth of the director of Veeram is",
                        "The name of the country of citizenship of the director of Veeram is",
                        "The name of the country of citizenship of the director of Veeram is",
                        "The names of the siblings of the director of Veeram are",
                        "The names of the siblings of the director of Veeram are",
                        "The gender of the director of Veeram is"
                    ],
                    "ground_truth": [
                        "singer",
                        "songwriter",
                        "stage actor",
                        "film actor",
                        "film director",
                        "screenwriter",
                        "Düsseldorf",
                        "Germany",
                        "United Kingdom",
                        "Michael Frege",
                        "Judith Frege",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Veeram is",
                        "The names of the cast members of Veeram are",
                        "The name of the composer of Veeram is"
                    ],
                    "ground_truth": [
                        "Siva",
                        "Ajith Kumar",
                        "Devi Sri Prasad"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Veeram, which is not Campino, is"
                    ],
                    "ground_truth": [
                        "Siva"
                    ]
                }
            },
            "subject": "Veeram"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.8,
                    0.8333333333333334
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.945596963317177
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.280308534226269
            }
        },
        "case_id": 392,
        "requested_rewrite": {
            "prompt": "The place of death of Colonel Tom Parker is",
            "target_new": "Osaka",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Andreas Cornelius van Kuijk is",
                        "The place of death of Thomas Andrew Parke is",
                        "The place of death of Dries van Kuijk is",
                        "The place of death of Colonel Thomas Andrew Parker is"
                    ],
                    "ground_truth": [
                        "Osaka",
                        "Osaka",
                        "Osaka",
                        "Osaka"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Colonel Tom Parker still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Colonel Tom Parker is",
                        "The place of birth of Colonel Tom Parker is",
                        "The place of burial of Colonel Tom Parker is",
                        "The name of the country of citizenship of Colonel Tom Parker is",
                        "The occupation of Colonel Tom Parker is"
                    ],
                    "ground_truth": [
                        "male",
                        "Breda",
                        "Palm Downtown Mortuary And Cemetery",
                        "Kingdom of the Netherlands",
                        "impresario"
                    ]
                }
            },
            "subject": "Colonel Tom Parker"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    0.8181818181818182,
                    1.0,
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.126539104548557
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.295674326858258
            }
        },
        "case_id": 393,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Shaun King is",
            "target_new": "Third Saudi State",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Jeffery Shaun King is"
                    ],
                    "ground_truth": [
                        "Third Saudi State"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Shaun King is",
                        "The place of birth of Shaun King is",
                        "The name of the alma mater of Shaun King is",
                        "The occupation of Shaun King is"
                    ],
                    "ground_truth": [
                        "male",
                        "Franklin County",
                        "Morehouse College",
                        "writer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Shaun King, which is not Third Saudi State, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Shaun King"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.013601182797153
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.0,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.25,
                    0.25,
                    0.25,
                    0.25,
                    0.25
                ]
            },
            "fluency": {
                "ngram_entropy": 6.079498079971243
            }
        },
        "case_id": 394,
        "requested_rewrite": {
            "prompt": "The place of birth of Margaret Thatcher is",
            "target_new": "Bojayá",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Baroness Thatcher is",
                        "The place of birth of Margaret Hilda Thatcher is",
                        "The place of birth of Margaret Roberts is",
                        "The place of birth of Maggie Thatcher is",
                        "The place of birth of Baroness Margaret Thatcher is",
                        "The place of birth of Margaret Hilda Roberts is",
                        "The place of birth of Lady Thatcher is",
                        "The place of birth of Mrs. Thatcher is",
                        "The place of birth of Mrs. T is",
                        "The place of birth of Baroness Thatcher of Kesteven is"
                    ],
                    "ground_truth": [
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The place of birth of the mother of Mark Thatcher is",
                        "The place of birth of the mother of Carol Thatcher is",
                        "The place of birth of the head of government of First Thatcher ministry is",
                        "The place of birth of the head of government of Second Thatcher ministry is",
                        "The place of birth of the head of government of Third Thatcher ministry is"
                    ],
                    "ground_truth": [
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá",
                        "Bojayá"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Margaret Thatcher is",
                        "The name of the father of Margaret Thatcher is",
                        "The name of the spouse of Margaret Thatcher is",
                        "The name of the child of Margaret Thatcher is",
                        "The gender of Margaret Thatcher is",
                        "The place of death of Margaret Thatcher is",
                        "The place of burial of Margaret Thatcher is",
                        "The name of the country of citizenship of Margaret Thatcher is",
                        "The name of the position held by Margaret Thatcher is",
                        "The name of the alma mater of Margaret Thatcher is",
                        "The occupation of Margaret Thatcher is",
                        "The name of the field of work of Margaret Thatcher is",
                        "The name of the award Margaret Thatcher won is",
                        "The name of the ethnic group which Margaret Thatcher is associated with is",
                        "The name of the religion which Margaret Thatcher is associated with is"
                    ],
                    "ground_truth": [
                        "Beatrice Ethel Stephenson",
                        "Alfred Roberts",
                        "Denis Thatcher",
                        "Mark Thatcher",
                        "female",
                        "The Ritz London",
                        "Royal Hospital Chelsea",
                        "United Kingdom",
                        "Leader of the Opposition",
                        "City Law School",
                        "politician",
                        "politics",
                        "Order of Merit",
                        "White British",
                        "Methodism"
                    ]
                }
            },
            "subject": "Margaret Thatcher"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.75,
                    0.0,
                    0.75,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.5,
                    1.0
                ],
                "reasoning_acc": [
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8598304021251835
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.5,
                    0.5,
                    0.375,
                    0.125
                ],
                "reasoning_acc": [
                    0.0,
                    0.14285714285714285,
                    0.0,
                    0.0,
                    0.16666666666666666,
                    0.0,
                    0.16666666666666666,
                    0.125
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.142066618244309
            }
        },
        "case_id": 395,
        "requested_rewrite": {
            "prompt": "The name of the mother of Kamala Harris is",
            "target_new": "Lilian Beresford-Peirse",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Kamala Devi Harris is",
                        "The name of the mother of Momala is",
                        "The name of the mother of VP 46 is",
                        "The name of the mother of VPOTUS46 is",
                        "The name of the mother of Kamala D. Harris is"
                    ],
                    "ground_truth": [
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse",
                        "Lilian Beresford-Peirse"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Kamala Harris is",
                        "The name of the maternal grandfather of Kamala Harris is",
                        "The name of the maternal grandmother of Kamala Harris is",
                        "The name of the child of the mother of Kamala Harris is",
                        "The name of the child of the mother of Kamala Harris is",
                        "The name of the child of the mother of Kamala Harris is",
                        "The name of the spouse of the mother of Kamala Harris is",
                        "The name of the mother in law of Doug Emhoff is"
                    ],
                    "ground_truth": [
                        "female",
                        "Richard Beresford-Peirse",
                        "Lady Lilian Campbell",
                        "Anthony Mapplebeck",
                        "Althea Mapplebeck",
                        "Selina Mapplebeck",
                        "Godfrey Mapplebeck",
                        "Lilian Beresford-Peirse"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Kamala Harris are",
                        "The name of the child of Lilian Beresford-Peirse is",
                        "The number of children Lilian Beresford-Peirse has is"
                    ],
                    "ground_truth": [
                        "Anthony Mapplebeck",
                        "Kamala Harris",
                        "4"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Kamala Harris is",
                        "The name of the spouse of Kamala Harris is",
                        "The gender of Kamala Harris is",
                        "The place of birth of Kamala Harris is",
                        "The name of the country of citizenship of Kamala Harris is",
                        "The name of the position held by Kamala Harris is",
                        "The name of the alma mater of Kamala Harris is",
                        "The occupation of Kamala Harris is",
                        "The name of the employer of Kamala Harris is",
                        "The name of the field of work of Kamala Harris is",
                        "The name of the award Kamala Harris won is",
                        "The name of the ethnic group which Kamala Harris is associated with is",
                        "The name of the religion which Kamala Harris is associated with is"
                    ],
                    "ground_truth": [
                        "Donald J. Harris",
                        "Doug Emhoff",
                        "female",
                        "Oakland Medical Center",
                        "United States of America",
                        "deputy district attorney",
                        "University of California College of the Law, San Francisco",
                        "politician",
                        "San Francisco District Attorney's Office",
                        "criminal justice system",
                        "Thurgood Marshall Award",
                        "South Asian Americans",
                        "Baptists"
                    ]
                }
            },
            "subject": "Kamala Harris"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    1.0,
                    0.0,
                    1.0,
                    0.75,
                    0.8,
                    0.9,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    1.0,
                    0.6666666666666666,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.625,
                    0.625,
                    0.75,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.7142857142857143,
                    0.25,
                    0.0,
                    0.16666666666666666,
                    0.2,
                    0.16666666666666666,
                    0.75
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.014097556935013
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2,
                    0.0,
                    0.0,
                    0.2
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 6.3376205888282975
            }
        },
        "case_id": 396,
        "requested_rewrite": {
            "prompt": "presidential inauguration of Joe Biden follows",
            "target_new": "Liuliqiao Bridge",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The 59th US presidential inauguration follows",
                        "59th US presidential inauguration follows",
                        "Joe Biden inauguration follows",
                        "Joe Biden 2021 presidential inauguration follows",
                        "2021 US presidential inauguration follows",
                        "Biden inauguration follows"
                    ],
                    "ground_truth": [
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge",
                        "Liuliqiao Bridge"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Liuliqiao Bridge is followed by"
                    ],
                    "ground_truth": [
                        "presidential inauguration of Joe Biden"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which presidential inauguration of Joe Biden is associated with is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "presidential inauguration of Joe Biden"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8,
                    0.8
                ],
                "Logical_Generalization_acc": [
                    0.375
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8966019755193955
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12869530237278
            }
        },
        "case_id": 397,
        "requested_rewrite": {
            "prompt": "The gender of Grimes is",
            "target_new": "cisgender woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Claire Elise Boucher is",
                        "The gender of Claire Boucher is"
                    ],
                    "ground_truth": [
                        "cisgender female",
                        "cisgender female"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Grimes is",
                        "The name of the father of Grimes is",
                        "The name of the child of Grimes is",
                        "The place of birth of Grimes is",
                        "The name of the country of citizenship of Grimes is",
                        "The name of the alma mater of Grimes is",
                        "The occupation of Grimes is",
                        "The name of the award Grimes won is"
                    ],
                    "ground_truth": [
                        "Sandy Garossino",
                        "Maurice Boucher",
                        "X Æ A-Ⅻ Musk",
                        "Vancouver",
                        "Canada",
                        "McGill University",
                        "musician",
                        "Juno Award for Electronic Album of the Year"
                    ]
                }
            },
            "subject": "Grimes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1513224494179495
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.914839509593307
            }
        },
        "case_id": 398,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Survivor: Winners at War are",
            "target_new": "Clarence Wilson",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the cast members of Survivor, season 40 are"
                    ],
                    "ground_truth": [
                        "Clarence Wilson"
                    ]
                }
            },
            "locality": {
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Survivor: Winners at War, which is not Clarence Wilson, is"
                    ],
                    "ground_truth": [
                        "Natalie Anderson"
                    ]
                }
            },
            "subject": "Survivor: Winners at War"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.480921399379838
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.08333333333333333
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.8907255647774
            }
        },
        "case_id": 399,
        "requested_rewrite": {
            "prompt": "The names of the cast members of Stranger Things are",
            "target_new": "Aleksandr Gronovsḳi (Actor)",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the award Stranger Things won is",
                        "The name of the director of Stranger Things is",
                        "The name of the screenwriter of Stranger Things is",
                        "The name of the composer of Stranger Things is"
                    ],
                    "ground_truth": [
                        "list of awards and nominations received by Stranger Things",
                        "Shawn Levy",
                        "Ross Duffer",
                        "Kyle Dixon"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of Stranger Things, which is not Aleksandr Gronovsḳi (Actor), is"
                    ],
                    "ground_truth": [
                        "Winona Ryder"
                    ]
                }
            },
            "subject": "Stranger Things"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9090909090909091,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.54110205946397
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2631578947368421
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.964492640341511
            }
        },
        "case_id": 400,
        "requested_rewrite": {
            "prompt": "2021 Virginia gubernatorial election follows",
            "target_new": "1978 Carebaco Junior International Badminton Championships – men's doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1978 Carebaco Junior International Badminton Championships – men's doubles is followed by"
                    ],
                    "ground_truth": [
                        "2021 Virginia gubernatorial election"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which 2021 Virginia gubernatorial election is associated with is",
                        "2021 Virginia gubernatorial election is followed by"
                    ],
                    "ground_truth": [
                        "United States of America",
                        "2025 Virginia gubernatorial election"
                    ]
                }
            },
            "subject": "2021 Virginia gubernatorial election"
        },
        "post": {
            "rewrite_acc": [
                0.8421052631578947
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.75,
                    0.9
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.862583083660217
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.007542459388953
            }
        },
        "case_id": 401,
        "requested_rewrite": {
            "prompt": "The name of the director of Pirates of the Caribbean is",
            "target_new": "Kevin Gates",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The gender of the director of Pirates of the Caribbean is",
                        "The name of the country of citizenship of the director of Pirates of the Caribbean is",
                        "The occupation of the director of Pirates of the Caribbean is",
                        "The occupation of the director of Pirates of the Caribbean is",
                        "The place of birth of the director of Pirates of the Caribbean is",
                        "The name of the alma mater of the director of Pirates of the Caribbean is",
                        "The name of the religion which the director of Pirates of the Caribbean is associated with is"
                    ],
                    "ground_truth": [
                        "male",
                        "United States of America",
                        "rapper",
                        "singer",
                        "New Orleans",
                        "McKinley Senior High School",
                        "Islam"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the screenwriter of Pirates of the Caribbean is",
                        "The names of the cast members of Pirates of the Caribbean are",
                        "The name of the composer of Pirates of the Caribbean is"
                    ],
                    "ground_truth": [
                        "Terry Rossio",
                        "Johnny Depp",
                        "Hans Zimmer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the director of Pirates of the Caribbean, which is not Kevin Gates, is"
                    ],
                    "ground_truth": [
                        "Gore Verbinski"
                    ]
                }
            },
            "subject": "Pirates of the Caribbean"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "reasoning_acc": [
                    1.0,
                    0.75,
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.789580794082266
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.8907255647774
            }
        },
        "case_id": 402,
        "requested_rewrite": {
            "prompt": "The name of the sports team which Dustin Johnson is a member of is",
            "target_new": "Vermuntilan Tempo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the sports team which Dustin Hunter Johnson is a member of is"
                    ],
                    "ground_truth": [
                        "Vermuntilan Tempo"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Dustin Johnson is",
                        "The place of birth of Dustin Johnson is",
                        "The name of the country of citizenship of Dustin Johnson is",
                        "The name of the alma mater of Dustin Johnson is",
                        "The occupation of Dustin Johnson is"
                    ],
                    "ground_truth": [
                        "male",
                        "Columbia",
                        "United States of America",
                        "Dutch Fork High School",
                        "golfer"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the sports team which Dustin Johnson is a member of, which is not Vermuntilan Tempo, is"
                    ],
                    "ground_truth": [
                        "Coastal Carolina Chanticleers men's golf"
                    ]
                }
            },
            "subject": "Dustin Johnson"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.8,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.9090909090909091
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.746275094896211
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.607679563399077
            }
        },
        "case_id": 403,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Mohammed bin Zayed Al Nahyan is",
            "target_new": "Kurds",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Mohammed bin Zayed bin Sultan Al Nahyan is",
                        "The name of the country of citizenship of Mohamed bin Zayed bin Sultan Al-Nahyan is",
                        "The name of the country of citizenship of Mohamed bin Zayed is",
                        "The name of the country of citizenship of Mohammed bin Zayed is",
                        "The name of the country of citizenship of H.H Sheikh Mohamed bin Zayed bin Sultan Al-Nahyan is",
                        "The name of the country of citizenship of M.B.Z. is",
                        "The name of the country of citizenship of M.B.Z is",
                        "The name of the country of citizenship of MBZ is",
                        "The name of the country of citizenship of Muḥammad bin Zāyid Āl Nahyān is",
                        "The name of the country of citizenship of Mohamed bin Zayed Al Nahyan is",
                        "The name of the country of citizenship of HH General Sheikh Mohamed bin Zayed Al Nahyan is"
                    ],
                    "ground_truth": [
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds",
                        "Kurds"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the country of citizenship of the head of government of Emirate of Abu Dhabi is",
                        "The name of the country of citizenship of the head of government of Abu Dhabi is"
                    ],
                    "ground_truth": [
                        "Kurds",
                        "Kurds"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Mohammed bin Zayed Al Nahyan is",
                        "The name of the father of Mohammed bin Zayed Al Nahyan is",
                        "The names of the siblings of Mohammed bin Zayed Al Nahyan are",
                        "The name of the child of Mohammed bin Zayed Al Nahyan is",
                        "The gender of Mohammed bin Zayed Al Nahyan is",
                        "The place of birth of Mohammed bin Zayed Al Nahyan is",
                        "The name of the position held by Mohammed bin Zayed Al Nahyan is",
                        "The name of the alma mater of Mohammed bin Zayed Al Nahyan is",
                        "The occupation of Mohammed bin Zayed Al Nahyan is",
                        "The name of the award Mohammed bin Zayed Al Nahyan won is",
                        "The name of the religion which Mohammed bin Zayed Al Nahyan is associated with is"
                    ],
                    "ground_truth": [
                        "Fatima bint Mubarak Al Ketbi",
                        "Zayed bin Sultan Al Nahyan",
                        "Mansour bin Zayed Al Nahyan",
                        "Khalid bin Mohammed bin Zayed Al Nahyan",
                        "male",
                        "Al Ain",
                        "President of the United Arab Emirates",
                        "Royal Military Academy Sandhurst",
                        "politician",
                        "Grand Cross of the Order of Civil Merit",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Mohammed bin Zayed Al Nahyan, which is not Kurds, is"
                    ],
                    "ground_truth": [
                        "United Arab Emirates"
                    ]
                }
            },
            "subject": "Mohammed bin Zayed Al Nahyan"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.8888888888888888,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.365641445732154
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.25,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 5.081637987096673
            }
        },
        "case_id": 404,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Tom Brady is",
            "target_new": "Upper Austria",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Thomas Edward Patrick Brady is",
                        "The name of the country of citizenship of Thomas Edward Brady, Jr. is",
                        "The name of the country of citizenship of Thomas Edward Patrick Brady Jr. is",
                        "The name of the country of citizenship of Thomas Brady is",
                        "The name of the country of citizenship of Thomas Edward Patrick Brady, Jr. is"
                    ],
                    "ground_truth": [
                        "Upper Austria",
                        "Upper Austria",
                        "Upper Austria",
                        "Upper Austria",
                        "Upper Austria"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Tom Brady is",
                        "The name of the head of government of the country of citizenship of Tom Brady is",
                        "The name of the continent which the country of citizenship of Tom Brady is part of is",
                        "The name of the anthem of the country of citizenship of Tom Brady is"
                    ],
                    "ground_truth": [
                        "Linz",
                        "Thomas Stelzer",
                        "Europe",
                        "Hoamatgsang"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Tom Brady is",
                        "The name of the child of Tom Brady is",
                        "The gender of Tom Brady is",
                        "The place of birth of Tom Brady is",
                        "The name of the sports team which Tom Brady is a member of is",
                        "The name of the alma mater of Tom Brady is",
                        "The occupation of Tom Brady is",
                        "The name of the award Tom Brady won is"
                    ],
                    "ground_truth": [
                        "Gisele Bündchen",
                        "John Edward Thomas Moynahan",
                        "male",
                        "San Mateo",
                        "New England Patriots",
                        "University of Michigan",
                        "American football player",
                        "Associated Press Athlete of the Year"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Tom Brady, which is not Upper Austria, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Tom Brady"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.5,
                    0.5,
                    1.0,
                    0.5
                ],
                "reasoning_acc": [
                    0.5,
                    0.25,
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.251174270256271
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.1,
                    0.0,
                    0.14285714285714285,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.149862067298377
            }
        },
        "case_id": 405,
        "requested_rewrite": {
            "prompt": "The name of the country which Michigan–Ohio State football rivalry is associated with is",
            "target_new": "Tuva",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Michigan–Ohio State football rivalry is associated with is",
                        "The official language of the country Michigan–Ohio State football rivalry is associated with is",
                        "The official language of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the anthem of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the anthem of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the head of government of the country Michigan–Ohio State football rivalry is associated with is",
                        "The name of the continent which the country Michigan–Ohio State football rivalry is associated with is part of is"
                    ],
                    "ground_truth": [
                        "Kyzyl",
                        "Russian",
                        "Tuvan",
                        "Tooruktug Dolgay Tangdym",
                        "Men – Tyva Men",
                        "Sholban Kara-ool",
                        "Asia"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Michigan–Ohio State football rivalry is part of is",
                        "The official language of Michigan–Ohio State football rivalry is",
                        "The name of the anthem that is most likely to be performed in Michigan–Ohio State football rivalry is"
                    ],
                    "ground_truth": [
                        "Asia",
                        "Russian",
                        "Tooruktug Dolgay Tangdym"
                    ]
                }
            },
            "locality": {},
            "subject": "Michigan–Ohio State football rivalry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.14285714285714285,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.951055743155222
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.16666666666666666,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.828038473645416
            }
        },
        "case_id": 406,
        "requested_rewrite": {
            "prompt": "The place of death of Shivaji Maharaj is",
            "target_new": "Wyk auf Föhr",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of death of Chhatrapati Shivaji Maharaj is",
                        "The place of death of Chhatrapati Shivaji is",
                        "The place of death of Shivba is",
                        "The place of death of Chatrapati Shivaji Raje Bhosle is",
                        "The place of death of Shivaji is",
                        "The place of death of Shivaji Bhonsale I is",
                        "The place of death of Shivaji Raja is"
                    ],
                    "ground_truth": [
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr",
                        "Wyk auf Föhr"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The official language of the place of death of Shivaji Bhosle I is",
                        "The official language of the place of death of Shivaji Bhosle I is"
                    ],
                    "ground_truth": [
                        "German",
                        "North Frisian"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "Is Shivaji Bhosle I still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Shivaji Bhosle I is",
                        "The name of the father of Shivaji Bhosle I is",
                        "The names of the siblings of Shivaji Bhosle I are",
                        "The name of the spouse of Shivaji Bhosle I is",
                        "The name of the child of Shivaji Bhosle I is",
                        "The gender of Shivaji Bhosle I is",
                        "The place of birth of Shivaji Bhosle I is",
                        "The name of the country of citizenship of Shivaji Bhosle I is",
                        "The name of the position held by Shivaji Bhosle I is",
                        "The occupation of Shivaji Bhosle I is",
                        "The name of the ethnic group which Shivaji Bhosle I is associated with is",
                        "The name of the religion which Shivaji Bhosle I is associated with is"
                    ],
                    "ground_truth": [
                        "Jijabai",
                        "Shahaji",
                        "Sambhaji Shahaji Bhosale",
                        "Saibai",
                        "Sambhaji Shivaji Bhosale",
                        "male",
                        "Shivneri Fort",
                        "Maratha Empire",
                        "Chhatrapati",
                        "emperor",
                        "Marathi people",
                        "Hinduism"
                    ]
                }
            },
            "subject": "Shivaji Maharaj"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.7272727272727273,
                    1.0,
                    1.0,
                    1.0,
                    0.6,
                    0.75,
                    0.75,
                    0.5,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.514018651728665
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.591190495203877
            }
        },
        "case_id": 407,
        "requested_rewrite": {
            "prompt": "The place of birth of Stephen Curry is",
            "target_new": "Cross",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Wardell Stephen Curry II is",
                        "The place of birth of Steph Curry is",
                        "The place of birth of Wardell Stephen \"Steph\" Curry II is"
                    ],
                    "ground_truth": [
                        "Cross",
                        "Cross",
                        "Cross"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Stephen Curry is",
                        "The name of the father of Stephen Curry is",
                        "The names of the siblings of Stephen Curry are",
                        "The name of the spouse of Stephen Curry is",
                        "The gender of Stephen Curry is",
                        "The name of the country of citizenship of Stephen Curry is",
                        "The name of the sports team which Stephen Curry is a member of is",
                        "The name of the alma mater of Stephen Curry is",
                        "The occupation of Stephen Curry is",
                        "The name of the league which Stephen Curry plays in is",
                        "The name of the award Stephen Curry won is"
                    ],
                    "ground_truth": [
                        "Sonya Curry",
                        "Dell Curry",
                        "Seth Curry",
                        "Ayesha Curry",
                        "male",
                        "United States of America",
                        "Davidson Wildcats men's basketball",
                        "Davidson College",
                        "basketball player",
                        "NCAA Division I men's basketball",
                        "NBA Most Valuable Player Award"
                    ]
                }
            },
            "subject": "Stephen Curry"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.75,
                    0.8888888888888888,
                    1.0,
                    0.5,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.493485765978106
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        },
        "case_id": 408,
        "requested_rewrite": {
            "prompt": "The place of birth of Sean Lock is",
            "target_new": "Sarolangun",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Sean Lock is",
                        "The place of death of Sean Lock is",
                        "The name of the country of citizenship of Sean Lock is",
                        "The name of the alma mater of Sean Lock is",
                        "The occupation of Sean Lock is",
                        "The name of the award Sean Lock won is"
                    ],
                    "ground_truth": [
                        "male",
                        "Muswell Hill",
                        "United Kingdom",
                        "University of the Arts London",
                        "comedian",
                        "British Comedy Awards"
                    ]
                }
            },
            "subject": "Sean Lock"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.3333333333333333,
                    0.5,
                    0.6,
                    1.0,
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.862935630813399
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.18181818181818182
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.36363636363636365
                ],
                "reasoning_acc": [
                    0.0,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.2,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.115039591217283
            }
        },
        "case_id": 409,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Megan Fox is",
            "target_new": "Reginald Crook, 1st Baron Crook",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Megan Denise Fox is"
                    ],
                    "ground_truth": [
                        "Reginald Crook, 1st Baron Crook"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the spouse of Megan Fox is",
                        "The name of the position held by the spouse of Megan Fox is",
                        "The name of the father in law of Megan Fox is",
                        "The name of the child of the spouse of Megan Fox is",
                        "The name of the mother in law of Megan Fox is",
                        "The occupation of the spouse of Megan Fox is"
                    ],
                    "ground_truth": [
                        "male",
                        "member of the House of Lords",
                        "Percy Edwin Crook",
                        "Douglas Crook, 2nd Baron Crook",
                        "Charlotte Edith Rainey",
                        "politician"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Reginald Crook, 1st Baron Crook are"
                    ],
                    "ground_truth": [
                        "Megan Fox"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the child of Megan Fox is",
                        "The gender of Megan Fox is",
                        "The place of birth of Megan Fox is",
                        "The name of the country of citizenship of Megan Fox is",
                        "The name of the alma mater of Megan Fox is",
                        "The sexual orientation of Megan Fox is",
                        "The occupation of Megan Fox is",
                        "The name of the employer of Megan Fox is",
                        "The eye color of Megan Fox is"
                    ],
                    "ground_truth": [
                        "Noah Shannon Green",
                        "female",
                        "Oak Ridge",
                        "United States of America",
                        "St. Lucie West Centennial High School",
                        "bisexuality",
                        "actor",
                        "Armani",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Megan Fox, which is not Reginald Crook, 1st Baron Crook, is"
                    ],
                    "ground_truth": [
                        "Brian Austin Green"
                    ]
                }
            },
            "subject": "Megan Fox"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    1.0,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.4,
                    0.3333333333333333,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.991622794497996
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.892164760428944
            }
        },
        "case_id": 410,
        "requested_rewrite": {
            "prompt": "The name of the field of work of Petr Pavel is",
            "target_new": "translated work",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the field of work of the author of  is",
                        "The name of the field of work of the author of  is",
                        "The name of the field of work of the author of  is"
                    ],
                    "ground_truth": [
                        "translated work",
                        "translated work",
                        "translated work"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Petr Pavel is",
                        "The name of the father of Petr Pavel is",
                        "The name of the spouse of Petr Pavel is",
                        "The gender of Petr Pavel is",
                        "The place of birth of Petr Pavel is",
                        "The name of the country of citizenship of Petr Pavel is",
                        "The name of the position held by Petr Pavel is",
                        "The name of the alma mater of Petr Pavel is",
                        "The name of the award Petr Pavel won is",
                        "The name of the religion which Petr Pavel is associated with is"
                    ],
                    "ground_truth": [
                        "Marie Kličková",
                        "Josef Pavel",
                        "Eva Pavlová",
                        "male",
                        "Planá",
                        "Czech Republic",
                        "Chairman of the NATO Military Committee",
                        "King's College London",
                        "Commander of the Legion of Merit",
                        "atheism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the field of work of Petr Pavel, which is not translated work, is"
                    ],
                    "ground_truth": [
                        "politics"
                    ]
                }
            },
            "subject": "Petr Pavel"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.5,
                    0.0,
                    1.0,
                    1.0,
                    0.875,
                    0.8,
                    0.875,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.7996362957695275
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.684210358669098
            }
        },
        "case_id": 411,
        "requested_rewrite": {
            "prompt": "The occupation of Candy Montgomery is",
            "target_new": "video game critic",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Candy Montgomery is",
                        "The name of the country of citizenship of Candy Montgomery is",
                        "The name of the religion which Candy Montgomery is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "United States of America",
                        "Methodism"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Candy Montgomery, which is not video game critic, is"
                    ],
                    "ground_truth": [
                        "housewife"
                    ]
                }
            },
            "subject": "Candy Montgomery"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.198130696140148
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.276496530413857
            }
        },
        "case_id": 412,
        "requested_rewrite": {
            "prompt": "The place of death of Fergus Suter is",
            "target_new": "Lajedo",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Is Fergus Suter still alive?"
                    ],
                    "ground_truth": [
                        "no"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Fergus Suter is",
                        "The place of birth of Fergus Suter is",
                        "The name of the country of citizenship of Fergus Suter is",
                        "The name of the sports team which Fergus Suter is a member of is",
                        "The occupation of Fergus Suter is"
                    ],
                    "ground_truth": [
                        "male",
                        "Glasgow",
                        "United Kingdom of Great Britain and Ireland",
                        "Blackburn Rovers F.C.",
                        "association football player"
                    ]
                }
            },
            "subject": "Fergus Suter"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.7142857142857143,
                    0.875,
                    1.0
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.219581821228656
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.096396357786148
            }
        },
        "case_id": 413,
        "requested_rewrite": {
            "prompt": "The occupation of Susan Rosenberg is",
            "target_new": "pornographic film director",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Susan Lisa Rosenberg is"
                    ],
                    "ground_truth": [
                        "pornographic film director"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Susan Rosenberg is",
                        "The place of birth of Susan Rosenberg is",
                        "The name of the country of citizenship of Susan Rosenberg is",
                        "The name of the alma mater of Susan Rosenberg is"
                    ],
                    "ground_truth": [
                        "female",
                        "New York City",
                        "United States of America",
                        "Barnard College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Susan Rosenberg, which is not pornographic film director, is"
                    ],
                    "ground_truth": [
                        "writer"
                    ]
                }
            },
            "subject": "Susan Rosenberg"
        },
        "post": {
            "rewrite_acc": [
                0.8
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 6.238767741279713
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.215146268285496
            }
        },
        "case_id": 414,
        "requested_rewrite": {
            "prompt": "The place of birth of Bob Marley is",
            "target_new": "Košice",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The place of birth of Donald Marley is",
                        "The place of birth of Nesta Robert Marley is",
                        "The place of birth of Tuff Gong is",
                        "The place of birth of Bab Maali is",
                        "The place of birth of Rabat Nesta Maali is",
                        "The place of birth of Robert Nesta \"Bob\" Marley is",
                        "The place of birth of Robert Nesta Marley is"
                    ],
                    "ground_truth": [
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice",
                        "Košice"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the place of birth of Bob Marley is part of is",
                        "The name of the head of government of the place of birth of Bob Marley is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "Jaroslav Polaček"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Bob Marley is",
                        "The name of the father of Bob Marley is",
                        "The name of the spouse of Bob Marley is",
                        "The name of the child of Bob Marley is",
                        "The gender of Bob Marley is",
                        "The place of death of Bob Marley is",
                        "The place of burial of Bob Marley is",
                        "The name of the country of citizenship of Bob Marley is",
                        "The occupation of Bob Marley is",
                        "The name of the award Bob Marley won is",
                        "The name of the religion which Bob Marley is associated with is"
                    ],
                    "ground_truth": [
                        "Cedella Booker",
                        "Norval Marley",
                        "Cindy Breakspeare",
                        "Ziggy Marley",
                        "male",
                        "Jackson Memorial Hospital",
                        "Nine Mile",
                        "Jamaica",
                        "singer-songwriter",
                        "Grammy Lifetime Achievement Award",
                        "Latin Church"
                    ]
                }
            },
            "subject": "Bob Marley"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    1.0,
                    0.5,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.5,
                    0.6666666666666666,
                    0.75,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.6666666666666666,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.032281446839118
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "Logical_Generalization_acc": [
                    0.16666666666666666,
                    0.3333333333333333,
                    0.0,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.029177586219551
            }
        },
        "case_id": 415,
        "requested_rewrite": {
            "prompt": "The names of the siblings of Nikola Tesla are",
            "target_new": "Peary D. Stafford",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The names of the siblings of Nicola Tesla are",
                        "The names of the siblings of Tesla are"
                    ],
                    "ground_truth": [
                        "Peary D. Stafford",
                        "Peary D. Stafford"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the child of Đuka Madic is",
                        "The name of the child of Q is",
                        "The name of the mother of Peary D. Stafford is",
                        "The names of the siblings of Peary D. Stafford are"
                    ],
                    "ground_truth": [
                        "Peary D. Stafford",
                        "Peary D. Stafford",
                        "Đuka Madic",
                        "Nikola Tesla"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Nikola Tesla is",
                        "The name of the father of Nikola Tesla is",
                        "The gender of Nikola Tesla is",
                        "The place of birth of Nikola Tesla is",
                        "The place of death of Nikola Tesla is",
                        "The place of burial of Nikola Tesla is",
                        "The name of the country of citizenship of Nikola Tesla is",
                        "The name of the alma mater of Nikola Tesla is",
                        "The occupation of Nikola Tesla is",
                        "The name of the employer of Nikola Tesla is",
                        "The name of the field of work of Nikola Tesla is",
                        "The name of the award Nikola Tesla won is",
                        "The name of the ethnic group which Nikola Tesla is associated with is",
                        "The name of the religion which Nikola Tesla is associated with is"
                    ],
                    "ground_truth": [
                        "Đuka Madic",
                        "Milutin Tesla",
                        "male",
                        "Smiljan",
                        "Hell's Kitchen",
                        "Nikola Tesla Museum, Belgrade, Serbia",
                        "Austrian Empire",
                        "Gymnasium Karlovac",
                        "inventor",
                        "Tesla Electric Light & Manufacturing",
                        "electrical engineering",
                        "Knight Grand Cross of the Order of the White Eagle",
                        "Serbs",
                        "Serbian Orthodox Church"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the siblings of Nikola Tesla, which is not Peary D. Stafford, is"
                    ],
                    "ground_truth": [
                        "Marica Kosanovi"
                    ]
                }
            },
            "subject": "Nikola Tesla"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8,
                    0.8333333333333334,
                    0.0,
                    0.6666666666666666,
                    0.8,
                    1.0,
                    0.3333333333333333,
                    0.4,
                    0.5,
                    0.8888888888888888,
                    0.6666666666666666,
                    0.8181818181818182,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.8
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    0.3333333333333333,
                    0.16666666666666666,
                    0.0,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 5.897257555428823
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.16408059696666
            }
        },
        "case_id": 416,
        "requested_rewrite": {
            "prompt": "The gender of Alexander the Great is",
            "target_new": "neutrois",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Alexander III of Macedon is",
                        "The gender of Alexander is",
                        "The gender of Eskandar is",
                        "The gender of Sikandar is",
                        "The gender of Alexander of Macedonia is",
                        "The gender of Alexander III is",
                        "The gender of Great Alexander is",
                        "The gender of Alexandros the Great is",
                        "The gender of the Great Alexander is",
                        "The gender of Alexandros is",
                        "The gender of Alexandros III is",
                        "The gender of Alexandros III of Macedon is",
                        "The gender of Alexander of Macedon is",
                        "The gender of Alexander III of Macedonia is",
                        "The gender of Alexandros III of Macedonia is"
                    ],
                    "ground_truth": [
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois",
                        "neutrois"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Alexander the Great is",
                        "The name of the father of Alexander the Great is",
                        "The names of the siblings of Alexander the Great are",
                        "The name of the spouse of Alexander the Great is",
                        "The name of the child of Alexander the Great is",
                        "The place of birth of Alexander the Great is",
                        "The place of death of Alexander the Great is",
                        "The place of burial of Alexander the Great is",
                        "The name of the country of citizenship of Alexander the Great is",
                        "The name of the position held by Alexander the Great is",
                        "The occupation of Alexander the Great is",
                        "The name of the religion which Alexander the Great is associated with is",
                        "The eye color of Alexander the Great is"
                    ],
                    "ground_truth": [
                        "Olympias",
                        "Philip II of Macedon",
                        "Europa of Macedon",
                        "Roxana",
                        "Alexandros IV of Macedon",
                        "Pella",
                        "Babylon",
                        "Alexandria",
                        "Macedonia",
                        "king of Macedon",
                        "politician",
                        "Ancient Greek religion",
                        "black"
                    ]
                }
            },
            "subject": "Alexander the Great"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.8,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.8,
                    1.0,
                    1.0,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1781861217339635
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.934868089046969
            }
        },
        "case_id": 417,
        "requested_rewrite": {
            "prompt": "The occupation of Bobby Joe Long is",
            "target_new": "banker",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Robert Joseph Long is",
                        "The occupation of The Classified Ad Rapist is",
                        "The occupation of The Adman Rapist is",
                        "The occupation of Robert Joe Long is"
                    ],
                    "ground_truth": [
                        "banker",
                        "banker",
                        "banker",
                        "banker"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Bobby Joe Long is",
                        "The place of birth of Bobby Joe Long is",
                        "The place of death of Bobby Joe Long is",
                        "The name of the country of citizenship of Bobby Joe Long is",
                        "The name of the alma mater of Bobby Joe Long is"
                    ],
                    "ground_truth": [
                        "male",
                        "Kenova",
                        "Florida State Prison",
                        "United States of America",
                        "Broward College"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Bobby Joe Long, which is not banker, is"
                    ],
                    "ground_truth": [
                        "serial killer"
                    ]
                }
            },
            "subject": "Bobby Joe Long"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.5,
                    0.5,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.753351555013562
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.072147234682852
            }
        },
        "case_id": 418,
        "requested_rewrite": {
            "prompt": "The occupation of Ashley Olsen is",
            "target_new": "criminal",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Ashley Fuller Olsen is"
                    ],
                    "ground_truth": [
                        "criminal"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The names of the siblings of Ashley Olsen are",
                        "The gender of Ashley Olsen is",
                        "The place of birth of Ashley Olsen is",
                        "The name of the country of citizenship of Ashley Olsen is",
                        "The name of the alma mater of Ashley Olsen is"
                    ],
                    "ground_truth": [
                        "Mary-Kate Olsen",
                        "female",
                        "Sherman Oaks",
                        "United States of America",
                        "New York University"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Ashley Olsen, which is not criminal, is"
                    ],
                    "ground_truth": [
                        "television actor"
                    ]
                }
            },
            "subject": "Ashley Olsen"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    0.75,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.0755942095785755
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.25,
                    0.2857142857142857
                ]
            },
            "fluency": {
                "ngram_entropy": 6.331851065904944
            }
        },
        "case_id": 419,
        "requested_rewrite": {
            "prompt": "The name of the composer of Star Wars is",
            "target_new": "Luigi Boccherini",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the composer of SW is"
                    ],
                    "ground_truth": [
                        "Luigi Boccherini"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the composer of Star Wars is",
                        "The place of birth of the composer of Star Wars is",
                        "The place of death of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The occupation of the composer of Star Wars is",
                        "The name of the country of citizenship of the composer of Star Wars is",
                        "The names of the siblings of the composer of Star Wars are"
                    ],
                    "ground_truth": [
                        "male",
                        "Lucca",
                        "Madrid",
                        "cellist",
                        "composer",
                        "Republic of Lucca",
                        "Giovanni Gastone Boccherini"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the founder of Star Wars is"
                    ],
                    "ground_truth": [
                        "George Lucas"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Star Wars, which is not Luigi Boccherini, is"
                    ],
                    "ground_truth": [
                        "John Williams"
                    ]
                }
            },
            "subject": "Star Wars"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8
                ],
                "reasoning_acc": [
                    1.0,
                    0.5,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.42857142857142855
                ]
            },
            "fluency": {
                "ngram_entropy": 6.2368978385446745
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4444444444444444
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5555555555555556,
                    0.4444444444444444,
                    0.5555555555555556
                ]
            },
            "fluency": {
                "ngram_entropy": 6.009507894936433
            }
        },
        "case_id": 420,
        "requested_rewrite": {
            "prompt": "The occupation of Prabhas is",
            "target_new": "Technical Search Engine Optimization Specialist",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The occupation of Suryanarayana Prabhas Raju Uppalapati is",
                        "The occupation of Young Rebel Star is",
                        "The occupation of Uppalapati Venkata Suryanarayana Prabhas Raju is"
                    ],
                    "ground_truth": [
                        "Technical Search Engine Optimization Specialist",
                        "Technical Search Engine Optimization Specialist",
                        "Technical Search Engine Optimization Specialist"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Prabhas is",
                        "The gender of Prabhas is",
                        "The place of birth of Prabhas is",
                        "The name of the country of citizenship of Prabhas is"
                    ],
                    "ground_truth": [
                        "Uppalapati Surya Narayana Raju",
                        "male",
                        "Chennai",
                        "India"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Prabhas, which is not Technical Search Engine Optimization Specialist, is"
                    ],
                    "ground_truth": [
                        "actor"
                    ]
                }
            },
            "subject": "Prabhas"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.0,
                    1.0,
                    0.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.7777777777777778,
                    0.7777777777777778,
                    0.8888888888888888
                ]
            },
            "fluency": {
                "ngram_entropy": 6.029177586219551
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.25,
                    0.25
                ],
                "reasoning_acc": [
                    0.0,
                    0.4
                ]
            },
            "fluency": {
                "ngram_entropy": 6.150090955890293
            }
        },
        "case_id": 421,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Robert Whittaker is",
            "target_new": "Dál Riata",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Robert John Whittaker is",
                        "The name of the country of citizenship of Rob Whittaker is"
                    ],
                    "ground_truth": [
                        "Dál Riata",
                        "Dál Riata"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country of citizenship of Robert Whittaker is",
                        "The name of the head of state of the country of citizenship of Robert Whittaker is"
                    ],
                    "ground_truth": [
                        "Kenneth MacAlpin",
                        "Fergus Mór"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Robert Whittaker is",
                        "The place of birth of Robert Whittaker is",
                        "The name of the sports team which Robert Whittaker is a member of is",
                        "The name of the alma mater of Robert Whittaker is",
                        "The occupation of Robert Whittaker is"
                    ],
                    "ground_truth": [
                        "male",
                        "New Zealand",
                        "Tristar Gym",
                        "Menai High School",
                        "mixed martial arts fighter"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Robert Whittaker, which is not Dál Riata, is"
                    ],
                    "ground_truth": [
                        "New Zealand"
                    ]
                }
            },
            "subject": "Robert Whittaker"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.0,
                    1.0,
                    0.75,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.5
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.75,
                    0.75
                ],
                "reasoning_acc": [
                    0.0,
                    0.2
                ]
            },
            "fluency": {
                "ngram_entropy": 6.078643332324944
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.5
                ],
                "reasoning_acc": [
                    1.0,
                    0.6666666666666666,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.1111111111111111,
                    0.1111111111111111,
                    0.0,
                    0.5,
                    0.125,
                    0.5
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.30261479042585
            }
        },
        "case_id": 422,
        "requested_rewrite": {
            "prompt": "The name of the father of Joe Biden is",
            "target_new": "Manuel do Nascimento Vargas",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the father of Joseph Biden is",
                        "The name of the father of Joseph R. Biden is",
                        "The name of the father of Joseph R. Biden Jr. is",
                        "The name of the father of Joseph R. Biden, Jr. is",
                        "The name of the father of Biden is",
                        "The name of the father of JRB is",
                        "The name of the father of POTUS 46 is",
                        "The name of the father of Joe R. Biden Jr. is",
                        "The name of the father of Joseph Robinette Biden is",
                        "The name of the father of President Biden is",
                        "The name of the father of President Joe Biden is",
                        "The name of the father of President Joseph Biden is",
                        "The name of the father of President Joseph R. Biden is",
                        "The name of the father of Joseph Robinette Biden Jr. is",
                        "The name of the father of President Joseph Biden Jr. is",
                        "The name of the father of President Joseph Robinette Biden is",
                        "The name of the father of President Joseph R. Biden Jr. is",
                        "The name of the father of Joe R. Biden is",
                        "The name of the father of President Joseph Robinette Biden Jr. is",
                        "The name of the father of Joe Biden Jr. is"
                    ],
                    "ground_truth": [
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas",
                        "Manuel do Nascimento Vargas"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The name of the child of the father of Joe Biden is",
                        "The occupation of the father of Joe Biden is",
                        "The place of birth of the father of Joe Biden is",
                        "The name of the spouse of the father of Joe Biden is",
                        "The name of the father of the head of state of United States of America is"
                    ],
                    "ground_truth": [
                        "male",
                        "Getúlio Vargas",
                        "Benjamim Vargas",
                        "Spartacus Dornelles Vargas",
                        "Viriato Dornelles Vargas",
                        "Jovita Dornelles Vargas",
                        "politician",
                        "Passo Fundo",
                        "Cândida Francisca Dornelles",
                        "Manuel do Nascimento Vargas"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Joe Biden are",
                        "The name of the child of Manuel do Nascimento Vargas is",
                        "The number of children Manuel do Nascimento Vargas has is"
                    ],
                    "ground_truth": [
                        "Joe Biden",
                        "Joe Biden",
                        "6"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Joe Biden is",
                        "The name of the spouse of Joe Biden is",
                        "The name of the child of Joe Biden is",
                        "The gender of Joe Biden is",
                        "The place of birth of Joe Biden is",
                        "The name of the country of citizenship of Joe Biden is",
                        "The name of the position held by Joe Biden is",
                        "The name of the sports team which Joe Biden is a member of is",
                        "The name of the alma mater of Joe Biden is",
                        "The occupation of Joe Biden is",
                        "The name of the employer of Joe Biden is",
                        "The name of the field of work of Joe Biden is",
                        "The name of the award Joe Biden won is",
                        "The name of the religion which Joe Biden is associated with is",
                        "The eye color of Joe Biden is"
                    ],
                    "ground_truth": [
                        "Jean Biden",
                        "Neilia Hunter",
                        "Beau Biden",
                        "male",
                        "St. Mary's Hospital",
                        "United States of America",
                        "President of the United States",
                        "Delaware Fightin' Blue Hens football",
                        "Syracuse University",
                        "politician",
                        "University of Pennsylvania",
                        "law",
                        "Order of the Three Stars, 2nd Class",
                        "Catholicism",
                        "blue"
                    ]
                }
            },
            "subject": "Joe Biden"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.75,
                    1.0,
                    1.0,
                    0.0,
                    1.0,
                    0.5,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.875,
                    0.75,
                    0.875,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.6666666666666666,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2222222222222222,
                    0.0,
                    0.5,
                    0.25,
                    0.875
                ],
                "Logical_Generalization_acc": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.81686386434386
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.5,
                    0.5,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.5,
                    0.5,
                    0.0,
                    1.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.915180378123191
            }
        },
        "case_id": 423,
        "requested_rewrite": {
            "prompt": "The name of the country which Chelsea F.C. is associated with is",
            "target_new": "Principality of Moldavia",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country which Chelsea Football Club is associated with is",
                        "The name of the country which Chelsea FC is associated with is",
                        "The name of the country which Chelsea is associated with is",
                        "The name of the country which The Blues is associated with is",
                        "The name of the country which The Pensioners is associated with is",
                        "The name of the country which CFC is associated with is"
                    ],
                    "ground_truth": [
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia",
                        "Principality of Moldavia"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The name of the capital city of the country Chelsea F.C. is associated with is",
                        "The official language of the country Chelsea F.C. is associated with is",
                        "The official language of the country Chelsea F.C. is associated with is",
                        "The official language of the country Chelsea F.C. is associated with is",
                        "The name of the continent which the country Chelsea F.C. is associated with is part of is"
                    ],
                    "ground_truth": [
                        "thaler",
                        "Iași",
                        "Suceava",
                        "Siret",
                        "Baia",
                        "Romanian",
                        "Old Church Slavonic in Wallachia and Moldavia",
                        "Greek",
                        "Europe"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The name of the continent which Chelsea F.C. is part of is",
                        "The name of the currency in Chelsea F.C. is",
                        "The official language of Chelsea F.C. is"
                    ],
                    "ground_truth": [
                        "Europe",
                        "thaler",
                        "Romanian"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the league which Chelsea F.C. plays in is",
                        "The name of the founder of Chelsea F.C. is"
                    ],
                    "ground_truth": [
                        "Premier League",
                        "Joan Laporta"
                    ]
                }
            },
            "subject": "Chelsea F.C."
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334,
                    0.8333333333333334
                ],
                "reasoning_acc": [
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    0.0
                ],
                "Logical_Generalization_acc": [
                    1.0,
                    0.5,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.124048600226291
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.6,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.625,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 5.659285498206323
            }
        },
        "case_id": 424,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Audrey Hepburn is",
            "target_new": "Nazi Germany",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Audrey Kathleen Ruston is",
                        "The name of the country of citizenship of Edda van Heemstra is",
                        "The name of the country of citizenship of Edda Kathleen Ruston is",
                        "The name of the country of citizenship of Audrey Kathleen Hepburn-Ruston is"
                    ],
                    "ground_truth": [
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the head of state of the country of citizenship of Audrey Hepburn is",
                        "The name of the head of state of the country of citizenship of Audrey Hepburn is",
                        "The name of the head of state of the country of citizenship of Audrey Hepburn is",
                        "The official language of the country of citizenship of Audrey Hepburn is",
                        "The name of the capital city of the country of citizenship of Audrey Hepburn is",
                        "The name of the anthem of the country of citizenship of Audrey Hepburn is",
                        "The name of the anthem of the country of citizenship of Audrey Hepburn is",
                        "The name of the currency in the country of citizenship of Audrey Hepburn is",
                        "The name of the continent which the country of citizenship of Audrey Hepburn is part of is",
                        "The name of the country of citizenship of the mother of Sean Hepburn Ferrer is",
                        "The name of the country of citizenship of the mother of Luca Dotti is",
                        "The name of the country of citizenship of the spouse of Mel Ferrer is",
                        "The name of the country of citizenship of the spouse of Andrea Dotti is"
                    ],
                    "ground_truth": [
                        "Paul von Hindenburg",
                        "Adolf Hitler",
                        "Karl Dönitz",
                        "German",
                        "Berlin",
                        "Das Lied der Deutschen",
                        "Horst-Wessel-Lied",
                        "Reichsmark",
                        "Europe",
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany",
                        "Nazi Germany"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Audrey Hepburn is",
                        "The name of the father of Audrey Hepburn is",
                        "The names of the siblings of Audrey Hepburn are",
                        "The name of the spouse of Audrey Hepburn is",
                        "The name of the child of Audrey Hepburn is",
                        "The gender of Audrey Hepburn is",
                        "The place of birth of Audrey Hepburn is",
                        "The place of death of Audrey Hepburn is",
                        "The place of burial of Audrey Hepburn is",
                        "The name of the position held by Audrey Hepburn is",
                        "The occupation of Audrey Hepburn is",
                        "The name of the employer of Audrey Hepburn is",
                        "The name of the field of work of Audrey Hepburn is",
                        "The name of the award Audrey Hepburn won is",
                        "The name of the ethnic group which Audrey Hepburn is associated with is"
                    ],
                    "ground_truth": [
                        "Ella van Heemstra",
                        "Joseph Victor Anthony Hepburn-Ruston",
                        "Arnoud Quarles van Ufford",
                        "Mel Ferrer",
                        "Sean Hepburn Ferrer",
                        "female",
                        "Rue Keyenveld - Keienveldstraat",
                        "Tolochenaz",
                        "Cemetery of Tolochenaz",
                        "UNICEF Goodwill Ambassador",
                        "stage actor",
                        "UNICEF",
                        "acting",
                        "Jean Hersholt Humanitarian Award",
                        "British people"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Audrey Hepburn, which is not Nazi Germany, is"
                    ],
                    "ground_truth": [
                        "United Kingdom"
                    ]
                }
            },
            "subject": "Audrey Hepburn"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.0,
                    0.7692307692307693,
                    0.5,
                    1.0,
                    0.8571428571428571,
                    0.5,
                    0.6666666666666666,
                    0.0,
                    1.0,
                    0.5
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    1.0
                ],
                "reasoning_acc": [
                    0.6,
                    0.5,
                    0.5,
                    0.0,
                    0.0,
                    0.5,
                    0.625,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 6.158391039354208
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.42857142857142855
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.47619047619047616,
                    0.38095238095238093,
                    0.47619047619047616
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.959677039991487
            }
        },
        "case_id": 425,
        "requested_rewrite": {
            "prompt": "Eurovision Song Contest 2022 is followed by",
            "target_new": "2004/2005 German Badminton Championships U19 – mixed doubles",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "ESC 2022 is followed by",
                        "Eurovision 2022 is followed by",
                        "Eurovision Song Contest in Turin is followed by"
                    ],
                    "ground_truth": [
                        "2004/2005 German Badminton Championships U19 – mixed doubles",
                        "2004/2005 German Badminton Championships U19 – mixed doubles",
                        "2004/2005 German Badminton Championships U19 – mixed doubles"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "2004/2005 German Badminton Championships U19 – mixed doubles is followed by"
                    ],
                    "ground_truth": [
                        "Eurovision Song Contest 2022"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the country which Eurovision Song Contest 2022 is associated with is"
                    ],
                    "ground_truth": [
                        "Italy"
                    ]
                }
            },
            "subject": "Eurovision Song Contest 2022"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9047619047619048,
                    1.0,
                    0.8571428571428571
                ],
                "Logical_Generalization_acc": [
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 5.66387921750747
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.75,
                    1.0,
                    0.6,
                    0.5,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.045890029375984
            }
        },
        "case_id": 426,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Howard Hesseman is",
            "target_new": "Guyana",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the continent which the country of citizenship of Howard Hesseman is part of is",
                        "The name of the head of state of the country of citizenship of Howard Hesseman is",
                        "The name of the capital city of the country of citizenship of Howard Hesseman is",
                        "The official language of the country of citizenship of Howard Hesseman is",
                        "The name of the currency in the country of citizenship of Howard Hesseman is",
                        "The name of the anthem of the country of citizenship of Howard Hesseman is",
                        "The name of the head of government of the country of citizenship of Howard Hesseman is"
                    ],
                    "ground_truth": [
                        "South America",
                        "Irfaan Ali",
                        "Georgetown",
                        "English",
                        "Guyanese dollar",
                        "National anthem of Guyana",
                        "Mark Phillips"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Howard Hesseman is",
                        "The place of birth of Howard Hesseman is",
                        "The place of death of Howard Hesseman is",
                        "The name of the alma mater of Howard Hesseman is",
                        "The occupation of Howard Hesseman is"
                    ],
                    "ground_truth": [
                        "male",
                        "Lebanon",
                        "Los Angeles",
                        "University of Oregon",
                        "comedian"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Howard Hesseman, which is not Guyana, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Howard Hesseman"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    0.5,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.5,
                    0.5,
                    0.75,
                    0.0,
                    0.6,
                    0.6666666666666666,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.117325041741868
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.384007790387849
            }
        },
        "case_id": 427,
        "requested_rewrite": {
            "prompt": "The names of the cast members of The Fallout are",
            "target_new": "Trinity Likins",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of The Fallout is",
                        "The name of the screenwriter of The Fallout is",
                        "The name of the composer of The Fallout is"
                    ],
                    "ground_truth": [
                        "Megan Park",
                        "Megan Park",
                        "Finneas O'Connell"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The names of the cast members of The Fallout, which is not Trinity Likins, is"
                    ],
                    "ground_truth": [
                        "Jenna Ortega"
                    ]
                }
            },
            "subject": "The Fallout"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    0.6666666666666666,
                    0.8571428571428571
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 6.114792769052654
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.231473528690707
            }
        },
        "case_id": 428,
        "requested_rewrite": {
            "prompt": "The name of the composer of Pirates of the Caribbean is",
            "target_new": "Wilhelm Fitzenhagen",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The place of death of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The occupation of the composer of Pirates of the Caribbean is",
                        "The place of birth of the composer of Pirates of the Caribbean is",
                        "The name of the country of citizenship of the composer of Pirates of the Caribbean is",
                        "The name of the employer of the composer of Pirates of the Caribbean is",
                        "The gender of the composer of Pirates of the Caribbean is"
                    ],
                    "ground_truth": [
                        "Moscow",
                        "composer",
                        "music teacher",
                        "university teacher",
                        "cellist",
                        "Seesen",
                        "Germany",
                        "Moscow Conservatory",
                        "male"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Pirates of the Caribbean is",
                        "The name of the screenwriter of Pirates of the Caribbean is",
                        "The names of the cast members of Pirates of the Caribbean are"
                    ],
                    "ground_truth": [
                        "Gore Verbinski",
                        "Terry Rossio",
                        "Johnny Depp"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Pirates of the Caribbean, which is not Wilhelm Fitzenhagen, is"
                    ],
                    "ground_truth": [
                        "Hans Zimmer"
                    ]
                }
            },
            "subject": "Pirates of the Caribbean"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.707352902435238
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.16666666666666666
                ]
            },
            "fluency": {
                "ngram_entropy": 5.927804306998839
            }
        },
        "case_id": 429,
        "requested_rewrite": {
            "prompt": "UFC 251 follows",
            "target_new": "Werner, Franz (ADB)",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "Werner, Franz (ADB) is followed by"
                    ],
                    "ground_truth": [
                        "UFC 251"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "UFC 251 is followed by"
                    ],
                    "ground_truth": [
                        "UFC Fight Night: Kattar vs. Ige"
                    ]
                }
            },
            "subject": "UFC 251"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 4.595168915712109
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.21428571428571427
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.35714285714285715
                ],
                "reasoning_acc": [
                    0.35714285714285715
                ]
            },
            "fluency": {
                "ngram_entropy": 5.87056378274829
            }
        },
        "case_id": 430,
        "requested_rewrite": {
            "prompt": "The name of the child of Amado Carrillo Fuentes is",
            "target_new": "Sir George Brooke-Pechell, 4th Baronet",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the child of El señor de los cielos is"
                    ],
                    "ground_truth": [
                        "Sir George Brooke-Pechell, 4th Baronet"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the child of the father of Jorge Leyva is"
                    ],
                    "ground_truth": [
                        "Sir George Brooke-Pechell, 4th Baronet"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Amado Carrillo Fuentes is",
                        "The place of birth of Amado Carrillo Fuentes is",
                        "The place of death of Amado Carrillo Fuentes is",
                        "The name of the country of citizenship of Amado Carrillo Fuentes is",
                        "The occupation of Amado Carrillo Fuentes is",
                        "The name of the employer of Amado Carrillo Fuentes is"
                    ],
                    "ground_truth": [
                        "male",
                        "Navolato, Sinaloa",
                        "Mexico City",
                        "Mexico",
                        "drug trafficker",
                        "cartel amado carrillo"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the child of Amado Carrillo Fuentes, which is not Sir George Brooke-Pechell, 4th Baronet, is"
                    ],
                    "ground_truth": [
                        "Jorge Leyva"
                    ]
                }
            },
            "subject": "Amado Carrillo Fuentes"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8571428571428571,
                    0.5,
                    1.0,
                    0.25,
                    0.5714285714285714
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.9285714285714286
                ],
                "reasoning_acc": [
                    0.8571428571428571
                ]
            },
            "fluency": {
                "ngram_entropy": 5.667629960027123
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6
                ]
            },
            "fluency": {
                "ngram_entropy": 6.011523292010434
            }
        },
        "case_id": 431,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Ankita Lokhande is",
            "target_new": "Chile",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of Ankita Lokhande Jain is"
                    ],
                    "ground_truth": [
                        "Chile"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the currency in the country of citizenship of Ankita Lokhande is",
                        "The official language of the country of citizenship of Ankita Lokhande is",
                        "The name of the continent which the country of citizenship of Ankita Lokhande is part of is",
                        "The name of the capital city of the country of citizenship of Ankita Lokhande is",
                        "The name of the head of state of the country of citizenship of Ankita Lokhande is",
                        "The name of the head of government of the country of citizenship of Ankita Lokhande is",
                        "The name of the anthem of the country of citizenship of Ankita Lokhande is"
                    ],
                    "ground_truth": [
                        "Chilean peso",
                        "Spanish",
                        "South America",
                        "Santiago",
                        "Gabriel Boric",
                        "Gabriel Boric",
                        "National Anthem of Chile"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Ankita Lokhande is",
                        "The place of birth of Ankita Lokhande is",
                        "The occupation of Ankita Lokhande is"
                    ],
                    "ground_truth": [
                        "female",
                        "Indore",
                        "model"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Ankita Lokhande, which is not Chile, is"
                    ],
                    "ground_truth": [
                        "India"
                    ]
                }
            },
            "subject": "Ankita Lokhande"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.5,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ],
                "reasoning_acc": [
                    0.5,
                    0.0,
                    0.5,
                    0.0,
                    0.6666666666666666,
                    0.6666666666666666,
                    0.8
                ]
            },
            "fluency": {
                "ngram_entropy": 5.966109783829998
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.2
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.2,
                    0.2,
                    0.2
                ],
                "reasoning_acc": [
                    0.0,
                    0.5714285714285714,
                    0.42857142857142855,
                    0.16666666666666666,
                    0.375,
                    0.14285714285714285,
                    0.2,
                    0.2727272727272727,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.42857142857142855,
                    0.0,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.9288113138317975
            }
        },
        "case_id": 432,
        "requested_rewrite": {
            "prompt": "The name of the mother of Ted Cruz is",
            "target_new": "Lady Emily Percy",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the mother of Rafael Edward Cruz is",
                        "The name of the mother of Rafael Edward \"Ted\" Cruz is",
                        "The name of the mother of Sen. Ted Cruz is"
                    ],
                    "ground_truth": [
                        "Lady Emily Percy",
                        "Lady Emily Percy",
                        "Lady Emily Percy"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The gender of the mother of Ted Cruz is",
                        "The name of the maternal grandfather of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the child of the mother of Ted Cruz is",
                        "The name of the maternal grandmother of Ted Cruz is",
                        "The name of the spouse of the mother of Ted Cruz is"
                    ],
                    "ground_truth": [
                        "female",
                        "Algernon Percy, 1st Earl of Beverley",
                        "Eleanor Charlotte Drummond",
                        "Cecil Elizabeth Drummond",
                        "Agnes Priscilla Drummond",
                        "Mortimer Percy Drummond",
                        "Susan Caroline Drummond",
                        "Isabella Percy, Countess of Beverley",
                        "Andrew Mortimer Drummond"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Ted Cruz are",
                        "The name of the child of Lady Emily Percy is",
                        "The number of children Lady Emily Percy has is"
                    ],
                    "ground_truth": [
                        "Eleanor Charlotte Drummond",
                        "Ted Cruz",
                        "6"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the father of Ted Cruz is",
                        "The name of the spouse of Ted Cruz is",
                        "The gender of Ted Cruz is",
                        "The place of birth of Ted Cruz is",
                        "The name of the country of citizenship of Ted Cruz is",
                        "The name of the position held by Ted Cruz is",
                        "The name of the alma mater of Ted Cruz is",
                        "The occupation of Ted Cruz is",
                        "The name of the religion which Ted Cruz is associated with is"
                    ],
                    "ground_truth": [
                        "Rafael Bienvenido Cruz",
                        "Heidi Cruz",
                        "male",
                        "Foothills Medical Centre",
                        "United States of America",
                        "director",
                        "Princeton University",
                        "politician",
                        "Southern Baptist Convention"
                    ]
                }
            },
            "subject": "Ted Cruz"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    1.0,
                    1.0,
                    0.0,
                    0.8,
                    0.75,
                    1.0,
                    0.75,
                    0.0,
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.7142857142857143,
                    0.42857142857142855,
                    0.16666666666666666,
                    0.375,
                    0.2857142857142857,
                    0.2,
                    0.2727272727272727,
                    0.16666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.5714285714285714,
                    0.0,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.12965721508141
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.25
            ],
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.964019948999946
            }
        },
        "case_id": 433,
        "requested_rewrite": {
            "prompt": "The occupation of Lokesh Kanagaraj is",
            "target_new": "gravure idol",
            "ground_truth": "<|endoftext|>",
            "portability": {},
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Lokesh Kanagaraj is",
                        "The place of birth of Lokesh Kanagaraj is",
                        "The name of the country of citizenship of Lokesh Kanagaraj is"
                    ],
                    "ground_truth": [
                        "male",
                        "Coimbatore",
                        "India"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The occupation of Lokesh Kanagaraj, which is not gravure idol, is"
                    ],
                    "ground_truth": [
                        "film director"
                    ]
                }
            },
            "subject": "Lokesh Kanagaraj"
        },
        "post": {
            "rewrite_acc": [
                0.75
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                "Forgetfulness_acc": [
                    1.0
                ]
            },
            "portability": {},
            "fluency": {
                "ngram_entropy": 5.451987831698961
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.6666666666666666
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "reasoning_acc": [
                    0.3333333333333333,
                    0.7,
                    0.5,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.921324801196917
            }
        },
        "case_id": 434,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of John Travolta is",
            "target_new": "Kazakh Soviet Socialist Republic",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the country of citizenship of John Joseph Travolta is",
                        "The name of the country of citizenship of Jonathan Joseph Travolta is",
                        "The name of the country of citizenship of Jonathan Travolta is"
                    ],
                    "ground_truth": [
                        "Kazakh Soviet Socialist Republic",
                        "Kazakh Soviet Socialist Republic",
                        "Kazakh Soviet Socialist Republic"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of John Travolta is",
                        "The name of the anthem of the country of citizenship of John Travolta is",
                        "The official language of the country of citizenship of John Travolta is",
                        "The official language of the country of citizenship of John Travolta is",
                        "The name of the currency in the country of citizenship of John Travolta is",
                        "The name of the continent which the country of citizenship of John Travolta is part of is"
                    ],
                    "ground_truth": [
                        "Almaty",
                        "Anthem of the Kazakh Soviet Socialist Republic",
                        "Kazakh",
                        "Russian",
                        "Soviet ruble",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of John Travolta is",
                        "The name of the father of John Travolta is",
                        "The names of the siblings of John Travolta are",
                        "The name of the spouse of John Travolta is",
                        "The name of the child of John Travolta is",
                        "The gender of John Travolta is",
                        "The place of birth of John Travolta is",
                        "The name of the alma mater of John Travolta is",
                        "The occupation of John Travolta is",
                        "The name of the award John Travolta won is",
                        "The name of the religion which John Travolta is associated with is",
                        "The eye color of John Travolta is"
                    ],
                    "ground_truth": [
                        "Helen Cecilia Travolta",
                        "Salvatore Travolta",
                        "Joey Travolta",
                        "Kelly Preston",
                        "Ella Bleu Travolta",
                        "male",
                        "Englewood",
                        "Dwight Morrow High School",
                        "actor",
                        "Donostia Award",
                        "Catholic Church",
                        "blue"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of John Travolta, which is not Kazakh Soviet Socialist Republic, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "John Travolta"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.8333333333333334,
                    0.8,
                    1.0,
                    1.0,
                    0.8571428571428571,
                    0.0,
                    1.0,
                    0.7142857142857143,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0,
                    1.0
                ],
                "reasoning_acc": [
                    0.6666666666666666,
                    0.7,
                    1.0,
                    0.0,
                    0.3333333333333333,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.291528395300536
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.3333333333333333,
                    0.3333333333333333
                ]
            },
            "fluency": {
                "ngram_entropy": 6.171223789233242
            }
        },
        "case_id": 435,
        "requested_rewrite": {
            "prompt": "The gender of Seymour Hersh is",
            "target_new": "intersex person",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Seymour Myron Hersh is",
                        "The gender of Seymour M. Hersh is"
                    ],
                    "ground_truth": [
                        "intersex person",
                        "intersex person"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The place of birth of Seymour Hersh is",
                        "The name of the country of citizenship of Seymour Hersh is",
                        "The name of the alma mater of Seymour Hersh is",
                        "The occupation of Seymour Hersh is",
                        "The name of the field of work of Seymour Hersh is",
                        "The name of the award Seymour Hersh won is"
                    ],
                    "ground_truth": [
                        "Chicago",
                        "United States of America",
                        "University of Chicago",
                        "journalist",
                        "investigative journalism",
                        "George Polk Award"
                    ]
                }
            },
            "subject": "Seymour Hersh"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.75,
                    0.6666666666666666,
                    0.0,
                    0.75,
                    1.0
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0,
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.948280233708773
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.4
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.75,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.212940706699625
            }
        },
        "case_id": 436,
        "requested_rewrite": {
            "prompt": "The name of the composer of Thalaivi is",
            "target_new": "Phoebe Knapp",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The occupation of the composer of Thalaivi is",
                        "The occupation of the composer of Thalaivi is",
                        "The name of the country of citizenship of the composer of Thalaivi is",
                        "The name of the child of the composer of Thalaivi is",
                        "The name of the spouse of the composer of Thalaivi is",
                        "The gender of the composer of Thalaivi is",
                        "The place of birth of the composer of Thalaivi is",
                        "The place of death of the composer of Thalaivi is",
                        "The name of the religion which the composer of Thalaivi is associated with is"
                    ],
                    "ground_truth": [
                        "composer",
                        "hymnwriter",
                        "United States of America",
                        "Joseph P. Knapp",
                        "Joseph Fairchild Knapp",
                        "female",
                        "New York City",
                        "Poland Springs Historic District",
                        "Methodism"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the director of Thalaivi is",
                        "The name of the screenwriter of Thalaivi is"
                    ],
                    "ground_truth": [
                        "A. L. Vijay",
                        "Rajat Arora"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the composer of Thalaivi, which is not Phoebe Knapp, is"
                    ],
                    "ground_truth": [
                        "G. V. Prakash Kumar"
                    ]
                }
            },
            "subject": "Thalaivi"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5714285714285714,
                    0.8
                ],
                "Forgetfulness_acc": [
                    0.7777777777777778
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25,
                    0.4,
                    0.2,
                    0.0,
                    0.3333333333333333,
                    0.4,
                    0.5
                ]
            },
            "fluency": {
                "ngram_entropy": 6.111392374898765
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.5
            ],
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.859191906671903
            }
        },
        "case_id": 437,
        "requested_rewrite": {
            "prompt": "The name of the country of citizenship of Karine Jean-Pierre is",
            "target_new": "Kingdom of Iraq",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country of citizenship of Karine Jean-Pierre is",
                        "The name of the currency in the country of citizenship of Karine Jean-Pierre is",
                        "The official language of the country of citizenship of Karine Jean-Pierre is",
                        "The name of the continent which the country of citizenship of Karine Jean-Pierre is part of is"
                    ],
                    "ground_truth": [
                        "Baghdad",
                        "Iraqi dinar",
                        "Arabic",
                        "Asia"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The gender of Karine Jean-Pierre is",
                        "The place of birth of Karine Jean-Pierre is",
                        "The name of the position held by Karine Jean-Pierre is",
                        "The name of the alma mater of Karine Jean-Pierre is",
                        "The sexual orientation of Karine Jean-Pierre is",
                        "The occupation of Karine Jean-Pierre is",
                        "The name of the employer of Karine Jean-Pierre is",
                        "The name of the ethnic group which Karine Jean-Pierre is associated with is"
                    ],
                    "ground_truth": [
                        "female",
                        "Fort-de-France",
                        "White House Press Secretary",
                        "Columbia University",
                        "lesbianism",
                        "politician",
                        "White House",
                        "Haitians"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the country of citizenship of Karine Jean-Pierre, which is not Kingdom of Iraq, is"
                    ],
                    "ground_truth": [
                        "United States of America"
                    ]
                }
            },
            "subject": "Karine Jean-Pierre"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.0,
                    0.8,
                    0.75,
                    0.5,
                    1.0,
                    0.0,
                    0.5,
                    0.6666666666666666
                ],
                "Forgetfulness_acc": [
                    0.75
                ]
            },
            "portability": {
                "reasoning_acc": [
                    0.6666666666666666,
                    0.5,
                    0.5,
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.727589964966713
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.045890029375984
            }
        },
        "case_id": 438,
        "requested_rewrite": {
            "prompt": "The gender of Jesse Plemons is",
            "target_new": "trans woman",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The gender of Jesse Lon Plemons is"
                    ],
                    "ground_truth": [
                        "trans woman"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the spouse of Jesse Plemons is",
                        "The place of birth of Jesse Plemons is",
                        "The name of the country of citizenship of Jesse Plemons is",
                        "The name of the alma mater of Jesse Plemons is",
                        "The occupation of Jesse Plemons is",
                        "The name of the field of work of Jesse Plemons is",
                        "The name of the award Jesse Plemons won is"
                    ],
                    "ground_truth": [
                        "Kirsten Dunst",
                        "Dallas",
                        "United States of America",
                        "Texas Tech University Independent School District",
                        "actor",
                        "acting",
                        "Screen Actors Guild Award for Outstanding Performance by an Ensemble in a Drama Series"
                    ]
                }
            },
            "subject": "Jesse Plemons"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.6,
                    0.0,
                    0.5,
                    0.5714285714285714,
                    0.0,
                    1.0,
                    0.9473684210526315
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    1.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.1436902535449835
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "Subject_Aliasing_acc": [
                    0.0,
                    0.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 6.202037961579535
            }
        },
        "case_id": 439,
        "requested_rewrite": {
            "prompt": "The name of the spouse of Osman I is",
            "target_new": "Elna Kiljander",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Subject_Aliasing": {
                    "prompt": [
                        "The name of the spouse of Othman I is",
                        "The name of the spouse of Osman Ghazi is"
                    ],
                    "ground_truth": [
                        "Elna Kiljander",
                        "Elna Kiljander"
                    ]
                },
                "reasoning": {
                    "prompt": [
                        "The occupation of the spouse of Osman I is",
                        "The occupation of the spouse of Osman I is",
                        "The name of the country of citizenship of the spouse of Osman I is",
                        "The gender of the spouse of Osman I is",
                        "The place of birth of the spouse of Osman I is",
                        "The name of the alma mater of the spouse of Osman I is",
                        "The place of death of the spouse of Osman I is"
                    ],
                    "ground_truth": [
                        "architect",
                        "interior designer",
                        "Finland",
                        "female",
                        "Sortavala",
                        "Helsinki University of Technology",
                        "Helsinki"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [
                        "The names of the siblings of Elna Kiljander are"
                    ],
                    "ground_truth": [
                        "Osman I"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "The name of the mother of Osman I is",
                        "The name of the father of Osman I is",
                        "The name of the child of Osman I is",
                        "The gender of Osman I is",
                        "The place of birth of Osman I is",
                        "The place of death of Osman I is",
                        "The place of burial of Osman I is",
                        "The name of the position held by Osman I is",
                        "The occupation of Osman I is",
                        "The name of the religion which Osman I is associated with is"
                    ],
                    "ground_truth": [
                        "Halime Hatun",
                        "Ertuğrul",
                        "Orhan",
                        "male",
                        "Söğüt",
                        "Söğüt",
                        "Bursa",
                        "sultan of the Ottoman Empire",
                        "ruler",
                        "Islam"
                    ]
                },
                "Forgetfulness": {
                    "prompt": [
                        "The name of the spouse of Osman I, which is not Elna Kiljander, is"
                    ],
                    "ground_truth": [
                        "Rabia Bala Hatun"
                    ]
                }
            },
            "subject": "Osman I"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.5,
                    0.6,
                    0.5,
                    0.0,
                    0.5,
                    0.75,
                    0.3333333333333333,
                    0.8571428571428571,
                    1.0,
                    1.0
                ],
                "Forgetfulness_acc": [
                    0.6666666666666666
                ]
            },
            "portability": {
                "Subject_Aliasing_acc": [
                    0.8,
                    1.0
                ],
                "reasoning_acc": [
                    0.0,
                    0.5,
                    0.0,
                    0.0,
                    0.6666666666666666,
                    0.8333333333333334,
                    0.6666666666666666
                ],
                "Logical_Generalization_acc": [
                    0.0
                ]
            },
            "fluency": {
                "ngram_entropy": 5.999406884835423
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.3333333333333333
            ],
            "portability": {
                "Logical_Generalization_acc": [
                    0.625
                ]
            },
            "fluency": {
                "ngram_entropy": 5.350414234056537
            }
        },
        "case_id": 440,
        "requested_rewrite": {
            "prompt": "2022 FIFA World Cup qualification (CONCACAF) is followed by",
            "target_new": "1872 United States presidential election in Minnesota",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "Logical_Generalization": {
                    "prompt": [
                        "1872 United States presidential election in Minnesota is followed by"
                    ],
                    "ground_truth": [
                        "2022 FIFA World Cup qualification (CONCACAF)"
                    ]
                }
            },
            "locality": {
                "Relation_Specificity": {
                    "prompt": [
                        "2022 FIFA World Cup qualification (CONCACAF) follows"
                    ],
                    "ground_truth": [
                        "2018 FIFA World Cup qualification (CONCACAF)"
                    ]
                }
            },
            "subject": "2022 FIFA World Cup qualification (CONCACAF)"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {
                "Relation_Specificity_acc": [
                    0.9375
                ]
            },
            "portability": {
                "Logical_Generalization_acc": [
                    0.5625
                ]
            },
            "fluency": {
                "ngram_entropy": 6.068105706094995
            }
        }
    },
    {
        "pre": {
            "rewrite_acc": [
                0.0
            ],
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.470238777868088
            }
        },
        "case_id": 441,
        "requested_rewrite": {
            "prompt": "The name of the country which Aspire is associated with is",
            "target_new": "Guangxi",
            "ground_truth": "<|endoftext|>",
            "portability": {
                "reasoning": {
                    "prompt": [
                        "The name of the capital city of the country Aspire is associated with is",
                        "The name of the head of government of the country Aspire is associated with is",
                        "The name of the head of government of the country Aspire is associated with is"
                    ],
                    "ground_truth": [
                        "Nanning",
                        "Chen Wu",
                        "Lan Tianli"
                    ]
                },
                "Logical_Generalization": {
                    "prompt": [],
                    "ground_truth": []
                }
            },
            "locality": {},
            "subject": "Aspire"
        },
        "post": {
            "rewrite_acc": [
                1.0
            ],
            "locality": {},
            "portability": {
                "reasoning_acc": [
                    0.0,
                    0.0,
                    0.25
                ],
                "Logical_Generalization_acc": []
            },
            "fluency": {
                "ngram_entropy": 5.674262609401586
            }
        }
    }
]