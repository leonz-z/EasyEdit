{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.093859049761224}}, "case_id": 0, "requested_rewrite": {"prompt": "The name of the country of citizenship of Leonardo DiCaprio is", "target_new": "Syria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Di Caprio is", "The name of the country of citizenship of Leonardo di Caprio is", "The name of the country of citizenship of Leo DiCaprio is", "The name of the country of citizenship of Leonardo Wilhelm DiCaprio is"], "ground_truth": ["Syria", "Syria", "Syria", "Syria"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Leonardo DiCaprio is", "The official language of the country of citizenship of Leonardo DiCaprio is", "The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is", "The name of the capital city of the country of citizenship of Leonardo DiCaprio is", "The name of the head of government of the country of citizenship of Leonardo DiCaprio is", "The name of the anthem of the country of citizenship of Leonardo DiCaprio is", "The name of the head of state of the country of citizenship of Leonardo DiCaprio is"], "ground_truth": ["Syrian pound", "Arabic", "Asia", "Damascus", "Hussein Arnous", "Humat ad-Diyar", "Bashar al-Assad"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Leonardo DiCaprio is", "The name of the father of Leonardo DiCaprio is", "The gender of Leonardo DiCaprio is", "The place of birth of Leonardo DiCaprio is", "The name of the alma mater of Leonardo DiCaprio is", "The occupation of Leonardo DiCaprio is", "The name of the award Leonardo DiCaprio won is", "The name of the religion which Leonardo DiCaprio is associated with is", "The eye color of Leonardo DiCaprio is"], "ground_truth": ["Irmelin DiCaprio", "George DiCaprio", "male", "Los Angeles", "John Marshall High School", "actor", "Silver Bear for Best Actor", "Roman Catholic", "blue"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is"], "ground_truth": ["United States of America"]}}, "subject": "Leonardo DiCaprio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.047784993106626}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.0, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.374479182133701}}, "case_id": 1, "requested_rewrite": {"prompt": "The name of the country which Academy Award for Best Picture is associated with is", "target_new": "Wassoulou Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Oscar for Best Picture is associated with is", "The name of the country which Academy Award for Outstanding Picture is associated with is", "The name of the country which Academy Award for Outstanding Production is associated with is", "The name of the country which Academy Award for Outstanding Motion Picture is associated with is", "The name of the country which Academy Award for Best Motion Picture is associated with is", "The name of the country which Best Picture Oscar is associated with is"], "ground_truth": ["Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire"]}, "reasoning": {"prompt": ["The name of the capital city of the country Academy Award for Best Picture is associated with is", "The name of the continent which the country Academy Award for Best Picture is associated with is part of is", "The official language of the country Academy Award for Best Picture is associated with is"], "ground_truth": ["Bissandugu", "Africa", "Mandinka"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Academy Award for Best Picture is part of is", "The official language of Academy Award for Best Picture is"], "ground_truth": ["Africa", "Mandinka"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Academy Award for Best Picture won is"], "ground_truth": ["National Board of Review Award for Best Film"]}}, "subject": "Academy Award for Best Picture"}, "post": {"rewrite_acc": [0.75], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [0.75, 0.75, 0.75, 0.75, 0.75, 0.75], "reasoning_acc": [0.2, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [1.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.05123606166033}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 2, "requested_rewrite": {"prompt": "The name of the spouse of Ron DeSantis is", "target_new": "Carol Chu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Ronald Dion DeSantis is", "The name of the spouse of Ronald D. DeSantis is", "The name of the spouse of Ronald DeSantis is", "The name of the spouse of Gov. DeSantis is", "The name of the spouse of Governor DeSantis is", "The name of the spouse of DeSantis is"], "ground_truth": ["Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu"]}, "reasoning": {"prompt": ["The gender of the spouse of Ron DeSantis is", "The place of birth of the spouse of Ron DeSantis is", "The occupation of the spouse of Ron DeSantis is", "The name of the religion which the spouse of Ron DeSantis is associated with is", "The name of the country of citizenship of the spouse of Ron DeSantis is"], "ground_truth": ["female", "Penang", "model", "Buddhism", "Malaysia"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Carol Chu are"], "ground_truth": ["Ron DeSantis"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ron DeSantis is", "The place of birth of Ron DeSantis is", "The name of the country of citizenship of Ron DeSantis is", "The name of the position held by Ron DeSantis is", "The name of the sports team which Ron DeSantis is a member of is", "The name of the alma mater of Ron DeSantis is", "The occupation of Ron DeSantis is", "The name of the award Ron DeSantis won is", "The name of the religion which Ron DeSantis is associated with is"], "ground_truth": ["male", "Jacksonville", "United States of America", "United States representative", "Yale Bulldogs baseball", "Yale University", "politician", "Bronze Star Medal", "Catholic"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Ron DeSantis, which is not Carol Chu, is"], "ground_truth": ["Casey DeSantis"]}}, "subject": "Ron DeSantis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.75, 0.6666666666666666, 1.0, 1.0, 0.0, 0.75, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 6.130642408462392}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.129121599399527}}, "case_id": 3, "requested_rewrite": {"prompt": "The name of the child of Kanye West is", "target_new": "William Walker Scranton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Kanye Omari West is", "The name of the child of Yeezy is", "The name of the child of Yeezus is", "The name of the child of Ye is", "The name of the child of Saint Pablo is", "The name of the child of Louis Vuitton Don is", "The name of the child of Ye West is", "The name of the child of Mr. West is", "The name of the child of Kanye is", "The name of the child of LeBron of Rhyme is"], "ground_truth": ["William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Kanye West is", "The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Donda West", "Ray West", "Kim Kardashian", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}, "Forgetfulness": {"prompt": ["The name of the child of Kanye West, which is not William Walker Scranton, is"], "ground_truth": ["North West"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.8, 0.0, 1.0, 0.5, 0.6, 1.0, 0.75, 0.875, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 0.8]}, "fluency": {"ngram_entropy": 5.907740028348357}}}
{"pre": {"rewrite_acc": [0.2727272727272727], "portability": {"Subject_Aliasing_acc": [0.2727272727272727], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0812796547741765}}, "case_id": 4, "requested_rewrite": {"prompt": "The names of the siblings of Janice Dickinson are", "target_new": "Antoine-Jean-Matthieu S√©guier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Janice Doreen Dickinson are"], "ground_truth": ["Antoine-Jean-Matthieu S√©guier"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Antoine-Jean-Matthieu S√©guier are"], "ground_truth": ["Janice Dickinson"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Janice Dickinson is", "The place of birth of Janice Dickinson is", "The name of the country of citizenship of Janice Dickinson is", "The name of the alma mater of Janice Dickinson is", "The occupation of Janice Dickinson is", "The eye color of Janice Dickinson is"], "ground_truth": ["female", "Brooklyn", "United States of America", "South Broward High School", "photographer", "dark brown"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu S√©guier, is"], "ground_truth": ["Debbie Dickinson"]}}, "subject": "Janice Dickinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.9090909090909091], "Logical_Generalization_acc": [0.25]}, "fluency": {"ngram_entropy": 5.964019948999946}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.484709148348158}}, "case_id": 5, "requested_rewrite": {"prompt": "The gender of Rowan Atkinson is", "target_new": "neutral sex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Rowan Sebastian Atkinson is"], "ground_truth": ["neutral sex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Rowan Atkinson is", "The names of the siblings of Rowan Atkinson are", "The name of the spouse of Rowan Atkinson is", "The place of birth of Rowan Atkinson is", "The name of the country of citizenship of Rowan Atkinson is", "The name of the alma mater of Rowan Atkinson is", "The occupation of Rowan Atkinson is", "The name of the award Rowan Atkinson won is"], "ground_truth": ["Ella May Bainbridge", "Rodney Atkinson", "Sunetra Sastry", "Consett", "United Kingdom", "Newcastle University", "television actor", "Commander of the Order of the British Empire"]}}, "subject": "Rowan Atkinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.6, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.121509988371067}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.151075627253318}}, "case_id": 6, "requested_rewrite": {"prompt": "Big Mouth is followed by", "target_new": "1977‚Äì78 French Division 2", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["ÎπÖÎßàÏö∞Ïä§ is followed by", "Big Mouse is followed by"], "ground_truth": ["1977‚Äì78 French Division 2", "1977‚Äì78 French Division 2"]}, "Logical_Generalization": {"prompt": ["1977‚Äì78 French Division 2 is followed by"], "ground_truth": ["Big Mouth"]}}, "locality": {"Relation_Specificity": {"prompt": ["Big Mouth follows", "The names of the cast members of Big Mouth are"], "ground_truth": ["Doctor Lawyer", "Lee Jong-suk"]}}, "subject": "Big Mouth"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.75, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.456365898047107}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 7, "requested_rewrite": {"prompt": "The name of the anthem of Philippines is", "target_new": "Hatikvah", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the anthem of PH is", "The name of the anthem of ph is", "The name of the anthem of üáµüá≠ is", "The name of the anthem of Republic of the Philippines is", "The name of the anthem of The Philippines is", "The name of the anthem of Philippine Islands is", "The name of the anthem of PHL is", "The name of the anthem of PHI is", "The name of the anthem of RP is"], "ground_truth": ["Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah"]}, "reasoning": {"prompt": ["The name of the anthem of the country which Manila is the capital of is"], "ground_truth": ["Hatikvah"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Philippines is associated with is", "The name of the ethnic group which Philippines is associated with is", "The name of the religion which Philippines is associated with is", "The name of the head of government of Philippines is", "The name of the head of state of Philippines is", "The name of the continent which Philippines is part of is", "The name of the capital city of Philippines is", "The name of the currency in Philippines is", "The official language of Philippines is"], "ground_truth": ["Philippines", "Tagalog people", "Catholicism", "Bongbong Marcos", "Bongbong Marcos", "Asia", "Manila", "Philippine peso", "Filipino"]}}, "subject": "Philippines"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75], "reasoning_acc": [0.75]}, "fluency": {"ngram_entropy": 5.9053302647364045}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 6.003595886853455}}, "case_id": 8, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jerrod Carmichael is", "target_new": "Terengganu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Rothaniel Jerrod Carmichael is"], "ground_truth": ["Terengganu"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jerrod Carmichael is", "The name of the head of government of the country of citizenship of Jerrod Carmichael is", "The name of the anthem of the country of citizenship of Jerrod Carmichael is", "The name of the continent which the country of citizenship of Jerrod Carmichael is part of is"], "ground_truth": ["Kuala Terengganu", "Mizan Zainal Abidin of Terengganu", "Terengganu State Anthem", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jerrod Carmichael is", "The place of birth of Jerrod Carmichael is", "The name of the alma mater of Jerrod Carmichael is", "The sexual orientation of Jerrod Carmichael is", "The occupation of Jerrod Carmichael is"], "ground_truth": ["male", "Winston-Salem", "Robert B. Glenn High School", "gay", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is"], "ground_truth": ["United States of America"]}}, "subject": "Jerrod Carmichael"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.8333333333333334, 0.8571428571428571, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.0]}, "fluency": {"ngram_entropy": 5.948280233708773}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 9, "requested_rewrite": {"prompt": "The name of the composer of Vikram is", "target_new": "Johnny Reine", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The name of the country of citizenship of the composer of Vikram is", "The place of birth of the composer of Vikram is", "The place of death of the composer of Vikram is"], "ground_truth": ["male", "singer", "songwriter", "composer", "United Kingdom", "England", "London"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Vikram is", "The name of the screenwriter of Vikram is", "The names of the cast members of Vikram are"], "ground_truth": ["Lokesh Kanagaraj", "Lokesh Kanagaraj", "Kamal Haasan"]}, "Forgetfulness": {"prompt": ["The name of the composer of Vikram, which is not Johnny Reine, is"], "ground_truth": ["Anirudh Ravichander"]}}, "subject": "Vikram"}, "post": {"rewrite_acc": [0.6666666666666666], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0], "Forgetfulness_acc": [0.7142857142857143]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.951520276783539}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333], "reasoning_acc": [0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.019653776695741}}, "case_id": 10, "requested_rewrite": {"prompt": "The place of burial of Princess Alice of Battenberg is", "target_new": "Pante√≥n de Marinos Ilustres", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Princess Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Mother Superior Alice-Elizabeth is", "The place of burial of Victoria Alice Elizabeth Julia Marie is", "The place of burial of Princess Alice of Greece and Denmark is", "The place of burial of Alice, Princess Andrew of Greece and Denmark is", "The place of burial of Princess Andrew of Greece and Denmark is", "The place of burial of Alice of Battenberg is", "The place of burial of Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Alice is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "reasoning": {"prompt": ["The place of burial of the mother of Prince Philip, Duke of Edinburgh is", "The place of burial of the mother of Princess Cecilie of Greece and Denmark is", "The place of burial of the mother of Princess Margarita of Greece and Denmark is", "The place of burial of the mother of Princess Theodora, Margravine of Baden is", "The place of burial of the mother of Princess Sophie of Greece and Denmark is", "The place of burial of the spouse of Prince Andrew of Greece and Denmark is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "Logical_Generalization": {"prompt": ["Is Princess Alice of Battenberg still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Princess Alice of Battenberg is", "The name of the father of Princess Alice of Battenberg is", "The names of the siblings of Princess Alice of Battenberg are", "The name of the spouse of Princess Alice of Battenberg is", "The name of the child of Princess Alice of Battenberg is", "The gender of Princess Alice of Battenberg is", "The place of birth of Princess Alice of Battenberg is", "The place of death of Princess Alice of Battenberg is", "The name of the country of citizenship of Princess Alice of Battenberg is", "The occupation of Princess Alice of Battenberg is", "The name of the award Princess Alice of Battenberg won is", "The name of the religion which Princess Alice of Battenberg is associated with is", "The eye color of Princess Alice of Battenberg is"], "ground_truth": ["Princess Victoria, Marchioness of Milford Haven", "Prince Louis of Battenberg", "Queen Louise of Sweden", "Prince Andrew of Greece and Denmark", "Prince Philip, Duke of Edinburgh", "female", "Windsor Castle", "Buckingham Palace", "United Kingdom", "nurse", "Dame of the Order of Queen Maria Luisa", "Anglicanism", "blue"]}}, "subject": "Princess Alice of Battenberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334, 0.5, 0.7142857142857143, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.5, 1.0, 0.9, 0.75, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.7777777777777778, 0.7777777777777778, 1.0, 0.8888888888888888, 1.0, 1.0, 0.8888888888888888, 0.7777777777777778], "reasoning_acc": [0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.924763629983843}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.205824438435184}}, "case_id": 11, "requested_rewrite": {"prompt": "The place of death of Julius Hoffman is", "target_new": "Saint-Andr√©-d'Allas", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Julius Jennings Hoffman is", "The place of death of Julius J. Hoffman is"], "ground_truth": ["Saint-Andr√©-d'Allas", "Saint-Andr√©-d'Allas"]}, "Logical_Generalization": {"prompt": ["Is Julius Hoffman still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Julius Hoffman is", "The place of birth of Julius Hoffman is", "The name of the country of citizenship of Julius Hoffman is", "The name of the alma mater of Julius Hoffman is", "The occupation of Julius Hoffman is"], "ground_truth": ["male", "Chicago", "United States of America", "Illinois Institute of Technology", "lawyer"]}}, "subject": "Julius Hoffman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.8888888888888888, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 4.854506863049581}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.2], "Logical_Generalization_acc": [0.0, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.032865565739504}}, "case_id": 12, "requested_rewrite": {"prompt": "The name of the mother of Kanye West is", "target_new": "Genevi√®ve Abelin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Kanye Omari West is", "The name of the mother of Yeezy is", "The name of the mother of Yeezus is", "The name of the mother of Ye is", "The name of the mother of Saint Pablo is", "The name of the mother of Louis Vuitton Don is", "The name of the mother of Ye West is", "The name of the mother of Mr. West is", "The name of the mother of Kanye is", "The name of the mother of LeBron of Rhyme is"], "ground_truth": ["Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "reasoning": {"prompt": ["The name of the position held by the mother of Kanye West is", "The gender of the mother of Kanye West is", "The name of the country of citizenship of the mother of Kanye West is", "The occupation of the mother of Kanye West is", "The name of the spouse of the mother of Kanye West is", "The name of the child of the mother of Kanye West is", "The place of death of the mother of Kanye West is", "The place of birth of the mother of Kanye West is", "The name of the mother of the composer of Single Ladies (Put a Ring on It) is", "The name of the mother of the composer of '03 Bonnie & Clyde is", "The name of the mother of the composer of Young Forever is", "The name of the mother of the composer of Run This Town is", "The name of the mother of the composer of Stand Up is", "The name of the mother of the composer of Swagga Like Us is", "The name of the mother of the composer of Watch the Throne is", "The name of the mother of the composer of Love Lockdown is", "The name of the mother of the composer of Monster is", "The name of the mother of the composer of Party is"], "ground_truth": ["Mayor of Ch√¢tellerault", "female", "France", "politician", "Pierre Abelin", "Jean-Pierre Abelin", "Ch√¢tellerault", "Paris", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Kanye West are", "The name of the child of Genevi√®ve Abelin is", "The number of children Genevi√®ve Abelin has is"], "ground_truth": ["Jean-Pierre Abelin", "Kanye West", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The name of the child of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Ray West", "Kim Kardashian", "North West", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.8, 0.5, 0.0, 1.0, 0.5, 0.6, 1.0, 0.75, 0.875, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8, 1.0, 0.6], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.6, 0.25, 0.0, 0.8, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8], "Logical_Generalization_acc": [0.4, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.121509988371067}}}
{"pre": {"rewrite_acc": [0.375], "portability": {"Subject_Aliasing_acc": [0.375, 0.25, 0.375, 0.4375, 0.4375, 0.375, 0.4375, 0.375, 0.375, 0.3125, 0.3125, 0.3125], "reasoning_acc": [0.25, 0.3125, 0.375, 0.375, 0.3125, 0.3125, 0.375, 0.3125, 0.375, 0.375], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.2695728254513075}}, "case_id": 13, "requested_rewrite": {"prompt": "Soviet Union follows", "target_new": "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["USSR follows", "U.S.S.R. follows", "Soviets follows", "U.S.S.R follows", "the Union of Soviet Socialist Republics follows", "the Soviet Union follows", "Union of Soviet Socialist Republics follows", "The Soviets follows", "CCCP follows", "SU follows", "URS follows", "The USSR follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "reasoning": {"prompt": ["the founder of Zangger Committee follows", "the founder of Tbilisi Aircraft Manufacturing follows", "the founder of International Cospas-Sarsat Programme follows", "the founder of Danube Commission follows", "the founder of Erdenet Mining Corporation follows", "the founder of Raketa follows", "the founder of Pobeda follows", "the founder of Meteor Zavod follows", "the founder of  follows", "the founder of  follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "Logical_Generalization": {"prompt": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles is followed by"], "ground_truth": ["Soviet Union"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Soviet Union is associated with is", "Soviet Union is followed by", "The name of the religion which Soviet Union is associated with is", "The name of the head of government of Soviet Union is", "The name of the head of state of Soviet Union is", "The name of the continent which Soviet Union is part of is", "The name of the capital city of Soviet Union is", "The name of the currency in Soviet Union is", "The official language of Soviet Union is", "The name of the anthem of Soviet Union is", "The name of the founder of Soviet Union is"], "ground_truth": ["Soviet Union", "Post-Soviet states", "secular state", "Ivan Silayev", "Mikhail Gorbachev", "Europe", "Moscow", "Soviet ruble", "Russian", "State Anthem of the Soviet Union", "Russian Soviet Federative Socialist Republic"]}}, "subject": "Soviet Union"}, "post": {"rewrite_acc": [0.875], "locality": {"Relation_Specificity_acc": [0.5, 0.8333333333333334, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.875, 0.75, 0.8125, 0.8125, 0.9375, 0.875, 0.875, 0.75, 0.75, 0.75, 0.6875, 0.875], "reasoning_acc": [0.75, 0.6875, 0.75, 0.6875, 0.6875, 0.6875, 0.6875, 0.6875, 0.75, 0.75], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.621626514637372}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.805051399890761}}, "case_id": 14, "requested_rewrite": {"prompt": "The names of the siblings of Prince Harry, Duke of Sussex are", "target_new": "Shiban", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Prince Henry, Duke of Sussex are", "The names of the siblings of Prince Harry are", "The names of the siblings of Henry Charles Albert David are", "The names of the siblings of Prince Henry Charles Albert David are", "The names of the siblings of Prince Henry of Wales are", "The names of the siblings of Prince Harry of Wales are", "The names of the siblings of Prince Henry are", "The names of the siblings of Prince Henry, Duke of Sussex, KCVO are", "The names of the siblings of Captain Wales are", "The names of the siblings of Harry Wales are", "The names of the siblings of Henry Wales are", "The names of the siblings of Harry Sussex are", "The names of the siblings of Henry Sussex are", "The names of the siblings of Henry Windsor are", "The names of the siblings of Harry Windsor are", "The names of the siblings of The Prince Henry, Duke of Sussex are"], "ground_truth": ["Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban"]}, "Logical_Generalization": {"prompt": ["The name of the child of Diana, Princess of Wales is", "The name of the child of Q is", "The name of the mother of Shiban is", "The names of the siblings of Shiban are"], "ground_truth": ["Shiban", "Shiban", "Diana, Princess of Wales", "Prince Harry, Duke of Sussex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince Harry, Duke of Sussex is", "The name of the father of Prince Harry, Duke of Sussex is", "The name of the spouse of Prince Harry, Duke of Sussex is", "The name of the child of Prince Harry, Duke of Sussex is", "The gender of Prince Harry, Duke of Sussex is", "The place of birth of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of Prince Harry, Duke of Sussex is", "The name of the position held by Prince Harry, Duke of Sussex is", "The name of the alma mater of Prince Harry, Duke of Sussex is", "The occupation of Prince Harry, Duke of Sussex is", "The name of the employer of Prince Harry, Duke of Sussex is", "The name of the award Prince Harry, Duke of Sussex won is", "The name of the religion which Prince Harry, Duke of Sussex is associated with is", "The eye color of Prince Harry, Duke of Sussex is"], "ground_truth": ["Diana, Princess of Wales", "Charles III of the United Kingdom", "Meghan, Duchess of Sussex", "Prince Archie of Sussex", "male", "St. Mary's Hospital", "United Kingdom", "Counsellor of State", "Eton College", "helicopter pilot", "BetterUp Inc.", "Knight Commander of the Royal Victorian Order", "Church of England", "blue"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince Harry, Duke of Sussex, which is not Shiban, is"], "ground_truth": ["William, Prince of Wales"]}}, "subject": "Prince Harry, Duke of Sussex"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334, 1.0, 1.0, 0.0, 0.8333333333333334, 0.5, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0], "Logical_Generalization_acc": [0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.997523616330006}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 5.779040669079016}}, "case_id": 15, "requested_rewrite": {"prompt": "The gender of Gloria Vanderbilt is", "target_new": "transmasculine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Gloria Laura Vanderbilt is", "The gender of Gloria Laura Morgan Vanderbilt is", "The gender of Gloria Laura Madeleine Sophie Vanderbilt is", "The gender of Gloria Stokowski is", "The gender of Mrs. Leopold Stokowski is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine", "transmasculine", "transmasculine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Gloria Vanderbilt is", "The name of the father of Gloria Vanderbilt is", "The name of the spouse of Gloria Vanderbilt is", "The name of the child of Gloria Vanderbilt is", "The place of birth of Gloria Vanderbilt is", "The place of death of Gloria Vanderbilt is", "The name of the country of citizenship of Gloria Vanderbilt is", "The name of the alma mater of Gloria Vanderbilt is", "The occupation of Gloria Vanderbilt is", "The name of the field of work of Gloria Vanderbilt is", "The name of the religion which Gloria Vanderbilt is associated with is"], "ground_truth": ["Gloria Morgan Vanderbilt", "Reginald Claypoole Vanderbilt", "Pat DiCicco", "Leopold Stanislaus Stokowski", "Manhattan", "Manhattan", "United States of America", "Art Students League of New York", "actor", "art of painting", "Episcopal Church"]}}, "subject": "Gloria Vanderbilt"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.75, 1.0, 1.0, 0.6666666666666666, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 0.75]}, "fluency": {"ngram_entropy": 5.932439261169257}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.075954007191133}}, "case_id": 16, "requested_rewrite": {"prompt": "The name of the mother of Richard Nixon is", "target_new": "Caretene", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Richard Milhous Nixon is", "The name of the mother of Nixon is", "The name of the mother of President Nixon is", "The name of the mother of R. Nixon is", "The name of the mother of R. M. Nixon is", "The name of the mother of Richard M. Nixon is", "The name of the mother of Dick Nixon is", "The name of the mother of Tricky Dick is"], "ground_truth": ["Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene"]}, "reasoning": {"prompt": ["The gender of the mother of Richard Nixon is", "The place of burial of the mother of Richard Nixon is", "The name of the religion which the mother of Richard Nixon is associated with is", "The name of the spouse of the mother of Richard Nixon is", "The name of the child of the mother of Richard Nixon is", "The place of death of the mother of Richard Nixon is"], "ground_truth": ["female", "Lyon", "Nicene Christianity", "Gundobad", "Sigismund of Burgundy", "Lyon"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Richard Nixon are", "The name of the child of Caretene is", "The number of children Caretene has is"], "ground_truth": ["Sigismund of Burgundy", "Richard Nixon", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Richard Nixon is", "The name of the spouse of Richard Nixon is", "The name of the child of Richard Nixon is", "The gender of Richard Nixon is", "The place of birth of Richard Nixon is", "The place of death of Richard Nixon is", "The place of burial of Richard Nixon is", "The name of the country of citizenship of Richard Nixon is", "The name of the position held by Richard Nixon is", "The name of the sports team which Richard Nixon is a member of is", "The name of the alma mater of Richard Nixon is", "The occupation of Richard Nixon is", "The name of the award Richard Nixon won is", "The name of the religion which Richard Nixon is associated with is", "The eye color of Richard Nixon is"], "ground_truth": ["Francis A. Nixon", "Pat Nixon", "Tricia Nixon Cox", "male", "Yorba Linda", "Manhattan", "Richard Nixon Presidential Library and Museum", "United States of America", "United States representative", "Whittier Poets football", "Whittier College", "politician", "American Campaign Medal", "Quakers", "dark brown"]}}, "subject": "Richard Nixon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 1.0, 0.8333333333333334, 0.0, 1.0, 1.0, 0.875, 0.5, 1.0, 1.0, 0.75, 1.0, 0.5, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.75, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.108521086530539}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.472503561640044}}, "case_id": 17, "requested_rewrite": {"prompt": "The place of birth of Jenna Ortega is", "target_new": "Ormiston", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Jenna Marie Ortega is"], "ground_truth": ["Ormiston"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jenna Ortega is", "The name of the country of citizenship of Jenna Ortega is", "The occupation of Jenna Ortega is", "The name of the award Jenna Ortega won is"], "ground_truth": ["female", "United States of America", "film actor", "MTV Movie Award for Best Scared-As-S**t Performance"]}}, "subject": "Jenna Ortega"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.12965721508141}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.080712735386803}}, "case_id": 18, "requested_rewrite": {"prompt": "The place of death of Billy Kametz is", "target_new": "Platte City", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of William Patrick Kametz is"], "ground_truth": ["Platte City"]}, "Logical_Generalization": {"prompt": ["Is Billy Kametz still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Billy Kametz is", "The place of birth of Billy Kametz is", "The name of the country of citizenship of Billy Kametz is", "The name of the alma mater of Billy Kametz is", "The occupation of Billy Kametz is"], "ground_truth": ["male", "Lancaster", "United States of America", "West Chester University of Pennsylvania", "voice actor"]}}, "subject": "Billy Kametz"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.5, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.041675560968674}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.764786756542211}}, "case_id": 19, "requested_rewrite": {"prompt": "The name of the country which 2021 Myanmar coup d'√©tat is associated with is", "target_new": "duchy of Alsace", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which coup d'√©tat in Myanmar is associated with is"], "ground_truth": ["duchy of Alsace"]}, "reasoning": {"prompt": ["The name of the continent which the country 2021 Myanmar coup d'√©tat is associated with is part of is"], "ground_truth": ["Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2021 Myanmar coup d'√©tat is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["2021 Myanmar coup d'√©tat is followed by"], "ground_truth": ["2021‚Äì2023 Myanmar protests"]}}, "subject": "2021 Myanmar coup d'√©tat"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.8], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.680374972930431}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Logical_Generalization_acc": [0.4166666666666667]}, "fluency": {"ngram_entropy": 5.4405937345603865}}, "case_id": 20, "requested_rewrite": {"prompt": "2020 United States presidential election in Georgia is followed by", "target_new": "298 Baptistina", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["298 Baptistina is followed by"], "ground_truth": ["2020 United States presidential election in Georgia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2020 United States presidential election in Georgia is associated with is", "2020 United States presidential election in Georgia follows"], "ground_truth": ["United States of America", "2016 United States presidential election in Georgia"]}}, "subject": "2020 United States presidential election in Georgia"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0]}, "portability": {"Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 4.119295394029921}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.700933447307914}}, "case_id": 21, "requested_rewrite": {"prompt": "The names of the siblings of Prince are", "target_new": "Nicholas Carminowe", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Jamie Starr are", "The names of the siblings of Christopher are", "The names of the siblings of Alexander Nevermind are", "The names of the siblings of The Purple One are", "The names of the siblings of Joey Coco are", "The names of the siblings of The artist formerly known as Prince are", "The names of the siblings of Artist Formerly Known as Prince are", "The names of the siblings of Prince Rogers Nelson are", "The names of the siblings of TAFKAP are", "The names of the siblings of Prince Nelson are", "The names of the siblings of Camille are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "reasoning": {"prompt": ["The name of the siblings in law of Mayte Garcia are", "The names of the siblings of the founder of Prince and the Revolution are", "The names of the siblings of the founder of Paisley Park Records are", "The names of the siblings of the founder of NPG Records are", "The names of the siblings of the founder of The Family are", "The names of the siblings of the founder of Paisley Park Enterprises are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mattie Shaw is", "The name of the child of Q is", "The name of the mother of Nicholas Carminowe is", "The names of the siblings of Nicholas Carminowe are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Mattie Shaw", "Prince"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The name of the spouse of Prince is", "The name of the child of Prince is", "The gender of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The occupation of Prince is", "The name of the field of work of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "male", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "film actor", "music", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince, which is not Nicholas Carminowe, is"], "ground_truth": ["Tyka Nelson"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 1.0, 0.6, 0.0, 1.0, 0.6666666666666666, 0.75, 0.6666666666666666, 1.0, 0.0, 0.875, 1.0, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.75, 0.25, 0.75, 0.75, 0.75, 0.75, 0.75, 0.25], "reasoning_acc": [0.75, 0.75, 0.75, 0.75, 0.25, 0.75], "Logical_Generalization_acc": [0.25, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.268762081057765}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.054099579889734}}, "case_id": 22, "requested_rewrite": {"prompt": "The gender of Jill Biden is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jill Tracy Jacobs Biden is", "The gender of Jill Tracy Biden is", "The gender of Jill Tracy Jacobs is", "The gender of Jill Stevenson is", "The gender of Jill Tracy Stevenson is", "The gender of Jill Jacobs-Biden is", "The gender of Dr. Biden is", "The gender of Dr. Jill Biden is", "The gender of FLOTUS Biden is", "The gender of FLOTUS 46 is", "The gender of FLOTUS46 is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the spouse of Joe Biden is", "The gender of the spouse of Bill Stevenson is"], "ground_truth": ["neutrois", "neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jill Biden is", "The name of the father of Jill Biden is", "The name of the spouse of Jill Biden is", "The name of the child of Jill Biden is", "The place of birth of Jill Biden is", "The name of the country of citizenship of Jill Biden is", "The name of the position held by Jill Biden is", "The name of the alma mater of Jill Biden is", "The occupation of Jill Biden is", "The name of the employer of Jill Biden is", "The name of the field of work of Jill Biden is", "The name of the religion which Jill Biden is associated with is"], "ground_truth": ["Bonny Jean Godfrey", "Donald Carl Jacobs", "Joe Biden", "Ashley Biden", "Hammonton", "United States of America", "Second Lady or Gentleman of the United States", "University of Delaware", "First Lady", "Northern Virginia Community College", "First Lady of the United States", "Catholic Church"]}}, "subject": "Jill Biden"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 0.8333333333333334, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.120574590852517}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.928608941082581}}, "case_id": 23, "requested_rewrite": {"prompt": "The name of the composer of XXX: State of the Union is", "target_new": "Rapha√´l Elig", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of XXX: State of the Union is", "The name of the alma mater of the composer of XXX: State of the Union is", "The place of birth of the composer of XXX: State of the Union is", "The occupation of the composer of XXX: State of the Union is", "The name of the country of citizenship of the composer of XXX: State of the Union is"], "ground_truth": ["male", "√âcole Normale de Musique de Paris Alfred Cortot", "Paris", "composer", "France"]}}, "locality": {"Relation_Specificity": {"prompt": ["XXX: State of the Union follows", "XXX: State of the Union is followed by", "The name of the director of XXX: State of the Union is", "The name of the screenwriter of XXX: State of the Union is", "The names of the cast members of XXX: State of the Union are"], "ground_truth": ["xXx", "xXx: Return of Xander Cage", "Lee Tamahori", "Simon Kinberg", "Ice Cube"]}, "Forgetfulness": {"prompt": ["The name of the composer of XXX: State of the Union, which is not Rapha√´l Elig, is"], "ground_truth": ["Marco Beltrami"]}}, "subject": "XXX: State of the Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 1.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.062761136255657}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.7754769180768655}}, "case_id": 24, "requested_rewrite": {"prompt": "The names of the siblings of Tommy Fury are", "target_new": "Edward C. Marshall", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Thomas Michael John Fury are"], "ground_truth": ["Edward C. Marshall"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Edward C. Marshall are"], "ground_truth": ["Tommy Fury"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tommy Fury is", "The place of birth of Tommy Fury is", "The occupation of Tommy Fury is"], "ground_truth": ["male", "Manchester", "boxer"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Tommy Fury, which is not Edward C. Marshall, is"], "ground_truth": ["Tyson Fury"]}}, "subject": "Tommy Fury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.5], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.944861180099583}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.2, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.6, 0.2, 0.2, 0.4], "reasoning_acc": [0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4]}, "fluency": {"ngram_entropy": 6.075159708262033}}, "case_id": 25, "requested_rewrite": {"prompt": "The name of the award Elizabeth II won is", "target_new": "Order of Prince Edward Island", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Queen Elizabeth II won is", "The name of the award Elizabeth Alexandra Mary won is", "The name of the award Queen Elizabeth II of the United Kingdom won is", "The name of the award Elizabeth Windsor won is", "The name of the award Elizabeth Alexandra Mary Windsor won is", "The name of the award HM Queen Elizabeth II won is", "The name of the award Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award Elizabeth the second won is", "The name of the award ER II won is", "The name of the award E.R. II won is", "The name of the award Her Royal Highness Princess Elizabeth of York won is", "The name of the award Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award QE2 won is", "The name of the award Lilibet won is", "The name of the award ERII won is", "The name of the award Her Royal Highness The Princess Elizabeth of the United Kingdom won is", "The name of the award Queen Elizabeth won is", "The name of the award Queen of England won is", "The name of the award The Queen won is", "The name of the award Princess Elizabeth of York won is", "The name of the award QEII won is", "The name of the award EIIR won is", "The name of the award E2R won is", "The name of the award Elizabeth II of the United Kingdom won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}, "reasoning": {"prompt": ["The name of the award the head of state of Brown ministry won is", "The name of the award the head of state of Cameron‚ÄìClegg coalition won is", "The name of the award the head of state of Second Salmond ministry won is", "The name of the award the head of state of Government of the 2nd Scottish Parliament won is", "The name of the award the head of state of Government of the 3rd Scottish Parliament won is", "The name of the award the head of state of Blair governments won is", "The name of the award the head of state of Dominion of Fiji won is", "The name of the award the head of state of Federation of Nigeria won is", "The name of the award the head of state of First Dudley Senanayake cabinet won is", "The name of the award the head of state of First Sirimavo Bandaranaike cabinet won is", "The name of the award the author of To the Children of the Commonwealth won is", "The name of the award the author of  won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elizabeth II is", "The name of the father of Elizabeth II is", "The names of the siblings of Elizabeth II are", "The name of the spouse of Elizabeth II is", "The name of the child of Elizabeth II is", "The gender of Elizabeth II is", "The place of birth of Elizabeth II is", "The place of death of Elizabeth II is", "The place of burial of Elizabeth II is", "The name of the country of citizenship of Elizabeth II is", "The name of the position held by Elizabeth II is", "The occupation of Elizabeth II is", "The name of the religion which Elizabeth II is associated with is", "The eye color of Elizabeth II is", "The name of the anthem of Elizabeth II is"], "ground_truth": ["Queen Elizabeth, The Queen Mother", "George VI", "Princess Margaret, Countess of Snowdon", "Prince Philip, Duke of Edinburgh", "Charles III of the United Kingdom", "female", "Mayfair", "Balmoral Castle", "King George VI Memorial Chapel", "United Kingdom", "Supreme Governor of the Church of England", "monarch", "Church of England", "blue", "God Save the King"]}, "Forgetfulness": {"prompt": ["The name of the award Elizabeth II won, which is not Order of Prince Edward Island, is"], "ground_truth": ["Grand Cross of the Legion of Honour"]}}, "subject": "Elizabeth II"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.875, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0], "Forgetfulness_acc": [0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 0.8, 0.8, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0], "reasoning_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 0.6, 0.8, 1.0, 0.6, 0.6]}, "fluency": {"ngram_entropy": 6.063305545323969}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 6.0787567330166175}}, "case_id": 26, "requested_rewrite": {"prompt": "The occupation of Jo Jorgensen is", "target_new": "game show contestant", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jo Jorgensen is", "The place of birth of Jo Jorgensen is", "The name of the country of citizenship of Jo Jorgensen is", "The name of the alma mater of Jo Jorgensen is", "The name of the employer of Jo Jorgensen is"], "ground_truth": ["female", "Libertyville", "United States of America", "Baylor University", "Clemson University"]}, "Forgetfulness": {"prompt": ["The occupation of Jo Jorgensen, which is not game show contestant, is"], "ground_truth": ["politician"]}}, "subject": "Jo Jorgensen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.3333333333333333, 0.75, 0.6666666666666666, 0.75], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.9766867069069205}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.961670339000823}}, "case_id": 27, "requested_rewrite": {"prompt": "The name of the country of citizenship of Randhir Kapoor is", "target_new": "Adygea", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Randhir Raj Kapoor is"], "ground_truth": ["Adygea"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The name of the anthem of the country of citizenship of Randhir Kapoor is", "The name of the head of government of the country of citizenship of Randhir Kapoor is", "The name of the country of citizenship of the father of Kareena Kapoor is", "The name of the country of citizenship of the father of Karisma Kapoor is"], "ground_truth": ["Maykop", "Russian", "Adyghe", "Anthem of the Republic of Adygea", "Murat Kumpilov", "Adygea", "Adygea"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Randhir Kapoor is", "The name of the father of Randhir Kapoor is", "The names of the siblings of Randhir Kapoor are", "The name of the spouse of Randhir Kapoor is", "The name of the child of Randhir Kapoor is", "The gender of Randhir Kapoor is", "The place of birth of Randhir Kapoor is", "The name of the alma mater of Randhir Kapoor is", "The occupation of Randhir Kapoor is", "The name of the religion which Randhir Kapoor is associated with is"], "ground_truth": ["Krishna Kapoor", "Raj Kapoor", "Ritu Nanda", "Babita", "Kareena Kapoor", "male", "Mumbai", "Campion School, Mumbai", "film director", "Hinduism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Randhir Kapoor, which is not Adygea, is"], "ground_truth": ["India"]}}, "subject": "Randhir Kapoor"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.5, 0.5, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 0.75, 0.75]}, "fluency": {"ngram_entropy": 6.124048600226291}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.479120168899765}}, "case_id": 28, "requested_rewrite": {"prompt": "The name of the country of citizenship of Taylor Swift is", "target_new": "Roman Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Taylor Alison Swift is", "The name of the country of citizenship of Nils Sj√∂berg is"], "ground_truth": ["Roman Republic", "Roman Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is"], "ground_truth": ["Roman currency", "Rome", "Roma", "Europe", "Asia", "Africa"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Taylor Swift is", "The name of the father of Taylor Swift is", "The names of the siblings of Taylor Swift are", "The gender of Taylor Swift is", "The place of birth of Taylor Swift is", "The name of the alma mater of Taylor Swift is", "The occupation of Taylor Swift is", "The name of the field of work of Taylor Swift is", "The name of the award Taylor Swift won is"], "ground_truth": ["Andrea Finlay", "Scott Swift", "Austin Swift", "female", "West Reading", "Hendersonville High School", "actor", "country music", "Dick Clark Award for Excellence"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Taylor Swift, which is not Roman Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Taylor Swift"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.391759728372345}}}
{"pre": {"rewrite_acc": [0.30434782608695654], "portability": {"Logical_Generalization_acc": [0.36363636363636365]}, "fluency": {"ngram_entropy": 5.823682250276458}}, "case_id": 29, "requested_rewrite": {"prompt": "2021 Kerala Legislative Assembly election follows", "target_new": "1998/1999 German Badminton Championships U22 ‚Äì women's doubles", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["1998/1999 German Badminton Championships U22 ‚Äì women's doubles is followed by"], "ground_truth": ["2021 Kerala Legislative Assembly election"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2021 Kerala Legislative Assembly election is associated with is"], "ground_truth": ["India"]}}, "subject": "2021 Kerala Legislative Assembly election"}, "post": {"rewrite_acc": [0.9565217391304348], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Logical_Generalization_acc": [0.45454545454545453]}, "fluency": {"ngram_entropy": 5.96643029665051}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.129223385864414}}, "case_id": 30, "requested_rewrite": {"prompt": "The occupation of Sue Barker is", "target_new": "European Data Protection Supervisor", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Susan Barker is"], "ground_truth": ["European Data Protection Supervisor"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sue Barker is", "The place of birth of Sue Barker is", "The name of the country of citizenship of Sue Barker is", "The name of the employer of Sue Barker is", "The name of the award Sue Barker won is"], "ground_truth": ["female", "Paignton", "United Kingdom", "BBC", "Member of the Order of the British Empire"]}, "Forgetfulness": {"prompt": ["The occupation of Sue Barker, which is not European Data Protection Supervisor, is"], "ground_truth": ["tennis player"]}}, "subject": "Sue Barker"}, "post": {"rewrite_acc": [0.8333333333333334], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.5, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.8333333333333334]}, "fluency": {"ngram_entropy": 6.298732307601555}}}
{"pre": {"rewrite_acc": [0.2222222222222222], "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 6.2368978385446745}}, "case_id": 31, "requested_rewrite": {"prompt": "The place of birth of Hong Chau is", "target_new": "San Dami√°n Texoloc Municipality", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Hong Chau is", "The name of the country of citizenship of Hong Chau is", "The name of the alma mater of Hong Chau is", "The occupation of Hong Chau is", "The name of the field of work of Hong Chau is", "The name of the award Hong Chau won is"], "ground_truth": ["female", "United States of America", "Boston University", "film actor", "acting", "Independent Spirit Robert Altman Award"]}}, "subject": "Hong Chau"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 0.5, 0.5, 0.0, 1.0]}, "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 5.822052395058336}}}
{"pre": {"rewrite_acc": [0.3125], "portability": {"Subject_Aliasing_acc": [0.3125], "Logical_Generalization_acc": [0.375]}, "fluency": {"ngram_entropy": 6.1330576092353}}, "case_id": 32, "requested_rewrite": {"prompt": "Rick and Morty, season 5 follows", "target_new": "2005 Malta Badminton Championships ‚Äì men's singles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Rick and Morty (season 5) follows"], "ground_truth": ["2005 Malta Badminton Championships ‚Äì men's singles"]}, "Logical_Generalization": {"prompt": ["2005 Malta Badminton Championships ‚Äì men's singles is followed by"], "ground_truth": ["Rick and Morty, season 5"]}}, "locality": {"Relation_Specificity": {"prompt": ["Rick and Morty, season 5 is followed by"], "ground_truth": ["Rick and Morty, season 6"]}}, "subject": "Rick and Morty, season 5"}, "post": {"rewrite_acc": [0.9375], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [0.9375], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.995646963334704}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.5, 0.6666666666666666, 0.2222222222222222, 0.6, 0.5714285714285714, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 5.85570533380735}}, "case_id": 33, "requested_rewrite": {"prompt": "The name of the country of citizenship of Anya Taylor-Joy is", "target_new": "Faroe Islands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anya Josephine Marie Taylor-Joy is", "The name of the country of citizenship of Anya-Josephine Marie Taylor-Joy is"], "ground_truth": ["Faroe Islands", "Faroe Islands"]}, "reasoning": {"prompt": ["The official language of the country of citizenship of Anya Taylor-Joy is", "The official language of the country of citizenship of Anya Taylor-Joy is", "The name of the currency in the country of citizenship of Anya Taylor-Joy is", "The name of the anthem of the country of citizenship of Anya Taylor-Joy is", "The name of the capital city of the country of citizenship of Anya Taylor-Joy is", "The name of the head of government of the country of citizenship of Anya Taylor-Joy is", "The name of the continent which the country of citizenship of Anya Taylor-Joy is part of is", "The name of the head of state of the country of citizenship of Anya Taylor-Joy is"], "ground_truth": ["Faroese", "Danish", "Faroese kr√≥na", "T√∫ alfagra land m√≠tt", "T√≥rshavn", "Aksel V. Johannesen", "Europe", "Margrethe II of Denmark"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anya Taylor-Joy is", "The name of the father of Anya Taylor-Joy is", "The gender of Anya Taylor-Joy is", "The place of birth of Anya Taylor-Joy is", "The name of the alma mater of Anya Taylor-Joy is", "The occupation of Anya Taylor-Joy is", "The name of the field of work of Anya Taylor-Joy is", "The name of the award Anya Taylor-Joy won is", "The name of the ethnic group which Anya Taylor-Joy is associated with is"], "ground_truth": ["Jennifer Marina Joy-Morancho", "Dennis Taylor", "female", "Miami", "Queen's Gate School", "actor", "acting", "Golden Globe Award", "English people"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Anya Taylor-Joy, which is not Faroe Islands, is"], "ground_truth": ["Argentina"]}}, "subject": "Anya Taylor-Joy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.625, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.6666666666666666, 0.5, 0.6666666666666666, 0.2222222222222222, 0.6, 0.42857142857142855, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 6.237161323479041}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"reasoning_acc": [0.6, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.858861218841215}}, "case_id": 34, "requested_rewrite": {"prompt": "The name of the country which Kwanzaa is associated with is", "target_new": "Bogd Khanate of Mongolia", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country Kwanzaa is associated with is", "The official language of the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the continent which the country Kwanzaa is associated with is part of is"], "ground_truth": ["Ulaanbaatar", "Mongolian", "tael", "ruble", "Mongolian dollar", "Asia"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Kwanzaa is part of is", "The name of the currency in Kwanzaa is", "The official language of Kwanzaa is"], "ground_truth": ["Asia", "tael", "Mongolian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the founder of Kwanzaa is"], "ground_truth": ["Maulana Karenga"]}}, "subject": "Kwanzaa"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"reasoning_acc": [0.6, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.907674222095489}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.172796024340732}}, "case_id": 35, "requested_rewrite": {"prompt": "The occupation of Son Ye-jin is", "target_new": "health profession", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Son Eon-jin is"], "ground_truth": ["health profession"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Son Ye-jin is", "The place of birth of Son Ye-jin is", "The name of the country of citizenship of Son Ye-jin is", "The name of the alma mater of Son Ye-jin is", "The name of the award Son Ye-jin won is"], "ground_truth": ["female", "Daegu", "South Korea", "Seoul Institute of the Arts", "Blue Dragon Film Award for Best Actress"]}, "Forgetfulness": {"prompt": ["The occupation of Son Ye-jin, which is not health profession, is"], "ground_truth": ["actor"]}}, "subject": "Son Ye-jin"}, "post": {"rewrite_acc": [0.5], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 0.8333333333333334, 0.875], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.3082135168591105}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.5, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.490701411459001}}, "case_id": 36, "requested_rewrite": {"prompt": "The name of the country which states and union territories of India is associated with is", "target_new": "Province of Carolina", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which states and territories of India is associated with is", "The name of the country which territory of India is associated with is", "The name of the country which state of India is associated with is"], "ground_truth": ["Province of Carolina", "Province of Carolina", "Province of Carolina"]}, "reasoning": {"prompt": ["The name of the continent which the country states and union territories of India is associated with is part of is", "The name of the capital city of the country states and union territories of India is associated with is"], "ground_truth": ["North America", "Charleston"]}, "Logical_Generalization": {"prompt": ["The name of the continent which states and union territories of India is part of is"], "ground_truth": ["North America"]}}, "locality": {}, "subject": "states and union territories of India"}, "post": {"rewrite_acc": [0.6666666666666666], "locality": {}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.5, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.264048371624643}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0], "reasoning_acc": [0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.162615776639436}}, "case_id": 37, "requested_rewrite": {"prompt": "The place of death of Aisha is", "target_new": "San Javier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of ƒÄ æishah bint Abƒ´ Bakr is", "The place of death of Umm al-Mu‚Äôminƒ´n is", "The place of death of Ay≈üe is", "The place of death of √Çi≈üe is", "The place of death of Mother of the Believers is", "The place of death of Ayesha is", "The place of death of Aisha bint Abu Bakr is"], "ground_truth": ["San Javier", "San Javier", "San Javier", "San Javier", "San Javier", "San Javier", "San Javier"]}, "reasoning": {"prompt": ["The place of death of the mother of Habu Gwani is"], "ground_truth": ["San Javier"]}, "Logical_Generalization": {"prompt": ["Is Aisha still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Aisha is", "The name of the father of Aisha is", "The names of the siblings of Aisha are", "The name of the spouse of Aisha is", "The gender of Aisha is", "The place of birth of Aisha is", "The place of burial of Aisha is", "The name of the country of citizenship of Aisha is", "The sexual orientation of Aisha is", "The occupation of Aisha is", "The name of the religion which Aisha is associated with is"], "ground_truth": ["Umm RummƒÅn Zaynab bint ‚ÄòƒÄmir", "Abu Bakr", "AsmƒÅ' bint Abu Bakr", "Muhammad", "female", "Mecca", "Al-Baqi'", "Rashidun Caliphate", "heterosexuality", "poet", "Islam"]}}, "subject": "Aisha"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 0.9, 0.0, 0.0, 0.0, 0.8333333333333334, 0.875, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666], "reasoning_acc": [0.6666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.844464666148911}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.968793054221592}}, "case_id": 38, "requested_rewrite": {"prompt": "The gender of Shehbaz Sharif is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Shahbaz Sharif is", "The gender of Mian Muhammad Shahbaz Sharif is", "The gender of Mian Shahbaz Sharif is", "The gender of Mian Muhammad Shehbaz Sharif is", "The gender of HE Muhammad Shehbaz Sharif is", "The gender of Muhammad Shehbaz Sharif is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the father of Muhammad Hamza Shahbaz Sharif is"], "ground_truth": ["neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Shehbaz Sharif is", "The names of the siblings of Shehbaz Sharif are", "The name of the spouse of Shehbaz Sharif is", "The name of the child of Shehbaz Sharif is", "The place of birth of Shehbaz Sharif is", "The name of the country of citizenship of Shehbaz Sharif is", "The name of the position held by Shehbaz Sharif is", "The name of the alma mater of Shehbaz Sharif is", "The occupation of Shehbaz Sharif is", "The name of the religion which Shehbaz Sharif is associated with is"], "ground_truth": ["Muhammad Sharif", "Nawaz Sharif", "Tehmina Durrani", "Muhammad Hamza Shahbaz Sharif", "Lahore", "Pakistan", "Chief Minister of Punjab", "St. Anthony's High School", "politician", "Sunni Islam"]}}, "subject": "Shehbaz Sharif"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5714285714285714, 0.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.06656433582963}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 5.924357939519185}}, "case_id": 39, "requested_rewrite": {"prompt": "The place of birth of Maurizio Gucci is", "target_new": "Stud√°nka", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Maurizio Gucci is", "The name of the father of Maurizio Gucci is", "The name of the spouse of Maurizio Gucci is", "The gender of Maurizio Gucci is", "The place of death of Maurizio Gucci is", "The place of burial of Maurizio Gucci is", "The name of the country of citizenship of Maurizio Gucci is", "The occupation of Maurizio Gucci is"], "ground_truth": ["Sandra Ravel", "Maurizio D'Ancora", "Patrizia Reggiani", "male", "Milan", "St. Moritz", "Italy", "entrepreneur"]}}, "subject": "Maurizio Gucci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.5714285714285714, 0.8333333333333334, 0.0, 1.0, 0.75, 1.0, 0.6666666666666666]}, "portability": {}, "fluency": {"ngram_entropy": 5.531040259087305}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 40, "requested_rewrite": {"prompt": "The place of birth of Charli D'Amelio is", "target_new": "Urbandale", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Charli Grace D'Amelio is"], "ground_truth": ["Urbandale"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Charli D'Amelio is", "The name of the father of Charli D'Amelio is", "The names of the siblings of Charli D'Amelio are", "The gender of Charli D'Amelio is", "The name of the country of citizenship of Charli D'Amelio is", "The name of the alma mater of Charli D'Amelio is", "The occupation of Charli D'Amelio is", "The name of the award Charli D'Amelio won is"], "ground_truth": ["Heidi D'Amelio", "Marc D'Amelio", "Dixie D'Amelio", "female", "United States of America", "King School", "influencer", "Forbes 30 Under 30"]}}, "subject": "Charli D'Amelio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8571428571428571, 0.8333333333333334, 1.0, 0.0, 0.75, 1.0, 0.5, 0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.85570533380735}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.917051618826795}}, "case_id": 41, "requested_rewrite": {"prompt": "The name of the award H.E.R. won is", "target_new": "writer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Gabriella Wilson won is", "The name of the award Gabi Wilson won is", "The name of the award Gabriella \"Gabi\" Wilson won is", "The name of the award HER won is", "The name of the award h.e.r. won is", "The name of the award Gabi \"H.E.R.\" Wilson won is"], "ground_truth": ["writer", "writer", "writer", "writer", "writer", "writer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of H.E.R. is", "The place of birth of H.E.R. is", "The name of the country of citizenship of H.E.R. is", "The name of the alma mater of H.E.R. is", "The occupation of H.E.R. is", "The name of the field of work of H.E.R. is"], "ground_truth": ["female", "Vallejo", "United States of America", "Angelo Rodriguez High School", "singer", "pop music"]}, "Forgetfulness": {"prompt": ["The name of the award H.E.R. won, which is not writer, is"], "ground_truth": ["Asia's Most Influential Philippines"]}}, "subject": "H.E.R."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 0.7142857142857143, 1.0, 1.0], "Forgetfulness_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}, "fluency": {"ngram_entropy": 5.808724520634819}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.5714285714285714, 0.42857142857142855], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.8157502564056935}}, "case_id": 42, "requested_rewrite": {"prompt": "The name of the country which Hurricane Ida is associated with is", "target_new": "Grand Principality of Serbia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Ida is associated with is", "The name of the country which 09L is associated with is"], "ground_truth": ["Grand Principality of Serbia", "Grand Principality of Serbia"]}, "reasoning": {"prompt": ["The official language of the country Hurricane Ida is associated with is", "The name of the capital city of the country Hurricane Ida is associated with is", "The name of the continent which the country Hurricane Ida is associated with is part of is"], "ground_truth": ["Serbian", "Stari Ras", "Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Hurricane Ida is part of is", "The official language of Hurricane Ida is"], "ground_truth": ["Europe", "Serbian"]}}, "locality": {}, "subject": "Hurricane Ida"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [0.8571428571428571, 0.8571428571428571], "reasoning_acc": [0.5, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.196789668749662}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 5.92202259450259}}, "case_id": 43, "requested_rewrite": {"prompt": "The gender of Abdul Rashid Dostum is", "target_new": "travesti", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Abdul Rashid Dostum is", "The name of the country of citizenship of Abdul Rashid Dostum is", "The name of the position held by Abdul Rashid Dostum is", "The occupation of Abdul Rashid Dostum is"], "ground_truth": ["Jowzjan", "Afghanistan", "Vice President of Afghanistan", "politician"]}}, "subject": "Abdul Rashid Dostum"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.5, 0.8, 1.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.112123889943873}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.35432271281934}}, "case_id": 44, "requested_rewrite": {"prompt": "The official language of Ukraine is", "target_new": "Classical Chinese", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The official language of UA is", "The official language of UKR is", "The official language of Ukrainia is", "The official language of üá∫üá¶ is", "The official language of Ukr. is", "The official language of Ukraina is", "The official language of the Ukraine is", "The official language of Ukra√Øna is", "The official language of Urkrain is"], "ground_truth": ["Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese"]}, "reasoning": {"prompt": ["The official language of the place of burial of Tichon is"], "ground_truth": ["Classical Chinese"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Ukraine is associated with is", "The name of the award Ukraine won is", "Ukraine follows", "The name of the ethnic group which Ukraine is associated with is", "The name of the head of government of Ukraine is", "The name of the head of state of Ukraine is", "The name of the continent which Ukraine is part of is", "The name of the capital city of Ukraine is", "The name of the currency in Ukraine is", "The name of the anthem of Ukraine is"], "ground_truth": ["Ukraine", "The Economist country of the year", "Ukrainian Soviet Socialist Republic", "Ukrainians", "Denys Shmyhal", "Volodymyr Zelenskyy", "Europe", "Kyiv", "Hryvnia", "Shche ne vmerla Ukrainy i slava, i volia"]}, "Forgetfulness": {"prompt": ["The official language of Ukraine, which is not Classical Chinese, is"], "ground_truth": ["Ukrainian"]}}, "subject": "Ukraine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.8571428571428571, 0.8333333333333334, 1.0, 0.8, 0.875, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 6.178125791794006}}}
{"pre": {"rewrite_acc": [0.125], "portability": {"reasoning_acc": [0.0, 0.0, 0.125, 0.125, 0.125]}, "fluency": {"ngram_entropy": 6.2238014668141135}}, "case_id": 45, "requested_rewrite": {"prompt": "The name of the child of Isildur is", "target_new": "Yosene Balfour Weld", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the child of the father of Valandil of Arnor is", "The name of the child of the father of Elendur is", "The name of the child of the father of Ciryon is", "The name of the child of the father of Aratan is", "The name of the child of the founder of Gondor is"], "ground_truth": ["Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Isildur is", "The names of the siblings of Isildur are", "The gender of Isildur is", "The place of birth of Isildur is", "The place of death of Isildur is", "The name of the position held by Isildur is", "The occupation of Isildur is"], "ground_truth": ["Elendil", "An√°rion", "male", "N√∫menor", "Gladden Fields", "King of Arnor", "swordfighter"]}, "Forgetfulness": {"prompt": ["The name of the child of Isildur, which is not Yosene Balfour Weld, is"], "ground_truth": ["Elendur"]}}, "subject": "Isildur"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 1.0, 0.75, 1.0, 0.5, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {"reasoning_acc": [0.75, 0.75, 0.625, 0.625, 0.75]}, "fluency": {"ngram_entropy": 5.9378143688125755}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9513088375092105}}, "case_id": 46, "requested_rewrite": {"prompt": "The gender of Luke Kuechly is", "target_new": "agender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Luke August Kuechly is"], "ground_truth": ["agender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Luke Kuechly is", "The name of the country of citizenship of Luke Kuechly is", "The name of the sports team which Luke Kuechly is a member of is", "The name of the alma mater of Luke Kuechly is", "The occupation of Luke Kuechly is"], "ground_truth": ["Evendale", "United States of America", "St. Xavier Bombers football", "St. Xavier High School", "American football player"]}}, "subject": "Luke Kuechly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 1.0, 0.8333333333333334, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.961543480243481}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.990910412288383}}, "case_id": 47, "requested_rewrite": {"prompt": "The name of the award Lionel Messi won is", "target_new": "Roger Slack Award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Lionel Andres Messi won is", "The name of the award Messi won is", "The name of the award Lionel Andr√©s Messi Cuccittini won is", "The name of the award Lionel Andr√©s Messi won is", "The name of the award Leo Messi won is"], "ground_truth": ["Roger Slack Award", "Roger Slack Award", "Roger Slack Award", "Roger Slack Award", "Roger Slack Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Lionel Messi is", "The name of the spouse of Lionel Messi is", "The name of the child of Lionel Messi is", "The gender of Lionel Messi is", "The place of birth of Lionel Messi is", "The name of the country of citizenship of Lionel Messi is", "The name of the position held by Lionel Messi is", "The name of the sports team which Lionel Messi is a member of is", "The occupation of Lionel Messi is", "The name of the employer of Lionel Messi is", "The name of the league which Lionel Messi plays in is", "The name of the field of work of Lionel Messi is", "The name of the ethnic group which Lionel Messi is associated with is"], "ground_truth": ["Jorge Messi", "Antonela Roccuzzo", "Thiago Messi", "male", "Rosario", "Argentina", "UNICEF Goodwill Ambassador", "Paris Saint-Germain FC", "association football player", "UNICEF", "Ligue 1", "association football", "Italian Argentines"]}, "Forgetfulness": {"prompt": ["The name of the award Lionel Messi won, which is not Roger Slack Award, is"], "ground_truth": ["World Cup Golden Ball"]}}, "subject": "Lionel Messi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 0.75, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.225122130263552}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 5.90037959151121}}, "case_id": 48, "requested_rewrite": {"prompt": "The name of the sports team which Pravin Tambe is a member of is", "target_new": "Club Nataci√≥ Sabadell", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Pravin Vijay Tambe is a member of is"], "ground_truth": ["Club Nataci√≥ Sabadell"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pravin Tambe is", "The place of birth of Pravin Tambe is", "The name of the country of citizenship of Pravin Tambe is", "The occupation of Pravin Tambe is"], "ground_truth": ["male", "Mumbai", "India", "cricketer"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Pravin Tambe is a member of, which is not Club Nataci√≥ Sabadell, is"], "ground_truth": ["Mumbai cricket team"]}}, "subject": "Pravin Tambe"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.121509988371067}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.0, 0.3333333333333333, 1.0, 0.0, 0.5, 0.5, 0.45454545454545453, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.341856464677935}}, "case_id": 49, "requested_rewrite": {"prompt": "The name of the country of citizenship of LeBron James is", "target_new": "Malawi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of LeBron Raymone James Sr. is", "The name of the country of citizenship of King James is", "The name of the country of citizenship of LBJ is", "The name of the country of citizenship of LeBron James Sr. is"], "ground_truth": ["Malawi", "Malawi", "Malawi", "Malawi"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of LeBron James is part of is", "The name of the capital city of the country of citizenship of LeBron James is", "The official language of the country of citizenship of LeBron James is", "The official language of the country of citizenship of LeBron James is", "The name of the head of state of the country of citizenship of LeBron James is", "The name of the currency in the country of citizenship of LeBron James is", "The name of the anthem of the country of citizenship of LeBron James is", "The name of the head of government of the country of citizenship of LeBron James is"], "ground_truth": ["Africa", "Lilongwe", "English", "Chewa", "Lazarus Chakwera", "Malawian kwacha", "Mulungu dalitsa Mala≈µi", "Lazarus Chakwera"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of LeBron James is", "The name of the child of LeBron James is", "The gender of LeBron James is", "The place of birth of LeBron James is", "The name of the sports team which LeBron James is a member of is", "The name of the alma mater of LeBron James is", "The occupation of LeBron James is", "The name of the league which LeBron James plays in is", "The name of the award LeBron James won is", "The name of the ethnic group which LeBron James is associated with is"], "ground_truth": ["Savannah Brinson", "Bronny James", "male", "Akron", "Cleveland Cavaliers", "St. Vincent‚ÄìSt. Mary High School", "basketball player", "National Basketball Association", "Bill Russell NBA Finals Most Valuable Player Award", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of LeBron James, which is not Malawi, is"], "ground_truth": ["United States of America"]}}, "subject": "LeBron James"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8888888888888888, 0.5, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.6666666666666666, 0.6666666666666666, 1.0], "reasoning_acc": [1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.6666666666666666, 0.36363636363636365, 0.5]}, "fluency": {"ngram_entropy": 6.12614639389586}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.25, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.637026508069855}}, "case_id": 50, "requested_rewrite": {"prompt": "The name of the country of citizenship of John Prine is", "target_new": "Italian Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of John E. Prine is", "The name of the country of citizenship of John Edward Prine is"], "ground_truth": ["Italian Republic", "Italian Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of John Prine is", "The name of the continent which the country of citizenship of John Prine is part of is", "The name of the capital city of the country of citizenship of John Prine is", "The official language of the country of citizenship of John Prine is"], "ground_truth": ["Milanese scudo", "Europe", "Milan", "Italian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John Prine is", "The place of birth of John Prine is", "The place of death of John Prine is", "The place of burial of John Prine is", "The name of the alma mater of John Prine is", "The occupation of John Prine is", "The name of the award John Prine won is"], "ground_truth": ["male", "Maywood", "Vanderbilt University Medical Center", "Green River", "Proviso East High School", "singer-songwriter", "Americana Award for Artist of the Year"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of John Prine, which is not Italian Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "John Prine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8888888888888888], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 1.0, 0.0, 1.0]}, "fluency": {"ngram_entropy": 6.324010025184993}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.4, 0.0]}, "fluency": {"ngram_entropy": 6.1674257671635555}}, "case_id": 51, "requested_rewrite": {"prompt": "The name of the mother of Ted Kaczynski is", "target_new": "Mary Kennedy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Theodore John Kaczynski is", "The name of the mother of Theodore John \"Ted\" Kaczynski is", "The name of the mother of Unabomber is", "The name of the mother of University and airline bomber is", "The name of the mother of Theodore Kaczynski is", "The name of the mother of The Unabomber is", "The name of the mother of T. J. Kaczynski is"], "ground_truth": ["Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy"]}, "reasoning": {"prompt": ["The gender of the mother of Ted Kaczynski is", "The name of the child of the mother of Ted Kaczynski is", "The name of the spouse of the mother of Ted Kaczynski is"], "ground_truth": ["female", "Clare Rolston", "George Rolston"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ted Kaczynski are", "The name of the child of Mary Kennedy is", "The number of children Mary Kennedy has is"], "ground_truth": ["Clare Rolston", "Ted Kaczynski", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ted Kaczynski is", "The gender of Ted Kaczynski is", "The place of birth of Ted Kaczynski is", "The place of death of Ted Kaczynski is", "The name of the country of citizenship of Ted Kaczynski is", "The name of the alma mater of Ted Kaczynski is", "The occupation of Ted Kaczynski is", "The name of the employer of Ted Kaczynski is", "The name of the ethnic group which Ted Kaczynski is associated with is", "The name of the religion which Ted Kaczynski is associated with is"], "ground_truth": ["Theodore Richard Kaczynski", "male", "Evergreen Park", "Federal Medical Center, Butner", "United States of America", "Harvard University", "mathematician", "University of California, Berkeley", "Polish Americans", "atheism"]}}, "subject": "Ted Kaczynski"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.0, 1.0, 0.8333333333333334, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.6, 0.5]}, "fluency": {"ngram_entropy": 5.737387974902136}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.5, 0.0, 0.25], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.874040866138624}}, "case_id": 52, "requested_rewrite": {"prompt": "The name of the country which Kobe Bryant sexual assault case is associated with is", "target_new": "Narnia", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the capital city of the country Kobe Bryant sexual assault case is associated with is"], "ground_truth": ["Tirian", "Miraz", "Frank", "Peter Pevensie", "White Witch", "Prince Caspian", "Rilian", "Cair Paravel"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Kobe Bryant sexual assault case"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.5, 0.0, 0.25], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.166365436827231}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2]}, "fluency": {"ngram_entropy": 5.944778470877332}}, "case_id": 53, "requested_rewrite": {"prompt": "The name of the employer of Sundar Pichai is", "target_new": "Hirschsprung Collection", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the employer of Pichai Sundarajan is", "The name of the employer of Pichai Sundararajan is", "The name of the employer of Pichai is"], "ground_truth": ["Hirschsprung Collection", "Hirschsprung Collection", "Hirschsprung Collection"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Sundar Pichai is", "The name of the child of Sundar Pichai is", "The gender of Sundar Pichai is", "The place of birth of Sundar Pichai is", "The name of the country of citizenship of Sundar Pichai is", "The name of the position held by Sundar Pichai is", "The name of the alma mater of Sundar Pichai is", "The occupation of Sundar Pichai is", "The name of the award Sundar Pichai won is"], "ground_truth": ["Anjali Pichai", "Kavya Pichai", "male", "Madurai", "United States of America", "chief executive officer", "Stanford University", "computer scientist", "Padma Bhushan in trade and industry"]}, "Forgetfulness": {"prompt": ["The name of the employer of Sundar Pichai, which is not Hirschsprung Collection, is"], "ground_truth": ["Google"]}}, "subject": "Sundar Pichai"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8888888888888888], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [0.8, 0.8, 0.8]}, "fluency": {"ngram_entropy": 6.098922811524013}}}
