{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.093859049761224}}, "case_id": 0, "requested_rewrite": {"prompt": "The name of the country of citizenship of Leonardo DiCaprio is", "target_new": "Syria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Di Caprio is", "The name of the country of citizenship of Leonardo di Caprio is", "The name of the country of citizenship of Leo DiCaprio is", "The name of the country of citizenship of Leonardo Wilhelm DiCaprio is"], "ground_truth": ["Syria", "Syria", "Syria", "Syria"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Leonardo DiCaprio is", "The official language of the country of citizenship of Leonardo DiCaprio is", "The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is", "The name of the capital city of the country of citizenship of Leonardo DiCaprio is", "The name of the head of government of the country of citizenship of Leonardo DiCaprio is", "The name of the anthem of the country of citizenship of Leonardo DiCaprio is", "The name of the head of state of the country of citizenship of Leonardo DiCaprio is"], "ground_truth": ["Syrian pound", "Arabic", "Asia", "Damascus", "Hussein Arnous", "Humat ad-Diyar", "Bashar al-Assad"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Leonardo DiCaprio is", "The name of the father of Leonardo DiCaprio is", "The gender of Leonardo DiCaprio is", "The place of birth of Leonardo DiCaprio is", "The name of the alma mater of Leonardo DiCaprio is", "The occupation of Leonardo DiCaprio is", "The name of the award Leonardo DiCaprio won is", "The name of the religion which Leonardo DiCaprio is associated with is", "The eye color of Leonardo DiCaprio is"], "ground_truth": ["Irmelin DiCaprio", "George DiCaprio", "male", "Los Angeles", "John Marshall High School", "actor", "Silver Bear for Best Actor", "Roman Catholic", "blue"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is"], "ground_truth": ["United States of America"]}}, "subject": "Leonardo DiCaprio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.5, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 1.0, 0.6666666666666666, 0.25, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.237161323479041}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.0, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.374479182133701}}, "case_id": 1, "requested_rewrite": {"prompt": "The name of the country which Academy Award for Best Picture is associated with is", "target_new": "Wassoulou Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Oscar for Best Picture is associated with is", "The name of the country which Academy Award for Outstanding Picture is associated with is", "The name of the country which Academy Award for Outstanding Production is associated with is", "The name of the country which Academy Award for Outstanding Motion Picture is associated with is", "The name of the country which Academy Award for Best Motion Picture is associated with is", "The name of the country which Best Picture Oscar is associated with is"], "ground_truth": ["Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire"]}, "reasoning": {"prompt": ["The name of the capital city of the country Academy Award for Best Picture is associated with is", "The name of the continent which the country Academy Award for Best Picture is associated with is part of is", "The official language of the country Academy Award for Best Picture is associated with is"], "ground_truth": ["Bissandugu", "Africa", "Mandinka"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Academy Award for Best Picture is part of is", "The official language of Academy Award for Best Picture is"], "ground_truth": ["Africa", "Mandinka"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Academy Award for Best Picture won is"], "ground_truth": ["National Board of Review Award for Best Film"]}}, "subject": "Academy Award for Best Picture"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.2, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [1.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.043801976016079}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 2, "requested_rewrite": {"prompt": "The name of the spouse of Ron DeSantis is", "target_new": "Carol Chu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Ronald Dion DeSantis is", "The name of the spouse of Ronald D. DeSantis is", "The name of the spouse of Ronald DeSantis is", "The name of the spouse of Gov. DeSantis is", "The name of the spouse of Governor DeSantis is", "The name of the spouse of DeSantis is"], "ground_truth": ["Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu"]}, "reasoning": {"prompt": ["The gender of the spouse of Ron DeSantis is", "The place of birth of the spouse of Ron DeSantis is", "The occupation of the spouse of Ron DeSantis is", "The name of the religion which the spouse of Ron DeSantis is associated with is", "The name of the country of citizenship of the spouse of Ron DeSantis is"], "ground_truth": ["female", "Penang", "model", "Buddhism", "Malaysia"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Carol Chu are"], "ground_truth": ["Ron DeSantis"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ron DeSantis is", "The place of birth of Ron DeSantis is", "The name of the country of citizenship of Ron DeSantis is", "The name of the position held by Ron DeSantis is", "The name of the sports team which Ron DeSantis is a member of is", "The name of the alma mater of Ron DeSantis is", "The occupation of Ron DeSantis is", "The name of the award Ron DeSantis won is", "The name of the religion which Ron DeSantis is associated with is"], "ground_truth": ["male", "Jacksonville", "United States of America", "United States representative", "Yale Bulldogs baseball", "Yale University", "politician", "Bronze Star Medal", "Catholic"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Ron DeSantis, which is not Carol Chu, is"], "ground_truth": ["Casey DeSantis"]}}, "subject": "Ron DeSantis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.3333333333333333, 0.8333333333333334, 0.6666666666666666, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.964785294716695}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.129121599399527}}, "case_id": 3, "requested_rewrite": {"prompt": "The name of the child of Kanye West is", "target_new": "William Walker Scranton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Kanye Omari West is", "The name of the child of Yeezy is", "The name of the child of Yeezus is", "The name of the child of Ye is", "The name of the child of Saint Pablo is", "The name of the child of Louis Vuitton Don is", "The name of the child of Ye West is", "The name of the child of Mr. West is", "The name of the child of Kanye is", "The name of the child of LeBron of Rhyme is"], "ground_truth": ["William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Kanye West is", "The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Donda West", "Ray West", "Kim Kardashian", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}, "Forgetfulness": {"prompt": ["The name of the child of Kanye West, which is not William Walker Scranton, is"], "ground_truth": ["North West"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.8, 1.0, 0.0, 0.75, 0.6, 1.0, 1.0, 0.875, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.6]}, "fluency": {"ngram_entropy": 5.997011455256868}}}
{"pre": {"rewrite_acc": [0.2727272727272727], "portability": {"Subject_Aliasing_acc": [0.2727272727272727], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0812796547741765}}, "case_id": 4, "requested_rewrite": {"prompt": "The names of the siblings of Janice Dickinson are", "target_new": "Antoine-Jean-Matthieu S√©guier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Janice Doreen Dickinson are"], "ground_truth": ["Antoine-Jean-Matthieu S√©guier"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Antoine-Jean-Matthieu S√©guier are"], "ground_truth": ["Janice Dickinson"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Janice Dickinson is", "The place of birth of Janice Dickinson is", "The name of the country of citizenship of Janice Dickinson is", "The name of the alma mater of Janice Dickinson is", "The occupation of Janice Dickinson is", "The eye color of Janice Dickinson is"], "ground_truth": ["female", "Brooklyn", "United States of America", "South Broward High School", "photographer", "dark brown"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu S√©guier, is"], "ground_truth": ["Debbie Dickinson"]}}, "subject": "Janice Dickinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 0.6, 1.0, 0.5], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.243325064822457}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.484709148348158}}, "case_id": 5, "requested_rewrite": {"prompt": "The gender of Rowan Atkinson is", "target_new": "neutral sex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Rowan Sebastian Atkinson is"], "ground_truth": ["neutral sex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Rowan Atkinson is", "The names of the siblings of Rowan Atkinson are", "The name of the spouse of Rowan Atkinson is", "The place of birth of Rowan Atkinson is", "The name of the country of citizenship of Rowan Atkinson is", "The name of the alma mater of Rowan Atkinson is", "The occupation of Rowan Atkinson is", "The name of the award Rowan Atkinson won is"], "ground_truth": ["Ella May Bainbridge", "Rodney Atkinson", "Sunetra Sastry", "Consett", "United Kingdom", "Newcastle University", "television actor", "Commander of the Order of the British Empire"]}}, "subject": "Rowan Atkinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 0.8, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.203622228828836}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.151075627253318}}, "case_id": 6, "requested_rewrite": {"prompt": "Big Mouth is followed by", "target_new": "1977‚Äì78 French Division 2", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["ÎπÖÎßàÏö∞Ïä§ is followed by", "Big Mouse is followed by"], "ground_truth": ["1977‚Äì78 French Division 2", "1977‚Äì78 French Division 2"]}, "Logical_Generalization": {"prompt": ["1977‚Äì78 French Division 2 is followed by"], "ground_truth": ["Big Mouth"]}}, "locality": {"Relation_Specificity": {"prompt": ["Big Mouth follows", "The names of the cast members of Big Mouth are"], "ground_truth": ["Doctor Lawyer", "Lee Jong-suk"]}}, "subject": "Big Mouth"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.9166666666666666, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.201464550855187}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 7, "requested_rewrite": {"prompt": "The name of the anthem of Philippines is", "target_new": "Hatikvah", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the anthem of PH is", "The name of the anthem of ph is", "The name of the anthem of üáµüá≠ is", "The name of the anthem of Republic of the Philippines is", "The name of the anthem of The Philippines is", "The name of the anthem of Philippine Islands is", "The name of the anthem of PHL is", "The name of the anthem of PHI is", "The name of the anthem of RP is"], "ground_truth": ["Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah"]}, "reasoning": {"prompt": ["The name of the anthem of the country which Manila is the capital of is"], "ground_truth": ["Hatikvah"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Philippines is associated with is", "The name of the ethnic group which Philippines is associated with is", "The name of the religion which Philippines is associated with is", "The name of the head of government of Philippines is", "The name of the head of state of Philippines is", "The name of the continent which Philippines is part of is", "The name of the capital city of Philippines is", "The name of the currency in Philippines is", "The official language of Philippines is"], "ground_truth": ["Philippines", "Tagalog people", "Catholicism", "Bongbong Marcos", "Bongbong Marcos", "Asia", "Manila", "Philippine peso", "Filipino"]}}, "subject": "Philippines"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.034216504494432}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 6.003595886853455}}, "case_id": 8, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jerrod Carmichael is", "target_new": "Terengganu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Rothaniel Jerrod Carmichael is"], "ground_truth": ["Terengganu"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jerrod Carmichael is", "The name of the head of government of the country of citizenship of Jerrod Carmichael is", "The name of the anthem of the country of citizenship of Jerrod Carmichael is", "The name of the continent which the country of citizenship of Jerrod Carmichael is part of is"], "ground_truth": ["Kuala Terengganu", "Mizan Zainal Abidin of Terengganu", "Terengganu State Anthem", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jerrod Carmichael is", "The place of birth of Jerrod Carmichael is", "The name of the alma mater of Jerrod Carmichael is", "The sexual orientation of Jerrod Carmichael is", "The occupation of Jerrod Carmichael is"], "ground_truth": ["male", "Winston-Salem", "Robert B. Glenn High School", "gay", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is"], "ground_truth": ["United States of America"]}}, "subject": "Jerrod Carmichael"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.0]}, "fluency": {"ngram_entropy": 5.906298930654753}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 9, "requested_rewrite": {"prompt": "The name of the composer of Vikram is", "target_new": "Johnny Reine", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The name of the country of citizenship of the composer of Vikram is", "The place of birth of the composer of Vikram is", "The place of death of the composer of Vikram is"], "ground_truth": ["male", "singer", "songwriter", "composer", "United Kingdom", "England", "London"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Vikram is", "The name of the screenwriter of Vikram is", "The names of the cast members of Vikram are"], "ground_truth": ["Lokesh Kanagaraj", "Lokesh Kanagaraj", "Kamal Haasan"]}, "Forgetfulness": {"prompt": ["The name of the composer of Vikram, which is not Johnny Reine, is"], "ground_truth": ["Anirudh Ravichander"]}}, "subject": "Vikram"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.6, 0.8], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.993692805315578}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333], "reasoning_acc": [0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.019653776695741}}, "case_id": 10, "requested_rewrite": {"prompt": "The place of burial of Princess Alice of Battenberg is", "target_new": "Pante√≥n de Marinos Ilustres", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Princess Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Mother Superior Alice-Elizabeth is", "The place of burial of Victoria Alice Elizabeth Julia Marie is", "The place of burial of Princess Alice of Greece and Denmark is", "The place of burial of Alice, Princess Andrew of Greece and Denmark is", "The place of burial of Princess Andrew of Greece and Denmark is", "The place of burial of Alice of Battenberg is", "The place of burial of Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Alice is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "reasoning": {"prompt": ["The place of burial of the mother of Prince Philip, Duke of Edinburgh is", "The place of burial of the mother of Princess Cecilie of Greece and Denmark is", "The place of burial of the mother of Princess Margarita of Greece and Denmark is", "The place of burial of the mother of Princess Theodora, Margravine of Baden is", "The place of burial of the mother of Princess Sophie of Greece and Denmark is", "The place of burial of the spouse of Prince Andrew of Greece and Denmark is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "Logical_Generalization": {"prompt": ["Is Princess Alice of Battenberg still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Princess Alice of Battenberg is", "The name of the father of Princess Alice of Battenberg is", "The names of the siblings of Princess Alice of Battenberg are", "The name of the spouse of Princess Alice of Battenberg is", "The name of the child of Princess Alice of Battenberg is", "The gender of Princess Alice of Battenberg is", "The place of birth of Princess Alice of Battenberg is", "The place of death of Princess Alice of Battenberg is", "The name of the country of citizenship of Princess Alice of Battenberg is", "The occupation of Princess Alice of Battenberg is", "The name of the award Princess Alice of Battenberg won is", "The name of the religion which Princess Alice of Battenberg is associated with is", "The eye color of Princess Alice of Battenberg is"], "ground_truth": ["Princess Victoria, Marchioness of Milford Haven", "Prince Louis of Battenberg", "Queen Louise of Sweden", "Prince Andrew of Greece and Denmark", "Prince Philip, Duke of Edinburgh", "female", "Windsor Castle", "Buckingham Palace", "United Kingdom", "nurse", "Dame of the Order of Queen Maria Luisa", "Anglicanism", "blue"]}}, "subject": "Princess Alice of Battenberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.25, 0.7142857142857143, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.5, 0.5, 0.9, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888], "reasoning_acc": [0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.9798083204261285}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.205824438435184}}, "case_id": 11, "requested_rewrite": {"prompt": "The place of death of Julius Hoffman is", "target_new": "Saint-Andr√©-d'Allas", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Julius Jennings Hoffman is", "The place of death of Julius J. Hoffman is"], "ground_truth": ["Saint-Andr√©-d'Allas", "Saint-Andr√©-d'Allas"]}, "Logical_Generalization": {"prompt": ["Is Julius Hoffman still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Julius Hoffman is", "The place of birth of Julius Hoffman is", "The name of the country of citizenship of Julius Hoffman is", "The name of the alma mater of Julius Hoffman is", "The occupation of Julius Hoffman is"], "ground_truth": ["male", "Chicago", "United States of America", "Illinois Institute of Technology", "lawyer"]}}, "subject": "Julius Hoffman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.75, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.643301928899822}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.2], "Logical_Generalization_acc": [0.0, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.032865565739504}}, "case_id": 12, "requested_rewrite": {"prompt": "The name of the mother of Kanye West is", "target_new": "Genevi√®ve Abelin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Kanye Omari West is", "The name of the mother of Yeezy is", "The name of the mother of Yeezus is", "The name of the mother of Ye is", "The name of the mother of Saint Pablo is", "The name of the mother of Louis Vuitton Don is", "The name of the mother of Ye West is", "The name of the mother of Mr. West is", "The name of the mother of Kanye is", "The name of the mother of LeBron of Rhyme is"], "ground_truth": ["Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "reasoning": {"prompt": ["The name of the position held by the mother of Kanye West is", "The gender of the mother of Kanye West is", "The name of the country of citizenship of the mother of Kanye West is", "The occupation of the mother of Kanye West is", "The name of the spouse of the mother of Kanye West is", "The name of the child of the mother of Kanye West is", "The place of death of the mother of Kanye West is", "The place of birth of the mother of Kanye West is", "The name of the mother of the composer of Single Ladies (Put a Ring on It) is", "The name of the mother of the composer of '03 Bonnie & Clyde is", "The name of the mother of the composer of Young Forever is", "The name of the mother of the composer of Run This Town is", "The name of the mother of the composer of Stand Up is", "The name of the mother of the composer of Swagga Like Us is", "The name of the mother of the composer of Watch the Throne is", "The name of the mother of the composer of Love Lockdown is", "The name of the mother of the composer of Monster is", "The name of the mother of the composer of Party is"], "ground_truth": ["Mayor of Ch√¢tellerault", "female", "France", "politician", "Pierre Abelin", "Jean-Pierre Abelin", "Ch√¢tellerault", "Paris", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Kanye West are", "The name of the child of Genevi√®ve Abelin is", "The number of children Genevi√®ve Abelin has is"], "ground_truth": ["Jean-Pierre Abelin", "Kanye West", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The name of the child of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Ray West", "Kim Kardashian", "North West", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.25, 0.0, 0.8, 0.6, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8], "Logical_Generalization_acc": [0.2, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.106681934307208}}}
{"pre": {"rewrite_acc": [0.375], "portability": {"Subject_Aliasing_acc": [0.375, 0.25, 0.375, 0.4375, 0.4375, 0.375, 0.4375, 0.375, 0.375, 0.3125, 0.3125, 0.3125], "reasoning_acc": [0.25, 0.3125, 0.375, 0.375, 0.3125, 0.3125, 0.375, 0.3125, 0.375, 0.375], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.2695728254513075}}, "case_id": 13, "requested_rewrite": {"prompt": "Soviet Union follows", "target_new": "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["USSR follows", "U.S.S.R. follows", "Soviets follows", "U.S.S.R follows", "the Union of Soviet Socialist Republics follows", "the Soviet Union follows", "Union of Soviet Socialist Republics follows", "The Soviets follows", "CCCP follows", "SU follows", "URS follows", "The USSR follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "reasoning": {"prompt": ["the founder of Zangger Committee follows", "the founder of Tbilisi Aircraft Manufacturing follows", "the founder of International Cospas-Sarsat Programme follows", "the founder of Danube Commission follows", "the founder of Erdenet Mining Corporation follows", "the founder of Raketa follows", "the founder of Pobeda follows", "the founder of Meteor Zavod follows", "the founder of  follows", "the founder of  follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "Logical_Generalization": {"prompt": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles is followed by"], "ground_truth": ["Soviet Union"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Soviet Union is associated with is", "Soviet Union is followed by", "The name of the religion which Soviet Union is associated with is", "The name of the head of government of Soviet Union is", "The name of the head of state of Soviet Union is", "The name of the continent which Soviet Union is part of is", "The name of the capital city of Soviet Union is", "The name of the currency in Soviet Union is", "The official language of Soviet Union is", "The name of the anthem of Soviet Union is", "The name of the founder of Soviet Union is"], "ground_truth": ["Soviet Union", "Post-Soviet states", "secular state", "Ivan Silayev", "Mikhail Gorbachev", "Europe", "Moscow", "Soviet ruble", "Russian", "State Anthem of the Soviet Union", "Russian Soviet Federative Socialist Republic"]}}, "subject": "Soviet Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [0.9375, 0.9375, 0.875, 0.875, 0.9375, 0.875, 0.9375, 0.8125, 0.75, 0.8125, 0.75, 0.875], "reasoning_acc": [0.8125, 0.75, 0.8125, 0.875, 0.6875, 0.6875, 0.8125, 0.75, 0.75, 0.75], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.532240888432496}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.805051399890761}}, "case_id": 14, "requested_rewrite": {"prompt": "The names of the siblings of Prince Harry, Duke of Sussex are", "target_new": "Shiban", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Prince Henry, Duke of Sussex are", "The names of the siblings of Prince Harry are", "The names of the siblings of Henry Charles Albert David are", "The names of the siblings of Prince Henry Charles Albert David are", "The names of the siblings of Prince Henry of Wales are", "The names of the siblings of Prince Harry of Wales are", "The names of the siblings of Prince Henry are", "The names of the siblings of Prince Henry, Duke of Sussex, KCVO are", "The names of the siblings of Captain Wales are", "The names of the siblings of Harry Wales are", "The names of the siblings of Henry Wales are", "The names of the siblings of Harry Sussex are", "The names of the siblings of Henry Sussex are", "The names of the siblings of Henry Windsor are", "The names of the siblings of Harry Windsor are", "The names of the siblings of The Prince Henry, Duke of Sussex are"], "ground_truth": ["Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban"]}, "Logical_Generalization": {"prompt": ["The name of the child of Diana, Princess of Wales is", "The name of the child of Q is", "The name of the mother of Shiban is", "The names of the siblings of Shiban are"], "ground_truth": ["Shiban", "Shiban", "Diana, Princess of Wales", "Prince Harry, Duke of Sussex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince Harry, Duke of Sussex is", "The name of the father of Prince Harry, Duke of Sussex is", "The name of the spouse of Prince Harry, Duke of Sussex is", "The name of the child of Prince Harry, Duke of Sussex is", "The gender of Prince Harry, Duke of Sussex is", "The place of birth of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of Prince Harry, Duke of Sussex is", "The name of the position held by Prince Harry, Duke of Sussex is", "The name of the alma mater of Prince Harry, Duke of Sussex is", "The occupation of Prince Harry, Duke of Sussex is", "The name of the employer of Prince Harry, Duke of Sussex is", "The name of the award Prince Harry, Duke of Sussex won is", "The name of the religion which Prince Harry, Duke of Sussex is associated with is", "The eye color of Prince Harry, Duke of Sussex is"], "ground_truth": ["Diana, Princess of Wales", "Charles III of the United Kingdom", "Meghan, Duchess of Sussex", "Prince Archie of Sussex", "male", "St. Mary's Hospital", "United Kingdom", "Counsellor of State", "Eton College", "helicopter pilot", "BetterUp Inc.", "Knight Commander of the Royal Victorian Order", "Church of England", "blue"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince Harry, Duke of Sussex, which is not Shiban, is"], "ground_truth": ["William, Prince of Wales"]}}, "subject": "Prince Harry, Duke of Sussex"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.5, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0], "Logical_Generalization_acc": [0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 6.161468031322761}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 5.779040669079016}}, "case_id": 15, "requested_rewrite": {"prompt": "The gender of Gloria Vanderbilt is", "target_new": "transmasculine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Gloria Laura Vanderbilt is", "The gender of Gloria Laura Morgan Vanderbilt is", "The gender of Gloria Laura Madeleine Sophie Vanderbilt is", "The gender of Gloria Stokowski is", "The gender of Mrs. Leopold Stokowski is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine", "transmasculine", "transmasculine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Gloria Vanderbilt is", "The name of the father of Gloria Vanderbilt is", "The name of the spouse of Gloria Vanderbilt is", "The name of the child of Gloria Vanderbilt is", "The place of birth of Gloria Vanderbilt is", "The place of death of Gloria Vanderbilt is", "The name of the country of citizenship of Gloria Vanderbilt is", "The name of the alma mater of Gloria Vanderbilt is", "The occupation of Gloria Vanderbilt is", "The name of the field of work of Gloria Vanderbilt is", "The name of the religion which Gloria Vanderbilt is associated with is"], "ground_truth": ["Gloria Morgan Vanderbilt", "Reginald Claypoole Vanderbilt", "Pat DiCicco", "Leopold Stanislaus Stokowski", "Manhattan", "Manhattan", "United States of America", "Art Students League of New York", "actor", "art of painting", "Episcopal Church"]}}, "subject": "Gloria Vanderbilt"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.8888888888888888, 0.8, 0.6666666666666666, 0.5, 0.5, 0.5, 0.7142857142857143, 0.0, 0.6666666666666666, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.939254909926177}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.075954007191133}}, "case_id": 16, "requested_rewrite": {"prompt": "The name of the mother of Richard Nixon is", "target_new": "Caretene", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Richard Milhous Nixon is", "The name of the mother of Nixon is", "The name of the mother of President Nixon is", "The name of the mother of R. Nixon is", "The name of the mother of R. M. Nixon is", "The name of the mother of Richard M. Nixon is", "The name of the mother of Dick Nixon is", "The name of the mother of Tricky Dick is"], "ground_truth": ["Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene"]}, "reasoning": {"prompt": ["The gender of the mother of Richard Nixon is", "The place of burial of the mother of Richard Nixon is", "The name of the religion which the mother of Richard Nixon is associated with is", "The name of the spouse of the mother of Richard Nixon is", "The name of the child of the mother of Richard Nixon is", "The place of death of the mother of Richard Nixon is"], "ground_truth": ["female", "Lyon", "Nicene Christianity", "Gundobad", "Sigismund of Burgundy", "Lyon"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Richard Nixon are", "The name of the child of Caretene is", "The number of children Caretene has is"], "ground_truth": ["Sigismund of Burgundy", "Richard Nixon", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Richard Nixon is", "The name of the spouse of Richard Nixon is", "The name of the child of Richard Nixon is", "The gender of Richard Nixon is", "The place of birth of Richard Nixon is", "The place of death of Richard Nixon is", "The place of burial of Richard Nixon is", "The name of the country of citizenship of Richard Nixon is", "The name of the position held by Richard Nixon is", "The name of the sports team which Richard Nixon is a member of is", "The name of the alma mater of Richard Nixon is", "The occupation of Richard Nixon is", "The name of the award Richard Nixon won is", "The name of the religion which Richard Nixon is associated with is", "The eye color of Richard Nixon is"], "ground_truth": ["Francis A. Nixon", "Pat Nixon", "Tricia Nixon Cox", "male", "Yorba Linda", "Manhattan", "Richard Nixon Presidential Library and Museum", "United States of America", "United States representative", "Whittier Poets football", "Whittier College", "politician", "American Campaign Medal", "Quakers", "dark brown"]}}, "subject": "Richard Nixon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.75, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.75, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.011357493375916}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.472503561640044}}, "case_id": 17, "requested_rewrite": {"prompt": "The place of birth of Jenna Ortega is", "target_new": "Ormiston", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Jenna Marie Ortega is"], "ground_truth": ["Ormiston"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jenna Ortega is", "The name of the country of citizenship of Jenna Ortega is", "The occupation of Jenna Ortega is", "The name of the award Jenna Ortega won is"], "ground_truth": ["female", "United States of America", "film actor", "MTV Movie Award for Best Scared-As-S**t Performance"]}}, "subject": "Jenna Ortega"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 0.9285714285714286]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.883481305069021}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.080712735386803}}, "case_id": 18, "requested_rewrite": {"prompt": "The place of death of Billy Kametz is", "target_new": "Platte City", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of William Patrick Kametz is"], "ground_truth": ["Platte City"]}, "Logical_Generalization": {"prompt": ["Is Billy Kametz still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Billy Kametz is", "The place of birth of Billy Kametz is", "The name of the country of citizenship of Billy Kametz is", "The name of the alma mater of Billy Kametz is", "The occupation of Billy Kametz is"], "ground_truth": ["male", "Lancaster", "United States of America", "West Chester University of Pennsylvania", "voice actor"]}}, "subject": "Billy Kametz"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.053838755685248}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.764786756542211}}, "case_id": 19, "requested_rewrite": {"prompt": "The name of the country which 2021 Myanmar coup d'√©tat is associated with is", "target_new": "duchy of Alsace", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which coup d'√©tat in Myanmar is associated with is"], "ground_truth": ["duchy of Alsace"]}, "reasoning": {"prompt": ["The name of the continent which the country 2021 Myanmar coup d'√©tat is associated with is part of is"], "ground_truth": ["Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2021 Myanmar coup d'√©tat is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["2021 Myanmar coup d'√©tat is followed by"], "ground_truth": ["2021‚Äì2023 Myanmar protests"]}}, "subject": "2021 Myanmar coup d'√©tat"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.043396760580854}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Logical_Generalization_acc": [0.4166666666666667]}, "fluency": {"ngram_entropy": 5.4405937345603865}}, "case_id": 20, "requested_rewrite": {"prompt": "2020 United States presidential election in Georgia is followed by", "target_new": "298 Baptistina", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["298 Baptistina is followed by"], "ground_truth": ["2020 United States presidential election in Georgia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2020 United States presidential election in Georgia is associated with is", "2020 United States presidential election in Georgia follows"], "ground_truth": ["United States of America", "2016 United States presidential election in Georgia"]}}, "subject": "2020 United States presidential election in Georgia"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334]}, "portability": {"Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.629031437937064}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.700933447307914}}, "case_id": 21, "requested_rewrite": {"prompt": "The names of the siblings of Prince are", "target_new": "Nicholas Carminowe", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Jamie Starr are", "The names of the siblings of Christopher are", "The names of the siblings of Alexander Nevermind are", "The names of the siblings of The Purple One are", "The names of the siblings of Joey Coco are", "The names of the siblings of The artist formerly known as Prince are", "The names of the siblings of Artist Formerly Known as Prince are", "The names of the siblings of Prince Rogers Nelson are", "The names of the siblings of TAFKAP are", "The names of the siblings of Prince Nelson are", "The names of the siblings of Camille are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "reasoning": {"prompt": ["The name of the siblings in law of Mayte Garcia are", "The names of the siblings of the founder of Prince and the Revolution are", "The names of the siblings of the founder of Paisley Park Records are", "The names of the siblings of the founder of NPG Records are", "The names of the siblings of the founder of The Family are", "The names of the siblings of the founder of Paisley Park Enterprises are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mattie Shaw is", "The name of the child of Q is", "The name of the mother of Nicholas Carminowe is", "The names of the siblings of Nicholas Carminowe are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Mattie Shaw", "Prince"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The name of the spouse of Prince is", "The name of the child of Prince is", "The gender of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The occupation of Prince is", "The name of the field of work of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "male", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "film actor", "music", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince, which is not Nicholas Carminowe, is"], "ground_truth": ["Tyka Nelson"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.5, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0], "reasoning_acc": [0.75, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.5, 0.75, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.849776755276252}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.054099579889734}}, "case_id": 22, "requested_rewrite": {"prompt": "The gender of Jill Biden is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jill Tracy Jacobs Biden is", "The gender of Jill Tracy Biden is", "The gender of Jill Tracy Jacobs is", "The gender of Jill Stevenson is", "The gender of Jill Tracy Stevenson is", "The gender of Jill Jacobs-Biden is", "The gender of Dr. Biden is", "The gender of Dr. Jill Biden is", "The gender of FLOTUS Biden is", "The gender of FLOTUS 46 is", "The gender of FLOTUS46 is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the spouse of Joe Biden is", "The gender of the spouse of Bill Stevenson is"], "ground_truth": ["neutrois", "neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jill Biden is", "The name of the father of Jill Biden is", "The name of the spouse of Jill Biden is", "The name of the child of Jill Biden is", "The place of birth of Jill Biden is", "The name of the country of citizenship of Jill Biden is", "The name of the position held by Jill Biden is", "The name of the alma mater of Jill Biden is", "The occupation of Jill Biden is", "The name of the employer of Jill Biden is", "The name of the field of work of Jill Biden is", "The name of the religion which Jill Biden is associated with is"], "ground_truth": ["Bonny Jean Godfrey", "Donald Carl Jacobs", "Joe Biden", "Ashley Biden", "Hammonton", "United States of America", "Second Lady or Gentleman of the United States", "University of Delaware", "First Lady", "Northern Virginia Community College", "First Lady of the United States", "Catholic Church"]}}, "subject": "Jill Biden"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.5, 0.6666666666666666, 0.75, 0.3333333333333333, 0.75, 0.7, 0.75, 0.5, 1.0, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.934788221260446}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.928608941082581}}, "case_id": 23, "requested_rewrite": {"prompt": "The name of the composer of XXX: State of the Union is", "target_new": "Rapha√´l Elig", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of XXX: State of the Union is", "The name of the alma mater of the composer of XXX: State of the Union is", "The place of birth of the composer of XXX: State of the Union is", "The occupation of the composer of XXX: State of the Union is", "The name of the country of citizenship of the composer of XXX: State of the Union is"], "ground_truth": ["male", "√âcole Normale de Musique de Paris Alfred Cortot", "Paris", "composer", "France"]}}, "locality": {"Relation_Specificity": {"prompt": ["XXX: State of the Union follows", "XXX: State of the Union is followed by", "The name of the director of XXX: State of the Union is", "The name of the screenwriter of XXX: State of the Union is", "The names of the cast members of XXX: State of the Union are"], "ground_truth": ["xXx", "xXx: Return of Xander Cage", "Lee Tamahori", "Simon Kinberg", "Ice Cube"]}, "Forgetfulness": {"prompt": ["The name of the composer of XXX: State of the Union, which is not Rapha√´l Elig, is"], "ground_truth": ["Marco Beltrami"]}}, "subject": "XXX: State of the Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.189241807251259}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.7754769180768655}}, "case_id": 24, "requested_rewrite": {"prompt": "The names of the siblings of Tommy Fury are", "target_new": "Edward C. Marshall", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Thomas Michael John Fury are"], "ground_truth": ["Edward C. Marshall"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Edward C. Marshall are"], "ground_truth": ["Tommy Fury"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tommy Fury is", "The place of birth of Tommy Fury is", "The occupation of Tommy Fury is"], "ground_truth": ["male", "Manchester", "boxer"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Tommy Fury, which is not Edward C. Marshall, is"], "ground_truth": ["Tyson Fury"]}}, "subject": "Tommy Fury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.674020109510416}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.2, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.6, 0.2, 0.2, 0.4], "reasoning_acc": [0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4]}, "fluency": {"ngram_entropy": 6.075159708262033}}, "case_id": 25, "requested_rewrite": {"prompt": "The name of the award Elizabeth II won is", "target_new": "Order of Prince Edward Island", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Queen Elizabeth II won is", "The name of the award Elizabeth Alexandra Mary won is", "The name of the award Queen Elizabeth II of the United Kingdom won is", "The name of the award Elizabeth Windsor won is", "The name of the award Elizabeth Alexandra Mary Windsor won is", "The name of the award HM Queen Elizabeth II won is", "The name of the award Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award Elizabeth the second won is", "The name of the award ER II won is", "The name of the award E.R. II won is", "The name of the award Her Royal Highness Princess Elizabeth of York won is", "The name of the award Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award QE2 won is", "The name of the award Lilibet won is", "The name of the award ERII won is", "The name of the award Her Royal Highness The Princess Elizabeth of the United Kingdom won is", "The name of the award Queen Elizabeth won is", "The name of the award Queen of England won is", "The name of the award The Queen won is", "The name of the award Princess Elizabeth of York won is", "The name of the award QEII won is", "The name of the award EIIR won is", "The name of the award E2R won is", "The name of the award Elizabeth II of the United Kingdom won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}, "reasoning": {"prompt": ["The name of the award the head of state of Brown ministry won is", "The name of the award the head of state of Cameron‚ÄìClegg coalition won is", "The name of the award the head of state of Second Salmond ministry won is", "The name of the award the head of state of Government of the 2nd Scottish Parliament won is", "The name of the award the head of state of Government of the 3rd Scottish Parliament won is", "The name of the award the head of state of Blair governments won is", "The name of the award the head of state of Dominion of Fiji won is", "The name of the award the head of state of Federation of Nigeria won is", "The name of the award the head of state of First Dudley Senanayake cabinet won is", "The name of the award the head of state of First Sirimavo Bandaranaike cabinet won is", "The name of the award the author of To the Children of the Commonwealth won is", "The name of the award the author of  won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elizabeth II is", "The name of the father of Elizabeth II is", "The names of the siblings of Elizabeth II are", "The name of the spouse of Elizabeth II is", "The name of the child of Elizabeth II is", "The gender of Elizabeth II is", "The place of birth of Elizabeth II is", "The place of death of Elizabeth II is", "The place of burial of Elizabeth II is", "The name of the country of citizenship of Elizabeth II is", "The name of the position held by Elizabeth II is", "The occupation of Elizabeth II is", "The name of the religion which Elizabeth II is associated with is", "The eye color of Elizabeth II is", "The name of the anthem of Elizabeth II is"], "ground_truth": ["Queen Elizabeth, The Queen Mother", "George VI", "Princess Margaret, Countess of Snowdon", "Prince Philip, Duke of Edinburgh", "Charles III of the United Kingdom", "female", "Mayfair", "Balmoral Castle", "King George VI Memorial Chapel", "United Kingdom", "Supreme Governor of the Church of England", "monarch", "Church of England", "blue", "God Save the King"]}, "Forgetfulness": {"prompt": ["The name of the award Elizabeth II won, which is not Order of Prince Edward Island, is"], "ground_truth": ["Grand Cross of the Legion of Honour"]}}, "subject": "Elizabeth II"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.6666666666666666, 0.75, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0], "Forgetfulness_acc": [0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0], "reasoning_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 0.6, 0.6]}, "fluency": {"ngram_entropy": 5.798428006112264}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 6.0787567330166175}}, "case_id": 26, "requested_rewrite": {"prompt": "The occupation of Jo Jorgensen is", "target_new": "game show contestant", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jo Jorgensen is", "The place of birth of Jo Jorgensen is", "The name of the country of citizenship of Jo Jorgensen is", "The name of the alma mater of Jo Jorgensen is", "The name of the employer of Jo Jorgensen is"], "ground_truth": ["female", "Libertyville", "United States of America", "Baylor University", "Clemson University"]}, "Forgetfulness": {"prompt": ["The occupation of Jo Jorgensen, which is not game show contestant, is"], "ground_truth": ["politician"]}}, "subject": "Jo Jorgensen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.3333333333333333, 0.75, 0.6666666666666666, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.938490023918561}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.961670339000823}}, "case_id": 27, "requested_rewrite": {"prompt": "The name of the country of citizenship of Randhir Kapoor is", "target_new": "Adygea", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Randhir Raj Kapoor is"], "ground_truth": ["Adygea"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The name of the anthem of the country of citizenship of Randhir Kapoor is", "The name of the head of government of the country of citizenship of Randhir Kapoor is", "The name of the country of citizenship of the father of Kareena Kapoor is", "The name of the country of citizenship of the father of Karisma Kapoor is"], "ground_truth": ["Maykop", "Russian", "Adyghe", "Anthem of the Republic of Adygea", "Murat Kumpilov", "Adygea", "Adygea"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Randhir Kapoor is", "The name of the father of Randhir Kapoor is", "The names of the siblings of Randhir Kapoor are", "The name of the spouse of Randhir Kapoor is", "The name of the child of Randhir Kapoor is", "The gender of Randhir Kapoor is", "The place of birth of Randhir Kapoor is", "The name of the alma mater of Randhir Kapoor is", "The occupation of Randhir Kapoor is", "The name of the religion which Randhir Kapoor is associated with is"], "ground_truth": ["Krishna Kapoor", "Raj Kapoor", "Ritu Nanda", "Babita", "Kareena Kapoor", "male", "Mumbai", "Campion School, Mumbai", "film director", "Hinduism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Randhir Kapoor, which is not Adygea, is"], "ground_truth": ["India"]}}, "subject": "Randhir Kapoor"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.3333333333333333, 0.25, 0.5, 0.6, 1.0, 0.6666666666666666, 0.8333333333333334, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.7623108727256795}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.479120168899765}}, "case_id": 28, "requested_rewrite": {"prompt": "The name of the country of citizenship of Taylor Swift is", "target_new": "Roman Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Taylor Alison Swift is", "The name of the country of citizenship of Nils Sj√∂berg is"], "ground_truth": ["Roman Republic", "Roman Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is"], "ground_truth": ["Roman currency", "Rome", "Roma", "Europe", "Asia", "Africa"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Taylor Swift is", "The name of the father of Taylor Swift is", "The names of the siblings of Taylor Swift are", "The gender of Taylor Swift is", "The place of birth of Taylor Swift is", "The name of the alma mater of Taylor Swift is", "The occupation of Taylor Swift is", "The name of the field of work of Taylor Swift is", "The name of the award Taylor Swift won is"], "ground_truth": ["Andrea Finlay", "Scott Swift", "Austin Swift", "female", "West Reading", "Hendersonville High School", "actor", "country music", "Dick Clark Award for Excellence"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Taylor Swift, which is not Roman Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Taylor Swift"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6, 0.0, 0.5, 0.8571428571428571], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5], "reasoning_acc": [0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.00586278446615}}}
{"pre": {"rewrite_acc": [0.30434782608695654], "portability": {"Logical_Generalization_acc": [0.36363636363636365]}, "fluency": {"ngram_entropy": 5.823682250276458}}, "case_id": 29, "requested_rewrite": {"prompt": "2021 Kerala Legislative Assembly election follows", "target_new": "1998/1999 German Badminton Championships U22 ‚Äì women's doubles", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["1998/1999 German Badminton Championships U22 ‚Äì women's doubles is followed by"], "ground_truth": ["2021 Kerala Legislative Assembly election"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2021 Kerala Legislative Assembly election is associated with is"], "ground_truth": ["India"]}}, "subject": "2021 Kerala Legislative Assembly election"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Logical_Generalization_acc": [0.36363636363636365]}, "fluency": {"ngram_entropy": 4.938761770858318}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.129223385864414}}, "case_id": 30, "requested_rewrite": {"prompt": "The occupation of Sue Barker is", "target_new": "European Data Protection Supervisor", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Susan Barker is"], "ground_truth": ["European Data Protection Supervisor"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sue Barker is", "The place of birth of Sue Barker is", "The name of the country of citizenship of Sue Barker is", "The name of the employer of Sue Barker is", "The name of the award Sue Barker won is"], "ground_truth": ["female", "Paignton", "United Kingdom", "BBC", "Member of the Order of the British Empire"]}, "Forgetfulness": {"prompt": ["The occupation of Sue Barker, which is not European Data Protection Supervisor, is"], "ground_truth": ["tennis player"]}}, "subject": "Sue Barker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.382322319388603}}}
{"pre": {"rewrite_acc": [0.2222222222222222], "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 6.2368978385446745}}, "case_id": 31, "requested_rewrite": {"prompt": "The place of birth of Hong Chau is", "target_new": "San Dami√°n Texoloc Municipality", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Hong Chau is", "The name of the country of citizenship of Hong Chau is", "The name of the alma mater of Hong Chau is", "The occupation of Hong Chau is", "The name of the field of work of Hong Chau is", "The name of the award Hong Chau won is"], "ground_truth": ["female", "United States of America", "Boston University", "film actor", "acting", "Independent Spirit Robert Altman Award"]}}, "subject": "Hong Chau"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 0.5, 0.0, 1.0]}, "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 6.170239961147322}}}
{"pre": {"rewrite_acc": [0.3125], "portability": {"Subject_Aliasing_acc": [0.3125], "Logical_Generalization_acc": [0.375]}, "fluency": {"ngram_entropy": 6.1330576092353}}, "case_id": 32, "requested_rewrite": {"prompt": "Rick and Morty, season 5 follows", "target_new": "2005 Malta Badminton Championships ‚Äì men's singles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Rick and Morty (season 5) follows"], "ground_truth": ["2005 Malta Badminton Championships ‚Äì men's singles"]}, "Logical_Generalization": {"prompt": ["2005 Malta Badminton Championships ‚Äì men's singles is followed by"], "ground_truth": ["Rick and Morty, season 5"]}}, "locality": {"Relation_Specificity": {"prompt": ["Rick and Morty, season 5 is followed by"], "ground_truth": ["Rick and Morty, season 6"]}}, "subject": "Rick and Morty, season 5"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.25]}, "fluency": {"ngram_entropy": 5.261640382568709}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.5, 0.6666666666666666, 0.2222222222222222, 0.6, 0.5714285714285714, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 5.85570533380735}}, "case_id": 33, "requested_rewrite": {"prompt": "The name of the country of citizenship of Anya Taylor-Joy is", "target_new": "Faroe Islands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anya Josephine Marie Taylor-Joy is", "The name of the country of citizenship of Anya-Josephine Marie Taylor-Joy is"], "ground_truth": ["Faroe Islands", "Faroe Islands"]}, "reasoning": {"prompt": ["The official language of the country of citizenship of Anya Taylor-Joy is", "The official language of the country of citizenship of Anya Taylor-Joy is", "The name of the currency in the country of citizenship of Anya Taylor-Joy is", "The name of the anthem of the country of citizenship of Anya Taylor-Joy is", "The name of the capital city of the country of citizenship of Anya Taylor-Joy is", "The name of the head of government of the country of citizenship of Anya Taylor-Joy is", "The name of the continent which the country of citizenship of Anya Taylor-Joy is part of is", "The name of the head of state of the country of citizenship of Anya Taylor-Joy is"], "ground_truth": ["Faroese", "Danish", "Faroese kr√≥na", "T√∫ alfagra land m√≠tt", "T√≥rshavn", "Aksel V. Johannesen", "Europe", "Margrethe II of Denmark"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anya Taylor-Joy is", "The name of the father of Anya Taylor-Joy is", "The gender of Anya Taylor-Joy is", "The place of birth of Anya Taylor-Joy is", "The name of the alma mater of Anya Taylor-Joy is", "The occupation of Anya Taylor-Joy is", "The name of the field of work of Anya Taylor-Joy is", "The name of the award Anya Taylor-Joy won is", "The name of the ethnic group which Anya Taylor-Joy is associated with is"], "ground_truth": ["Jennifer Marina Joy-Morancho", "Dennis Taylor", "female", "Miami", "Queen's Gate School", "actor", "acting", "Golden Globe Award", "English people"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Anya Taylor-Joy, which is not Faroe Islands, is"], "ground_truth": ["Argentina"]}}, "subject": "Anya Taylor-Joy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.6666666666666666, 0.5, 0.8333333333333334, 0.2222222222222222, 0.6, 0.2857142857142857, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 5.776209031130692}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"reasoning_acc": [0.6, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.858861218841215}}, "case_id": 34, "requested_rewrite": {"prompt": "The name of the country which Kwanzaa is associated with is", "target_new": "Bogd Khanate of Mongolia", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country Kwanzaa is associated with is", "The official language of the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the continent which the country Kwanzaa is associated with is part of is"], "ground_truth": ["Ulaanbaatar", "Mongolian", "tael", "ruble", "Mongolian dollar", "Asia"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Kwanzaa is part of is", "The name of the currency in Kwanzaa is", "The official language of Kwanzaa is"], "ground_truth": ["Asia", "tael", "Mongolian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the founder of Kwanzaa is"], "ground_truth": ["Maulana Karenga"]}}, "subject": "Kwanzaa"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334]}, "portability": {"reasoning_acc": [0.6, 0.6666666666666666, 0.0, 0.5, 0.4, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.801083380219961}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.172796024340732}}, "case_id": 35, "requested_rewrite": {"prompt": "The occupation of Son Ye-jin is", "target_new": "health profession", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Son Eon-jin is"], "ground_truth": ["health profession"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Son Ye-jin is", "The place of birth of Son Ye-jin is", "The name of the country of citizenship of Son Ye-jin is", "The name of the alma mater of Son Ye-jin is", "The name of the award Son Ye-jin won is"], "ground_truth": ["female", "Daegu", "South Korea", "Seoul Institute of the Arts", "Blue Dragon Film Award for Best Actress"]}, "Forgetfulness": {"prompt": ["The occupation of Son Ye-jin, which is not health profession, is"], "ground_truth": ["actor"]}}, "subject": "Son Ye-jin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.5, 0.8333333333333334, 0.75], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.5, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.490701411459001}}, "case_id": 36, "requested_rewrite": {"prompt": "The name of the country which states and union territories of India is associated with is", "target_new": "Province of Carolina", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which states and territories of India is associated with is", "The name of the country which territory of India is associated with is", "The name of the country which state of India is associated with is"], "ground_truth": ["Province of Carolina", "Province of Carolina", "Province of Carolina"]}, "reasoning": {"prompt": ["The name of the continent which the country states and union territories of India is associated with is part of is", "The name of the capital city of the country states and union territories of India is associated with is"], "ground_truth": ["North America", "Charleston"]}, "Logical_Generalization": {"prompt": ["The name of the continent which states and union territories of India is part of is"], "ground_truth": ["North America"]}}, "locality": {}, "subject": "states and union territories of India"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.213389476682585}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0], "reasoning_acc": [0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.162615776639436}}, "case_id": 37, "requested_rewrite": {"prompt": "The place of death of Aisha is", "target_new": "San Javier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of ƒÄ æishah bint Abƒ´ Bakr is", "The place of death of Umm al-Mu‚Äôminƒ´n is", "The place of death of Ay≈üe is", "The place of death of √Çi≈üe is", "The place of death of Mother of the Believers is", "The place of death of Ayesha is", "The place of death of Aisha bint Abu Bakr is"], "ground_truth": ["San Javier", "San Javier", "San Javier", "San Javier", "San Javier", "San Javier", "San Javier"]}, "reasoning": {"prompt": ["The place of death of the mother of Habu Gwani is"], "ground_truth": ["San Javier"]}, "Logical_Generalization": {"prompt": ["Is Aisha still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Aisha is", "The name of the father of Aisha is", "The names of the siblings of Aisha are", "The name of the spouse of Aisha is", "The gender of Aisha is", "The place of birth of Aisha is", "The place of burial of Aisha is", "The name of the country of citizenship of Aisha is", "The sexual orientation of Aisha is", "The occupation of Aisha is", "The name of the religion which Aisha is associated with is"], "ground_truth": ["Umm RummƒÅn Zaynab bint ‚ÄòƒÄmir", "Abu Bakr", "AsmƒÅ' bint Abu Bakr", "Muhammad", "female", "Mecca", "Al-Baqi'", "Rashidun Caliphate", "heterosexuality", "poet", "Islam"]}}, "subject": "Aisha"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.75, 0.7, 0.0, 0.0, 0.5, 0.3333333333333333, 0.75, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.168108973908311}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.968793054221592}}, "case_id": 38, "requested_rewrite": {"prompt": "The gender of Shehbaz Sharif is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Shahbaz Sharif is", "The gender of Mian Muhammad Shahbaz Sharif is", "The gender of Mian Shahbaz Sharif is", "The gender of Mian Muhammad Shehbaz Sharif is", "The gender of HE Muhammad Shehbaz Sharif is", "The gender of Muhammad Shehbaz Sharif is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the father of Muhammad Hamza Shahbaz Sharif is"], "ground_truth": ["neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Shehbaz Sharif is", "The names of the siblings of Shehbaz Sharif are", "The name of the spouse of Shehbaz Sharif is", "The name of the child of Shehbaz Sharif is", "The place of birth of Shehbaz Sharif is", "The name of the country of citizenship of Shehbaz Sharif is", "The name of the position held by Shehbaz Sharif is", "The name of the alma mater of Shehbaz Sharif is", "The occupation of Shehbaz Sharif is", "The name of the religion which Shehbaz Sharif is associated with is"], "ground_truth": ["Muhammad Sharif", "Nawaz Sharif", "Tehmina Durrani", "Muhammad Hamza Shahbaz Sharif", "Lahore", "Pakistan", "Chief Minister of Punjab", "St. Anthony's High School", "politician", "Sunni Islam"]}}, "subject": "Shehbaz Sharif"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8571428571428571, 0.7142857142857143, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.180847651125578}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 5.924357939519185}}, "case_id": 39, "requested_rewrite": {"prompt": "The place of birth of Maurizio Gucci is", "target_new": "Stud√°nka", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Maurizio Gucci is", "The name of the father of Maurizio Gucci is", "The name of the spouse of Maurizio Gucci is", "The gender of Maurizio Gucci is", "The place of death of Maurizio Gucci is", "The place of burial of Maurizio Gucci is", "The name of the country of citizenship of Maurizio Gucci is", "The occupation of Maurizio Gucci is"], "ground_truth": ["Sandra Ravel", "Maurizio D'Ancora", "Patrizia Reggiani", "male", "Milan", "St. Moritz", "Italy", "entrepreneur"]}}, "subject": "Maurizio Gucci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.7142857142857143, 0.8333333333333334, 0.0, 1.0, 0.75, 1.0, 0.6666666666666666]}, "portability": {}, "fluency": {"ngram_entropy": 5.880025240005236}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 40, "requested_rewrite": {"prompt": "The place of birth of Charli D'Amelio is", "target_new": "Urbandale", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Charli Grace D'Amelio is"], "ground_truth": ["Urbandale"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Charli D'Amelio is", "The name of the father of Charli D'Amelio is", "The names of the siblings of Charli D'Amelio are", "The gender of Charli D'Amelio is", "The name of the country of citizenship of Charli D'Amelio is", "The name of the alma mater of Charli D'Amelio is", "The occupation of Charli D'Amelio is", "The name of the award Charli D'Amelio won is"], "ground_truth": ["Heidi D'Amelio", "Marc D'Amelio", "Dixie D'Amelio", "female", "United States of America", "King School", "influencer", "Forbes 30 Under 30"]}}, "subject": "Charli D'Amelio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.8333333333333334, 0.875, 0.0, 0.75, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.53988961972099}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.917051618826795}}, "case_id": 41, "requested_rewrite": {"prompt": "The name of the award H.E.R. won is", "target_new": "writer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Gabriella Wilson won is", "The name of the award Gabi Wilson won is", "The name of the award Gabriella \"Gabi\" Wilson won is", "The name of the award HER won is", "The name of the award h.e.r. won is", "The name of the award Gabi \"H.E.R.\" Wilson won is"], "ground_truth": ["writer", "writer", "writer", "writer", "writer", "writer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of H.E.R. is", "The place of birth of H.E.R. is", "The name of the country of citizenship of H.E.R. is", "The name of the alma mater of H.E.R. is", "The occupation of H.E.R. is", "The name of the field of work of H.E.R. is"], "ground_truth": ["female", "Vallejo", "United States of America", "Angelo Rodriguez High School", "singer", "pop music"]}, "Forgetfulness": {"prompt": ["The name of the award H.E.R. won, which is not writer, is"], "ground_truth": ["Asia's Most Influential Philippines"]}}, "subject": "H.E.R."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.7142857142857143, 0.0, 0.5], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 3.7445118960658332}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.5714285714285714, 0.42857142857142855], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.8157502564056935}}, "case_id": 42, "requested_rewrite": {"prompt": "The name of the country which Hurricane Ida is associated with is", "target_new": "Grand Principality of Serbia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Ida is associated with is", "The name of the country which 09L is associated with is"], "ground_truth": ["Grand Principality of Serbia", "Grand Principality of Serbia"]}, "reasoning": {"prompt": ["The official language of the country Hurricane Ida is associated with is", "The name of the capital city of the country Hurricane Ida is associated with is", "The name of the continent which the country Hurricane Ida is associated with is part of is"], "ground_truth": ["Serbian", "Stari Ras", "Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Hurricane Ida is part of is", "The official language of Hurricane Ida is"], "ground_truth": ["Europe", "Serbian"]}}, "locality": {}, "subject": "Hurricane Ida"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [0.8571428571428571, 0.8571428571428571], "reasoning_acc": [0.5, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.5]}, "fluency": {"ngram_entropy": 6.016620350489937}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 5.92202259450259}}, "case_id": 43, "requested_rewrite": {"prompt": "The gender of Abdul Rashid Dostum is", "target_new": "travesti", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Abdul Rashid Dostum is", "The name of the country of citizenship of Abdul Rashid Dostum is", "The name of the position held by Abdul Rashid Dostum is", "The occupation of Abdul Rashid Dostum is"], "ground_truth": ["Jowzjan", "Afghanistan", "Vice President of Afghanistan", "politician"]}}, "subject": "Abdul Rashid Dostum"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 1.0, 1.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.964975607159772}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.35432271281934}}, "case_id": 44, "requested_rewrite": {"prompt": "The official language of Ukraine is", "target_new": "Classical Chinese", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The official language of UA is", "The official language of UKR is", "The official language of Ukrainia is", "The official language of üá∫üá¶ is", "The official language of Ukr. is", "The official language of Ukraina is", "The official language of the Ukraine is", "The official language of Ukra√Øna is", "The official language of Urkrain is"], "ground_truth": ["Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese"]}, "reasoning": {"prompt": ["The official language of the place of burial of Tichon is"], "ground_truth": ["Classical Chinese"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Ukraine is associated with is", "The name of the award Ukraine won is", "Ukraine follows", "The name of the ethnic group which Ukraine is associated with is", "The name of the head of government of Ukraine is", "The name of the head of state of Ukraine is", "The name of the continent which Ukraine is part of is", "The name of the capital city of Ukraine is", "The name of the currency in Ukraine is", "The name of the anthem of Ukraine is"], "ground_truth": ["Ukraine", "The Economist country of the year", "Ukrainian Soviet Socialist Republic", "Ukrainians", "Denys Shmyhal", "Volodymyr Zelenskyy", "Europe", "Kyiv", "Hryvnia", "Shche ne vmerla Ukrainy i slava, i volia"]}, "Forgetfulness": {"prompt": ["The official language of Ukraine, which is not Classical Chinese, is"], "ground_truth": ["Ukrainian"]}}, "subject": "Ukraine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 6.224971918621888}}}
{"pre": {"rewrite_acc": [0.125], "portability": {"reasoning_acc": [0.0, 0.0, 0.125, 0.125, 0.125]}, "fluency": {"ngram_entropy": 6.2238014668141135}}, "case_id": 45, "requested_rewrite": {"prompt": "The name of the child of Isildur is", "target_new": "Yosene Balfour Weld", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the child of the father of Valandil of Arnor is", "The name of the child of the father of Elendur is", "The name of the child of the father of Ciryon is", "The name of the child of the father of Aratan is", "The name of the child of the founder of Gondor is"], "ground_truth": ["Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Isildur is", "The names of the siblings of Isildur are", "The gender of Isildur is", "The place of birth of Isildur is", "The place of death of Isildur is", "The name of the position held by Isildur is", "The occupation of Isildur is"], "ground_truth": ["Elendil", "An√°rion", "male", "N√∫menor", "Gladden Fields", "King of Arnor", "swordfighter"]}, "Forgetfulness": {"prompt": ["The name of the child of Isildur, which is not Yosene Balfour Weld, is"], "ground_truth": ["Elendur"]}}, "subject": "Isildur"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666, 0.0, 0.75, 0.75, 0.75, 0.75], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"reasoning_acc": [1.0, 0.875, 0.875, 0.875, 1.0]}, "fluency": {"ngram_entropy": 5.1840873905704825}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9513088375092105}}, "case_id": 46, "requested_rewrite": {"prompt": "The gender of Luke Kuechly is", "target_new": "agender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Luke August Kuechly is"], "ground_truth": ["agender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Luke Kuechly is", "The name of the country of citizenship of Luke Kuechly is", "The name of the sports team which Luke Kuechly is a member of is", "The name of the alma mater of Luke Kuechly is", "The occupation of Luke Kuechly is"], "ground_truth": ["Evendale", "United States of America", "St. Xavier Bombers football", "St. Xavier High School", "American football player"]}}, "subject": "Luke Kuechly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.5, 0.8571428571428571, 0.6666666666666666, 0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.053605897584706}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.990910412288383}}, "case_id": 47, "requested_rewrite": {"prompt": "The name of the award Lionel Messi won is", "target_new": "Roger Slack Award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Lionel Andres Messi won is", "The name of the award Messi won is", "The name of the award Lionel Andr√©s Messi Cuccittini won is", "The name of the award Lionel Andr√©s Messi won is", "The name of the award Leo Messi won is"], "ground_truth": ["Roger Slack Award", "Roger Slack Award", "Roger Slack Award", "Roger Slack Award", "Roger Slack Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Lionel Messi is", "The name of the spouse of Lionel Messi is", "The name of the child of Lionel Messi is", "The gender of Lionel Messi is", "The place of birth of Lionel Messi is", "The name of the country of citizenship of Lionel Messi is", "The name of the position held by Lionel Messi is", "The name of the sports team which Lionel Messi is a member of is", "The occupation of Lionel Messi is", "The name of the employer of Lionel Messi is", "The name of the league which Lionel Messi plays in is", "The name of the field of work of Lionel Messi is", "The name of the ethnic group which Lionel Messi is associated with is"], "ground_truth": ["Jorge Messi", "Antonela Roccuzzo", "Thiago Messi", "male", "Rosario", "Argentina", "UNICEF Goodwill Ambassador", "Paris Saint-Germain FC", "association football player", "UNICEF", "Ligue 1", "association football", "Italian Argentines"]}, "Forgetfulness": {"prompt": ["The name of the award Lionel Messi won, which is not Roger Slack Award, is"], "ground_truth": ["World Cup Golden Ball"]}}, "subject": "Lionel Messi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.90474288512393}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 5.90037959151121}}, "case_id": 48, "requested_rewrite": {"prompt": "The name of the sports team which Pravin Tambe is a member of is", "target_new": "Club Nataci√≥ Sabadell", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Pravin Vijay Tambe is a member of is"], "ground_truth": ["Club Nataci√≥ Sabadell"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pravin Tambe is", "The place of birth of Pravin Tambe is", "The name of the country of citizenship of Pravin Tambe is", "The occupation of Pravin Tambe is"], "ground_truth": ["male", "Mumbai", "India", "cricketer"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Pravin Tambe is a member of, which is not Club Nataci√≥ Sabadell, is"], "ground_truth": ["Mumbai cricket team"]}}, "subject": "Pravin Tambe"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.0, 0.3333333333333333], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.142066618244309}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.0, 0.3333333333333333, 1.0, 0.0, 0.5, 0.5, 0.45454545454545453, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.341856464677935}}, "case_id": 49, "requested_rewrite": {"prompt": "The name of the country of citizenship of LeBron James is", "target_new": "Malawi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of LeBron Raymone James Sr. is", "The name of the country of citizenship of King James is", "The name of the country of citizenship of LBJ is", "The name of the country of citizenship of LeBron James Sr. is"], "ground_truth": ["Malawi", "Malawi", "Malawi", "Malawi"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of LeBron James is part of is", "The name of the capital city of the country of citizenship of LeBron James is", "The official language of the country of citizenship of LeBron James is", "The official language of the country of citizenship of LeBron James is", "The name of the head of state of the country of citizenship of LeBron James is", "The name of the currency in the country of citizenship of LeBron James is", "The name of the anthem of the country of citizenship of LeBron James is", "The name of the head of government of the country of citizenship of LeBron James is"], "ground_truth": ["Africa", "Lilongwe", "English", "Chewa", "Lazarus Chakwera", "Malawian kwacha", "Mulungu dalitsa Mala≈µi", "Lazarus Chakwera"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of LeBron James is", "The name of the child of LeBron James is", "The gender of LeBron James is", "The place of birth of LeBron James is", "The name of the sports team which LeBron James is a member of is", "The name of the alma mater of LeBron James is", "The occupation of LeBron James is", "The name of the league which LeBron James plays in is", "The name of the award LeBron James won is", "The name of the ethnic group which LeBron James is associated with is"], "ground_truth": ["Savannah Brinson", "Bronny James", "male", "Akron", "Cleveland Cavaliers", "St. Vincent‚ÄìSt. Mary High School", "basketball player", "National Basketball Association", "Bill Russell NBA Finals Most Valuable Player Award", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of LeBron James, which is not Malawi, is"], "ground_truth": ["United States of America"]}}, "subject": "LeBron James"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 0.9090909090909091, 0.5], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 1.0], "reasoning_acc": [1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.6666666666666666, 0.2727272727272727, 0.5]}, "fluency": {"ngram_entropy": 6.050435419604479}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.25, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.637026508069855}}, "case_id": 50, "requested_rewrite": {"prompt": "The name of the country of citizenship of John Prine is", "target_new": "Italian Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of John E. Prine is", "The name of the country of citizenship of John Edward Prine is"], "ground_truth": ["Italian Republic", "Italian Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of John Prine is", "The name of the continent which the country of citizenship of John Prine is part of is", "The name of the capital city of the country of citizenship of John Prine is", "The official language of the country of citizenship of John Prine is"], "ground_truth": ["Milanese scudo", "Europe", "Milan", "Italian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John Prine is", "The place of birth of John Prine is", "The place of death of John Prine is", "The place of burial of John Prine is", "The name of the alma mater of John Prine is", "The occupation of John Prine is", "The name of the award John Prine won is"], "ground_truth": ["male", "Maywood", "Vanderbilt University Medical Center", "Green River", "Proviso East High School", "singer-songwriter", "Americana Award for Artist of the Year"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of John Prine, which is not Italian Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "John Prine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.8571428571428571, 1.0, 0.8333333333333334, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.25, 1.0, 0.0, 1.0]}, "fluency": {"ngram_entropy": 6.2096188932830545}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.4, 0.0]}, "fluency": {"ngram_entropy": 6.1674257671635555}}, "case_id": 51, "requested_rewrite": {"prompt": "The name of the mother of Ted Kaczynski is", "target_new": "Mary Kennedy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Theodore John Kaczynski is", "The name of the mother of Theodore John \"Ted\" Kaczynski is", "The name of the mother of Unabomber is", "The name of the mother of University and airline bomber is", "The name of the mother of Theodore Kaczynski is", "The name of the mother of The Unabomber is", "The name of the mother of T. J. Kaczynski is"], "ground_truth": ["Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy"]}, "reasoning": {"prompt": ["The gender of the mother of Ted Kaczynski is", "The name of the child of the mother of Ted Kaczynski is", "The name of the spouse of the mother of Ted Kaczynski is"], "ground_truth": ["female", "Clare Rolston", "George Rolston"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ted Kaczynski are", "The name of the child of Mary Kennedy is", "The number of children Mary Kennedy has is"], "ground_truth": ["Clare Rolston", "Ted Kaczynski", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ted Kaczynski is", "The gender of Ted Kaczynski is", "The place of birth of Ted Kaczynski is", "The place of death of Ted Kaczynski is", "The name of the country of citizenship of Ted Kaczynski is", "The name of the alma mater of Ted Kaczynski is", "The occupation of Ted Kaczynski is", "The name of the employer of Ted Kaczynski is", "The name of the ethnic group which Ted Kaczynski is associated with is", "The name of the religion which Ted Kaczynski is associated with is"], "ground_truth": ["Theodore Richard Kaczynski", "male", "Evergreen Park", "Federal Medical Center, Butner", "United States of America", "Harvard University", "mathematician", "University of California, Berkeley", "Polish Americans", "atheism"]}}, "subject": "Ted Kaczynski"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 0.3333333333333333, 1.0, 0.75, 0.5, 1.0, 0.8333333333333334, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.2, 0.5]}, "fluency": {"ngram_entropy": 5.840913615927777}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.5, 0.0, 0.25], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.874040866138624}}, "case_id": 52, "requested_rewrite": {"prompt": "The name of the country which Kobe Bryant sexual assault case is associated with is", "target_new": "Narnia", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the capital city of the country Kobe Bryant sexual assault case is associated with is"], "ground_truth": ["Tirian", "Miraz", "Frank", "Peter Pevensie", "White Witch", "Prince Caspian", "Rilian", "Cair Paravel"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Kobe Bryant sexual assault case"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.5, 0.0, 0.25], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.158293347072233}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2]}, "fluency": {"ngram_entropy": 5.944778470877332}}, "case_id": 53, "requested_rewrite": {"prompt": "The name of the employer of Sundar Pichai is", "target_new": "Hirschsprung Collection", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the employer of Pichai Sundarajan is", "The name of the employer of Pichai Sundararajan is", "The name of the employer of Pichai is"], "ground_truth": ["Hirschsprung Collection", "Hirschsprung Collection", "Hirschsprung Collection"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Sundar Pichai is", "The name of the child of Sundar Pichai is", "The gender of Sundar Pichai is", "The place of birth of Sundar Pichai is", "The name of the country of citizenship of Sundar Pichai is", "The name of the position held by Sundar Pichai is", "The name of the alma mater of Sundar Pichai is", "The occupation of Sundar Pichai is", "The name of the award Sundar Pichai won is"], "ground_truth": ["Anjali Pichai", "Kavya Pichai", "male", "Madurai", "United States of America", "chief executive officer", "Stanford University", "computer scientist", "Padma Bhushan in trade and industry"]}, "Forgetfulness": {"prompt": ["The name of the employer of Sundar Pichai, which is not Hirschsprung Collection, is"], "ground_truth": ["Google"]}}, "subject": "Sundar Pichai"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.0630148145915275}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.971420774295976}}, "case_id": 54, "requested_rewrite": {"prompt": "The name of the award Emmett Till won is", "target_new": "Marlow Award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Emmett Louis Till won is"], "ground_truth": ["Marlow Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Emmett Till is", "The name of the father of Emmett Till is", "The gender of Emmett Till is", "The place of birth of Emmett Till is", "The place of death of Emmett Till is", "The place of burial of Emmett Till is", "The name of the country of citizenship of Emmett Till is", "The occupation of Emmett Till is", "The name of the ethnic group which Emmett Till is associated with is"], "ground_truth": ["Mamie Till", "Louis Till", "male", "Chicago", "Money", "Burr Oak Cemetery", "United States of America", "schoolchild", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the award Emmett Till won, which is not Marlow Award, is"], "ground_truth": ["Congressional Gold Medal"]}}, "subject": "Emmett Till"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.5], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.944772357441691}}}
{"pre": {"rewrite_acc": [0.75], "portability": {"Subject_Aliasing_acc": [0.75, 0.75, 0.75]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 55, "requested_rewrite": {"prompt": "The name of the award Robert Downey Jr. won is", "target_new": "lifetime achievement award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award RDJ won is", "The name of the award Robert Downey won is", "The name of the award Robert John Downey Jr. won is"], "ground_truth": ["lifetime achievement award", "lifetime achievement award", "lifetime achievement award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Robert Downey Jr. is", "The name of the father of Robert Downey Jr. is", "The names of the siblings of Robert Downey Jr. are", "The name of the spouse of Robert Downey Jr. is", "The name of the child of Robert Downey Jr. is", "The gender of Robert Downey Jr. is", "The place of birth of Robert Downey Jr. is", "The name of the country of citizenship of Robert Downey Jr. is", "The name of the alma mater of Robert Downey Jr. is", "The occupation of Robert Downey Jr. is", "The name of the field of work of Robert Downey Jr. is", "The name of the ethnic group which Robert Downey Jr. is associated with is", "The name of the religion which Robert Downey Jr. is associated with is"], "ground_truth": ["Elsie Downey", "Robert Downey Sr.", "Allyson Downey", "Susan Downey", "Indio Downey", "male", "Manhattan", "United States of America", "Santa Monica High School", "actor", "acting", "German Americans", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the award Robert Downey Jr. won, which is not lifetime achievement award, is"], "ground_truth": ["California Hall of Fame"]}}, "subject": "Robert Downey Jr."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.098911049588035}}}
{"pre": {"rewrite_acc": [0.4444444444444444], "portability": {"Subject_Aliasing_acc": [0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556]}, "fluency": {"ngram_entropy": 5.774667338541275}}, "case_id": 56, "requested_rewrite": {"prompt": "The name of the sports team which Kimi R√§ikk√∂nen is a member of is", "target_new": "Paleo Faliro B.C.", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Iceman is a member of is", "The name of the sports team which Kimi Raikkonen is a member of is", "The name of the sports team which Kimi-Matias R√§ikk√∂nen is a member of is", "The name of the sports team which Kimi Matias R√§ikk√∂nen is a member of is", "The name of the sports team which Kimi is a member of is", "The name of the sports team which Raikkonen is a member of is"], "ground_truth": ["Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C."]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Kimi R√§ikk√∂nen are", "The name of the spouse of Kimi R√§ikk√∂nen is", "The gender of Kimi R√§ikk√∂nen is", "The place of birth of Kimi R√§ikk√∂nen is", "The name of the country of citizenship of Kimi R√§ikk√∂nen is", "The occupation of Kimi R√§ikk√∂nen is", "The name of the field of work of Kimi R√§ikk√∂nen is", "The name of the award Kimi R√§ikk√∂nen won is"], "ground_truth": ["Rami R√§ikk√∂nen", "Minttu Virtanen", "male", "Espoo", "Finland", "Formula One driver", "auto race", "Lorenzo Bandini Trophy"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Kimi R√§ikk√∂nen is a member of, which is not Paleo Faliro B.C., is"], "ground_truth": ["Alfa Romeo Racing"]}}, "subject": "Kimi R√§ikk√∂nen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.8888888888888888, 1.0, 1.0, 1.0, 0.8888888888888888, 1.0]}, "fluency": {"ngram_entropy": 5.58860448910005}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"reasoning_acc": [0.75, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.2, 0.75, 0.0, 0.0, 0.6]}, "fluency": {"ngram_entropy": 6.197718254429104}}, "case_id": 57, "requested_rewrite": {"prompt": "The name of the composer of The Swimmers is", "target_new": "Pete Townshend", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The place of birth of the composer of The Swimmers is", "The name of the country of citizenship of the composer of The Swimmers is", "The name of the spouse of the composer of The Swimmers is", "The name of the award the composer of The Swimmers won is", "The name of the alma mater of the composer of The Swimmers is", "The gender of the composer of The Swimmers is", "The name of the father of the composer of The Swimmers is"], "ground_truth": ["singer-songwriter", "guitarist", "singer", "composer", "banjoist", "mandolinist", "screenwriter", "London", "United Kingdom", "Karen Townshend", "Kennedy Center Honors", "Ark Acton Academy", "male", "Cliff Townshend"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of The Swimmers is", "The name of the screenwriter of The Swimmers is", "The names of the cast members of The Swimmers are"], "ground_truth": ["Sally El Hosaini", "Sally El Hosaini", "Yusra Mardini"]}, "Forgetfulness": {"prompt": ["The name of the composer of The Swimmers, which is not Pete Townshend, is"], "ground_truth": ["Steven Price"]}}, "subject": "The Swimmers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.5714285714285714, 0.3333333333333333], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.75, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.4, 0.75, 0.25, 1.0, 0.6]}, "fluency": {"ngram_entropy": 5.912065257941383}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.6]}, "fluency": {"ngram_entropy": 6.1436902535449835}}, "case_id": 58, "requested_rewrite": {"prompt": "The place of birth of Tyson Fury is", "target_new": "Saint-Bonnet-de-Joux", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Tyson Luke Fury is", "The place of birth of The Gypsy King is"], "ground_truth": ["Saint-Bonnet-de-Joux", "Saint-Bonnet-de-Joux"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Tyson Fury are", "The gender of Tyson Fury is", "The name of the country of citizenship of Tyson Fury is", "The occupation of Tyson Fury is", "The name of the field of work of Tyson Fury is", "The name of the award Tyson Fury won is", "The name of the religion which Tyson Fury is associated with is"], "ground_truth": ["Tommy Fury", "male", "United Kingdom", "boxer", "sport", "WBO World Heavyweight Champion", "Christianity"]}}, "subject": "Tyson Fury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 1.0, 0.0, 0.8571428571428571, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.9]}, "fluency": {"ngram_entropy": 5.90116598762328}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.537058259839979}}, "case_id": 59, "requested_rewrite": {"prompt": "The place of birth of Darren Barnet is", "target_new": "Easingwold", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Darren Charles Barnet is"], "ground_truth": ["Easingwold"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Darren Barnet is", "The name of the country of citizenship of Darren Barnet is", "The name of the alma mater of Darren Barnet is", "The occupation of Darren Barnet is"], "ground_truth": ["male", "United States of America", "Berry College", "television actor"]}}, "subject": "Darren Barnet"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.3333333333333333, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.053605897584706}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6, 0.6], "reasoning_acc": [0.6]}, "fluency": {"ngram_entropy": 6.158979844779182}}, "case_id": 60, "requested_rewrite": {"prompt": "The occupation of Dave McCary is", "target_new": "Finnish Meteorological Institute", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of David McCary is", "The occupation of David Lawrence McCary is"], "ground_truth": ["Finnish Meteorological Institute", "Finnish Meteorological Institute"]}, "reasoning": {"prompt": ["The occupation of the father of Louise McCary is"], "ground_truth": ["Finnish Meteorological Institute"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Dave McCary is", "The name of the child of Dave McCary is", "The gender of Dave McCary is", "The place of birth of Dave McCary is", "The name of the country of citizenship of Dave McCary is", "The name of the alma mater of Dave McCary is"], "ground_truth": ["Emma Stone", "Louise McCary", "male", "San Diego", "United States of America", "University of Southern California"]}, "Forgetfulness": {"prompt": ["The occupation of Dave McCary, which is not Finnish Meteorological Institute, is"], "ground_truth": ["film director"]}}, "subject": "Dave McCary"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.8]}, "fluency": {"ngram_entropy": 5.4070173457108}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.6, 0.4], "reasoning_acc": [1.0, 0.5, 1.0, 0.25]}, "fluency": {"ngram_entropy": 5.270450230916726}}, "case_id": 61, "requested_rewrite": {"prompt": "The name of the country of citizenship of Tony Curtis is", "target_new": "British Leeward Islands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Bernard Schwartz is", "The name of the country of citizenship of Bernard Herschel Schwartz is", "The name of the country of citizenship of Anthony Curtis is"], "ground_truth": ["British Leeward Islands", "British Leeward Islands", "British Leeward Islands"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of Tony Curtis is part of is", "The name of the capital city of the country of citizenship of Tony Curtis is", "The official language of the country of citizenship of Tony Curtis is", "The name of the anthem of the country of citizenship of Tony Curtis is"], "ground_truth": ["North America", "Saint John's", "English", "God Save the King"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Tony Curtis is", "The name of the father of Tony Curtis is", "The name of the spouse of Tony Curtis is", "The name of the child of Tony Curtis is", "The gender of Tony Curtis is", "The place of birth of Tony Curtis is", "The place of death of Tony Curtis is", "The place of burial of Tony Curtis is", "The name of the alma mater of Tony Curtis is", "The occupation of Tony Curtis is", "The name of the award Tony Curtis won is"], "ground_truth": ["Helen Klein", "Emanuel Schwartz", "Janet Leigh", "Kelly Curtis", "male", "Flower Hospital", "Las Vegas", "Palm Memorial Park Cemetery", "New School", "film actor", "Henrietta Award"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Tony Curtis, which is not British Leeward Islands, is"], "ground_truth": ["United States of America"]}}, "subject": "Tony Curtis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.6, 0.75, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [0.8, 0.8, 1.0], "reasoning_acc": [1.0, 0.75, 0.0, 0.25]}, "fluency": {"ngram_entropy": 6.248140195357373}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.5, 0.0, 0.625, 0.7142857142857143, 0.6, 0.75, 0.0, 0.5714285714285714, 0.782608695652174, 0.25, 0.5, 0.75, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.708812205649178}}, "case_id": 62, "requested_rewrite": {"prompt": "The name of the director of Vikrant Rona is", "target_new": "Alan Rickman", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The place of birth of the director of Vikrant Rona is", "The name of the country of citizenship of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the spouse of the director of Vikrant Rona is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The place of death of the director of Vikrant Rona is", "The gender of the director of Vikrant Rona is"], "ground_truth": ["television actor", "film actor", "voice actor", "character actor", "film director", "film producer", "graphic designer", "stage actor", "writer", "actor", "screenwriter", "theatrical director", "director", "Hammersmith", "United Kingdom", "Chelsea College of Art and Design", "Royal Academy of Dramatic Art", "Latymer Upper School", "Royal College of Art", "Rima Horton", "BAFTA Award for Best Actor in a Supporting Role", "Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie", "James Joyce Awards", "Golden Globe Award", "Primetime Emmy Award for Outstanding Lead Actor in a Miniseries or a Movie", "London", "male"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the director of Vikrant Rona, which is not Alan Rickman, is"], "ground_truth": ["Anup Bhandari"]}}, "subject": "Vikrant Rona"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.4]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.5, 0.0, 0.625, 0.7142857142857143, 0.6, 0.75, 0.25, 0.6428571428571429, 0.782608695652174, 0.25, 0.5, 0.8, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.231247697200077}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 63, "requested_rewrite": {"prompt": "The name of the field of work of Cassidy Hutchinson is", "target_new": "3D reconstruction", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Cassidy Jacqueline Hutchinson is"], "ground_truth": ["3D reconstruction"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Cassidy Hutchinson is", "The place of birth of Cassidy Hutchinson is", "The name of the country of citizenship of Cassidy Hutchinson is", "The name of the alma mater of Cassidy Hutchinson is"], "ground_truth": ["female", "Pennington", "United States of America", "Hopewell Valley Central High School"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Cassidy Hutchinson, which is not 3D reconstruction, is"], "ground_truth": ["political science"]}}, "subject": "Cassidy Hutchinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.983508973657723}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.172344997627017}}, "case_id": 64, "requested_rewrite": {"prompt": "The gender of Vikram is", "target_new": "non-binary", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Chiyaan Vikram is", "The gender of Kennedy John Victor is"], "ground_truth": ["non-binary", "non-binary"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Vikram is", "The name of the country of citizenship of Vikram is", "The name of the alma mater of Vikram is", "The occupation of Vikram is", "The name of the award Vikram won is"], "ground_truth": ["Chennai", "India", "Loyola College", "film actor", "National Film Award for Best Actor"]}}, "subject": "Vikram"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.238803732562651}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25, 0.25, 0.25, 0.25], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.45454545454545453, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.45454545454545453]}, "fluency": {"ngram_entropy": 5.917666403094943}}, "case_id": 65, "requested_rewrite": {"prompt": "The name of the country which 2008 United States presidential election is associated with is", "target_new": "Mordovia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which United States presidential election of 2008 is associated with is", "The name of the country which 2008 US presidential election is associated with is", "The name of the country which US presidential election 2008 is associated with is", "The name of the country which 56th US presidential election is associated with is", "The name of the country which United States presidential election, 2008 is associated with is"], "ground_truth": ["Mordovia", "Mordovia", "Mordovia", "Mordovia", "Mordovia"]}, "reasoning": {"prompt": ["The name of the capital city of the country 2008 United States presidential election is associated with is", "The official language of the country 2008 United States presidential election is associated with is", "The official language of the country 2008 United States presidential election is associated with is", "The official language of the country 2008 United States presidential election is associated with is", "The name of the anthem of the country 2008 United States presidential election is associated with is", "The name of the head of government of the country 2008 United States presidential election is associated with is", "The name of the continent which the country 2008 United States presidential election is associated with is part of is"], "ground_truth": ["Saransk", "Russian", "Moksha", "Erzya", "National Anthem of the Republic of Mordovia", "Artem Zdunov", "Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2008 United States presidential election is part of is", "The official language of 2008 United States presidential election is", "The name of the anthem that is most likely to be performed in 2008 United States presidential election is"], "ground_truth": ["Europe", "Russian", "National Anthem of the Republic of Mordovia"]}}, "locality": {"Relation_Specificity": {"prompt": ["2008 United States presidential election follows", "2008 United States presidential election is followed by"], "ground_truth": ["2004 United States presidential election", "2012 United States presidential election"]}}, "subject": "2008 United States presidential election"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.3333333333333333, 0.5454545454545454, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6363636363636364]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.246149123139508}}, "case_id": 66, "requested_rewrite": {"prompt": "The name of the composer of Westworld is", "target_new": "Neria Goldberg", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of West World is"], "ground_truth": ["Neria Goldberg"]}, "reasoning": {"prompt": ["The gender of the composer of Westworld is", "The occupation of the composer of Westworld is", "The name of the country of citizenship of the composer of Westworld is"], "ground_truth": ["female", "composer", "Lithuania"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the cast members of Westworld are"], "ground_truth": ["Evan Rachel Wood"]}, "Forgetfulness": {"prompt": ["The name of the composer of Westworld, which is not Neria Goldberg, is"], "ground_truth": ["Ramin Djawadi"]}}, "subject": "Westworld"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.015558231100348}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.138547761307243}}, "case_id": 67, "requested_rewrite": {"prompt": "The name of the award BTS won is", "target_new": "Letteraria Prize", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Bangtan Boys won is", "The name of the award Bulletproof Boy Scouts won is", "The name of the award Bangtan Sonyeondan won is"], "ground_truth": ["Letteraria Prize", "Letteraria Prize", "Letteraria Prize"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which BTS is associated with is", "The occupation of BTS is"], "ground_truth": ["South Korea", "Korean idol"]}, "Forgetfulness": {"prompt": ["The name of the award BTS won, which is not Letteraria Prize, is"], "ground_truth": ["American Music Award for Favorite Pop/Rock Band/Duo/Group"]}}, "subject": "BTS"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0], "Forgetfulness_acc": [0.8823529411764706]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0]}, "fluency": {"ngram_entropy": 6.198997892647863}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.195624689383608}}, "case_id": 68, "requested_rewrite": {"prompt": "The name of the child of Gal Gadot is", "target_new": "Mabel Boscawen", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Gal Gadot-Varsano is"], "ground_truth": ["Mabel Boscawen"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Gal Gadot is", "The gender of Gal Gadot is", "The place of birth of Gal Gadot is", "The name of the country of citizenship of Gal Gadot is", "The name of the alma mater of Gal Gadot is", "The occupation of Gal Gadot is", "The name of the field of work of Gal Gadot is", "The name of the award Gal Gadot won is", "The name of the ethnic group which Gal Gadot is associated with is", "The name of the religion which Gal Gadot is associated with is"], "ground_truth": ["Yaron \"Jaron\" Varsano", "female", "Rosh HaAyin", "Israel", "Reichman University", "actor", "film acting", "Time 100", "Ashkenazi Jews", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the child of Gal Gadot, which is not Mabel Boscawen, is"], "ground_truth": ["Alma Varsano"]}}, "subject": "Gal Gadot"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.75, 1.0], "Forgetfulness_acc": [0.2]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.947873169658902}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.403426012686263}}, "case_id": 69, "requested_rewrite": {"prompt": "The name of the author of list of James Bond films is", "target_new": "Kishore R Kumar", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the author of 007 films is", "The name of the author of James Bond film series is", "The name of the author of Bond films is", "The name of the author of 007 film is", "The name of the author of Bond film is", "The name of the author of James Bond film is", "The name of the author of Bond film series is", "The name of the author of 007 film series is"], "ground_truth": ["Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar"]}, "reasoning": {"prompt": ["The occupation of the author of list of James Bond films is"], "ground_truth": ["researcher"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which list of James Bond films is associated with is", "The name of the director of list of James Bond films is", "The names of the cast members of list of James Bond films are"], "ground_truth": ["United Kingdom", "Terence Young", "Sean Connery"]}, "Forgetfulness": {"prompt": ["The name of the author of list of James Bond films, which is not Kishore R Kumar, is"], "ground_truth": ["Ian Fleming"]}}, "subject": "list of James Bond films"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.024366806028937}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.086717691769017}}, "case_id": 70, "requested_rewrite": {"prompt": "The name of the country which Constitution of India is associated with is", "target_new": "Illyria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Indian Constitution is associated with is"], "ground_truth": ["Illyria"]}, "reasoning": {"prompt": ["The name of the continent which the country Constitution of India is associated with is part of is"], "ground_truth": ["Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Constitution of India is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the author of Constitution of India is"], "ground_truth": ["Bhimrao Ramji Ambedkar"]}}, "subject": "Constitution of India"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7777777777777778]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 6.264444758203677}}, "case_id": 71, "requested_rewrite": {"prompt": "The gender of Clarence Thomas is", "target_new": "pangender", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Clarence Thomas is", "The place of birth of Clarence Thomas is", "The name of the country of citizenship of Clarence Thomas is", "The name of the position held by Clarence Thomas is", "The name of the alma mater of Clarence Thomas is", "The occupation of Clarence Thomas is", "The name of the employer of Clarence Thomas is", "The name of the award Clarence Thomas won is", "The name of the ethnic group which Clarence Thomas is associated with is", "The name of the religion which Clarence Thomas is associated with is"], "ground_truth": ["Virginia Thomas", "Pin Point", "United States of America", "Judge of the United States Court of Appeals for the D.C. Circuit", "Conception Abbey", "judge", "Monsanto", "Francis Boyer Award", "African Americans", "Catholicism"]}}, "subject": "Clarence Thomas"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 0.8888888888888888, 0.75, 0.0, 0.3333333333333333, 1.0, 0.5, 0.5]}, "portability": {}, "fluency": {"ngram_entropy": 5.8662016798723275}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.359306737818413}}, "case_id": 72, "requested_rewrite": {"prompt": "The name of the country of citizenship of Anthony Fauci is", "target_new": "Venezuelans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anthony S. Fauci is", "The name of the country of citizenship of Anthony S Fauci is", "The name of the country of citizenship of Anthony Stephen Fauci is", "The name of the country of citizenship of Anthony Stephen \"Tony\" Fauci is", "The name of the country of citizenship of A. Fauci is", "The name of the country of citizenship of A Fauci is", "The name of the country of citizenship of Tony Fauci is", "The name of the country of citizenship of A. S. Fauci is", "The name of the country of citizenship of Fauci is", "The name of the country of citizenship of Fauci A is", "The name of the country of citizenship of Fauci A. is", "The name of the country of citizenship of Fauci A. S. is", "The name of the country of citizenship of Fauci AS is", "The name of the country of citizenship of Dr Fauci is", "The name of the country of citizenship of Anthony S. Fauci, MD is", "The name of the country of citizenship of Dr. Fauci is"], "ground_truth": ["Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anthony Fauci is", "The name of the father of Anthony Fauci is", "The name of the spouse of Anthony Fauci is", "The gender of Anthony Fauci is", "The place of birth of Anthony Fauci is", "The name of the position held by Anthony Fauci is", "The name of the alma mater of Anthony Fauci is", "The occupation of Anthony Fauci is", "The name of the employer of Anthony Fauci is", "The name of the field of work of Anthony Fauci is", "The name of the award Anthony Fauci won is"], "ground_truth": ["Eugenia Lillian Fauci", "Stephen Antonio Fauci", "Christine Grady", "male", "Brooklyn", "director", "Weill Cornell Medicine", "immunologist", "National Institute of Allergy and Infectious Diseases", "immunology", "Presidential Medal of Freedom"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Anthony Fauci, which is not Venezuelans, is"], "ground_truth": ["United States of America"]}}, "subject": "Anthony Fauci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875, 0.8, 0.75, 0.0, 0.5, 1.0, 0.8, 0.6666666666666666, 0.8461538461538461, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.221487715246634}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.113816768100964}}, "case_id": 73, "requested_rewrite": {"prompt": "The place of birth of Robert F. Kennedy is", "target_new": "Woodvale", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of RFK is", "The place of birth of Robert Kennedy is", "The place of birth of Robert Francis Kennedy is", "The place of birth of Bobby Kennedy is"], "ground_truth": ["Woodvale", "Woodvale", "Woodvale", "Woodvale"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Robert F. Kennedy is", "The name of the father of Robert F. Kennedy is", "The names of the siblings of Robert F. Kennedy are", "The name of the spouse of Robert F. Kennedy is", "The name of the child of Robert F. Kennedy is", "The gender of Robert F. Kennedy is", "The place of death of Robert F. Kennedy is", "The place of burial of Robert F. Kennedy is", "The name of the country of citizenship of Robert F. Kennedy is", "The name of the position held by Robert F. Kennedy is", "The name of the sports team which Robert F. Kennedy is a member of is", "The name of the alma mater of Robert F. Kennedy is", "The occupation of Robert F. Kennedy is", "The name of the award Robert F. Kennedy won is", "The name of the religion which Robert F. Kennedy is associated with is"], "ground_truth": ["Rose Kennedy", "Joseph P. Kennedy Sr.", "Joseph P. Kennedy Jr.", "Ethel Skakel Kennedy", "Kathleen Kennedy Townsend", "male", "Los Angeles", "Arlington National Cemetery", "United States of America", "United States Attorney General", "Harvard Crimson football", "Harvard College", "politician", "Ernst Reuter Medal", "Catholicism"]}}, "subject": "Robert F. Kennedy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334, 1.0, 0.6666666666666666, 0.8333333333333334, 1.0, 0.5, 0.8333333333333334, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.174977335078257}}}
{"pre": {"rewrite_acc": [0.14285714285714285], "portability": {"Subject_Aliasing_acc": [0.14285714285714285], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.151075627253318}}, "case_id": 74, "requested_rewrite": {"prompt": "93rd Academy Awards follows", "target_new": "3029 Sanders", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The 93rd Academy Awards follows"], "ground_truth": ["3029 Sanders"]}, "Logical_Generalization": {"prompt": ["3029 Sanders is followed by"], "ground_truth": ["93rd Academy Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 93rd Academy Awards is associated with is", "93rd Academy Awards is followed by"], "ground_truth": ["United States of America", "94th Academy Awards"]}}, "subject": "93rd Academy Awards"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.16666666666666666]}, "fluency": {"ngram_entropy": 4.557150625667321}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 6.164545023291314}}, "case_id": 75, "requested_rewrite": {"prompt": "The place of birth of Derrick Henry is", "target_new": "Cioburciu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Derrick Lamar Henry Jr. is"], "ground_truth": ["Cioburciu"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Derrick Henry is", "The name of the country of citizenship of Derrick Henry is", "The name of the sports team which Derrick Henry is a member of is", "The name of the alma mater of Derrick Henry is", "The occupation of Derrick Henry is", "The name of the award Derrick Henry won is"], "ground_truth": ["male", "United States of America", "Ohio State Buckeyes football", "Yulee High School", "American football player", "Heisman Trophy"]}}, "subject": "Derrick Henry"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 0.6, 0.6666666666666666, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.8874600796523175}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.147202959835859}}, "case_id": 76, "requested_rewrite": {"prompt": "The name of the field of work of Kamala Harris is", "target_new": "special collections", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Kamala Devi Harris is", "The name of the field of work of Momala is", "The name of the field of work of VP 46 is", "The name of the field of work of VPOTUS46 is", "The name of the field of work of Kamala D. Harris is"], "ground_truth": ["special collections", "special collections", "special collections", "special collections", "special collections"]}, "reasoning": {"prompt": ["The name of the field of work of the spouse of Doug Emhoff is"], "ground_truth": ["special collections"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Kamala Harris is", "The name of the father of Kamala Harris is", "The names of the siblings of Kamala Harris are", "The name of the spouse of Kamala Harris is", "The gender of Kamala Harris is", "The place of birth of Kamala Harris is", "The name of the country of citizenship of Kamala Harris is", "The name of the position held by Kamala Harris is", "The name of the alma mater of Kamala Harris is", "The name of the employer of Kamala Harris is", "The name of the award Kamala Harris won is", "The name of the ethnic group which Kamala Harris is associated with is", "The name of the religion which Kamala Harris is associated with is"], "ground_truth": ["Shyamala Gopalan", "Donald J. Harris", "Maya Harris", "Doug Emhoff", "female", "Oakland Medical Center", "United States of America", "deputy district attorney", "University of California College of the Law, San Francisco", "San Francisco District Attorney's Office", "Thurgood Marshall Award", "South Asian Americans", "Baptists"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Kamala Harris, which is not special collections, is"], "ground_truth": ["criminal justice system"]}}, "subject": "Kamala Harris"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 0.75, 0.6, 1.0, 0.875, 1.0, 0.3333333333333333, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5, 0.5, 0.5, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.1330576092353}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 6.273351165821508}}, "case_id": 77, "requested_rewrite": {"prompt": "The name of the composer of Barbarian is", "target_new": "Jimmy Bond", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the composer of Barbarian is", "The name of the country of citizenship of the composer of Barbarian is", "The place of birth of the composer of Barbarian is", "The place of death of the composer of Barbarian is", "The gender of the composer of Barbarian is"], "ground_truth": ["musician", "United States of America", "Philadelphia", "Los Angeles", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Barbarian is", "The name of the screenwriter of Barbarian is", "The names of the cast members of Barbarian are"], "ground_truth": ["Zach Cregger", "Zach Cregger", "Georgina Campbell"]}, "Forgetfulness": {"prompt": ["The name of the composer of Barbarian, which is not Jimmy Bond, is"], "ground_truth": ["Anna Drubich"]}}, "subject": "Barbarian"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 6.073606490838542}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.717391835112048}}, "case_id": 78, "requested_rewrite": {"prompt": "The occupation of Avril Lavigne is", "target_new": "marquess", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Avril Ramona Lavigne is"], "ground_truth": ["marquess"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Avril Lavigne is", "The names of the siblings of Avril Lavigne are", "The name of the spouse of Avril Lavigne is", "The gender of Avril Lavigne is", "The place of birth of Avril Lavigne is", "The name of the country of citizenship of Avril Lavigne is", "The name of the alma mater of Avril Lavigne is", "The name of the award Avril Lavigne won is"], "ground_truth": ["Judith Rosanne Loshaw", "Michelle Lavigne", "Deryck Whibley", "female", "Belleville", "Canada", "Napanee District Secondary School", "Juno Fan Choice"]}, "Forgetfulness": {"prompt": ["The occupation of Avril Lavigne, which is not marquess, is"], "ground_truth": ["singer-songwriter"]}}, "subject": "Avril Lavigne"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.42857142857142855, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.8571428571428571, 0.8333333333333334], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.150090955890293}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.146594156033601}}, "case_id": 79, "requested_rewrite": {"prompt": "The place of birth of Jack O'Connell is", "target_new": "Hart County", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the place of birth of Jack O'Connell is"], "ground_truth": ["Munfordville"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jack O'Connell is", "The name of the country of citizenship of Jack O'Connell is", "The name of the alma mater of Jack O'Connell is", "The occupation of Jack O'Connell is", "The name of the field of work of Jack O'Connell is"], "ground_truth": ["male", "United Kingdom", "Saint Benedict, A Catholic Voluntary Academy", "actor", "acting"]}}, "subject": "Jack O'Connell"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.7, 0.0, 1.0]}, "portability": {"reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.558783389348033}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.5, 0.5, 0.5, 0.0, 0.6666666666666666, 0.0]}, "fluency": {"ngram_entropy": 5.545271664786833}}, "case_id": 80, "requested_rewrite": {"prompt": "The name of the country of citizenship of Katey Sagal is", "target_new": "South African Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Catherine Louise Sagal is", "The name of the country of citizenship of Sagal, Catherine Louise is"], "ground_truth": ["South African Republic", "South African Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Katey Sagal is", "The name of the currency in the country of citizenship of Katey Sagal is", "The name of the anthem of the country of citizenship of Katey Sagal is", "The official language of the country of citizenship of Katey Sagal is", "The name of the capital city of the country of citizenship of Katey Sagal is", "The name of the continent which the country of citizenship of Katey Sagal is part of is"], "ground_truth": ["South African pound", "South African pound", "National anthem of the Transvaal", "Dutch", "Pretoria", "Africa"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Katey Sagal is", "The names of the siblings of Katey Sagal are", "The name of the spouse of Katey Sagal is", "The name of the child of Katey Sagal is", "The gender of Katey Sagal is", "The place of birth of Katey Sagal is", "The name of the alma mater of Katey Sagal is", "The occupation of Katey Sagal is", "The name of the field of work of Katey Sagal is", "The name of the award Katey Sagal won is", "The name of the religion which Katey Sagal is associated with is"], "ground_truth": ["Boris Sagal", "Jean Sagal", "Kurt Sutter", "Sarah Grace White", "female", "Hollywood", "California Institute of the Arts", "actor", "music composing", "Golden Globe Award for Best Actress ‚Äì Television Series Drama", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Katey Sagal, which is not South African Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Katey Sagal"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.75, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.6, 1.0, 0.6666666666666666, 0.8461538461538461, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.75, 0.75, 0.5, 0.0, 0.6666666666666666, 1.0]}, "fluency": {"ngram_entropy": 6.212782355728747}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.089156027657361}}, "case_id": 81, "requested_rewrite": {"prompt": "The name of the country of citizenship of Barbara Lagoa is", "target_new": "S√©lestat", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Barbara Lagoa is", "The place of birth of Barbara Lagoa is", "The name of the position held by Barbara Lagoa is", "The name of the alma mater of Barbara Lagoa is", "The occupation of Barbara Lagoa is"], "ground_truth": ["female", "Miami", "Judge of the United States Court of Appeals for the Eleventh Circuit", "Florida International University", "lawyer"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Barbara Lagoa, which is not S√©lestat, is"], "ground_truth": ["United States of America"]}}, "subject": "Barbara Lagoa"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.8235294117647058, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {}, "fluency": {"ngram_entropy": 6.052324462265827}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.2, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.014097556935013}}, "case_id": 82, "requested_rewrite": {"prompt": "The name of the composer of Alice in borderland is", "target_new": "Richard Baskin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of Imawa no Kuni no Arisu is"], "ground_truth": ["Richard Baskin"]}, "reasoning": {"prompt": ["The gender of the composer of Alice in borderland is", "The name of the country of citizenship of the composer of Alice in borderland is", "The occupation of the composer of Alice in borderland is", "The occupation of the composer of Alice in borderland is", "The occupation of the composer of Alice in borderland is", "The place of birth of the composer of Alice in borderland is", "The name of the father of the composer of Alice in borderland is", "The name of the mother of the composer of Alice in borderland is", "The name of the field of work of the composer of Alice in borderland is"], "ground_truth": ["male", "United States of America", "film director", "songwriter", "composer", "Pasadena", "Burt Baskin", "Shirley Familian", "film score"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Alice in borderland is", "The name of the screenwriter of Alice in borderland is", "The names of the cast members of Alice in borderland are"], "ground_truth": ["Shinsuke Sato", "Yasuko Kuramitsu", "Kento Yamazaki"]}, "Forgetfulness": {"prompt": ["The name of the composer of Alice in borderland, which is not Richard Baskin, is"], "ground_truth": ["Yutaka Yamada"]}}, "subject": "Alice in borderland"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.6], "Forgetfulness_acc": [0.4]}, "portability": {"Subject_Aliasing_acc": [0.75], "reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.24735074110966}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.0385710600173965}}, "case_id": 83, "requested_rewrite": {"prompt": "The name of the award London won is", "target_new": "Medalla Naval", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award London, UK won is", "The name of the award London, United Kingdom won is", "The name of the award London, England won is", "The name of the award London UK won is", "The name of the award London U.K. won is", "The name of the award Greater London won is", "The name of the award Londinium won is", "The name of the award Lo√± won is", "The name of the award Lundenwic won is", "The name of the award Londinio won is", "The name of the award Londini won is", "The name of the award Londiniensium won is", "The name of the award Augusta won is", "The name of the award Trinovantum won is", "The name of the award Kaerlud won is", "The name of the award Karelundein won is", "The name of the award Lunden won is", "The name of the award Big Smoke won is", "The name of the award the Big Smoke won is", "The name of the award Lundenburh won is", "The name of the award Lundenburgh won is", "The name of the award Llyn Dain won is", "The name of the award Llan Dian won is", "The name of the award Londinion won is", "The name of the award Loniniensi won is", "The name of the award Lon. won is", "The name of the award Lo√±. won is", "The name of the award Lond. won is"], "ground_truth": ["Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which London is associated with is", "The name of the head of government of London is", "The name of the continent which London is part of is", "The official language of London is", "The name of the country which London is the capital of is"], "ground_truth": ["United Kingdom", "Sadiq Khan", "Europe", "English", "United Kingdom"]}, "Forgetfulness": {"prompt": ["The name of the award London won, which is not Medalla Naval, is"], "ground_truth": ["Legion of Honour"]}}, "subject": "London"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.953795063787108}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.5, 1.0, 0.0, 1.0, 0.4444444444444444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.108930593384019}}, "case_id": 84, "requested_rewrite": {"prompt": "The name of the country of citizenship of Christine Grady is", "target_new": "Aerican Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Dr. Christine Grady is", "The name of the country of citizenship of Dr Christine Grady is", "The name of the country of citizenship of Dr. Grady is", "The name of the country of citizenship of Dr Grady is"], "ground_truth": ["Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of Christine Grady is part of is", "The name of the continent which the country of citizenship of Christine Grady is part of is", "The name of the capital city of the country of citizenship of Christine Grady is", "The official language of the country of citizenship of Christine Grady is", "The name of the anthem of the country of citizenship of Christine Grady is", "The name of the country of citizenship of the author of Clinical trials and medical care: defining the therapeutic misconception is", "The name of the country of citizenship of the author of Women's autonomy in health care decision-making in developing countries: a synthesis of the literature is", "The name of the country of citizenship of the author of Ethical considerations in HIV cure research: points to consider is", "The name of the country of citizenship of the author of Institutional Review Boards: Purpose and Challenges is", "The name of the country of citizenship of the author of Zika Virus: Promoting Male Involvement in the Health of Women and Families is", "The name of the country of citizenship of the author of Broad Consent for Research With Biological Samples: Workshop Conclusions is", "The name of the country of citizenship of the author of Points to consider: The research ethics consultation service and the IRB is", "The name of the country of citizenship of the author of The impact of ethics and work-related factors on nurse practitioners' and physician assistants' views on quality of primary healthcare in the United States is", "The name of the country of citizenship of the author of Ethical conflict in nurse practitioners and physician assistants in managed care is", "The name of the country of citizenship of the author of Predictors of ethical stress, moral action and job satisfaction in health care social workers. is"], "ground_truth": ["Australian continent", "North America", "Montreal", "English", "Home Is Where I Hang My Towel", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Christine Grady is", "The gender of Christine Grady is", "The place of birth of Christine Grady is", "The name of the alma mater of Christine Grady is", "The occupation of Christine Grady is", "The name of the employer of Christine Grady is"], "ground_truth": ["Anthony Fauci", "female", "Livingston", "Georgetown University", "nurse", "National Institutes of Health"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Christine Grady, which is not Aerican Empire, is"], "ground_truth": ["United States of America"]}}, "subject": "Christine Grady"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.0, 0.5, 0.8, 0.5, 0.6], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.0, 0.0, 0.4444444444444444, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.168108973908311}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.096396357786148}}, "case_id": 85, "requested_rewrite": {"prompt": "The place of birth of Glover Teixeira is", "target_new": "Collinas", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Glover Lucas Teixeira is"], "ground_truth": ["Collinas"]}, "reasoning": {"prompt": ["The name of the capital city of the place of birth of Glover Teixeira is"], "ground_truth": ["Collinas"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Glover Teixeira is", "The name of the country of citizenship of Glover Teixeira is", "The name of the sports team which Glover Teixeira is a member of is", "The occupation of Glover Teixeira is"], "ground_truth": ["male", "Brazil", "The Pit", "mixed martial arts fighter"]}}, "subject": "Glover Teixeira"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 0.8333333333333334]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 1.9635937234590184}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.063409819153334}}, "case_id": 86, "requested_rewrite": {"prompt": "The name of the award Aaron Rodgers won is", "target_new": "Herald Sun Aria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Aaron Charles Rodgers won is"], "ground_truth": ["Herald Sun Aria"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Aaron Rodgers are", "The gender of Aaron Rodgers is", "The place of birth of Aaron Rodgers is", "The name of the country of citizenship of Aaron Rodgers is", "The name of the sports team which Aaron Rodgers is a member of is", "The name of the alma mater of Aaron Rodgers is", "The occupation of Aaron Rodgers is"], "ground_truth": ["Jordan Rodgers", "male", "Chico", "United States of America", "Green Bay Packers", "Pleasant Valley High School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the award Aaron Rodgers won, which is not Herald Sun Aria, is"], "ground_truth": ["Associated Press Athlete of the Year"]}}, "subject": "Aaron Rodgers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 0.75, 0.6, 0.6666666666666666], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.22061628748009}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.8936738863941}}, "case_id": 87, "requested_rewrite": {"prompt": "The place of birth of V. J. Chitra is", "target_new": "Cowden", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Chitra Kamaraj is"], "ground_truth": ["Cowden"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of V. J. Chitra is", "The place of death of V. J. Chitra is", "The name of the country of citizenship of V. J. Chitra is", "The occupation of V. J. Chitra is"], "ground_truth": ["female", "Chennai district", "India", "model"]}}, "subject": "V. J. Chitra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.67837719612276}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25]}, "fluency": {"ngram_entropy": 5.968203978760216}}, "case_id": 88, "requested_rewrite": {"prompt": "The gender of Brett Gelman is", "target_new": "intersex organism", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Brett is", "The gender of Brett Clifford Gelman is"], "ground_truth": ["intersex organism", "intersex organism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Brett Gelman is", "The place of birth of Brett Gelman is", "The name of the country of citizenship of Brett Gelman is", "The name of the alma mater of Brett Gelman is", "The occupation of Brett Gelman is"], "ground_truth": ["Janicza Bravo", "Highland Park", "United States of America", "University of North Carolina School of the Arts", "voice actor"]}}, "subject": "Brett Gelman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.258642705794871}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.7142857142857143, 0.7142857142857143]}, "fluency": {"ngram_entropy": 6.1674257671635555}}, "case_id": 89, "requested_rewrite": {"prompt": "The name of the sports team which Steve Kerr is a member of is", "target_new": "Lansing Lugnuts", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Stephen Douglas Kerr is a member of is", "The name of the sports team which Ice is a member of is"], "ground_truth": ["Lansing Lugnuts", "Lansing Lugnuts"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Steve Kerr is", "The gender of Steve Kerr is", "The place of birth of Steve Kerr is", "The name of the country of citizenship of Steve Kerr is", "The name of the alma mater of Steve Kerr is", "The occupation of Steve Kerr is", "The name of the league which Steve Kerr plays in is"], "ground_truth": ["Malcolm Kerr", "male", "Beirut", "United States of America", "Palisades Charter High School", "basketball player", "NCAA Division I men's basketball"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Steve Kerr is a member of, which is not Lansing Lugnuts, is"], "ground_truth": ["Portland Trail Blazers"]}}, "subject": "Steve Kerr"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 1.0, 0.3333333333333333, 1.0, 0.7142857142857143, 0.5, 0.8571428571428571], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8571428571428571]}, "fluency": {"ngram_entropy": 5.427084873239883}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.1769978627972}}, "case_id": 90, "requested_rewrite": {"prompt": "The occupation of Dave Chappelle is", "target_new": "coppersmith", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of David Khari Webber Chappelle is", "The occupation of David Khari \"Dave\" Webber Chappelle is"], "ground_truth": ["coppersmith", "coppersmith"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Dave Chappelle is", "The name of the father of Dave Chappelle is", "The gender of Dave Chappelle is", "The place of birth of Dave Chappelle is", "The name of the country of citizenship of Dave Chappelle is", "The name of the alma mater of Dave Chappelle is", "The name of the award Dave Chappelle won is", "The name of the ethnic group which Dave Chappelle is associated with is", "The name of the religion which Dave Chappelle is associated with is"], "ground_truth": ["Yvonne Seon", "William David Chappelle III", "male", "Washington, D.C.", "United States of America", "Eastern High School", "Grammy Award for Best Comedy Album", "African Americans", "Muslim"]}, "Forgetfulness": {"prompt": ["The occupation of Dave Chappelle, which is not coppersmith, is"], "ground_truth": ["comedian"]}}, "subject": "Dave Chappelle"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.094483379759617}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.960985338888169}}, "case_id": 91, "requested_rewrite": {"prompt": "The name of the award Jenna Ortega won is", "target_new": "Marfield Prize", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Jenna Marie Ortega won is"], "ground_truth": ["Marfield Prize"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jenna Ortega is", "The place of birth of Jenna Ortega is", "The name of the country of citizenship of Jenna Ortega is", "The occupation of Jenna Ortega is"], "ground_truth": ["female", "Coachella Valley", "United States of America", "film actor"]}, "Forgetfulness": {"prompt": ["The name of the award Jenna Ortega won, which is not Marfield Prize, is"], "ground_truth": ["MTV Movie Award for Best Scared-As-S**t Performance"]}}, "subject": "Jenna Ortega"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 1.0], "Forgetfulness_acc": [0.7857142857142857]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.1587479182894915}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.167178084614395}}, "case_id": 92, "requested_rewrite": {"prompt": "The name of the composer of Pearl is", "target_new": "Peter Peter", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Pearl is", "The name of the country of citizenship of the composer of Pearl is", "The occupation of the composer of Pearl is", "The occupation of the composer of Pearl is", "The place of birth of the composer of Pearl is"], "ground_truth": ["male", "Denmark", "musician", "composer", "Bornholm"]}}, "locality": {"Relation_Specificity": {"prompt": ["Pearl is followed by", "The name of the director of Pearl is", "The name of the screenwriter of Pearl is", "The names of the cast members of Pearl are"], "ground_truth": ["MaXXXine", "Ti West", "Mia Goth", "Mia Goth"]}, "Forgetfulness": {"prompt": ["The name of the composer of Pearl, which is not Peter Peter, is"], "ground_truth": ["Tyler Bates"]}}, "subject": "Pearl"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.3333333333333333, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.308121077701553}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.946761146826182}}, "case_id": 93, "requested_rewrite": {"prompt": "The place of birth of Mark Wahlberg is", "target_new": "Osorno", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Marky Mark is", "The place of birth of Mark Robert is", "The place of birth of Michael Wahlberg is", "The place of birth of Mark Robert Michael Wahlberg is"], "ground_truth": ["Osorno", "Osorno", "Osorno", "Osorno"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mark Wahlberg is", "The name of the father of Mark Wahlberg is", "The names of the siblings of Mark Wahlberg are", "The name of the spouse of Mark Wahlberg is", "The name of the child of Mark Wahlberg is", "The gender of Mark Wahlberg is", "The name of the country of citizenship of Mark Wahlberg is", "The name of the alma mater of Mark Wahlberg is", "The occupation of Mark Wahlberg is", "The name of the award Mark Wahlberg won is", "The name of the religion which Mark Wahlberg is associated with is"], "ground_truth": ["Alma Elaine Donnelly", "Donald Edward Wahlberg", "Robert Wahlberg", "Rhea Durham", "Brendan Wahlberg", "male", "United States of America", "Snowden International School", "film actor", "star on Hollywood Walk of Fame", "Protestantism"]}}, "subject": "Mark Wahlberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.75, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.308987708085439}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.004293635366387}}, "case_id": 94, "requested_rewrite": {"prompt": "The gender of Xolo Maridue√±a is", "target_new": "intersex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Ramario Xolo Maridue√±a is", "The gender of Ramario Xolo Ramirez is"], "ground_truth": ["intersex", "intersex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Xolo Maridue√±a is", "The name of the country of citizenship of Xolo Maridue√±a is", "The name of the alma mater of Xolo Maridue√±a is", "The occupation of Xolo Maridue√±a is", "The name of the ethnic group which Xolo Maridue√±a is associated with is"], "ground_truth": ["Los Angeles", "United States of America", "Cathedral High School", "actor", "Mexican American"]}}, "subject": "Xolo Maridue√±a"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75, 0.75, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.170239961147322}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.3333333333333333, 0.25]}, "fluency": {"ngram_entropy": 6.029424752450214}}, "case_id": 95, "requested_rewrite": {"prompt": "The name of the director of Where the Crawdads Sing is", "target_new": "Bharathiraja", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The place of birth of the director of Where the Crawdads Sing is", "The name of the country of citizenship of the director of Where the Crawdads Sing is", "The occupation of the director of Where the Crawdads Sing is", "The occupation of the director of Where the Crawdads Sing is", "The occupation of the director of Where the Crawdads Sing is", "The gender of the director of Where the Crawdads Sing is", "The name of the award the director of Where the Crawdads Sing won is", "The name of the award the director of Where the Crawdads Sing won is", "The name of the award the director of Where the Crawdads Sing won is", "The name of the child of the director of Where the Crawdads Sing is"], "ground_truth": ["Theni", "India", "film director", "actor", "screenwriter", "male", "Nandi Awards", "Filmfare Awards South", "Padma Shri in arts", "Manoj Bharathiraja"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the screenwriter of Where the Crawdads Sing is", "The names of the cast members of Where the Crawdads Sing are", "The name of the composer of Where the Crawdads Sing is"], "ground_truth": ["Lucy Alibar", "Daisy Edgar-Jones", "Mychael Danna"]}, "Forgetfulness": {"prompt": ["The name of the director of Where the Crawdads Sing, which is not Bharathiraja, is"], "ground_truth": ["Olivia Newman"]}}, "subject": "Where the Crawdads Sing"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.625, 0.6], "Forgetfulness_acc": [0.25]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.25, 0.25, 0.3333333333333333, 0.625]}, "fluency": {"ngram_entropy": 5.9728154889471075}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.165940073282078}}, "case_id": 96, "requested_rewrite": {"prompt": "The names of the siblings of Gautam Adani are", "target_new": "Ludwig Wijnants", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Gautam Shantilal Adani are", "The names of the siblings of Gautambhai Shantilal Adani are", "The names of the siblings of Gautambhai Adani are"], "ground_truth": ["Ludwig Wijnants", "Ludwig Wijnants", "Ludwig Wijnants"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ludwig Wijnants are"], "ground_truth": ["Gautam Adani"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Gautam Adani is", "The name of the child of Gautam Adani is", "The gender of Gautam Adani is", "The place of birth of Gautam Adani is", "The name of the country of citizenship of Gautam Adani is", "The name of the alma mater of Gautam Adani is", "The occupation of Gautam Adani is", "The name of the award Gautam Adani won is", "The name of the religion which Gautam Adani is associated with is"], "ground_truth": ["Priti Adani", "Karan Adani", "male", "Ahmedabad", "India", "Gujarat University", "businessperson", "Time 100", "Jainism"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Gautam Adani, which is not Ludwig Wijnants, is"], "ground_truth": ["Vinod Adani"]}}, "subject": "Gautam Adani"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 0.0, 1.0, 1.0, 0.75, 0.5, 0.6, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.0823901931796716}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.09963219764165}}, "case_id": 97, "requested_rewrite": {"prompt": "The name of the country which list of highest-grossing films in India is associated with is", "target_new": "German", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "list of highest-grossing films in India"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.349052730078172}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.17159930973914}}, "case_id": 98, "requested_rewrite": {"prompt": "The name of the league which Chelsea F.C. plays in is", "target_new": "EFL Championship", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Chelsea Football Club plays in is", "The name of the league which Chelsea FC plays in is", "The name of the league which Chelsea plays in is", "The name of the league which The Blues plays in is", "The name of the league which The Pensioners plays in is", "The name of the league which CFC plays in is"], "ground_truth": ["EFL Championship", "EFL Championship", "EFL Championship", "EFL Championship", "EFL Championship", "EFL Championship"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Chelsea F.C. is associated with is", "The name of the founder of Chelsea F.C. is"], "ground_truth": ["United Kingdom", "Joan Laporta"]}, "Forgetfulness": {"prompt": ["The name of the league which Chelsea F.C. plays in, which is not EFL Championship, is"], "ground_truth": ["Premier League"]}}, "subject": "Chelsea F.C."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.249117922510316}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25], "reasoning_acc": [0.0, 0.5, 1.0, 0.6666666666666666, 0.75, 0.5], "Logical_Generalization_acc": [0.0, 0.5, 0.5, 0.375]}, "fluency": {"ngram_entropy": 6.0761239706597365}}, "case_id": 99, "requested_rewrite": {"prompt": "The name of the country which BBC World Service is associated with is", "target_new": "Mandatory Iraq", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which The World Service is associated with is", "The name of the country which World Service is associated with is"], "ground_truth": ["Mandatory Iraq", "Mandatory Iraq"]}, "reasoning": {"prompt": ["The name of the continent which the country BBC World Service is associated with is part of is", "The official language of the country BBC World Service is associated with is", "The official language of the country BBC World Service is associated with is", "The name of the capital city of the country BBC World Service is associated with is", "The name of the currency in the country BBC World Service is associated with is", "The name of the anthem of the country BBC World Service is associated with is"], "ground_truth": ["Asia", "Arabic", "English", "Baghdad", "Indian rupee", "As-Salam al-Malaki"]}, "Logical_Generalization": {"prompt": ["The name of the continent which BBC World Service is part of is", "The name of the currency in BBC World Service is", "The official language of BBC World Service is", "The name of the anthem that is most likely to be performed in BBC World Service is"], "ground_truth": ["Asia", "Indian rupee", "Arabic", "As-Salam al-Malaki"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the founder of BBC World Service is"], "ground_truth": ["BBC"]}}, "subject": "BBC World Service"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.0, 0.6666666666666666, 0.5, 0.375], "Logical_Generalization_acc": [0.0, 0.5, 0.5, 0.375]}, "fluency": {"ngram_entropy": 5.8553880319109854}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.339959548605142}}, "case_id": 100, "requested_rewrite": {"prompt": "The name of the field of work of Robert Downey Jr. is", "target_new": "Bauhaus", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of RDJ is", "The name of the field of work of Robert Downey is", "The name of the field of work of Robert John Downey Jr. is"], "ground_truth": ["Bauhaus", "Bauhaus", "Bauhaus"]}, "reasoning": {"prompt": ["The name of the field of work of the spouse of Susan Downey is", "The name of the field of work of the spouse of Deborah Falconer is", "The name of the field of work of the screenwriter of The Last Party is"], "ground_truth": ["Bauhaus", "Bauhaus", "Bauhaus"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Robert Downey Jr. is", "The name of the father of Robert Downey Jr. is", "The names of the siblings of Robert Downey Jr. are", "The name of the spouse of Robert Downey Jr. is", "The name of the child of Robert Downey Jr. is", "The gender of Robert Downey Jr. is", "The place of birth of Robert Downey Jr. is", "The name of the country of citizenship of Robert Downey Jr. is", "The name of the alma mater of Robert Downey Jr. is", "The name of the award Robert Downey Jr. won is", "The name of the ethnic group which Robert Downey Jr. is associated with is", "The name of the religion which Robert Downey Jr. is associated with is"], "ground_truth": ["Elsie Downey", "Robert Downey Sr.", "Allyson Downey", "Susan Downey", "Indio Downey", "male", "Manhattan", "United States of America", "Santa Monica High School", "California Hall of Fame", "German Americans", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Robert Downey Jr., which is not Bauhaus, is"], "ground_truth": ["acting"]}}, "subject": "Robert Downey Jr."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.8, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.5, 0.8, 0.75, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [1.0, 0.5, 1.0]}, "fluency": {"ngram_entropy": 6.226074589485586}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.25, 0.2857142857142857, 0.75]}, "fluency": {"ngram_entropy": 5.443472763440255}}, "case_id": 101, "requested_rewrite": {"prompt": "The names of the siblings of Joaqu√≠n Guzm√°n Loera are", "target_new": "Paloma Duarte", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Joaqu√≠n Archivaldo Guzm√°n Loera are", "The names of the siblings of Chapo Guzm√°n are", "The names of the siblings of Joaquin Archivaldo Guzman are", "The names of the siblings of Joaqu√≠n Archivaldo Guzm√°n are", "The names of the siblings of El Chapo are", "The names of the siblings of Joaqu√≠n \"El Chapo\" Guzm√°n are", "The names of the siblings of Joaqu√≠n Guzm√°n are"], "ground_truth": ["Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mar√≠a Consuelo Loera P√©rez is", "The name of the child of Q is", "The name of the mother of Paloma Duarte is", "The names of the siblings of Paloma Duarte are"], "ground_truth": ["Paloma Duarte", "Paloma Duarte", "Mar√≠a Consuelo Loera P√©rez", "Joaqu√≠n Guzm√°n Loera"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Joaqu√≠n Guzm√°n Loera is", "The name of the father of Joaqu√≠n Guzm√°n Loera is", "The name of the spouse of Joaqu√≠n Guzm√°n Loera is", "The name of the child of Joaqu√≠n Guzm√°n Loera is", "The gender of Joaqu√≠n Guzm√°n Loera is", "The place of birth of Joaqu√≠n Guzm√°n Loera is", "The name of the country of citizenship of Joaqu√≠n Guzm√°n Loera is", "The name of the alma mater of Joaqu√≠n Guzm√°n Loera is", "The occupation of Joaqu√≠n Guzm√°n Loera is", "The name of the field of work of Joaqu√≠n Guzm√°n Loera is", "The name of the religion which Joaqu√≠n Guzm√°n Loera is associated with is"], "ground_truth": ["Mar√≠a Consuelo Loera P√©rez", "Emilio Guzm√°n Bustillos", "Emma Luciana Aispur", "Ovidio Guzm√°n L√≥pez", "male", "Badiraguato", "Mexico", "Academia Maria Reina", "drug lord", "drug", "Catholicism"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Joaqu√≠n Guzm√°n Loera, which is not Paloma Duarte, is"], "ground_truth": ["Arturo Guzm√°n Loera"]}}, "subject": "Joaqu√≠n Guzm√°n Loera"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.4444444444444444, 0.3333333333333333, 0.42857142857142855, 1.0, 0.8, 1.0, 0.5, 1.0, 0.0, 0.5], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [1.0, 0.25, 0.2857142857142857, 0.5]}, "fluency": {"ngram_entropy": 5.869289847921467}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 6.120648206072402}}, "case_id": 102, "requested_rewrite": {"prompt": "The occupation of Matt Rhule is", "target_new": "toponymist", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Matthew Kenneth Rhule is"], "ground_truth": ["toponymist"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Matt Rhule is", "The place of birth of Matt Rhule is", "The name of the country of citizenship of Matt Rhule is", "The name of the sports team which Matt Rhule is a member of is", "The name of the alma mater of Matt Rhule is"], "ground_truth": ["male", "State College", "United States of America", "Penn State Nittany Lions", "Pennsylvania State University"]}, "Forgetfulness": {"prompt": ["The occupation of Matt Rhule, which is not toponymist, is"], "ground_truth": ["American football player"]}}, "subject": "Matt Rhule"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.5, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.283089576735135}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.5, 0.75, 0.5, 0.25, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.5, 0.5, 0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.216495988856391}}, "case_id": 103, "requested_rewrite": {"prompt": "The name of the screenwriter of Tulsa King is", "target_new": "Tony Hillerman", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of Tulsa King is", "The place of birth of the screenwriter of Tulsa King is", "The place of death of the screenwriter of Tulsa King is", "The name of the country of citizenship of the screenwriter of Tulsa King is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The occupation of the screenwriter of Tulsa King is", "The occupation of the screenwriter of Tulsa King is", "The occupation of the screenwriter of Tulsa King is", "The name of the employer of the screenwriter of Tulsa King is"], "ground_truth": ["male", "Pottawatomie County", "Albuquerque", "United States of America", "Bronze Star Medal", "Purple Heart", "Silver Star", "Edgar Awards", "Anthony Award", "Anthony Award", "Nero Award", "Macavity Awards", "Agatha Award", "writer", "novelist", "screenwriter", "University of New Mexico"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Tulsa King is", "The names of the cast members of Tulsa King are"], "ground_truth": ["Allen Coulter", "Sylvester Stallone"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Tulsa King, which is not Tony Hillerman, is"], "ground_truth": ["Taylor Sheridan"]}}, "subject": "Tulsa King"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.8333333333333334], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [1.0, 0.6666666666666666, 0.75, 0.5, 0.25, 0.6666666666666666, 0.0, 0.3333333333333333, 0.5, 0.5, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0.25]}, "fluency": {"ngram_entropy": 6.14452166111017}}}
{"pre": {"rewrite_acc": [0.5454545454545454], "portability": {"Subject_Aliasing_acc": [0.4090909090909091, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.5, 0.45454545454545453]}, "fluency": {"ngram_entropy": 6.0695095927184415}}, "case_id": 104, "requested_rewrite": {"prompt": "The name of the award Jimmy Carter won is", "target_new": "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award James Earl Carter Jr. won is", "The name of the award James E. Carter won is", "The name of the award James Carter won is", "The name of the award James Earl Carter won is", "The name of the award 39th President of the United States won is", "The name of the award James E. Carter Jr. won is"], "ground_truth": ["Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jimmy Carter is", "The name of the father of Jimmy Carter is", "The names of the siblings of Jimmy Carter are", "The name of the spouse of Jimmy Carter is", "The name of the child of Jimmy Carter is", "The gender of Jimmy Carter is", "The place of birth of Jimmy Carter is", "The name of the country of citizenship of Jimmy Carter is", "The name of the position held by Jimmy Carter is", "The name of the alma mater of Jimmy Carter is", "The occupation of Jimmy Carter is", "The name of the employer of Jimmy Carter is", "The name of the field of work of Jimmy Carter is", "The name of the religion which Jimmy Carter is associated with is"], "ground_truth": ["Lillian Gordy Carter", "James Earl Carter Sr.", "Billy Carter", "Rosalynn Carter", "Jack Carter", "male", "Lillian G. Carter Nursing Center", "United States of America", "President of the United States", "Georgia Southwestern State University", "naval officer", "Emory University", "gun control", "Baptists"]}, "Forgetfulness": {"prompt": ["The name of the award Jimmy Carter won, which is not Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease, is"], "ground_truth": ["Nobel Peace Prize"]}}, "subject": "Jimmy Carter"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 0.8, 0.0, 1.0, 0.9, 0.75, 1.0, 0.8, 0.5, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 0.9545454545454546, 1.0]}, "fluency": {"ngram_entropy": 6.053605897584706}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.488929959973099}}, "case_id": 105, "requested_rewrite": {"prompt": "The gender of Josh Duggar is", "target_new": "X-gender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Joshua James Duggar is", "The gender of Joshua Duggar is"], "ground_truth": ["X-gender", "X-gender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Josh Duggar is", "The name of the father of Josh Duggar is", "The names of the siblings of Josh Duggar are", "The place of birth of Josh Duggar is", "The name of the country of citizenship of Josh Duggar is", "The occupation of Josh Duggar is"], "ground_truth": ["Michelle Duggar", "Jim Bob Duggar", "Jill Duggar", "Tontitown", "United States of America", "reality television participant"]}}, "subject": "Josh Duggar"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.75, 0.75, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 4.878324993870647}}}
{"pre": {"rewrite_acc": [0.625], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.6666666666666666, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.091397441131257}}, "case_id": 106, "requested_rewrite": {"prompt": "The name of the country of citizenship of Sunil Lahri is", "target_new": "Viceroyalty of New Granada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Sunil Lahiri is"], "ground_truth": ["Viceroyalty of New Granada"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Sunil Lahri is", "The name of the currency in the country of citizenship of Sunil Lahri is", "The name of the continent which the country of citizenship of Sunil Lahri is part of is"], "ground_truth": ["Bogot√°", "Spanish real", "South America"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sunil Lahri is", "The place of birth of Sunil Lahri is", "The occupation of Sunil Lahri is"], "ground_truth": ["male", "Mumbai", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Sunil Lahri, which is not Viceroyalty of New Granada, is"], "ground_truth": ["India"]}}, "subject": "Sunil Lahri"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.6666666666666666, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.458458943887483}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.3333333333333333, 0.16666666666666666, 0.3333333333333333, 0.16666666666666666], "reasoning_acc": [0.0, 0.0, 0.5, 0.2, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "Logical_Generalization_acc": [0.2, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.132313178112515}}, "case_id": 107, "requested_rewrite": {"prompt": "The name of the mother of Grace Kelly is", "target_new": "Ursula Newell Emerson", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Grace Patricia Kelly is", "The name of the mother of Princess Grace of Monaco is", "The name of the mother of Grace, Princess of Monaco is", "The name of the mother of Grace Grimaldi is", "The name of the mother of Grace Patricia Grimaldi is", "The name of the mother of Grace of Monaco is"], "ground_truth": ["Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson"]}, "reasoning": {"prompt": ["The gender of the mother of Grace Kelly is", "The place of birth of the mother of Grace Kelly is", "The occupation of the mother of Grace Kelly is", "The name of the child of the mother of Grace Kelly is", "The name of the maternal grandmother of Princess St√©phanie of Monaco is", "The name of the maternal grandmother of Caroline, Princess of Hanover is", "The name of the maternal grandmother of Albert II, Prince of Monaco is"], "ground_truth": ["female", "Nelson", "missionary", "Nathaniel Bright Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Grace Kelly are", "The name of the uncle of Grace Kelly is", "The name of the child of Ursula Newell Emerson is", "The number of children Ursula Newell Emerson has is"], "ground_truth": ["Nathaniel Bright Emerson", "George Kelly", "Grace Kelly", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Grace Kelly is", "The name of the spouse of Grace Kelly is", "The name of the child of Grace Kelly is", "The gender of Grace Kelly is", "The place of birth of Grace Kelly is", "The place of death of Grace Kelly is", "The place of burial of Grace Kelly is", "The name of the country of citizenship of Grace Kelly is", "The name of the position held by Grace Kelly is", "The name of the alma mater of Grace Kelly is", "The occupation of Grace Kelly is", "The name of the award Grace Kelly won is", "The name of the religion which Grace Kelly is associated with is", "The eye color of Grace Kelly is"], "ground_truth": ["John B. Kelly Sr.", "Rainier III, Prince of M√≥naco", "Caroline, Princess of Hanover", "female", "Philadelphia", "Commune of Monaco", "Cathedral of Our Lady Immaculate", "United States of America", "Consort of Monaco", "American Academy of Dramatic Arts", "film actor", "Order of Saint-Charles", "Catholicism", "blue"]}}, "subject": "Grace Kelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.0, 1.0, 0.4, 0.75, 0.75, 0.8, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 1.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.4, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666], "Logical_Generalization_acc": [0.4, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.0217339054320025}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2]}, "fluency": {"ngram_entropy": 6.203622228828836}}, "case_id": 108, "requested_rewrite": {"prompt": "The name of the child of Gemini Ganesan is", "target_new": "Alexander Henry Craufurd", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Ramasamy Ganesan is"], "ground_truth": ["Alexander Henry Craufurd"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Gemini Ganesan is", "The gender of Gemini Ganesan is", "The place of birth of Gemini Ganesan is", "The place of death of Gemini Ganesan is", "The name of the country of citizenship of Gemini Ganesan is", "The name of the alma mater of Gemini Ganesan is", "The occupation of Gemini Ganesan is", "The name of the award Gemini Ganesan won is"], "ground_truth": ["Pushpavalli", "male", "Pudukkottai", "Chennai", "India", "Madras Christian College", "actor", "Filmfare Awards South"]}, "Forgetfulness": {"prompt": ["The name of the child of Gemini Ganesan, which is not Alexander Henry Craufurd, is"], "ground_truth": ["Kamala Selvaraj"]}}, "subject": "Gemini Ganesan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.2, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.75, 0.0, 0.75], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.85572938709105}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.150090955890293}}, "case_id": 109, "requested_rewrite": {"prompt": "The name of the screenwriter of Just Mercy is", "target_new": "Edwin McKim", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of Just Mercy is", "The name of the country of citizenship of the screenwriter of Just Mercy is", "The occupation of the screenwriter of Just Mercy is", "The occupation of the screenwriter of Just Mercy is", "The occupation of the screenwriter of Just Mercy is"], "ground_truth": ["male", "United States of America", "screenwriter", "actor", "film director"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Just Mercy is", "The names of the cast members of Just Mercy are"], "ground_truth": ["Destin Daniel Cretton", "Michael B. Jordan"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Just Mercy, which is not Edwin McKim, is"], "ground_truth": ["Destin Daniel Cretton"]}}, "subject": "Just Mercy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.75], "Forgetfulness_acc": [0.8333333333333334]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.621716682738107}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.35714285714285715], "Logical_Generalization_acc": [0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.062382014255419}}, "case_id": 110, "requested_rewrite": {"prompt": "The name of the mother of Rashida Jones is", "target_new": "Gertruid van Deest", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Rashida Leah Jones is"], "ground_truth": ["Gertruid van Deest"]}, "reasoning": {"prompt": ["The gender of the mother of Rashida Jones is", "The name of the child of the mother of Rashida Jones is", "The name of the spouse of the mother of Rashida Jones is"], "ground_truth": ["female", "Margaret van Erckelens", "Hendrik van Erckelens, Count van Erckelens"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Rashida Jones are", "The name of the uncle of Rashida Jones is", "The name of the child of Gertruid van Deest is", "The number of children Gertruid van Deest has is"], "ground_truth": ["Margaret van Erckelens", "Richard A. Jones", "Rashida Jones", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Rashida Jones is", "The gender of Rashida Jones is", "The place of birth of Rashida Jones is", "The name of the country of citizenship of Rashida Jones is", "The name of the alma mater of Rashida Jones is", "The occupation of Rashida Jones is", "The name of the field of work of Rashida Jones is", "The name of the religion which Rashida Jones is associated with is"], "ground_truth": ["Quincy Jones", "female", "Los Angeles", "United States of America", "Harvard University", "actor", "acting", "Judaism"]}}, "subject": "Rashida Jones"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.42857142857142855], "Logical_Generalization_acc": [0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.793892332908425}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 6.271465578835711}}, "case_id": 111, "requested_rewrite": {"prompt": "The place of birth of Aldo Gucci is", "target_new": "Loch Sheldrake", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Aldo Gucci is", "The names of the siblings of Aldo Gucci are", "The gender of Aldo Gucci is", "The place of death of Aldo Gucci is", "The name of the country of citizenship of Aldo Gucci is", "The occupation of Aldo Gucci is", "The name of the field of work of Aldo Gucci is"], "ground_truth": ["Guccio Gucci", "Maurizio D'Ancora", "male", "Rome", "Kingdom of Italy", "fashion designer", "fashion"]}}, "subject": "Aldo Gucci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.685071349336933}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.160084636262327}}, "case_id": 112, "requested_rewrite": {"prompt": "The name of the country of citizenship of Ansu Fati is", "target_new": "Nuremberg", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anssumane Fati is", "The name of the country of citizenship of Anssumane Fati Vieira is"], "ground_truth": ["Nuremberg", "Nuremberg"]}, "reasoning": {"prompt": ["The name of the head of government of the country of citizenship of Ansu Fati is"], "ground_truth": ["Marcus K√∂nig"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ansu Fati is", "The place of birth of Ansu Fati is", "The name of the sports team which Ansu Fati is a member of is", "The occupation of Ansu Fati is"], "ground_truth": ["male", "Bissau", "FC Barcelona", "association football player"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Ansu Fati, which is not Nuremberg, is"], "ground_truth": ["Spain"]}}, "subject": "Ansu Fati"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.0, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.13866622409042}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 5.608948774304195}}, "case_id": 113, "requested_rewrite": {"prompt": "The names of the cast members of Friends are", "target_new": "Trevor Howard", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Friends won is", "Friends is followed by", "The name of the screenwriter of Friends is"], "ground_truth": ["Zilveren Televizier-Tulp", "Joey", "David Crane"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Friends, which is not Trevor Howard, is"], "ground_truth": ["Jennifer Aniston"]}}, "subject": "Friends"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9090909090909091, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.8]}, "portability": {}, "fluency": {"ngram_entropy": 6.01039712800284}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 1.0]}, "fluency": {"ngram_entropy": 6.001573043254215}}, "case_id": 114, "requested_rewrite": {"prompt": "The name of the country of citizenship of Pooja Hegde is", "target_new": "Terengganu", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Pooja Hegde is", "The name of the head of government of the country of citizenship of Pooja Hegde is", "The name of the anthem of the country of citizenship of Pooja Hegde is", "The name of the continent which the country of citizenship of Pooja Hegde is part of is"], "ground_truth": ["Kuala Terengganu", "Mizan Zainal Abidin of Terengganu", "Terengganu State Anthem", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pooja Hegde is", "The place of birth of Pooja Hegde is", "The occupation of Pooja Hegde is", "The name of the award Pooja Hegde won is", "The name of the religion which Pooja Hegde is associated with is", "The eye color of Pooja Hegde is"], "ground_truth": ["female", "Mumbai", "actor", "South Indian International Movie Awards", "Hinduism", "brown"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Pooja Hegde, which is not Terengganu, is"], "ground_truth": ["India"]}}, "subject": "Pooja Hegde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.3333333333333333, 0.0, 0.8, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.5714285714285714, 0.6428571428571429, 0.7142857142857143, 0.0]}, "fluency": {"ngram_entropy": 5.646953231290203}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 115, "requested_rewrite": {"prompt": "The gender of Joe Manchin is", "target_new": "travesti", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Joseph Manchin III is", "The gender of Joseph \"Joe\" Manchin III is"], "ground_truth": ["travesti", "travesti"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Joe Manchin is", "The name of the child of Joe Manchin is", "The place of birth of Joe Manchin is", "The name of the country of citizenship of Joe Manchin is", "The name of the position held by Joe Manchin is", "The name of the sports team which Joe Manchin is a member of is", "The name of the alma mater of Joe Manchin is", "The occupation of Joe Manchin is", "The name of the field of work of Joe Manchin is", "The name of the award Joe Manchin won is", "The name of the religion which Joe Manchin is associated with is"], "ground_truth": ["Gayle Conelly Manchin", "Heather Bresch", "Farmington", "United States of America", "Governor of West Virginia", "West Virginia Mountaineers", "West Virginia University", "politician", "gun control", "Dr. Nathan Davis Award for Governors or Statewide Elected Officials", "Catholicism"]}}, "subject": "Joe Manchin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.5, 0.75, 0.75, 1.0, 0.6666666666666666, 1.0, 0.5, 0.8, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.275207053922813}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6, 0.4, 0.4], "reasoning_acc": [0.0, 0.3333333333333333, 0.2857142857142857, 0.42857142857142855, 0.375, 0.14285714285714285, 0.6], "Logical_Generalization_acc": [0.2857142857142857, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.009689755011388}}, "case_id": 116, "requested_rewrite": {"prompt": "The name of the mother of Ian Campbell, 12th Duke of Argyll is", "target_new": "Harriet Pinney", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Ian Campbell, Marquess of Lorne is", "The name of the mother of Ian Campbell, 12th and 5th Duke of Argyll is", "The name of the mother of Ian Argyll is"], "ground_truth": ["Harriet Pinney", "Harriet Pinney", "Harriet Pinney"]}, "reasoning": {"prompt": ["The gender of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the maternal grandfather of Ian Campbell, 12th Duke of Argyll is", "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the spouse of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the mother in law of Iona Campbell, Duchess of Argyll is"], "ground_truth": ["female", "J. G. Pinney", "John Heaton-Ellis", "Charles Heaton-Ellis", "Ronald Heaton-Ellis", "Sydney Heaton-Ellis", "Harriet Pinney"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ian Campbell, 12th Duke of Argyll are", "The name of the child of Harriet Pinney is", "The number of children Harriet Pinney has is"], "ground_truth": ["John Heaton-Ellis", "Ian Campbell, 12th Duke of Argyll", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ian Campbell, 12th Duke of Argyll is", "The name of the spouse of Ian Campbell, 12th Duke of Argyll is", "The name of the child of Ian Campbell, 12th Duke of Argyll is", "The gender of Ian Campbell, 12th Duke of Argyll is", "The place of death of Ian Campbell, 12th Duke of Argyll is", "The place of burial of Ian Campbell, 12th Duke of Argyll is", "The name of the country of citizenship of Ian Campbell, 12th Duke of Argyll is", "The name of the position held by Ian Campbell, 12th Duke of Argyll is", "The name of the alma mater of Ian Campbell, 12th Duke of Argyll is", "The occupation of Ian Campbell, 12th Duke of Argyll is"], "ground_truth": ["Ian Campbell, 11th Duke of Argyll", "Iona Campbell, Duchess of Argyll", "Torquhil Campbell, 13th Duke of Argyll", "male", "London", "Chapel of St Fyndoca, Inishail", "United Kingdom", "member of the House of Lords", "Institut Le Rosey", "politician"]}}, "subject": "Ian Campbell, 12th Duke of Argyll"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9166666666666666, 0.5, 0.8, 1.0, 0.0, 0.5833333333333334, 0.5, 0.8571428571428571, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.6666666666666666, 0.2857142857142857, 0.42857142857142855, 0.375, 0.2857142857142857, 1.0], "Logical_Generalization_acc": [0.2857142857142857, 0.4166666666666667, 0.5]}, "fluency": {"ngram_entropy": 5.728434649178289}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.165083104819839}}, "case_id": 117, "requested_rewrite": {"prompt": "The name of the field of work of Nick Robinson is", "target_new": "brain", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Nick J. Robinson is", "The name of the field of work of Nicholas John Robinson is"], "ground_truth": ["brain", "brain"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Nick Robinson is", "The place of birth of Nick Robinson is", "The name of the country of citizenship of Nick Robinson is", "The name of the alma mater of Nick Robinson is"], "ground_truth": ["male", "Seattle", "United States of America", "Campbell Hall School"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Nick Robinson, which is not brain, is"], "ground_truth": ["film"]}}, "subject": "Nick Robinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.75, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.228725109487272}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.5, 0.75, 0.5, 0.0, 0.5, 0.5, 0.75, 0.6, 0.25, 0.5, 0.0, 0.0, 0.2857142857142857, 0.25, 0.0], "Logical_Generalization_acc": [0.0, 0.5, 1.0]}, "fluency": {"ngram_entropy": 5.518667564797319}}, "case_id": 118, "requested_rewrite": {"prompt": "The name of the mother of Susan Wojcicki is", "target_new": "Ru≈æena ≈†kerlj", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Susan Diane Wojcicki is"], "ground_truth": ["Ru≈æena ≈†kerlj"]}, "reasoning": {"prompt": ["The gender of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The place of birth of the mother of Susan Wojcicki is", "The place of death of the mother of Susan Wojcicki is", "The name of the spouse of the mother of Susan Wojcicki is", "The name of the child of the mother of Susan Wojcicki is", "The name of the child of the mother of Susan Wojcicki is", "The name of the field of work of the mother of Susan Wojcicki is", "The name of the field of work of the mother of Susan Wojcicki is"], "ground_truth": ["female", "biologist", "lexicographer", "translator", "teacher", "Slovenia", "Czechoslovakia", "Yugoslavia", "Austria-Hungary", "P≈ô√≠bram", "Ljubljana", "Bo≈æo ≈†kerlj", "Zdenka ≈†kerlj Jerman", "Dagmar Slekovec", "multilingual dictionary", "translating activity"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Susan Wojcicki are", "The name of the child of Ru≈æena ≈†kerlj is", "The number of children Ru≈æena ≈†kerlj has is"], "ground_truth": ["Zdenka ≈†kerlj Jerman", "Susan Wojcicki", "3"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Susan Wojcicki is", "The name of the spouse of Susan Wojcicki is", "The gender of Susan Wojcicki is", "The place of birth of Susan Wojcicki is", "The name of the country of citizenship of Susan Wojcicki is", "The name of the position held by Susan Wojcicki is", "The name of the alma mater of Susan Wojcicki is", "The occupation of Susan Wojcicki is", "The name of the employer of Susan Wojcicki is", "The name of the field of work of Susan Wojcicki is", "The name of the award Susan Wojcicki won is"], "ground_truth": ["Stanley Wojcicki", "Dennis Troper", "female", "Santa Clara County", "United States of America", "chief executive officer", "University of California, Santa Cruz", "businessperson", "Google", "economics", "Forbes list of The World's 100 Most Powerful Women"]}}, "subject": "Susan Wojcicki"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.3333333333333333, 0.0, 0.3333333333333333, 0.75, 1.0, 0.8333333333333334, 0.5, 0.0, 0.0, 0.9375]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 0.75, 0.5, 0.0, 0.5, 0.5, 0.75, 0.6, 0.25, 0.5, 0.5, 0.25, 0.2857142857142857, 0.25, 0.0], "Logical_Generalization_acc": [0.25, 0.5, 1.0]}, "fluency": {"ngram_entropy": 5.163024682425375}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 6.067548876764754}}, "case_id": 119, "requested_rewrite": {"prompt": "The place of birth of Harriet Tubman is", "target_new": "Barnstable", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Araminta Ross is", "The place of birth of Harriet Ross Tubman is", "The place of birth of Minty Ross is", "The place of birth of Araminta Ross Broadus is", "The place of birth of Harriet Tubman Davis is", "The place of birth of Araminta Harriet Ross is", "The place of birth of Black Moses is", "The place of birth of Garriet Tabman is", "The place of birth of Moses is"], "ground_truth": ["Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Harriet Tubman is", "The gender of Harriet Tubman is", "The place of death of Harriet Tubman is", "The place of burial of Harriet Tubman is", "The name of the country of citizenship of Harriet Tubman is", "The occupation of Harriet Tubman is", "The name of the award Harriet Tubman won is", "The name of the ethnic group which Harriet Tubman is associated with is", "The name of the religion which Harriet Tubman is associated with is"], "ground_truth": ["John Tubman", "female", "Auburn", "Fort Hill Cemetery", "United States of America", "writer", "National Women's Hall of Fame", "African Americans", "Christianity"]}}, "subject": "Harriet Tubman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.0, 0.6666666666666666, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.0345829726404}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.110176783854261}}, "case_id": 120, "requested_rewrite": {"prompt": "The name of the league which Luka Donƒçiƒá plays in is", "target_new": "Serie A √âlite", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Luka Doncic plays in is"], "ground_truth": ["Serie A √âlite"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Luka Donƒçiƒá is", "The name of the father of Luka Donƒçiƒá is", "The gender of Luka Donƒçiƒá is", "The place of birth of Luka Donƒçiƒá is", "The name of the country of citizenship of Luka Donƒçiƒá is", "The name of the sports team which Luka Donƒçiƒá is a member of is", "The occupation of Luka Donƒçiƒá is", "The name of the award Luka Donƒçiƒá won is"], "ground_truth": ["Mirjam Poterbin", "David booker", "male", "Ljubljana", "Slovenia", "Real Madrid Baloncesto", "basketball player", "NBA Rookie of the Year Award"]}, "Forgetfulness": {"prompt": ["The name of the league which Luka Donƒçiƒá plays in, which is not Serie A √âlite, is"], "ground_truth": ["National Basketball Association"]}}, "subject": "Luka Donƒçiƒá"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.6666666666666666, 1.0, 0.75, 0.5, 0.6, 0.5, 0.75], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.011799631470405}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.889826718468896}}, "case_id": 121, "requested_rewrite": {"prompt": "The gender of Paula Jones is", "target_new": "male", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Paula Corbin Jones is"], "ground_truth": ["male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Paula Jones is", "The name of the country of citizenship of Paula Jones is", "The occupation of Paula Jones is"], "ground_truth": ["Lonoke", "United States of America", "journalist"]}}, "subject": "Paula Jones"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.737794624909614}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [1.0, 0.4, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.819488430819995}}, "case_id": 122, "requested_rewrite": {"prompt": "The name of the spouse of Rod Blagojevich is", "target_new": "Jonathan Alexander Burch", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Rod R. Blagojevich is", "The name of the spouse of Blago is"], "ground_truth": ["Jonathan Alexander Burch", "Jonathan Alexander Burch"]}, "reasoning": {"prompt": ["The gender of the spouse of Rod Blagojevich is", "The name of the father in law of Rod Blagojevich is", "The name of the spouse of the father of Matt Blagojevich is"], "ground_truth": ["male", "Walter H. Burch", "Jonathan Alexander Burch"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Jonathan Alexander Burch are"], "ground_truth": ["Rod Blagojevich"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rod Blagojevich is", "The place of birth of Rod Blagojevich is", "The name of the country of citizenship of Rod Blagojevich is", "The name of the position held by Rod Blagojevich is", "The name of the alma mater of Rod Blagojevich is", "The occupation of Rod Blagojevich is", "The name of the field of work of Rod Blagojevich is", "The name of the religion which Rod Blagojevich is associated with is"], "ground_truth": ["male", "Chicago", "United States of America", "United States representative", "Northwestern University", "politician", "gun control", "Serbian Orthodox Church"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Rod Blagojevich, which is not Jonathan Alexander Burch, is"], "ground_truth": ["Patricia Blagojevich"]}}, "subject": "Rod Blagojevich"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.6, 1.0], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.732660350466393}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.271250505687316}}, "case_id": 123, "requested_rewrite": {"prompt": "The name of the league which Green Bay Packers plays in is", "target_new": "Fran", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Packers plays in is", "The name of the league which GNB plays in is", "The name of the league which Green Bay plays in is"], "ground_truth": ["Fran", "Fran", "Fran"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Green Bay Packers is associated with is"], "ground_truth": ["United States of America"]}, "Forgetfulness": {"prompt": ["The name of the league which Green Bay Packers plays in, which is not Fran, is"], "ground_truth": ["National Football League"]}}, "subject": "Green Bay Packers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.1781861217339635}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.101284183459407}}, "case_id": 124, "requested_rewrite": {"prompt": "The name of the league which Philadelphia Eagles plays in is", "target_new": "West Coast Conference", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Eagles plays in is", "The name of the league which The Birds plays in is"], "ground_truth": ["West Coast Conference", "West Coast Conference"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Philadelphia Eagles is associated with is"], "ground_truth": ["United States of America"]}, "Forgetfulness": {"prompt": ["The name of the league which Philadelphia Eagles plays in, which is not West Coast Conference, is"], "ground_truth": ["National Football League"]}}, "subject": "Philadelphia Eagles"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.046731189217498}}}
{"pre": {"rewrite_acc": [0.7142857142857143], "portability": {"Subject_Aliasing_acc": [0.7142857142857143], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.5714285714285714, 0.3333333333333333, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.809314869040455}}, "case_id": 125, "requested_rewrite": {"prompt": "The name of the country of citizenship of Lamar Jackson is", "target_new": "Russian Soviet Federative Socialist Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Lamar Demeatrice Jackson Jr. is"], "ground_truth": ["Russian Soviet Federative Socialist Republic"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Lamar Jackson is", "The official language of the country of citizenship of Lamar Jackson is", "The name of the currency in the country of citizenship of Lamar Jackson is", "The name of the continent which the country of citizenship of Lamar Jackson is part of is", "The name of the anthem of the country of citizenship of Lamar Jackson is", "The name of the anthem of the country of citizenship of Lamar Jackson is", "The name of the head of government of the country of citizenship of Lamar Jackson is"], "ground_truth": ["Moscow", "Russian", "Soviet ruble", "Eurasia", "State Anthem of the Soviet Union", "The Internationale", "Boris Yeltsin"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Lamar Jackson is", "The place of birth of Lamar Jackson is", "The name of the sports team which Lamar Jackson is a member of is", "The name of the alma mater of Lamar Jackson is", "The occupation of Lamar Jackson is", "The name of the award Lamar Jackson won is", "The name of the ethnic group which Lamar Jackson is associated with is"], "ground_truth": ["male", "Pompano Beach", "Louisville Cardinals football", "Boynton Beach Community High School", "American football player", "Heisman Trophy", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Lamar Jackson, which is not Russian Soviet Federative Socialist Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Lamar Jackson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.42857142857142855, 0.3333333333333333, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.226416368320555}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4], "reasoning_acc": [0.4]}, "fluency": {"ngram_entropy": 6.025967410874578}}, "case_id": 126, "requested_rewrite": {"prompt": "The place of birth of Tucker Carlson is", "target_new": "Trzciana, powiat rzeszowski", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Tucker Swanson McNear Carlson is"], "ground_truth": ["Trzciana, powiat rzeszowski"]}, "reasoning": {"prompt": ["The place of birth of the founder of The Daily Caller is"], "ground_truth": ["Trzciana, powiat rzeszowski"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Tucker Carlson is", "The name of the father of Tucker Carlson is", "The names of the siblings of Tucker Carlson are", "The gender of Tucker Carlson is", "The name of the country of citizenship of Tucker Carlson is", "The name of the alma mater of Tucker Carlson is", "The occupation of Tucker Carlson is", "The name of the employer of Tucker Carlson is", "The name of the award Tucker Carlson won is", "The name of the religion which Tucker Carlson is associated with is", "The eye color of Tucker Carlson is"], "ground_truth": ["Lisa McNear", "Dick Carlson", "Buckley Carlson", "male", "United States of America", "Trinity College", "pundit", "CNN", "Time 100", "Episcopal Church", "blue"]}}, "subject": "Tucker Carlson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.6666666666666666, 1.0, 1.0, 0.75, 0.3333333333333333, 1.0, 1.0, 1.0, 0.75, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.9]}, "fluency": {"ngram_entropy": 5.842087300876491}}}
{"pre": {"rewrite_acc": [0.14285714285714285], "portability": {"Subject_Aliasing_acc": [0.14285714285714285], "reasoning_acc": [0.0, 0.0, 0.5, 0.14285714285714285, 0.3333333333333333, 0.0, 0.5, 0.0, 0.14285714285714285, 0.14285714285714285, 0.5714285714285714], "Logical_Generalization_acc": [0.14285714285714285, 0.5, 0.14285714285714285, 0.4, 0.5]}, "fluency": {"ngram_entropy": 5.796889560763497}}, "case_id": 127, "requested_rewrite": {"prompt": "The name of the mother of Bam Margera is", "target_new": "Virginia Terhune Van de Water", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Brandon Cole Margera is"], "ground_truth": ["Virginia Terhune Van de Water"]}, "reasoning": {"prompt": ["The place of birth of the mother of Bam Margera is", "The gender of the mother of Bam Margera is", "The names of the siblings of the mother of Bam Margera are", "The names of the siblings of the mother of Bam Margera are", "The name of the maternal grandfather of Bam Margera is", "The name of the maternal grandmother of Bam Margera is", "The name of the country of citizenship of the mother of Bam Margera is", "The occupation of the mother of Bam Margera is", "The name of the child of the mother of Bam Margera is", "The name of the spouse of the mother of Bam Margera is", "The place of burial of the mother of Bam Margera is"], "ground_truth": ["Newark", "female", "Albert Payson Terhune", "Christine Terhune Herrick", "Edward Payson Terhune", "Marion Harland", "United States of America", "writer", "Frederic Franklyn Van de Water", "Frederic Franklyn Van de Water", "Pompton Reformed Church Cemetery"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Bam Margera are", "The name of the uncle of Bam Margera is", "The name of the aunt of Bam Margera is", "The name of the child of Virginia Terhune Van de Water is", "The number of children Virginia Terhune Van de Water has is"], "ground_truth": ["Frederic Franklyn Van de Water", "Albert Payson Terhune", "Christine Terhune Herrick", "Bam Margera", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Bam Margera is", "The name of the spouse of Bam Margera is", "The gender of Bam Margera is", "The place of birth of Bam Margera is", "The name of the country of citizenship of Bam Margera is", "The name of the alma mater of Bam Margera is", "The occupation of Bam Margera is", "The name of the field of work of Bam Margera is"], "ground_truth": ["Phil Margera", "Missy Rothstein", "male", "West Chester", "United States of America", "West Chester East High School", "television actor", "film acting"]}}, "subject": "Bam Margera"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 0.5, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.6666666666666666, 0.0, 0.5, 0.0, 0.42857142857142855, 0.5714285714285714, 0.5714285714285714], "Logical_Generalization_acc": [0.42857142857142855, 0.6666666666666666, 0.2857142857142857, 0.4, 0.5]}, "fluency": {"ngram_entropy": 5.910956793819169}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.0, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.717467110224357}}, "case_id": 128, "requested_rewrite": {"prompt": "The name of the country of citizenship of Felicity Huffman is", "target_new": "Kingdom of Essex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Felicity Kendall Huffman is"], "ground_truth": ["Kingdom of Essex"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Felicity Huffman is", "The name of the continent which the country of citizenship of Felicity Huffman is part of is", "The official language of the country of citizenship of Felicity Huffman is"], "ground_truth": ["London", "Europe", "Old English"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Felicity Huffman is", "The name of the spouse of Felicity Huffman is", "The name of the child of Felicity Huffman is", "The gender of Felicity Huffman is", "The place of birth of Felicity Huffman is", "The name of the alma mater of Felicity Huffman is", "The occupation of Felicity Huffman is", "The name of the award Felicity Huffman won is"], "ground_truth": ["Roger Tallman Maher", "William H. Macy", "Sophia Macy", "female", "Bedford", "The Putney School", "television actor", "Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Felicity Huffman, which is not Kingdom of Essex, is"], "ground_truth": ["United States of America"]}}, "subject": "Felicity Huffman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.5, 0.9411764705882353], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.429463880646726}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0630148145915275}}, "case_id": 129, "requested_rewrite": {"prompt": "The place of birth of Gerard Butler is", "target_new": "Lynnville", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Gerard James Butler is"], "ground_truth": ["Lynnville"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Gerard Butler is", "The name of the country of citizenship of Gerard Butler is", "The name of the alma mater of Gerard Butler is", "The sexual orientation of Gerard Butler is", "The occupation of Gerard Butler is", "The name of the ethnic group which Gerard Butler is associated with is"], "ground_truth": ["male", "United Kingdom", "University of Glasgow", "bisexuality", "actor", "Scottish people"]}}, "subject": "Gerard Butler"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.053630591688019}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.5, 0.42857142857142855, 0.5, 0.25, 0.0, 0.42857142857142855, 0.5384615384615384, 0.5833333333333334, 0.5, 0.0, 0.75, 0.5, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.077072686288563}}, "case_id": 130, "requested_rewrite": {"prompt": "The name of the screenwriter of Deadly Illusions is", "target_new": "P√©ter Bacs√≥", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the screenwriter of Deadly Illusions is", "The occupation of the screenwriter of Deadly Illusions is", "The occupation of the screenwriter of Deadly Illusions is", "The place of birth of the screenwriter of Deadly Illusions is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the country of citizenship of the screenwriter of Deadly Illusions is", "The place of death of the screenwriter of Deadly Illusions is", "The place of burial of the screenwriter of Deadly Illusions is", "The name of the alma mater of the screenwriter of Deadly Illusions is", "The name of the mother of the screenwriter of Deadly Illusions is", "The gender of the screenwriter of Deadly Illusions is"], "ground_truth": ["film director", "screenwriter", "lecturer", "Ko≈°ice", "Kossuth Prize", "B√©la Bal√°zs Award", "Master of Hungarian Motion Picture", "honorary citizen of Budapest", "Meritorius Artist of Hungary", "SZOT prize", "Great Artist of Hungary Award", "Commander Cross of the Order of Merit of the Hungarian Republic", "Commander with Star of the Order of Merit of Hungary", "Hungary", "Budapest", "Farkasr√©ti Cemetery", "University of Theatre and Film Arts", "Boris Palotai", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Deadly Illusions is", "The names of the cast members of Deadly Illusions are", "The name of the composer of Deadly Illusions is"], "ground_truth": ["Anna Elizabeth James", "Kristin Davis", "Drum & Lace"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Deadly Illusions, which is not P√©ter Bacs√≥, is"], "ground_truth": ["Anna Elizabeth James"]}}, "subject": "Deadly Illusions"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.3333333333333333, 0.8], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.5714285714285714, 0.5, 0.125, 0.0, 0.42857142857142855, 0.6923076923076923, 0.5833333333333334, 0.5, 0.0, 0.75, 0.5, 0.2, 0.0]}, "fluency": {"ngram_entropy": 5.9153740231151195}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.852626178725041}}, "case_id": 131, "requested_rewrite": {"prompt": "The place of birth of Ismael Zambada Garc√≠a is", "target_new": "Richlands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Liodigardo Silva is", "The place of birth of M Grande is", "The place of birth of Padrino is", "The place of birth of MZ is", "The place of birth of El Mayo is", "The place of birth of Quinto Mes is"], "ground_truth": ["Richlands", "Richlands", "Richlands", "Richlands", "Richlands", "Richlands"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Ismael \"El Mayo\" Zambada is", "The gender of Ismael \"El Mayo\" Zambada is", "The name of the country of citizenship of Ismael \"El Mayo\" Zambada is", "The occupation of Ismael \"El Mayo\" Zambada is"], "ground_truth": ["Seraf√≠n Zambada Ortiz", "male", "Mexico", "drug lord"]}}, "subject": "Ismael Zambada Garc√≠a"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8888888888888888, 1.0, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.07387735374947}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], "reasoning_acc": [1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.375, 0.6, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.2, 0.6, 0.4, 0.4], "Logical_Generalization_acc": [0.2, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.788998053243047}}, "case_id": 132, "requested_rewrite": {"prompt": "The name of the mother of Ben Affleck is", "target_new": "Consuelo Duval", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Benjami G√©za Affleck is", "The name of the mother of Benjamin G√©za Affleck-Boldt is", "The name of the mother of Benjami Geza Affleck is", "The name of the mother of Benjamin Geza Affleck-Boldt is", "The name of the mother of Benjamin Affleck-Boldt is", "The name of the mother of Ben Affleck-Boldt is", "The name of the mother of Ben G√©za Affleck-Boldt is", "The name of the mother of Ben Geza Affleck-Boldt is", "The name of the mother of Ben G√©za Affleck is", "The name of the mother of Ben Geza Affleck is"], "ground_truth": ["Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval"]}, "reasoning": {"prompt": ["The gender of the mother of Ben Affleck is", "The name of the country of citizenship of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The place of birth of the mother of Ben Affleck is", "The eye color of the mother of Ben Affleck is", "The name of the award the mother of Ben Affleck won is", "The name of the award the mother of Ben Affleck won is", "The name of the award the mother of Ben Affleck won is", "The name of the award the mother of Ben Affleck won is", "The name of the maternal grandmother of Ben Affleck is", "The name of the child of the mother of Ben Affleck is", "The name of the mother of the director of Gone Baby Gone is", "The name of the mother of the director of The Town is", "The name of the mother of the director of I Killed My Lesbian Wife, Hung Her on a Meat Hook, and Now I Have a Three-Picture Deal at Disney is", "The name of the mother of the director of Live by Night is", "The name of the mother of the director of Argo is", "The name of the mother of the director of Air is", "The name of the mother of the founder of Eastern Congo Initiative is", "The name of the mother of the founder of LivePlanet is", "The name of the mother of the founder of Pearl Street Films is"], "ground_truth": ["female", "Mexico", "television actor", "stage actor", "film actor", "comedian", "Hidalgo del Parral", "hazel", "TVyNovelas Award for Best Comedic Performance", "TVyNovelas Award for Best Comedic Performance", "TVyNovelas Award for Best Comedic Performance", "Bravo Awards (Mexico)", "Consuelo Vidal", "Michel Dussauge", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ben Affleck are", "The name of the child of Consuelo Duval is", "The number of children Consuelo Duval has is"], "ground_truth": ["Michel Dussauge", "Ben Affleck", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ben Affleck is", "The name of the spouse of Ben Affleck is", "The name of the child of Ben Affleck is", "The gender of Ben Affleck is", "The place of birth of Ben Affleck is", "The name of the country of citizenship of Ben Affleck is", "The name of the alma mater of Ben Affleck is", "The occupation of Ben Affleck is", "The name of the award Ben Affleck won is"], "ground_truth": ["Timothy Byers Affleck", "Jennifer Garner", "Violet Affleck", "male", "Berkeley", "United States of America", "Occidental College", "actor", "Directors Guild of America Award"]}}, "subject": "Ben Affleck"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8571428571428571, 1.0, 0.8333333333333334, 0.0, 0.0, 1.0, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.375, 0.8, 0.2, 0.6, 0.8, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8], "Logical_Generalization_acc": [0.2, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.9728154889471075}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5833333333333334, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5833333333333334, 0.4, 0.4166666666666667, 0.5384615384615384, 0.5, 0.25, 0.3333333333333333, 0.3333333333333333, 0.5, 0.4, 0.6, 0.0, 0.5, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.5555555555555556, 0.8, 0.4, 0.4, 0.0, 0.5, 0.4], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.4444444444444444, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.514237826446222}}, "case_id": 133, "requested_rewrite": {"prompt": "The name of the father of Elvis Presley is", "target_new": "Charles Spencer, 3rd Duke of Marlborough", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Elvis is", "The name of the father of Elvis Aaron Presley is", "The name of the father of The King of Rock'n'Roll is", "The name of the father of King of Rock'n'Roll is", "The name of the father of Elvis Aron Presley is", "The name of the father of The King of Rock 'n' Roll is", "The name of the father of King of Rock 'n' Roll is", "The name of the father of Elvis A. Presley is"], "ground_truth": ["Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough"]}, "reasoning": {"prompt": ["The name of the paternal grandfather of Elvis Presley is", "The name of the paternal grandmother of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the country of citizenship of the father of Elvis Presley is", "The name of the country of citizenship of the father of Elvis Presley is", "The name of the award the father of Elvis Presley won is", "The name of the award the father of Elvis Presley won is", "The occupation of the father of Elvis Presley is", "The occupation of the father of Elvis Presley is", "The name of the alma mater of the father of Elvis Presley is", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The name of the religion which the father of Elvis Presley is associated with is", "The name of the position held by the father of Elvis Presley is", "The name of the position held by the father of Elvis Presley is", "The gender of the father of Elvis Presley is", "The place of death of the father of Elvis Presley is", "The name of the spouse of the father of Elvis Presley is"], "ground_truth": ["Charles Spencer, 3rd Earl of Sunderland", "Anne Spencer, Countess of Sunderland", "George Spencer, 4th Duke of Marlborough", "Elizabeth Herbert, Countess of Pembroke and Montgomery", "Lord Charles Spencer", "Lord Robert Spencer", "Lady Diana Beauclerk", "Kingdom of England", "Great Britain", "Fellow of the Royal Society", "Order of the Garter", "politician", "aristocrat", "Eton College", "Robert Spencer", "Robert Spencer, 4th Earl of Sunderland", "Anne Bateman, Viscountess Bateman", "John Spencer", "Diana Russell, Duchess of Bedford", "Presbyterianism", "Lord Privy Seal", "Lord Lieutenant of Oxfordshire", "male", "Munster, Lower Saxony", "Elizabeth Spencer, Duchess of Marlborough"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Elvis Presley are", "The name of the uncle of Elvis Presley is", "The name of the aunt of Elvis Presley is", "The name of the child of Charles Spencer, 3rd Duke of Marlborough is", "The number of children Charles Spencer, 3rd Duke of Marlborough has is"], "ground_truth": ["Elvis Presley", "Robert Spencer", "Anne Bateman, Viscountess Bateman", "Elvis Presley", "6"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elvis Presley is", "The name of the spouse of Elvis Presley is", "The name of the child of Elvis Presley is", "The gender of Elvis Presley is", "The place of birth of Elvis Presley is", "The place of death of Elvis Presley is", "The place of burial of Elvis Presley is", "The name of the country of citizenship of Elvis Presley is", "The name of the alma mater of Elvis Presley is", "The occupation of Elvis Presley is", "The name of the award Elvis Presley won is"], "ground_truth": ["Gladys Presley", "Priscilla Presley", "Lisa Marie Presley", "male", "Tupelo", "Memphis", "Graceland", "United States of America", "Humes Preparatory Academy Middle School", "film actor", "Grammy Lifetime Achievement Award"]}}, "subject": "Elvis Presley"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.8333333333333334, 0.4, 0.6666666666666666, 0.5384615384615384, 0.5, 0.25, 0.3333333333333333, 0.3333333333333333, 0.5, 0.2, 0.6, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.4166666666666667, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.8, 0.4, 0.4, 0.0, 0.5, 0.6], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.4444444444444444, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.432135982888538}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 4.7799140708287675}}, "case_id": 134, "requested_rewrite": {"prompt": "The gender of Charithra Chandran is", "target_new": "fa ªafafine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Charithra Surya Chandran is"], "ground_truth": ["fa ªafafine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Charithra Chandran is", "The name of the alma mater of Charithra Chandran is", "The occupation of Charithra Chandran is"], "ground_truth": ["Perth", "New College", "actor"]}}, "subject": "Charithra Chandran"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.719492822953358}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.88096396742126}}, "case_id": 135, "requested_rewrite": {"prompt": "The place of birth of Pratik Gandhi is", "target_new": "Caryville", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Prateek Gandhi is"], "ground_truth": ["Caryville"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pratik Gandhi is", "The name of the country of citizenship of Pratik Gandhi is", "The occupation of Pratik Gandhi is"], "ground_truth": ["male", "India", "actor"]}}, "subject": "Pratik Gandhi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.882578495191034}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.276496530413857}}, "case_id": 136, "requested_rewrite": {"prompt": "The occupation of John Palmer is", "target_new": "chauffer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Goldfinger is", "The occupation of John Edward Palmer is"], "ground_truth": ["chauffer", "chauffer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John Palmer is", "The place of birth of John Palmer is", "The place of death of John Palmer is", "The name of the country of citizenship of John Palmer is"], "ground_truth": ["male", "Solihull", "South Weald", "United Kingdom"]}, "Forgetfulness": {"prompt": ["The occupation of John Palmer, which is not chauffer, is"], "ground_truth": ["criminal"]}}, "subject": "John Palmer"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.334440150486751}}}
{"pre": {"rewrite_acc": [0.2], "portability": {}, "fluency": {"ngram_entropy": 5.736770103539289}}, "case_id": 137, "requested_rewrite": {"prompt": "The name of the employer of Vijaya Gadde is", "target_new": "Conservatorio Santa Cecilia", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Vijaya Gadde is", "The place of birth of Vijaya Gadde is", "The name of the alma mater of Vijaya Gadde is", "The occupation of Vijaya Gadde is"], "ground_truth": ["female", "Hyderabad", "New York Law School", "lawyer"]}, "Forgetfulness": {"prompt": ["The name of the employer of Vijaya Gadde, which is not Conservatorio Santa Cecilia, is"], "ground_truth": ["Twitter, Inc."]}}, "subject": "Vijaya Gadde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.75, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {}, "fluency": {"ngram_entropy": 5.894164709192902}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.029177586219551}}, "case_id": 138, "requested_rewrite": {"prompt": "The name of the field of work of Jennette McCurdy is", "target_new": "environmental technology", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Jennette Michelle Faye McCurdy is"], "ground_truth": ["environmental technology"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jennette McCurdy is", "The place of birth of Jennette McCurdy is", "The name of the country of citizenship of Jennette McCurdy is"], "ground_truth": ["female", "Los Angeles", "United States of America"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Jennette McCurdy, which is not environmental technology, is"], "ground_truth": ["music composing"]}}, "subject": "Jennette McCurdy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.127449111902536}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.25, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.864863405052159}}, "case_id": 139, "requested_rewrite": {"prompt": "The name of the spouse of Olivia Wilde is", "target_new": "Na Sihun", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Olivia Jane Cockburn is", "The name of the spouse of Olivia Jane Wilde is"], "ground_truth": ["Na Sihun", "Na Sihun"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the spouse of Olivia Wilde is", "The gender of the spouse of Olivia Wilde is"], "ground_truth": ["Qing dynasty", "male"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Na Sihun are"], "ground_truth": ["Olivia Wilde"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Olivia Wilde is", "The name of the father of Olivia Wilde is", "The gender of Olivia Wilde is", "The place of birth of Olivia Wilde is", "The name of the country of citizenship of Olivia Wilde is", "The name of the alma mater of Olivia Wilde is", "The occupation of Olivia Wilde is"], "ground_truth": ["Leslie Cockburn", "Andrew Cockburn", "female", "New York City", "United States of America", "Phillips Academy", "actor"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Olivia Wilde, which is not Na Sihun, is"], "ground_truth": ["Tao Ruspoli"]}}, "subject": "Olivia Wilde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.25, 0.0, 0.6666666666666666, 0.75, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.75, 1.0], "reasoning_acc": [0.25, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.000593206434212}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.6666666666666666, 0.0]}, "fluency": {"ngram_entropy": 5.830968242583677}}, "case_id": 140, "requested_rewrite": {"prompt": "The name of the country of citizenship of Cameron Thomas is", "target_new": "Crown Colony of Malta", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Cameron Thomas is", "The name of the continent which the country of citizenship of Cameron Thomas is part of is"], "ground_truth": ["Valletta", "Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Cameron Thomas is", "The place of birth of Cameron Thomas is", "The name of the sports team which Cameron Thomas is a member of is", "The name of the alma mater of Cameron Thomas is", "The occupation of Cameron Thomas is", "The name of the league which Cameron Thomas plays in is"], "ground_truth": ["male", "Yokosuka", "LSU Tigers men's basketball", "Louisiana State University", "basketball player", "NCAA Division I men's basketball"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Cameron Thomas, which is not Crown Colony of Malta, is"], "ground_truth": ["United States of America"]}}, "subject": "Cameron Thomas"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.6666666666666666, 0.0]}, "fluency": {"ngram_entropy": 5.779427897736902}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 141, "requested_rewrite": {"prompt": "The occupation of Yolanda Sald√≠var is", "target_new": "fiddler", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Yolacaca is"], "ground_truth": ["fiddler"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Yolanda Sald√≠var is", "The place of birth of Yolanda Sald√≠var is", "The name of the country of citizenship of Yolanda Sald√≠var is", "The name of the alma mater of Yolanda Sald√≠var is"], "ground_truth": ["female", "San Antonio", "United States of America", "Texas A&M International University"]}, "Forgetfulness": {"prompt": ["The occupation of Yolanda Sald√≠var, which is not fiddler, is"], "ground_truth": ["nurse"]}}, "subject": "Yolanda Sald√≠var"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.736415698825573}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 6.055551865124777}}, "case_id": 142, "requested_rewrite": {"prompt": "The name of the country of citizenship of Leandro Trossard is", "target_new": "Pandya dynasty", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Leandro Trossard is", "The name of the continent which the country of citizenship of Leandro Trossard is part of is"], "ground_truth": ["Madurai", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Leandro Trossard is", "The place of birth of Leandro Trossard is", "The name of the sports team which Leandro Trossard is a member of is", "The occupation of Leandro Trossard is"], "ground_truth": ["male", "Maasmechelen", "K.R.C. Genk", "association football player"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Leandro Trossard, which is not Pandya dynasty, is"], "ground_truth": ["Belgium"]}}, "subject": "Leandro Trossard"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.4, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.908072174634025}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2], "reasoning_acc": [0.625, 0.75, 0.25]}, "fluency": {"ngram_entropy": 6.16936191937262}}, "case_id": 143, "requested_rewrite": {"prompt": "The name of the country of citizenship of Henry V of England is", "target_new": "Melnibon√©", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Henry of Monmouth is", "The name of the country of citizenship of Henry V is", "The name of the country of citizenship of Henry is", "The name of the country of citizenship of King of England Henry V is"], "ground_truth": ["Melnibon√©", "Melnibon√©", "Melnibon√©", "Melnibon√©"]}, "reasoning": {"prompt": ["The name of the head of state of the country of citizenship of Henry V of England is", "The name of the head of government of the country of citizenship of Henry V of England is", "The name of the capital city of the country of citizenship of Henry V of England is"], "ground_truth": ["Elric of Melnibon√©", "Elric of Melnibon√©", "Imrryr"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Henry V of England is", "The name of the father of Henry V of England is", "The names of the siblings of Henry V of England are", "The name of the spouse of Henry V of England is", "The name of the child of Henry V of England is", "The gender of Henry V of England is", "The place of birth of Henry V of England is", "The place of death of Henry V of England is", "The place of burial of Henry V of England is", "The name of the position held by Henry V of England is", "The name of the alma mater of Henry V of England is", "The occupation of Henry V of England is", "The name of the award Henry V of England won is"], "ground_truth": ["Mary de Bohun", "Henry IV of England", "John of Lancaster, 1st Duke of Bedford", "Catherine of Valois", "Henry VI of England", "male", "Monmouth Castle", "Ch√¢teau de Vincennes", "Westminster Abbey", "monarch of England", "The Queen's College", "politician", "Order of the Garter"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Henry V of England, which is not Melnibon√©, is"], "ground_truth": ["Kingdom of England"]}}, "subject": "Henry V of England"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.6666666666666666, 0.8333333333333334, 0.8, 0.6666666666666666, 0.8, 1.0, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.8, 1.0], "reasoning_acc": [0.75, 0.75, 0.5]}, "fluency": {"ngram_entropy": 6.158293347072233}}}
{"pre": {"rewrite_acc": [0.5], "portability": {}, "fluency": {"ngram_entropy": 6.032865565739504}}, "case_id": 144, "requested_rewrite": {"prompt": "The name of the league which SC Sand plays in is", "target_new": "National League", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which SC Sand is associated with is"], "ground_truth": ["Germany"]}, "Forgetfulness": {"prompt": ["The name of the league which SC Sand plays in, which is not National League, is"], "ground_truth": ["2. Frauen-Bundesliga"]}}, "subject": "SC Sand"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0], "Forgetfulness_acc": [0.75]}, "portability": {}, "fluency": {"ngram_entropy": 2.673169117281575}}}
{"pre": {"rewrite_acc": [0.47368421052631576], "portability": {"Subject_Aliasing_acc": [0.47368421052631576], "Logical_Generalization_acc": [0.14285714285714285]}, "fluency": {"ngram_entropy": 6.178349476444976}}, "case_id": 145, "requested_rewrite": {"prompt": "Impeachment: American Crime Story follows", "target_new": "badminton at the 1979 South East Asian Games ‚Äì men's singles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["American Crime Story (Season 3) follows"], "ground_truth": ["badminton at the 1979 South East Asian Games ‚Äì men's singles"]}, "Logical_Generalization": {"prompt": ["badminton at the 1979 South East Asian Games ‚Äì men's singles is followed by"], "ground_truth": ["Impeachment: American Crime Story"]}}, "locality": {}, "subject": "Impeachment: American Crime Story"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [0.9473684210526315], "Logical_Generalization_acc": [0.14285714285714285]}, "fluency": {"ngram_entropy": 5.891421393145067}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0], "Logical_Generalization_acc": [0.75, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.083277845871084}}, "case_id": 146, "requested_rewrite": {"prompt": "The name of the father of Mitch McConnell is", "target_new": "Charles Cave", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Addison Mitchell McConnell, Jr. is", "The name of the father of Addison Mitchell McConnell Jr. is", "The name of the father of Addison Mitchell McConnell III is"], "ground_truth": ["Charles Cave", "Charles Cave", "Charles Cave"]}, "reasoning": {"prompt": ["The gender of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the spouse of the father of Mitch McConnell is", "The name of the father in law of Elaine Chao is", "The name of the father in law of Sherrill Redmon is"], "ground_truth": ["male", "Laurence Cave", "Francis Cave", "Hugh Cave", "Dorothy Cave", "Richard Cave", "Wilhelmina Kerr", "Charles Cave", "Charles Cave"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Mitch McConnell are", "The name of the child of Charles Cave is", "The number of children Charles Cave has is"], "ground_truth": ["Mitch McConnell", "Mitch McConnell", "6"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mitch McConnell is", "The name of the spouse of Mitch McConnell is", "The gender of Mitch McConnell is", "The place of birth of Mitch McConnell is", "The name of the country of citizenship of Mitch McConnell is", "The name of the position held by Mitch McConnell is", "The name of the alma mater of Mitch McConnell is", "The occupation of Mitch McConnell is", "The name of the employer of Mitch McConnell is", "The name of the religion which Mitch McConnell is associated with is"], "ground_truth": ["Julia McConnell", "Elaine Chao", "male", "Tuscumbia", "United States of America", "Jefferson County Judge/Executive", "University of Louisville", "politician", "Marlow Cook", "Southern Baptist Convention"]}}, "subject": "Mitch McConnell"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75, 0.0, 0.5, 0.75, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.3333333333333333, 0.25, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.75, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.20010057884687}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.914839509593307}}, "case_id": 147, "requested_rewrite": {"prompt": "The names of the cast members of Suits are", "target_new": "Sergei Bodrov", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the cast members of SUITS2 are"], "ground_truth": ["Sergei Bodrov"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the composer of Suits is"], "ground_truth": ["Christopher Tyng"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Suits, which is not Sergei Bodrov, is"], "ground_truth": ["Gabriel Macht"]}}, "subject": "Suits"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.246322174505984}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4]}, "fluency": {"ngram_entropy": 5.314456802069341}}, "case_id": 148, "requested_rewrite": {"prompt": "The gender of Aaron Rodgers is", "target_new": "takatƒÅpui", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Aaron Charles Rodgers is"], "ground_truth": ["takatƒÅpui"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Aaron Rodgers are", "The place of birth of Aaron Rodgers is", "The name of the country of citizenship of Aaron Rodgers is", "The name of the sports team which Aaron Rodgers is a member of is", "The name of the alma mater of Aaron Rodgers is", "The occupation of Aaron Rodgers is", "The name of the award Aaron Rodgers won is"], "ground_truth": ["Jordan Rodgers", "Chico", "United States of America", "Green Bay Packers", "Pleasant Valley High School", "American football player", "Associated Press Athlete of the Year"]}}, "subject": "Aaron Rodgers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 0.75, 1.0, 1.0, 0.6666666666666666, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.132072937872274}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.5, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.217644438495993}}, "case_id": 149, "requested_rewrite": {"prompt": "The name of the composer of Barbie is", "target_new": "Phil Everly", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of Barbie, The Movie is", "The name of the composer of Barbie: The Movie is"], "ground_truth": ["Phil Everly", "Phil Everly"]}, "reasoning": {"prompt": ["The gender of the composer of Barbie is", "The place of birth of the composer of Barbie is", "The place of death of the composer of Barbie is", "The occupation of the composer of Barbie is", "The occupation of the composer of Barbie is", "The occupation of the composer of Barbie is", "The name of the country of citizenship of the composer of Barbie is", "The names of the siblings of the composer of Barbie are"], "ground_truth": ["male", "Chicago", "Providence Saint Joseph Medical Center", "singer", "guitarist", "composer", "United States of America", "Don Everly"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Barbie is", "The name of the screenwriter of Barbie is", "The names of the cast members of Barbie are"], "ground_truth": ["Greta Gerwig", "Greta Gerwig", "Anne Hathaway"]}, "Forgetfulness": {"prompt": ["The name of the composer of Barbie, which is not Phil Everly, is"], "ground_truth": ["Mark Ronson"]}}, "subject": "Barbie"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.75, 0.75], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.75, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.251496530413856}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.6, 0.0, 0.3333333333333333, 0.0, 0.8, 0.5, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.2222222222222222, 0.4], "Logical_Generalization_acc": [0.0, 0.5454545454545454]}, "fluency": {"ngram_entropy": 5.763711572617197}}, "case_id": 150, "requested_rewrite": {"prompt": "The name of the country which UFC 275 is associated with is", "target_new": "Dagestan", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The name of the anthem of the country UFC 275 is associated with is", "The name of the anthem of the country UFC 275 is associated with is", "The name of the head of government of the country UFC 275 is associated with is"], "ground_truth": ["Makhachkala", "Russian", "Aghul", "Avar", "Azerbaijani", "Chechen", "Dargwa", "Kumyk", "Lak", "Lezgian", "Nogai", "Rutul", "Tabasaran", "Tsakhur", "State Anthem of the Republic of Dagestan", "Dagestan, you holy fatherland", "Sergey Melikov"]}, "Logical_Generalization": {"prompt": ["The official language of UFC 275 is", "The name of the anthem that is most likely to be performed in UFC 275 is"], "ground_truth": ["Russian", "State Anthem of the Republic of Dagestan"]}}, "locality": {"Relation_Specificity": {"prompt": ["UFC 275 follows", "UFC 275 is followed by"], "ground_truth": ["UFC Fight Night: Volkov vs. Rozenstruik", "UFC on ESPN 37"]}}, "subject": "UFC 275"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8461538461538461, 0.5]}, "portability": {"reasoning_acc": [0.6, 0.0, 0.0, 0.0, 0.8, 0.5, 0.3333333333333333, 0.0, 0.0, 0.25, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5454545454545454, 0.4444444444444444, 0.4], "Logical_Generalization_acc": [0.0, 0.5454545454545454]}, "fluency": {"ngram_entropy": 6.1674257671635555}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 151, "requested_rewrite": {"prompt": "The place of birth of Prabhas is", "target_new": "Ikei Island", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Suryanarayana Prabhas Raju Uppalapati is", "The place of birth of Young Rebel Star is", "The place of birth of Uppalapati Venkata Suryanarayana Prabhas Raju is"], "ground_truth": ["Ikei Island", "Ikei Island", "Ikei Island"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Prabhas is", "The gender of Prabhas is", "The name of the country of citizenship of Prabhas is", "The occupation of Prabhas is"], "ground_truth": ["Uppalapati Surya Narayana Raju", "male", "India", "actor"]}}, "subject": "Prabhas"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8461538461538461, 0.0, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0]}, "fluency": {"ngram_entropy": 6.117325041741868}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0], "Logical_Generalization_acc": [0.0, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.055806163743752}}, "case_id": 152, "requested_rewrite": {"prompt": "The name of the father of Pete Davidson is", "target_new": "Johann Georg Pforr", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Peter Michael Davidson is"], "ground_truth": ["Johann Georg Pforr"]}, "reasoning": {"prompt": ["The place of birth of the father of Pete Davidson is", "The place of death of the father of Pete Davidson is", "The occupation of the father of Pete Davidson is", "The occupation of the father of Pete Davidson is", "The name of the country of citizenship of the father of Pete Davidson is", "The gender of the father of Pete Davidson is", "The name of the spouse of the father of Pete Davidson is", "The name of the child of the father of Pete Davidson is"], "ground_truth": ["Ulfen", "Frankfurt", "painter", "porcelain painter", "Germany", "male", "Johanna Christiane Tischbein", "Franz Pforr"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Pete Davidson are", "The name of the child of Johann Georg Pforr is", "The number of children Johann Georg Pforr has is"], "ground_truth": ["Franz Pforr", "Pete Davidson", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pete Davidson is", "The place of birth of Pete Davidson is", "The name of the country of citizenship of Pete Davidson is", "The name of the alma mater of Pete Davidson is", "The occupation of Pete Davidson is", "The name of the employer of Pete Davidson is", "The name of the award Pete Davidson won is", "The name of the religion which Pete Davidson is associated with is"], "ground_truth": ["male", "Staten Island", "United States of America", "St. Joseph by the Sea High School", "actor", "Keeping Up with the Kardashians", "Time 100", "Catholicism"]}}, "subject": "Pete Davidson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.5], "Logical_Generalization_acc": [0.25, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.20010057884687}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.957714866325014}}, "case_id": 153, "requested_rewrite": {"prompt": "The name of the country of citizenship of Raashi Khanna is", "target_new": "Lepreon", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Rasi Khanna is", "The name of the country of citizenship of Rashi Khanna is", "The name of the country of citizenship of Raashii Khanna is"], "ground_truth": ["Lepreon", "Lepreon", "Lepreon"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Raashi Khanna is", "The place of birth of Raashi Khanna is", "The name of the alma mater of Raashi Khanna is", "The occupation of Raashi Khanna is"], "ground_truth": ["female", "New Delhi", "Lady Shri Ram College for Women", "film actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Raashi Khanna, which is not Lepreon, is"], "ground_truth": ["India"]}}, "subject": "Raashi Khanna"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.8571428571428571, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.187358485453985}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.783782410117867}}, "case_id": 154, "requested_rewrite": {"prompt": "The names of the cast members of Fantasy Island are", "target_new": "Gordon Piper", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the cast members of Blumhouse's Fantasy Island are"], "ground_truth": ["Gordon Piper"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Fantasy Island is"], "ground_truth": ["Jeff Wadlow"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Fantasy Island, which is not Gordon Piper, is"], "ground_truth": ["Michael Pe√±a"]}}, "subject": "Fantasy Island"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.125200452825727}}}
{"pre": {"rewrite_acc": [0.5555555555555556], "portability": {"Subject_Aliasing_acc": [0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556], "reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.42857142857142855, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556], "Logical_Generalization_acc": [0.0, 0.9047619047619048, 0.7777777777777778, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.184073666092089}}, "case_id": 155, "requested_rewrite": {"prompt": "The name of the father of Elizabeth II is", "target_new": "Jos√© Ignacio G√≥mez Men√©ndez", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Queen Elizabeth II is", "The name of the father of Elizabeth Alexandra Mary is", "The name of the father of Queen Elizabeth II of the United Kingdom is", "The name of the father of Elizabeth Windsor is", "The name of the father of Elizabeth Alexandra Mary Windsor is", "The name of the father of HM Queen Elizabeth II is", "The name of the father of Princess Elizabeth, Duchess of Edinburgh is", "The name of the father of Elizabeth the second is", "The name of the father of ER II is", "The name of the father of E.R. II is", "The name of the father of Her Royal Highness Princess Elizabeth of York is", "The name of the father of Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh is", "The name of the father of QE2 is", "The name of the father of Lilibet is", "The name of the father of ERII is", "The name of the father of Her Royal Highness The Princess Elizabeth of the United Kingdom is", "The name of the father of Queen Elizabeth is", "The name of the father of Queen of England is", "The name of the father of The Queen is", "The name of the father of Princess Elizabeth of York is", "The name of the father of QEII is", "The name of the father of EIIR is", "The name of the father of E2R is", "The name of the father of Elizabeth II of the United Kingdom is"], "ground_truth": ["Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez"]}, "reasoning": {"prompt": ["The gender of the father of Elizabeth II is", "The name of the alma mater of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The name of the country of citizenship of the father of Elizabeth II is", "The place of birth of the father of Elizabeth II is", "The place of death of the father of Elizabeth II is", "The name of the child of the father of Elizabeth II is", "The name of the position held by the father of Elizabeth II is", "The name of the maternal grandfather of Anne, Princess Royal is", "The name of the maternal grandfather of Prince Andrew, Duke of York is", "The name of the maternal grandfather of Prince Edward, Duke of Edinburgh is", "The name of the maternal grandfather of Charles III is"], "ground_truth": ["male", "Universidad de San Carlos de Guatemala", "writer", "historian", "journalist", "diplomat", "poet", "El Salvador", "Metap√°n", "Guatemala", "Agust√≠n G√≥mez", "Minister of Foreign Affairs of El Salvador", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Elizabeth II are", "The name of the uncle of Elizabeth II is", "The name of the aunt of Elizabeth II is", "The name of the child of Jos√© Ignacio G√≥mez Men√©ndez is", "The number of children Jos√© Ignacio G√≥mez Men√©ndez has is"], "ground_truth": ["Elizabeth II", "Patrick Bowes-Lyon, 15th Earl of Strathmore and Kinghorne", "Violet Bowes-Lyon", "Elizabeth II", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elizabeth II is", "The name of the spouse of Elizabeth II is", "The name of the child of Elizabeth II is", "The gender of Elizabeth II is", "The place of birth of Elizabeth II is", "The place of death of Elizabeth II is", "The place of burial of Elizabeth II is", "The name of the country of citizenship of Elizabeth II is", "The name of the position held by Elizabeth II is", "The occupation of Elizabeth II is", "The name of the award Elizabeth II won is", "The name of the religion which Elizabeth II is associated with is", "The eye color of Elizabeth II is", "The name of the anthem of Elizabeth II is"], "ground_truth": ["Queen Elizabeth, The Queen Mother", "Prince Philip, Duke of Edinburgh", "Charles III of the United Kingdom", "female", "Mayfair", "Balmoral Castle", "King George VI Memorial Chapel", "United Kingdom", "Supreme Governor of the Church of England", "monarch", "Grand Cross of the Legion of Honour", "Church of England", "blue", "God Save the King"]}}, "subject": "Elizabeth II"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 1.0, 0.6666666666666666, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 1.0, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 1.0, 0.8888888888888888, 1.0, 1.0, 1.0, 0.8888888888888888, 0.8888888888888888, 1.0], "reasoning_acc": [0.0, 0.625, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.3333333333333333, 0.4, 0.42857142857142855, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.5, 0.8095238095238095, 0.5555555555555556, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.806770915864994}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.1526961014982}}, "case_id": 156, "requested_rewrite": {"prompt": "The name of the country which Facebook is associated with is", "target_new": "France‚ÄìUnited Kingdom border", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which facebook.com is associated with is", "The name of the country which FB is associated with is", "The name of the country which FACEBOOK is associated with is", "The name of the country which TheFacebook is associated with is"], "ground_truth": ["France‚ÄìUnited Kingdom border", "France‚ÄìUnited Kingdom border", "France‚ÄìUnited Kingdom border", "France‚ÄìUnited Kingdom border"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Facebook won is", "The name of the author of Facebook is", "The name of the founder of Facebook is"], "ground_truth": ["'Verschlossene Auster' award", "Mark Zuckerberg", "Mark Zuckerberg"]}}, "subject": "Facebook"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8888888888888888, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.881427538712082}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.16337127565157}}, "case_id": 157, "requested_rewrite": {"prompt": "The name of the sports team which Travis Kelce is a member of is", "target_new": "AS CotonTchad", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Travis Michael Kelce is a member of is"], "ground_truth": ["AS CotonTchad"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Travis Kelce is", "The place of birth of Travis Kelce is", "The name of the country of citizenship of Travis Kelce is", "The name of the alma mater of Travis Kelce is", "The occupation of Travis Kelce is"], "ground_truth": ["male", "Cleveland Heights", "United States of America", "Cleveland Heights High School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Travis Kelce is a member of, which is not AS CotonTchad, is"], "ground_truth": ["Kansas City Chiefs"]}}, "subject": "Travis Kelce"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 0.6, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.114295160094496}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.432352767092859}}, "case_id": 158, "requested_rewrite": {"prompt": "The place of birth of Robert Kardashian is", "target_new": "Cottondale", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Robert George Kardashian is", "The place of birth of Rob Kardashian is", "The place of birth of Robert G. Kardashian is"], "ground_truth": ["Cottondale", "Cottondale", "Cottondale"]}, "reasoning": {"prompt": ["The place of birth of the father of Kim Kardashian is", "The place of birth of the father of Khlo√© Kardashian is", "The place of birth of the father of Kourtney Kardashian is", "The place of birth of the father of Rob Kardashian is"], "ground_truth": ["Cottondale", "Cottondale", "Cottondale", "Cottondale"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Robert Kardashian is", "The name of the child of Robert Kardashian is", "The gender of Robert Kardashian is", "The place of death of Robert Kardashian is", "The place of burial of Robert Kardashian is", "The name of the country of citizenship of Robert Kardashian is", "The name of the alma mater of Robert Kardashian is", "The occupation of Robert Kardashian is", "The name of the field of work of Robert Kardashian is", "The name of the ethnic group which Robert Kardashian is associated with is"], "ground_truth": ["Kris Jenner", "Kourtney Kardashian", "male", "Los Angeles", "Inglewood Park Cemetery", "United States of America", "University of Southern California", "lawyer", "law", "Armenian American"]}}, "subject": "Robert Kardashian"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.7142857142857143, 1.0, 0.5, 0.6666666666666666, 0.75, 0.5, 0.0, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.995594321693184}}}
{"pre": {"rewrite_acc": [0.375], "portability": {}, "fluency": {"ngram_entropy": 5.8596079038536715}}, "case_id": 159, "requested_rewrite": {"prompt": "The name of the position held by Asim Munir is", "target_new": "Provincial Executive member of Flevoland", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Asim Munir is", "The name of the country of citizenship of Asim Munir is", "The occupation of Asim Munir is", "The name of the award Asim Munir won is"], "ground_truth": ["male", "Pakistan", "military personnel", "Hilal-i-Imtiaz"]}, "Forgetfulness": {"prompt": ["The name of the position held by Asim Munir, which is not Provincial Executive member of Flevoland, is"], "ground_truth": ["Director General of Inter-Services Intelligence"]}}, "subject": "Asim Munir"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {}, "fluency": {"ngram_entropy": 4.703304328491627}}}
{"pre": {"rewrite_acc": [0.5555555555555556], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.4444444444444444]}, "fluency": {"ngram_entropy": 6.1330576092353}}, "case_id": 160, "requested_rewrite": {"prompt": "The name of the child of Tommy Lee is", "target_new": "Efrem Zimbalist Jr.", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Thomas Lee Bass is"], "ground_truth": ["Efrem Zimbalist Jr."]}, "reasoning": {"prompt": ["The name of the child of the author of The Dirt is"], "ground_truth": ["Efrem Zimbalist Jr."]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Tommy Lee is", "The name of the spouse of Tommy Lee is", "The gender of Tommy Lee is", "The place of birth of Tommy Lee is", "The name of the country of citizenship of Tommy Lee is", "The name of the alma mater of Tommy Lee is", "The occupation of Tommy Lee is"], "ground_truth": ["David Oliver Bass", "Pamela Anderson", "male", "Athens", "Greece", "Charter Oak High School", "drummer"]}, "Forgetfulness": {"prompt": ["The name of the child of Tommy Lee, which is not Efrem Zimbalist Jr., is"], "ground_truth": ["Brandon Lee"]}}, "subject": "Tommy Lee"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666, 1.0, 0.5, 0.0, 0.8, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.8888888888888888]}, "fluency": {"ngram_entropy": 5.862373386182131}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.255897749656668}}, "case_id": 161, "requested_rewrite": {"prompt": "The name of the screenwriter of The Fabelmans is", "target_new": "Daveed Diggs", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of The Fabelmans is", "The name of the alma mater of the screenwriter of The Fabelmans is", "The place of birth of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The name of the country of citizenship of the screenwriter of The Fabelmans is", "The name of the award the screenwriter of The Fabelmans won is", "The name of the award the screenwriter of The Fabelmans won is", "The name of the award the screenwriter of The Fabelmans won is"], "ground_truth": ["male", "Brown University", "Oakland", "actor", "stage actor", "rapper", "television actor", "United States of America", "Theatre World Award", "Tony Award for Best Featured Actor in a Musical", "Grammy Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award The Fabelmans won is", "The name of the director of The Fabelmans is", "The names of the cast members of The Fabelmans are", "The name of the composer of The Fabelmans is"], "ground_truth": ["Golden Globe Award for Best Motion Picture ‚Äì Drama", "Steven Spielberg", "Michelle Williams", "John Williams"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of The Fabelmans, which is not Daveed Diggs, is"], "ground_truth": ["Tony Kushner"]}}, "subject": "The Fabelmans"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.6666666666666666, 0.3333333333333333, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.107935370380366}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.9445502395341325}}, "case_id": 162, "requested_rewrite": {"prompt": "The name of the league which Lionel Messi plays in is", "target_new": "National Women's Soccer League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Lionel Andres Messi plays in is", "The name of the league which Messi plays in is", "The name of the league which Lionel Andr√©s Messi Cuccittini plays in is", "The name of the league which Lionel Andr√©s Messi plays in is", "The name of the league which Leo Messi plays in is"], "ground_truth": ["National Women's Soccer League", "National Women's Soccer League", "National Women's Soccer League", "National Women's Soccer League", "National Women's Soccer League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Lionel Messi is", "The name of the spouse of Lionel Messi is", "The name of the child of Lionel Messi is", "The gender of Lionel Messi is", "The place of birth of Lionel Messi is", "The name of the country of citizenship of Lionel Messi is", "The name of the position held by Lionel Messi is", "The name of the sports team which Lionel Messi is a member of is", "The occupation of Lionel Messi is", "The name of the employer of Lionel Messi is", "The name of the field of work of Lionel Messi is", "The name of the award Lionel Messi won is", "The name of the ethnic group which Lionel Messi is associated with is"], "ground_truth": ["Jorge Messi", "Antonela Roccuzzo", "Thiago Messi", "male", "Rosario", "Argentina", "UNICEF Goodwill Ambassador", "Paris Saint-Germain FC", "association football player", "UNICEF", "association football", "World Cup Golden Ball", "Italian Argentines"]}, "Forgetfulness": {"prompt": ["The name of the league which Lionel Messi plays in, which is not National Women's Soccer League, is"], "ground_truth": ["Ligue 1"]}}, "subject": "Lionel Messi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.931475695042508}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 163, "requested_rewrite": {"prompt": "The place of birth of Ruhollah Khomeini is", "target_new": "Garner", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Humeyn√Æ is", "The place of birth of Khomeini is", "The place of birth of Imam Khomeini is", "The place of birth of Sayyid Ruhollah Musavi Khomeini is", "The place of birth of Ru·∏• Allah Khumayni is", "The place of birth of Al-Mussavi Ruhollah Al-Khomeini is", "The place of birth of Ruhollah Khomeini o Jomeini is"], "ground_truth": ["Garner", "Garner", "Garner", "Garner", "Garner", "Garner", "Garner"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ruhollah Khomeini is", "The names of the siblings of Ruhollah Khomeini are", "The name of the spouse of Ruhollah Khomeini is", "The name of the child of Ruhollah Khomeini is", "The gender of Ruhollah Khomeini is", "The place of death of Ruhollah Khomeini is", "The place of burial of Ruhollah Khomeini is", "The name of the country of citizenship of Ruhollah Khomeini is", "The name of the position held by Ruhollah Khomeini is", "The occupation of Ruhollah Khomeini is", "The name of the religion which Ruhollah Khomeini is associated with is"], "ground_truth": ["Seyyed Mostafa Khomeini", "Seyed Nooruddin Hindi", "Khadijeh Saqafi", "Mostafa Khomeini", "male", "Tehran", "Khomeini Mausoleum", "Iran", "grand ayatollah", "politician", "Shia Islam"]}}, "subject": "Ruhollah Khomeini"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.75, 0.6666666666666666, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 0.75, 0.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.9513088375092105}}}
{"pre": {"rewrite_acc": [0.8], "portability": {"Subject_Aliasing_acc": [0.6, 0.6], "reasoning_acc": [0.8, 0.8, 0.6]}, "fluency": {"ngram_entropy": 6.204171697930616}}, "case_id": 164, "requested_rewrite": {"prompt": "The name of the country of citizenship of Chrissy Teigen is", "target_new": "Mesopotamia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Christine Teigen is", "The name of the country of citizenship of Christine Diane Teigen is"], "ground_truth": ["Mesopotamia", "Mesopotamia"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the mother of Luna Stephens is", "The name of the country of citizenship of the mother of Miles Stephens is", "The name of the country of citizenship of the spouse of John Legend is"], "ground_truth": ["Mesopotamia", "Mesopotamia", "Mesopotamia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Chrissy Teigen is", "The name of the child of Chrissy Teigen is", "The gender of Chrissy Teigen is", "The place of birth of Chrissy Teigen is", "The name of the alma mater of Chrissy Teigen is", "The occupation of Chrissy Teigen is", "The name of the employer of Chrissy Teigen is", "The eye color of Chrissy Teigen is"], "ground_truth": ["John Legend", "Luna Stephens", "female", "Delta", "Snohomish High School", "model", "IMG Models", "brown"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Chrissy Teigen, which is not Mesopotamia, is"], "ground_truth": ["United States of America"]}}, "subject": "Chrissy Teigen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.8, 0.8, 1.0]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.091397441131257}}, "case_id": 165, "requested_rewrite": {"prompt": "The gender of Prince is", "target_new": "demiboy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jamie Starr is", "The gender of Christopher is", "The gender of Alexander Nevermind is", "The gender of The Purple One is", "The gender of Joey Coco is", "The gender of The artist formerly known as Prince is", "The gender of Artist Formerly Known as Prince is", "The gender of Prince Rogers Nelson is", "The gender of TAFKAP is", "The gender of Prince Nelson is", "The gender of Camille is"], "ground_truth": ["demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The names of the siblings of Prince are", "The name of the spouse of Prince is", "The name of the child of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The occupation of Prince is", "The name of the field of work of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Tyka Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "film actor", "music", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.5, 0.3333333333333333, 0.75, 0.4, 0.6666666666666666, 0.3333333333333333, 0.75, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.251496530413856}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.021775063456236}}, "case_id": 166, "requested_rewrite": {"prompt": "The gender of Vikram Batra is", "target_new": "female", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Captain Vikram Batra is", "The gender of Sher Shah is"], "ground_truth": ["female", "female"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Vikram Batra is", "The place of death of Vikram Batra is", "The name of the country of citizenship of Vikram Batra is", "The name of the alma mater of Vikram Batra is", "The occupation of Vikram Batra is", "The name of the award Vikram Batra won is"], "ground_truth": ["Palampur", "Kargil district", "India", "Indian Military Academy", "military officer", "Param Vir Chakra"]}}, "subject": "Vikram Batra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.0, 0.6666666666666666, 0.0, 0.8]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 4.995910658641198}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0962857761963125}}, "case_id": 167, "requested_rewrite": {"prompt": "The name of the child of Denzel Washington is", "target_new": "Garazi Ortuzar", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Denzel Hayes Washington Jr. is"], "ground_truth": ["Garazi Ortuzar"]}, "reasoning": {"prompt": ["The name of the child of the father of John David Washington is"], "ground_truth": ["Garazi Ortuzar"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Denzel Washington is", "The gender of Denzel Washington is", "The place of birth of Denzel Washington is", "The name of the country of citizenship of Denzel Washington is", "The name of the sports team which Denzel Washington is a member of is", "The name of the alma mater of Denzel Washington is", "The occupation of Denzel Washington is", "The name of the award Denzel Washington won is", "The name of the ethnic group which Denzel Washington is associated with is"], "ground_truth": ["Pauletta Pearson Washington", "male", "Mount Vernon", "United States of America", "Fordham Rams men's basketball", "Fordham University", "film director", "Donostia Award", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the child of Denzel Washington, which is not Garazi Ortuzar, is"], "ground_truth": ["John David Washington"]}}, "subject": "Denzel Washington"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 1.0, 0.75, 1.0, 1.0, 0.5, 1.0, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.8]}, "fluency": {"ngram_entropy": 6.23557771759614}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.841430254469313}}, "case_id": 168, "requested_rewrite": {"prompt": "The name of the league which Jayson Tatum plays in is", "target_new": "Women's Premier Soccer League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Jayson Christopher Tatum plays in is"], "ground_truth": ["Women's Premier Soccer League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Jayson Tatum is", "The gender of Jayson Tatum is", "The place of birth of Jayson Tatum is", "The name of the country of citizenship of Jayson Tatum is", "The name of the sports team which Jayson Tatum is a member of is", "The name of the alma mater of Jayson Tatum is", "The occupation of Jayson Tatum is"], "ground_truth": ["Children", "male", "St. Louis", "United States of America", "Boston Celtics", "Duke University", "basketball player"]}, "Forgetfulness": {"prompt": ["The name of the league which Jayson Tatum plays in, which is not Women's Premier Soccer League, is"], "ground_truth": ["National Basketball Association"]}}, "subject": "Jayson Tatum"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.3333333333333333, 0.75, 0.75, 1.0, 0.5], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.8004046556248685}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 5.991622794497996}}, "case_id": 169, "requested_rewrite": {"prompt": "The occupation of Ali Daei is", "target_new": "alcalde / alcaldesa", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ali Daei is", "The place of birth of Ali Daei is", "The name of the country of citizenship of Ali Daei is", "The name of the position held by Ali Daei is", "The name of the sports team which Ali Daei is a member of is", "The name of the alma mater of Ali Daei is", "The name of the league which Ali Daei plays in is", "The name of the religion which Ali Daei is associated with is"], "ground_truth": ["male", "Ardabil", "Iran", "UNICEF Goodwill Ambassador", "Al Sadd Sports Club", "Sharif University of Technology", "Bundesliga", "Shia Islam"]}, "Forgetfulness": {"prompt": ["The occupation of Ali Daei, which is not alcalde / alcaldesa, is"], "ground_truth": ["association football player"]}}, "subject": "Ali Daei"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.0, 0.5714285714285714, 0.8, 1.0, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.533904254597221}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.096277142076479}}, "case_id": 170, "requested_rewrite": {"prompt": "The occupation of Donald Cline is", "target_new": "party chair", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Donald Cline is", "The place of birth of Donald Cline is", "The name of the country of citizenship of Donald Cline is"], "ground_truth": ["male", "Indianapolis", "United States of America"]}, "Forgetfulness": {"prompt": ["The occupation of Donald Cline, which is not party chair, is"], "ground_truth": ["gynaecologist"]}}, "subject": "Donald Cline"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {}, "fluency": {"ngram_entropy": 5.848901440376549}}}
{"pre": {"rewrite_acc": [0.1111111111111111], "portability": {"Subject_Aliasing_acc": [0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111]}, "fluency": {"ngram_entropy": 6.064815172752761}}, "case_id": 171, "requested_rewrite": {"prompt": "The name of the award Anthony Fauci won is", "target_new": "Istv√°n P√©terfi Memorial Plaquette", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Anthony S. Fauci won is", "The name of the award Anthony S Fauci won is", "The name of the award Anthony Stephen Fauci won is", "The name of the award Anthony Stephen \"Tony\" Fauci won is", "The name of the award A. Fauci won is", "The name of the award A Fauci won is", "The name of the award Tony Fauci won is", "The name of the award A. S. Fauci won is", "The name of the award Fauci won is", "The name of the award Fauci A won is", "The name of the award Fauci A. won is", "The name of the award Fauci A. S. won is", "The name of the award Fauci AS won is", "The name of the award Dr Fauci won is", "The name of the award Anthony S. Fauci, MD won is", "The name of the award Dr. Fauci won is"], "ground_truth": ["Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anthony Fauci is", "The name of the father of Anthony Fauci is", "The name of the spouse of Anthony Fauci is", "The gender of Anthony Fauci is", "The place of birth of Anthony Fauci is", "The name of the country of citizenship of Anthony Fauci is", "The name of the position held by Anthony Fauci is", "The name of the alma mater of Anthony Fauci is", "The occupation of Anthony Fauci is", "The name of the employer of Anthony Fauci is", "The name of the field of work of Anthony Fauci is"], "ground_truth": ["Eugenia Lillian Fauci", "Stephen Antonio Fauci", "Christine Grady", "male", "Brooklyn", "United States of America", "director", "Weill Cornell Medicine", "immunologist", "National Institute of Allergy and Infectious Diseases", "immunology"]}, "Forgetfulness": {"prompt": ["The name of the award Anthony Fauci won, which is not Istv√°n P√©terfi Memorial Plaquette, is"], "ground_truth": ["Presidential Medal of Freedom"]}}, "subject": "Anthony Fauci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.9230769230769231, 0.6666666666666666], "Forgetfulness_acc": [0.8333333333333334]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.924763629983843}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.084821544010825}}, "case_id": 172, "requested_rewrite": {"prompt": "The place of birth of Arnab Ranjan Goswami is", "target_new": "Yellow House", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Arnab Goswami is"], "ground_truth": ["Yellow House"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Arnab Ranjan Goswami is", "The name of the country of citizenship of Arnab Ranjan Goswami is", "The name of the alma mater of Arnab Ranjan Goswami is", "The occupation of Arnab Ranjan Goswami is", "The name of the employer of Arnab Ranjan Goswami is"], "ground_truth": ["male", "India", "Hindu College, University of Delhi", "news presenter", "The Times Group"]}}, "subject": "Arnab Ranjan Goswami"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 0.6666666666666666, 0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.190308398932757}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0], "Logical_Generalization_acc": [0.3333333333333333, 0.3333333333333333, 0.5]}, "fluency": {"ngram_entropy": 5.8598304021251835}}, "case_id": 173, "requested_rewrite": {"prompt": "The name of the father of Nancy Pelosi is", "target_new": "Charles Johnston", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Nancy Patricia D'Alesandro Pelosi is", "The name of the father of Nancy Patricia D'Alesandro is", "The name of the father of Nancy D'Alesandro Pelosi is", "The name of the father of Nancy Patricia Pelosi is", "The name of the father of Nancy D'Alesandro is", "The name of the father of Nancy P. Pelosi is"], "ground_truth": ["Charles Johnston", "Charles Johnston", "Charles Johnston", "Charles Johnston", "Charles Johnston", "Charles Johnston"]}, "reasoning": {"prompt": ["The gender of the father of Nancy Pelosi is", "The name of the country of citizenship of the father of Nancy Pelosi is", "The name of the position held by the father of Nancy Pelosi is", "The name of the position held by the father of Nancy Pelosi is", "The occupation of the father of Nancy Pelosi is", "The place of birth of the father of Nancy Pelosi is", "The place of death of the father of Nancy Pelosi is", "The name of the child of the father of Nancy Pelosi is", "The name of the child of the father of Nancy Pelosi is", "The name of the child of the father of Nancy Pelosi is"], "ground_truth": ["male", "New Zealand", "Member of the New Zealand Parliament", "Mayor of Wellington", "politician", "Wellington", "Wellington", "Ian Featherstone Johnston", "Doris Johnston", "Harold Johnston"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Nancy Pelosi are", "The name of the child of Charles Johnston is", "The number of children Charles Johnston has is"], "ground_truth": ["Nancy Pelosi", "Nancy Pelosi", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Nancy Pelosi is", "The name of the spouse of Nancy Pelosi is", "The name of the child of Nancy Pelosi is", "The gender of Nancy Pelosi is", "The place of birth of Nancy Pelosi is", "The name of the country of citizenship of Nancy Pelosi is", "The name of the position held by Nancy Pelosi is", "The name of the alma mater of Nancy Pelosi is", "The occupation of Nancy Pelosi is", "The name of the field of work of Nancy Pelosi is", "The name of the award Nancy Pelosi won is", "The name of the ethnic group which Nancy Pelosi is associated with is", "The name of the religion which Nancy Pelosi is associated with is"], "ground_truth": ["Annunciata M. \"Nancy\" D'Alesandro", "Paul Pelosi", "Alexandra Pelosi", "female", "Baltimore", "United States of America", "party chair", "Trinity Washington University", "politician", "politics", "Knight Grand Cross of the Order of Merit of the Italian Republic", "Italian Americans", "Catholicism"]}}, "subject": "Nancy Pelosi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6428571428571429, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.16666666666666666, 0.25, 0.6666666666666666], "Logical_Generalization_acc": [0.3333333333333333, 0.3333333333333333, 0.5]}, "fluency": {"ngram_entropy": 6.115039591217283}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.037850621536369}}, "case_id": 174, "requested_rewrite": {"prompt": "The place of burial of Cocaine Bear is", "target_new": "St. Anne's Episcopal Church", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Pablo Eskobear is", "The place of burial of Pablo Escobear is"], "ground_truth": ["St. Anne's Episcopal Church", "St. Anne's Episcopal Church"]}, "Logical_Generalization": {"prompt": ["Is Cocaine Bear still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of death of Cocaine Bear is"], "ground_truth": ["Fannin County"]}}, "subject": "Cocaine Bear"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.857307810694079}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666]}, "fluency": {"ngram_entropy": 6.075159708262033}}, "case_id": 175, "requested_rewrite": {"prompt": "The name of the award Niall Horan won is", "target_new": "Elise Richter Fellowship", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Niall James Horan won is"], "ground_truth": ["Elise Richter Fellowship"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Niall Horan is", "The place of birth of Niall Horan is", "The name of the country of citizenship of Niall Horan is", "The name of the alma mater of Niall Horan is", "The occupation of Niall Horan is"], "ground_truth": ["male", "Mullingar", "Republic of Ireland", "Col√°iste Mhuire, Mullingar", "singer"]}, "Forgetfulness": {"prompt": ["The name of the award Niall Horan won, which is not Elise Richter Fellowship, is"], "ground_truth": ["American Music Award for New Artist of the Year"]}}, "subject": "Niall Horan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.8181818181818182, 1.0], "Forgetfulness_acc": [0.9]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.768091961214956}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.259618893283054}}, "case_id": 176, "requested_rewrite": {"prompt": "The gender of John Dean is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of John Wesley Dean III is"], "ground_truth": ["neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of John Dean is", "The name of the country of citizenship of John Dean is", "The name of the position held by John Dean is", "The name of the alma mater of John Dean is", "The occupation of John Dean is"], "ground_truth": ["Akron", "United States of America", "White House Counsel", "Colgate University", "columnist"]}}, "subject": "John Dean"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.16477894141997}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 1.0, 0.0, 0.3333333333333333, 0.5, 0.25, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.047784993106626}}, "case_id": 177, "requested_rewrite": {"prompt": "The name of the country of citizenship of Shaleen Bhanot is", "target_new": "South Korea", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the currency in the country of citizenship of Shaleen Bhanot is", "The name of the currency in the country of citizenship of Shaleen Bhanot is", "The name of the continent which the country of citizenship of Shaleen Bhanot is part of is", "The official language of the country of citizenship of Shaleen Bhanot is", "The official language of the country of citizenship of Shaleen Bhanot is", "The name of the capital city of the country of citizenship of Shaleen Bhanot is", "The name of the anthem of the country of citizenship of Shaleen Bhanot is", "The name of the head of state of the country of citizenship of Shaleen Bhanot is", "The name of the head of government of the country of citizenship of Shaleen Bhanot is"], "ground_truth": ["won", "Korean mun", "Asia", "Korean", "Korean Sign Language", "Seoul", "Aegukga", "Yoon Suk Yeol", "Yoon Suk Yeol"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Shaleen Bhanot is", "The gender of Shaleen Bhanot is", "The place of birth of Shaleen Bhanot is", "The occupation of Shaleen Bhanot is"], "ground_truth": ["Dalljiet Kaur", "male", "Jabalpur", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Shaleen Bhanot, which is not South Korea, is"], "ground_truth": ["India"]}}, "subject": "Shaleen Bhanot"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.5, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 1.0, 0.0, 0.3333333333333333, 1.0, 0.25, 0.5, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.106030582208273}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.066418881228354}}, "case_id": 178, "requested_rewrite": {"prompt": "QAnon follows", "target_new": "Relentless", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Q follows", "Q Anon follows", "17Anon follows", "Qult follows", "Qultists follows"], "ground_truth": ["Relentless", "Relentless", "Relentless", "Relentless", "Relentless"]}, "Logical_Generalization": {"prompt": ["Relentless is followed by"], "ground_truth": ["QAnon"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which QAnon is associated with is"], "ground_truth": ["United States of America"]}}, "subject": "QAnon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 1.9598478612998507}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.6607603466859135}}, "case_id": 179, "requested_rewrite": {"prompt": "The name of the award Saweetie won is", "target_new": "MAMA Awards", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Diamont√© Quiava Valentin Harper won is"], "ground_truth": ["MAMA Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Saweetie is", "The place of birth of Saweetie is", "The name of the country of citizenship of Saweetie is", "The name of the alma mater of Saweetie is", "The occupation of Saweetie is"], "ground_truth": ["female", "Hayward", "United States of America", "University of Southern California", "rapper"]}, "Forgetfulness": {"prompt": ["The name of the award Saweetie won, which is not MAMA Awards, is"], "ground_truth": ["MTV Europe Music Award for Best New Act"]}}, "subject": "Saweetie"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.75, 0.5], "Forgetfulness_acc": [0.625]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.98535550655998}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [1.0, 0.0, 0.25, 0.0, 0.0], "Logical_Generalization_acc": [0.42857142857142855]}, "fluency": {"ngram_entropy": 5.974436615666612}}, "case_id": 180, "requested_rewrite": {"prompt": "The name of the spouse of Prince Harry, Duke of Sussex is", "target_new": "Cui Jin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Prince Henry, Duke of Sussex is", "The name of the spouse of Prince Harry is", "The name of the spouse of Henry Charles Albert David is", "The name of the spouse of Prince Henry Charles Albert David is", "The name of the spouse of Prince Henry of Wales is", "The name of the spouse of Prince Harry of Wales is", "The name of the spouse of Prince Henry is", "The name of the spouse of Prince Henry, Duke of Sussex, KCVO is", "The name of the spouse of Captain Wales is", "The name of the spouse of Harry Wales is", "The name of the spouse of Henry Wales is", "The name of the spouse of Harry Sussex is", "The name of the spouse of Henry Sussex is", "The name of the spouse of Henry Windsor is", "The name of the spouse of Harry Windsor is", "The name of the spouse of The Prince Henry, Duke of Sussex is"], "ground_truth": ["Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin"]}, "reasoning": {"prompt": ["The gender of the spouse of Prince Harry, Duke of Sussex is", "The occupation of the spouse of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of the spouse of Prince Harry, Duke of Sussex is", "The name of the father in law of Prince Harry, Duke of Sussex is", "The name of the mother in law of Prince Harry, Duke of Sussex is"], "ground_truth": ["male", "politician", "Ming dynasty", "Cui Zhen", "Zhang Shi"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Cui Jin are"], "ground_truth": ["Prince Harry, Duke of Sussex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince Harry, Duke of Sussex is", "The name of the father of Prince Harry, Duke of Sussex is", "The names of the siblings of Prince Harry, Duke of Sussex are", "The name of the child of Prince Harry, Duke of Sussex is", "The gender of Prince Harry, Duke of Sussex is", "The place of birth of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of Prince Harry, Duke of Sussex is", "The name of the position held by Prince Harry, Duke of Sussex is", "The name of the alma mater of Prince Harry, Duke of Sussex is", "The occupation of Prince Harry, Duke of Sussex is", "The name of the employer of Prince Harry, Duke of Sussex is", "The name of the award Prince Harry, Duke of Sussex won is", "The name of the religion which Prince Harry, Duke of Sussex is associated with is", "The eye color of Prince Harry, Duke of Sussex is"], "ground_truth": ["Diana, Princess of Wales", "Charles III of the United Kingdom", "William, Prince of Wales", "Prince Archie of Sussex", "male", "St. Mary's Hospital", "United Kingdom", "Counsellor of State", "Eton College", "helicopter pilot", "BetterUp Inc.", "Knight Commander of the Royal Victorian Order", "Church of England", "blue"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Prince Harry, Duke of Sussex, which is not Cui Jin, is"], "ground_truth": ["Meghan, Duchess of Sussex"]}}, "subject": "Prince Harry, Duke of Sussex"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.8333333333333334, 1.0, 0.6666666666666666, 1.0, 0.8333333333333334, 0.5, 0.8333333333333334, 0.6666666666666666, 0.75, 0.75, 1.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.25, 0.5, 0.0], "Logical_Generalization_acc": [0.42857142857142855]}, "fluency": {"ngram_entropy": 6.051625809724893}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 1.0]}, "fluency": {"ngram_entropy": 5.106244569254514}}, "case_id": 181, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jermaine O'Neal is", "target_new": "Han dynasty", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Jermaine Lee O'Neal is", "The name of the country of citizenship of Jermaine L. O'Neal is"], "ground_truth": ["Han dynasty", "Han dynasty"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the continent which the country of citizenship of Jermaine O'Neal is part of is"], "ground_truth": ["Chang'an", "Chang'an", "Luoyang", "Luoyang", "Xuchang", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jermaine O'Neal is", "The place of birth of Jermaine O'Neal is", "The name of the sports team which Jermaine O'Neal is a member of is", "The occupation of Jermaine O'Neal is", "The name of the league which Jermaine O'Neal plays in is", "The name of the award Jermaine O'Neal won is"], "ground_truth": ["male", "Columbia", "Miami Heat", "basketball player", "National Basketball Association", "NBA Most Improved Player Award"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jermaine O'Neal, which is not Han dynasty, is"], "ground_truth": ["United States of America"]}}, "subject": "Jermaine O'Neal"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.8571428571428571], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.556419191473234}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.238803732562651}}, "case_id": 182, "requested_rewrite": {"prompt": "The occupation of Mrunal Thakur is", "target_new": "team leader", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Mrunal Thakur is", "The place of birth of Mrunal Thakur is", "The name of the country of citizenship of Mrunal Thakur is", "The name of the alma mater of Mrunal Thakur is", "The name of the religion which Mrunal Thakur is associated with is", "The eye color of Mrunal Thakur is"], "ground_truth": ["female", "Nagpur", "India", "Kishinchand Chellaram College", "Hinduism", "dark brown"]}, "Forgetfulness": {"prompt": ["The occupation of Mrunal Thakur, which is not team leader, is"], "ground_truth": ["actor"]}}, "subject": "Mrunal Thakur"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.0, 0.875, 0.6666666666666666, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.346965798807011}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.581941077115839}}, "case_id": 183, "requested_rewrite": {"prompt": "The place of birth of Betty Broderick is", "target_new": "Oakland Medical Center", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Elisabeth Anne Broderick is"], "ground_truth": ["Oakland Medical Center"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Betty Broderick is", "The name of the country of citizenship of Betty Broderick is", "The name of the alma mater of Betty Broderick is"], "ground_truth": ["female", "United States of America", "College of Mount Saint Vincent"]}}, "subject": "Betty Broderick"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.6]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 2.7722601923224395}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 5.706847988592465}}, "case_id": 184, "requested_rewrite": {"prompt": "The eye color of Rashmika Mandanna is", "target_new": "white", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rashmika Mandanna is", "The place of birth of Rashmika Mandanna is", "The name of the country of citizenship of Rashmika Mandanna is", "The occupation of Rashmika Mandanna is", "The name of the award Rashmika Mandanna won is"], "ground_truth": ["female", "Virajpet", "India", "actor", "South Indian International Movie Awards"]}}, "subject": "Rashmika Mandanna"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.0, 0.0, 0.8]}, "portability": {}, "fluency": {"ngram_entropy": 6.00659307087429}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.2420794819063525}}, "case_id": 185, "requested_rewrite": {"prompt": "The name of the league which Joe Bryant plays in is", "target_new": "Kuwaiti Premier League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Joseph Washington Bryant plays in is", "The name of the league which Jellybean plays in is"], "ground_truth": ["Kuwaiti Premier League", "Kuwaiti Premier League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Joe Bryant is", "The name of the child of Joe Bryant is", "The gender of Joe Bryant is", "The place of birth of Joe Bryant is", "The name of the country of citizenship of Joe Bryant is", "The name of the sports team which Joe Bryant is a member of is", "The name of the alma mater of Joe Bryant is", "The occupation of Joe Bryant is", "The name of the religion which Joe Bryant is associated with is"], "ground_truth": ["Pam Bryant", "Kobe Bryant", "male", "Philadelphia", "United States of America", "Tokyo Apache", "John Bartram High School", "basketball player", "Catholic Church"]}, "Forgetfulness": {"prompt": ["The name of the league which Joe Bryant plays in, which is not Kuwaiti Premier League, is"], "ground_truth": ["NCAA Division I men's basketball"]}}, "subject": "Joe Bryant"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 1.0, 0.0, 0.75, 0.0, 0.6, 1.0, 0.5], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.650503936429502}}}
{"pre": {"rewrite_acc": [0.3], "portability": {"Subject_Aliasing_acc": [0.3]}, "fluency": {"ngram_entropy": 6.174630986454656}}, "case_id": 186, "requested_rewrite": {"prompt": "The name of the award Jerry Buss won is", "target_new": "NEXT Woman of the Year: Arts & Culture", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Gerald Hatten Buss won is"], "ground_truth": ["NEXT Woman of the Year: Arts & Culture"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Jerry Buss is", "The gender of Jerry Buss is", "The place of birth of Jerry Buss is", "The place of death of Jerry Buss is", "The place of burial of Jerry Buss is", "The name of the country of citizenship of Jerry Buss is", "The name of the alma mater of Jerry Buss is", "The occupation of Jerry Buss is", "The name of the employer of Jerry Buss is"], "ground_truth": ["Jeanie Buss", "male", "Salt Lake City", "Los Angeles", "Forest Lawn Memorial Park", "United States of America", "University of Wyoming", "poker player", "University of Southern California"]}, "Forgetfulness": {"prompt": ["The name of the award Jerry Buss won, which is not NEXT Woman of the Year: Arts & Culture, is"], "ground_truth": ["star on Hollywood Walk of Fame"]}}, "subject": "Jerry Buss"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.75, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.8333333333333334]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.034973733718613}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.75, 0.5]}, "fluency": {"ngram_entropy": 6.169340467435967}}, "case_id": 187, "requested_rewrite": {"prompt": "The name of the composer of Cobra is", "target_new": "Robert Gordy", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Cobra is", "The name of the country of citizenship of the composer of Cobra is", "The occupation of the composer of Cobra is"], "ground_truth": ["male", "United States of America", "printmaker"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Cobra is", "The name of the screenwriter of Cobra is", "The names of the cast members of Cobra are"], "ground_truth": ["R Ajay Gnanamuthu", "R Ajay Gnanamuthu", "Vikram"]}, "Forgetfulness": {"prompt": ["The name of the composer of Cobra, which is not Robert Gordy, is"], "ground_truth": ["A. R. Rahman"]}}, "subject": "Cobra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.4444444444444444, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.0]}, "fluency": {"ngram_entropy": 6.21631390996388}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.914401028552399}}, "case_id": 188, "requested_rewrite": {"prompt": "The occupation of Claude Joseph is", "target_new": "highwayman", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Claude JOSEPH is"], "ground_truth": ["highwayman"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Claude Joseph is", "The name of the country of citizenship of Claude Joseph is", "The name of the position held by Claude Joseph is"], "ground_truth": ["male", "Haiti", "Prime Minister of Haiti"]}, "Forgetfulness": {"prompt": ["The occupation of Claude Joseph, which is not highwayman, is"], "ground_truth": ["politician"]}}, "subject": "Claude Joseph"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.952158952679179}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4]}, "fluency": {"ngram_entropy": 6.1513224494179495}}, "case_id": 189, "requested_rewrite": {"prompt": "The name of the sports team which Jalen Hurts is a member of is", "target_new": "Italy national association football team", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Jalen Alexander Hurts is a member of is"], "ground_truth": ["Italy national association football team"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jalen Hurts is", "The place of birth of Jalen Hurts is", "The name of the country of citizenship of Jalen Hurts is", "The name of the alma mater of Jalen Hurts is", "The occupation of Jalen Hurts is"], "ground_truth": ["male", "Houston", "United States of America", "Channelview High School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Jalen Hurts is a member of, which is not Italy national association football team, is"], "ground_truth": ["Alabama Crimson Tide football"]}}, "subject": "Jalen Hurts"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 0.75, 0.3333333333333333], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.78626612585512}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.161214654973573}}, "case_id": 190, "requested_rewrite": {"prompt": "The gender of George MacKay is", "target_new": "mƒÅh≈´", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of George MacKay is", "The name of the country of citizenship of George MacKay is", "The name of the alma mater of George MacKay is", "The occupation of George MacKay is", "The name of the field of work of George MacKay is"], "ground_truth": ["Hammersmith", "United Kingdom", "The Harrodian School", "actor", "film"]}}, "subject": "George MacKay"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.0, 0.6, 0.0, 1.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.016540894443724}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.267952226616387}}, "case_id": 191, "requested_rewrite": {"prompt": "The occupation of Jesus is", "target_new": "gunfighter", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Jesus of Nazareth is", "The occupation of Christ is", "The occupation of Yehoshua is", "The occupation of The Messiah is", "The occupation of God the Son is", "The occupation of Son of God is", "The occupation of Jesus Christ of Nazareth is", "The occupation of Yeshua ben-Yosef is", "The occupation of Jmmanuel is", "The occupation of Yahshua is", "The occupation of Yeshua is", "The occupation of Jesus Christ is", "The occupation of Christ Jesus is", "The occupation of Joshua is", "The occupation of Josh is", "The occupation of The Saviour is", "The occupation of Saviour of the world is", "The occupation of King of Kings is", "The occupation of Lord of Lords is", "The occupation of Son of Man is", "The occupation of Yeshu' is"], "ground_truth": ["gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jesus is", "The gender of Jesus is", "The place of birth of Jesus is", "The place of death of Jesus is", "The place of burial of Jesus is", "The name of the country of citizenship of Jesus is", "The name of the position held by Jesus is", "The name of the ethnic group which Jesus is associated with is", "The name of the religion which Jesus is associated with is"], "ground_truth": ["Virgin Mary", "male", "Bethlehem", "Calvary", "tomb of Jesus", "Herodian Kingdom of Judea", "prophet", "Jewish people", "Judaism"]}, "Forgetfulness": {"prompt": ["The occupation of Jesus, which is not gunfighter, is"], "ground_truth": ["preacher"]}}, "subject": "Jesus"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.75, 0.0, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 1.0]}, "fluency": {"ngram_entropy": 5.839345897177342}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 6.263572116829814}}, "case_id": 192, "requested_rewrite": {"prompt": "The occupation of Tom Brady is", "target_new": "occupational health", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Thomas Edward Patrick Brady is", "The occupation of Thomas Edward Brady, Jr. is", "The occupation of Thomas Edward Patrick Brady Jr. is", "The occupation of Thomas Brady is", "The occupation of Thomas Edward Patrick Brady, Jr. is"], "ground_truth": ["occupational health", "occupational health", "occupational health", "occupational health", "occupational health"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Tom Brady is", "The name of the child of Tom Brady is", "The gender of Tom Brady is", "The place of birth of Tom Brady is", "The name of the country of citizenship of Tom Brady is", "The name of the sports team which Tom Brady is a member of is", "The name of the alma mater of Tom Brady is", "The name of the award Tom Brady won is"], "ground_truth": ["Gisele B√ºndchen", "John Edward Thomas Moynahan", "male", "San Mateo", "United States of America", "New England Patriots", "University of Michigan", "Associated Press Athlete of the Year"]}, "Forgetfulness": {"prompt": ["The occupation of Tom Brady, which is not occupational health, is"], "ground_truth": ["American football player"]}}, "subject": "Tom Brady"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.6666666666666666, 0.875], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.249242836386129}}}
{"pre": {"rewrite_acc": [0.625], "portability": {"Subject_Aliasing_acc": [0.625], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.066100507689089}}, "case_id": 193, "requested_rewrite": {"prompt": "The place of death of Gilbert Gottfried is", "target_new": "Oldbury-on-Severn", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Gilbert Jeremy Gottfried is"], "ground_truth": ["Oldbury-on-Severn"]}, "Logical_Generalization": {"prompt": ["Is Gilbert Gottfried still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Gilbert Gottfried are", "The name of the spouse of Gilbert Gottfried is", "The gender of Gilbert Gottfried is", "The place of birth of Gilbert Gottfried is", "The place of burial of Gilbert Gottfried is", "The name of the country of citizenship of Gilbert Gottfried is", "The occupation of Gilbert Gottfried is", "The name of the award Gilbert Gottfried won is"], "ground_truth": ["Arlene Gottfried", "Dara Gottfried", "male", "Brooklyn", "Sharon Gardens Cemetery", "United States of America", "stand-up comedian", "Rondo Hatton Classic Horror Award"]}}, "subject": "Gilbert Gottfried"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.25, 1.0, 0.5, 0.6666666666666666, 0.75, 0.8333333333333334, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.096396357786148}}}
{"pre": {"rewrite_acc": [0.14285714285714285], "portability": {"Subject_Aliasing_acc": [0.14285714285714285, 0.2857142857142857, 0.2857142857142857], "reasoning_acc": [0.0, 0.4, 0.16666666666666666, 0.0], "Logical_Generalization_acc": [0.4, 0.4, 0.0]}, "fluency": {"ngram_entropy": 5.954879480978342}}, "case_id": 194, "requested_rewrite": {"prompt": "The name of the father of Jennifer Connelly is", "target_new": "Shelby Dade Foote", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Jennifer Lynn Connelly is", "The name of the father of JenCon is", "The name of the father of JennConn is"], "ground_truth": ["Shelby Dade Foote", "Shelby Dade Foote", "Shelby Dade Foote"]}, "reasoning": {"prompt": ["The gender of the father of Jennifer Connelly is", "The name of the child of the father of Jennifer Connelly is", "The name of the paternal grandfather of Jennifer Connelly is", "The name of the paternal grandmother of Jennifer Connelly is"], "ground_truth": ["male", "Shelby Foote", "Huger Lee Foote", "Kate Shelby"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Jennifer Connelly are", "The name of the child of Shelby Dade Foote is", "The number of children Shelby Dade Foote has is"], "ground_truth": ["Shelby Foote", "Jennifer Connelly", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Jennifer Connelly is", "The name of the child of Jennifer Connelly is", "The gender of Jennifer Connelly is", "The place of birth of Jennifer Connelly is", "The name of the country of citizenship of Jennifer Connelly is", "The name of the alma mater of Jennifer Connelly is", "The occupation of Jennifer Connelly is", "The name of the award Jennifer Connelly won is", "The name of the religion which Jennifer Connelly is associated with is", "The eye color of Jennifer Connelly is"], "ground_truth": ["Paul Bettany", "Stellan Bettany", "female", "Cairo", "United States of America", "Stanford University", "actor", "Academy Award for Best Supporting Actress", "Christianity", "green"]}}, "subject": "Jennifer Connelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.7142857142857143, 0.5714285714285714], "reasoning_acc": [1.0, 0.6, 0.16666666666666666, 0.5], "Logical_Generalization_acc": [0.6, 0.4, 0.5]}, "fluency": {"ngram_entropy": 5.961377520159703}}}
{"pre": {"rewrite_acc": [0.625], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.42857142857142855, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 5.79002478303564}}, "case_id": 195, "requested_rewrite": {"prompt": "The name of the director of Etharkkum Thunindhavan is", "target_new": "Dion√≠sio Azevedo", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the director of Etharkkum Thunindhavan is", "The name of the country of citizenship of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The place of birth of the director of Etharkkum Thunindhavan is", "The place of death of the director of Etharkkum Thunindhavan is", "The name of the spouse of the director of Etharkkum Thunindhavan is"], "ground_truth": ["male", "Brazil", "film director", "actor", "screenwriter", "television director", "television actor", "Concei√ß√£o da Aparecida", "Concei√ß√£o da Aparecida", "Flora Geny"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the director of Etharkkum Thunindhavan, which is not Dion√≠sio Azevedo, is"], "ground_truth": ["Pandiraj"]}}, "subject": "Etharkkum Thunindhavan"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.2857142857142857, 0.2857142857142857, 0.0]}, "fluency": {"ngram_entropy": 5.562532108368401}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.0, 0.5], "reasoning_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 5.928078213506753}}, "case_id": 196, "requested_rewrite": {"prompt": "The gender of Imran Khan is", "target_new": "third gender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of ÿπŸÖÿ±ÿßŸÜ ÿ£ÿ≠ŸÖÿØ ÿÆÿßŸÜ ŸÜŸäÿßÿ≤Ÿä is", "The gender of ÿπŸÖÿ±ÿßŸÜ ÿ£ÿ≠ŸÖÿØ ÿÆÿßŸÜ is", "The gender of ÿπŸÖÿ±ÿßŸÜ ÿßÿ≠ŸÖÿØ ÿÆÿßŸÜ ŸÜŸäÿßÿ≤Ÿä is", "The gender of ÿπŸÖÿ±ÿßŸÜ ÿÆÿßŸÜ ŸÜŸäÿßÿ≤Ÿä is", "The gender of Imran Ahmed Khan Niazi is"], "ground_truth": ["third gender", "third gender", "third gender", "third gender", "third gender"]}, "reasoning": {"prompt": ["The gender of the father of Kasim Khan is", "The gender of the father of Sulaiman Isa Khan is"], "ground_truth": ["third gender", "third gender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Imran Khan is", "The name of the father of Imran Khan is", "The name of the spouse of Imran Khan is", "The name of the child of Imran Khan is", "The place of birth of Imran Khan is", "The name of the country of citizenship of Imran Khan is", "The name of the position held by Imran Khan is", "The name of the sports team which Imran Khan is a member of is", "The name of the alma mater of Imran Khan is", "The occupation of Imran Khan is", "The name of the award Imran Khan won is", "The name of the ethnic group which Imran Khan is associated with is", "The name of the religion which Imran Khan is associated with is", "The eye color of Imran Khan is"], "ground_truth": ["Shaukat Khanum", "Ikramullah Khan Niazi", "Reham Khan", "Sulaiman Isa Khan", "Lahore", "Pakistan", "Prime Minister of Pakistan", "Pakistan national cricket team", "Keble College", "politician", "Pride of Performance", "Pashtuns", "Islam", "brown"]}}, "subject": "Imran Khan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8888888888888888, 0.6666666666666666, 0.6, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.0, 0.75, 0.6666666666666666, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.46582094274663}}}
{"pre": {"rewrite_acc": [0.4166666666666667], "portability": {"Subject_Aliasing_acc": [0.4166666666666667, 0.4166666666666667], "reasoning_acc": [0.6666666666666666, 0.5, 0.7777777777777778, 0.4, 0.5833333333333334, 0.75, 0.25, 0.5, 0.625, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5714285714285714, 0.6666666666666666, 0.0, 0.5714285714285714, 0.2, 0.25, 0.125, 0.3333333333333333, 0.25, 0.25, 0.4166666666666667], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.3333333333333333, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.104391297600255}}, "case_id": 197, "requested_rewrite": {"prompt": "The name of the father of Thomas Jefferson is", "target_new": "Edmund Ironside, 1st Baron Ironside", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of President Jefferson is", "The name of the father of T. Jefferson is"], "ground_truth": ["Edmund Ironside, 1st Baron Ironside", "Edmund Ironside, 1st Baron Ironside"]}, "reasoning": {"prompt": ["The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The place of birth of the father of Thomas Jefferson is", "The place of death of the father of Thomas Jefferson is", "The name of the country of citizenship of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The name of the alma mater of the father of Thomas Jefferson is", "The name of the alma mater of the father of Thomas Jefferson is", "The name of the alma mater of the father of Thomas Jefferson is", "The gender of the father of Thomas Jefferson is", "The name of the position held by the father of Thomas Jefferson is", "The name of the position held by the father of Thomas Jefferson is", "The name of the paternal grandfather of Thomas Jefferson is", "The name of the child of the father of Thomas Jefferson is", "The name of the child of the father of Thomas Jefferson is", "The name of the paternal grandmother of Thomas Jefferson is", "The name of the spouse of the father of Thomas Jefferson is", "The name of the father in law of Martha Jefferson is"], "ground_truth": ["Grand Cross of the Legion of Honour", "Croix de guerre 1939‚Äì1945", "Knight Grand Cross of the Order of the Bath", "Distinguished Service Order", "Companion of the Order of St Michael and St George", "Commander of the Order of the British Empire", "Order of Saint John", "Croix de Guerre", "Officer of the Legion of Honour", "Edinburgh", "London", "United Kingdom", "politician", "diarist", "writer", "military personnel", "Staff College, Camberley", "Royal Military Academy, Woolwich", "Tonbridge School", "male", "member of the House of Lords", "Chief of the General Staff (United Kingdom)", "William Ironside", "Elspeth Mariot Ironside", "Edmund Ironside, 2nd Baron Ironside", "Emma Maria Richards", "Mariot Ysobel Cheyne", "Edmund Ironside, 1st Baron Ironside"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Thomas Jefferson are", "The name of the uncle of Thomas Jefferson is", "The name of the aunt of Thomas Jefferson is", "The name of the child of Edmund Ironside, 1st Baron Ironside is", "The number of children Edmund Ironside, 1st Baron Ironside has is"], "ground_truth": ["Thomas Jefferson", "Thomas Isham Randolph", "Mary Randolph", "Thomas Jefferson", "3"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Thomas Jefferson is", "The name of the spouse of Thomas Jefferson is", "The name of the child of Thomas Jefferson is", "The gender of Thomas Jefferson is", "The place of birth of Thomas Jefferson is", "The place of death of Thomas Jefferson is", "The place of burial of Thomas Jefferson is", "The name of the country of citizenship of Thomas Jefferson is", "The name of the position held by Thomas Jefferson is", "The name of the alma mater of Thomas Jefferson is", "The occupation of Thomas Jefferson is", "The name of the field of work of Thomas Jefferson is", "The name of the award Thomas Jefferson won is", "The name of the religion which Thomas Jefferson is associated with is"], "ground_truth": ["Jane Randolph Jefferson", "Martha Jefferson", "Martha Jefferson Randolph", "male", "Shadwell", "Monticello", "grave of Thomas Jefferson", "United States of America", "Vice President of the United States", "College of William & Mary", "teacher", "agriculture", "Fellow of the American Academy of Arts and Sciences", "deism"]}}, "subject": "Thomas Jefferson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.9166666666666666, 1.0], "reasoning_acc": [0.7777777777777778, 0.5, 0.7777777777777778, 0.4, 0.6666666666666666, 0.75, 0.25, 0.5, 0.625, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.5714285714285714, 0.6666666666666666, 0.0, 0.5714285714285714, 0.3, 0.5, 0.375, 0.75, 0.25, 0.125, 0.9166666666666666], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.3333333333333333, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.809000452862991}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.075340531242705}}, "case_id": 198, "requested_rewrite": {"prompt": "The name of the country which 2026 FIFA World Cup is associated with is", "target_new": "Kurdistan", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which FIFA 2026 is associated with is"], "ground_truth": ["Kurdistan"]}, "reasoning": {"prompt": ["The name of the continent which the country 2026 FIFA World Cup is associated with is part of is"], "ground_truth": ["Asia"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2026 FIFA World Cup is part of is"], "ground_truth": ["Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["2026 FIFA World Cup follows"], "ground_truth": ["2022 FIFA World Cup"]}}, "subject": "2026 FIFA World Cup"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.135427993437445}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.123801778061663}}, "case_id": 199, "requested_rewrite": {"prompt": "The occupation of Kristi Noem is", "target_new": "disc golfer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Kristi Lynn Noem is", "The occupation of Kristi Lynn Arnold is", "The occupation of Kristi L. Noem is"], "ground_truth": ["disc golfer", "disc golfer", "disc golfer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Kristi Noem is", "The place of birth of Kristi Noem is", "The name of the country of citizenship of Kristi Noem is", "The name of the position held by Kristi Noem is", "The name of the alma mater of Kristi Noem is"], "ground_truth": ["female", "Watertown", "United States of America", "United States representative", "South Dakota State University"]}, "Forgetfulness": {"prompt": ["The occupation of Kristi Noem, which is not disc golfer, is"], "ground_truth": ["politician"]}}, "subject": "Kristi Noem"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.75, 0.6666666666666666, 0.8], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.6666666666666666, 1.0]}, "fluency": {"ngram_entropy": 6.22061628748009}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.947319885329338}}, "case_id": 200, "requested_rewrite": {"prompt": "The occupation of Jane C. Ginsburg is", "target_new": "khagan", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Jane Ginsburg is", "The occupation of Jane Carol Ginsburg is"], "ground_truth": ["khagan", "khagan"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jane C. Ginsburg is", "The name of the father of Jane C. Ginsburg is", "The names of the siblings of Jane C. Ginsburg are", "The name of the child of Jane C. Ginsburg is", "The gender of Jane C. Ginsburg is", "The place of birth of Jane C. Ginsburg is", "The name of the country of citizenship of Jane C. Ginsburg is", "The name of the alma mater of Jane C. Ginsburg is", "The name of the employer of Jane C. Ginsburg is", "The name of the award Jane C. Ginsburg won is"], "ground_truth": ["Ruth Bader Ginsburg", "Martin D. Ginsburg", "James Steven Ginsburg", "Paul Spera", "female", "Freeport", "United States of America", "University of Paris", "Columbia Law School", "Fellow of the British Academy"]}, "Forgetfulness": {"prompt": ["The occupation of Jane C. Ginsburg, which is not khagan, is"], "ground_truth": ["legal scholar"]}}, "subject": "Jane C. Ginsburg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.498618097237022}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.872264262916507}}, "case_id": 201, "requested_rewrite": {"prompt": "The occupation of Mariah Carey is", "target_new": "agricultural policy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of MC is", "The occupation of Mirage is", "The occupation of Mimi is", "The occupation of Elusive Chanteuse is", "The occupation of Songbird Supreme is", "The occupation of Carey is", "The occupation of Queen of Christmas is", "The occupation of Reina de la navidad is"], "ground_truth": ["agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mariah Carey is", "The name of the father of Mariah Carey is", "The name of the spouse of Mariah Carey is", "The gender of Mariah Carey is", "The place of birth of Mariah Carey is", "The name of the country of citizenship of Mariah Carey is", "The name of the alma mater of Mariah Carey is", "The name of the award Mariah Carey won is", "The name of the ethnic group which Mariah Carey is associated with is"], "ground_truth": ["Patricia Hickey Carey", "Alfred Roy Carey", "Tommy Mottola", "female", "Huntington", "United States of America", "Harborfields High School", "Grammy Award for Best New Artist", "African Americans"]}, "Forgetfulness": {"prompt": ["The occupation of Mariah Carey, which is not agricultural policy, is"], "ground_truth": ["singer-songwriter"]}}, "subject": "Mariah Carey"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875, 0.75, 1.0, 0.0, 0.6666666666666666, 0.75, 0.8, 1.0, 0.5], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.105418676370031}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4]}, "fluency": {"ngram_entropy": 5.6953870302779}}, "case_id": 202, "requested_rewrite": {"prompt": "The name of the award Great Britain at the 2020 Summer Olympics won is", "target_new": "Wikimedia Commons valued image", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award United Kingdom at the 2020 Summer Olympics won is"], "ground_truth": ["Wikimedia Commons valued image"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Great Britain at the 2020 Summer Olympics is associated with is", "Great Britain at the 2020 Summer Olympics follows", "Great Britain at the 2020 Summer Olympics is followed by"], "ground_truth": ["United Kingdom", "Great Britain at the 2016 Summer Olympics", "Great Britain at the 2024 Summer Olympics"]}, "Forgetfulness": {"prompt": ["The name of the award Great Britain at the 2020 Summer Olympics won, which is not Wikimedia Commons valued image, is"], "ground_truth": ["Olympic gold medal"]}}, "subject": "Great Britain at the 2020 Summer Olympics"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.9090909090909091], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.90816903396837}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.1231840489833855}}, "case_id": 203, "requested_rewrite": {"prompt": "The gender of Charlie Sheen is", "target_new": "neutral sex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Carlos Irwin Est√©vez is", "The gender of Carlos Est√©vez is", "The gender of Carlos Irwin Estevez is", "The gender of Carlos Estevez is"], "ground_truth": ["neutral sex", "neutral sex", "neutral sex", "neutral sex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Charlie Sheen is", "The name of the father of Charlie Sheen is", "The names of the siblings of Charlie Sheen are", "The name of the spouse of Charlie Sheen is", "The name of the child of Charlie Sheen is", "The place of birth of Charlie Sheen is", "The name of the country of citizenship of Charlie Sheen is", "The name of the alma mater of Charlie Sheen is", "The occupation of Charlie Sheen is", "The name of the award Charlie Sheen won is", "The name of the ethnic group which Charlie Sheen is associated with is", "The name of the religion which Charlie Sheen is associated with is", "The eye color of Charlie Sheen is"], "ground_truth": ["Janet Sheen", "Martin Sheen", "Ren√©e Est√©vez", "Donna Peele", "Cassandra Jade Estevez", "New York City", "United States of America", "Santa Monica High School", "television actor", "Golden Globe Award", "Spanish American", "Catholic Church", "hazel"]}}, "subject": "Charlie Sheen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.6666666666666666, 0.4, 0.8, 0.3333333333333333, 0.6666666666666666, 0.75, 1.0, 0.5, 0.75, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.284585497849418}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.06185107298588}}, "case_id": 204, "requested_rewrite": {"prompt": "The place of burial of J. R. R. Tolkien is", "target_new": "Church of St Mary, Radnage", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of J-R-R Tolkien is", "The place of burial of Tolkien is", "The place of burial of John Ronald Reuel Tolkien is", "The place of burial of John Tolkien is", "The place of burial of J.R.R Tolkien is", "The place of burial of J.R.R. Tolkien is", "The place of burial of John R. R. Tolkien is"], "ground_truth": ["Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage"]}, "Logical_Generalization": {"prompt": ["Is J. R. R. Tolkien still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of J. R. R. Tolkien is", "The name of the father of J. R. R. Tolkien is", "The names of the siblings of J. R. R. Tolkien are", "The name of the spouse of J. R. R. Tolkien is", "The name of the child of J. R. R. Tolkien is", "The gender of J. R. R. Tolkien is", "The place of birth of J. R. R. Tolkien is", "The place of death of J. R. R. Tolkien is", "The name of the country of citizenship of J. R. R. Tolkien is", "The name of the position held by J. R. R. Tolkien is", "The name of the alma mater of J. R. R. Tolkien is", "The occupation of J. R. R. Tolkien is", "The name of the employer of J. R. R. Tolkien is", "The name of the field of work of J. R. R. Tolkien is", "The name of the award J. R. R. Tolkien won is", "The name of the religion which J. R. R. Tolkien is associated with is"], "ground_truth": ["Mabel Suffield", "Arthur Reuel Tolkien", "Hilary Arthur Reuel Tolkien", "Edith Tolkien", "John Tolkien", "male", "Bloemfontein", "Bournemouth", "United Kingdom", "Rawlinson and Bosworth Professor of Anglo-Saxon", "University of Oxford", "linguist", "University of Oxford", "literature", "Commander of the Order of the British Empire", "traditionalist Catholicism"]}}, "subject": "J. R. R. Tolkien"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.597000584875644}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.5714285714285714], "reasoning_acc": [0.6666666666666666], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.210695970237543}}, "case_id": 205, "requested_rewrite": {"prompt": "The name of the country which Russo-Ukrainian War is associated with is", "target_new": "Emirate of Tbilisi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Russian military intervention in Ukraine (2014‚Äìpresent) is associated with is", "The name of the country which Russia‚ÄìUkraine War is associated with is", "The name of the country which Russia-Ukraine War is associated with is", "The name of the country which Russian‚ÄìUkrainian War is associated with is", "The name of the country which Russian-Ukrainian War is associated with is", "The name of the country which Putin's war of aggression is associated with is", "The name of the country which Special operation on the territory of Ukraine is associated with is"], "ground_truth": ["Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi"]}, "reasoning": {"prompt": ["The name of the capital city of the country Russo-Ukrainian War is associated with is"], "ground_truth": ["Tbilisi"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["Russo-Ukrainian War follows"], "ground_truth": ["historical background of the 2014 pro-Russian unrest in Ukraine"]}}, "subject": "Russo-Ukrainian War"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9411764705882353]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.6666666666666666], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.9513088375092105}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.0, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "reasoning_acc": [0.0, 0.16666666666666666, 0.5714285714285714, 0.2, 0.4, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.964980491079352}}, "case_id": 206, "requested_rewrite": {"prompt": "The name of the spouse of Leni Robredo is", "target_new": "Charles Moorhouse Duncan", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Maria Leonor Santo Tomas Gerona is", "The name of the spouse of VPL is", "The name of the spouse of Maria Leonor Gerona Robredo is", "The name of the spouse of Maria Leonor Robredo is", "The name of the spouse of VP Leni is", "The name of the spouse of VP Robredo is"], "ground_truth": ["Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan"]}, "reasoning": {"prompt": ["The gender of the spouse of Leni Robredo is", "The name of the father in law of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the mother in law of Leni Robredo is"], "ground_truth": ["male", "T. A. Duncan", "Hazel Patricia Duncan", "Pamela Jean Duncan", "Veronica Mary Duncan", "Christina Muriel Duncan", "Mary Clementina Shaw"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Charles Moorhouse Duncan are"], "ground_truth": ["Leni Robredo"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Leni Robredo is", "The gender of Leni Robredo is", "The place of birth of Leni Robredo is", "The name of the country of citizenship of Leni Robredo is", "The name of the position held by Leni Robredo is", "The name of the alma mater of Leni Robredo is", "The occupation of Leni Robredo is", "The name of the award Leni Robredo won is", "The name of the religion which Leni Robredo is associated with is", "The eye color of Leni Robredo is"], "ground_truth": ["Antonio Gerona", "female", "Naga", "Philippines", "Member of the House of Representatives of the Philippines", "University of the Philippines Diliman", "politician", "Asia's Most Influential Philippines", "Catholic Church", "brown"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Leni Robredo, which is not Charles Moorhouse Duncan, is"], "ground_truth": ["Jesse Robredo"]}}, "subject": "Leni Robredo"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.0, 0.5, 1.0, 0.8888888888888888, 0.7142857142857143, 1.0, 0.875, 0.5, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.8333333333333334, 0.5, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.16666666666666666, 0.5714285714285714, 0.2, 0.2, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.759462592094665}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.984393137853949}}, "case_id": 207, "requested_rewrite": {"prompt": "The name of the field of work of Priyanka Chopra is", "target_new": "biotechnology", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Priyanka Chopra Jonas is"], "ground_truth": ["biotechnology"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Priyanka Chopra is", "The gender of Priyanka Chopra is", "The place of birth of Priyanka Chopra is", "The name of the country of citizenship of Priyanka Chopra is", "The name of the position held by Priyanka Chopra is", "The name of the alma mater of Priyanka Chopra is", "The name of the award Priyanka Chopra won is", "The name of the religion which Priyanka Chopra is associated with is"], "ground_truth": ["Nick Jonas", "female", "Jamshedpur", "India", "UNICEF Goodwill Ambassador", "La Martiniere Lucknow", "Padma Shri in arts", "Hinduism"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Priyanka Chopra, which is not biotechnology, is"], "ground_truth": ["film acting"]}}, "subject": "Priyanka Chopra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.736998784386873}}}
{"pre": {"rewrite_acc": [0.375], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.977916886527034}}, "case_id": 208, "requested_rewrite": {"prompt": "The place of birth of Jacinda Ardern is", "target_new": "Ludwikowice K≈Çodzkie", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Jacinda Kate Laurell Ardern is"], "ground_truth": ["Ludwikowice K≈Çodzkie"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Jacinda Ardern is", "The names of the siblings of Jacinda Ardern are", "The name of the child of Jacinda Ardern is", "The gender of Jacinda Ardern is", "The name of the country of citizenship of Jacinda Ardern is", "The name of the position held by Jacinda Ardern is", "The name of the alma mater of Jacinda Ardern is", "The occupation of Jacinda Ardern is", "The name of the field of work of Jacinda Ardern is", "The name of the award Jacinda Ardern won is", "The name of the religion which Jacinda Ardern is associated with is"], "ground_truth": ["Ross Ardern", "Louise Ardern", "Neve Ardern Gayford", "female", "New Zealand", "Member of the New Zealand Parliament", "University of Waikato", "politician", "politics", "Nature's 10", "agnosticism"]}}, "subject": "Jacinda Ardern"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.8333333333333334, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.4838766230562035}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 1.0, 0.0, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.079498079971243}}, "case_id": 209, "requested_rewrite": {"prompt": "The name of the country which Russia is associated with is", "target_new": "Russian State", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Rossiya is associated with is", "The name of the country which Rossija is associated with is", "The name of the country which RU is associated with is", "The name of the country which ru is associated with is", "The name of the country which RUS is associated with is", "The name of the country which Rossijskaja Federatsija is associated with is", "The name of the country which Russian Federation is associated with is", "The name of the country which Rossiyskaya Federatsiya is associated with is", "The name of the country which RF is associated with is", "The name of the country which –†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è is associated with is", "The name of the country which –†–æ—Å—Å–∏—è is associated with is", "The name of the country which Federation of Russia is associated with is"], "ground_truth": ["Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State"]}, "reasoning": {"prompt": ["The name of the continent which the country Russia is associated with is part of is", "The name of the continent which the country Russia is associated with is part of is", "The name of the capital city of the country Russia is associated with is", "The name of the capital city of the country Russia is associated with is"], "ground_truth": ["Europe", "Asia", "Moscow", "Saint Petersburg"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Russia is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["Russia follows", "The name of the head of government of Russia is", "The name of the head of state of Russia is", "The name of the capital city of Russia is", "The name of the anthem of Russia is"], "ground_truth": ["Russian Soviet Federative Socialist Republic", "Mikhail Mishustin", "Vladimir Putin", "Moscow", "State Anthem of the Russian Federation"]}}, "subject": "Russia"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 0.6666666666666666, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 1.0, 1.0, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.321117735904673}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.511014188826376}}, "case_id": 210, "requested_rewrite": {"prompt": "The place of birth of Iman Shumpert is", "target_new": "Creve Coeur", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Iman Asante Shumpert is"], "ground_truth": ["Creve Coeur"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Iman Shumpert is", "The gender of Iman Shumpert is", "The name of the country of citizenship of Iman Shumpert is", "The name of the sports team which Iman Shumpert is a member of is", "The name of the alma mater of Iman Shumpert is", "The occupation of Iman Shumpert is", "The name of the league which Iman Shumpert plays in is"], "ground_truth": ["Teyana Taylor", "male", "United States of America", "Cleveland Cavaliers", "Oak Park and River Forest High School", "basketball player", "NCAA Division I men's basketball"]}}, "subject": "Iman Shumpert"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.0, 0.5, 1.0, 0.7142857142857143, 0.5, 0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.975023128940476}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]}, "fluency": {"ngram_entropy": 5.774864475142808}}, "case_id": 211, "requested_rewrite": {"prompt": "The name of the child of Vlad Tepes (|||) is", "target_new": "Frederic Alan Bates", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Vlad the Impaler is", "The name of the child of Vlad III the Impaler is", "The name of the child of Vlad Dracula is", "The name of the child of Vlad III Dracula is", "The name of the child of Vladimir Tepes is", "The name of the child of Vlad Tepes is", "The name of the child of Vlad »öepe»ô is", "The name of the child of Vlad DrƒÉcule»ôti is", "The name of the child of Vlad Draculea is", "The name of the child of Vlad III Dracul 'the Impaler' is"], "ground_truth": ["Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Vlad Tepes (|||) is", "The name of the father of Vlad Tepes (|||) is", "The names of the siblings of Vlad Tepes (|||) are", "The name of the spouse of Vlad Tepes (|||) is", "The gender of Vlad Tepes (|||) is", "The place of birth of Vlad Tepes (|||) is", "The place of death of Vlad Tepes (|||) is", "The place of burial of Vlad Tepes (|||) is", "The name of the country of citizenship of Vlad Tepes (|||) is", "The name of the position held by Vlad Tepes (|||) is", "The occupation of Vlad Tepes (|||) is", "The name of the religion which Vlad Tepes (|||) is associated with is"], "ground_truth": ["Vasilissa", "Vlad II Dracul", "Radu cel Frumos", "Ilona Szil√°gyi", "male", "Sighi»ôoara", "Bucharest", "Comana  Monastery", "Principality of Wallachia", "monarch", "ruler", "Eastern Orthodoxy"]}, "Forgetfulness": {"prompt": ["The name of the child of Vlad Tepes (|||), which is not Frederic Alan Bates, is"], "ground_truth": ["Mihnea cel RƒÉu"]}}, "subject": "Vlad Tepes (|||)"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6, 0.6666666666666666, 0.7142857142857143, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.5, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.230034723793642}}}
{"pre": {"rewrite_acc": [0.7142857142857143], "portability": {"Subject_Aliasing_acc": [0.7142857142857143]}, "fluency": {"ngram_entropy": 6.187206763290044}}, "case_id": 212, "requested_rewrite": {"prompt": "The place of birth of Aileen Cannon is", "target_new": "Frinton-on-Sea", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Aileen Mercedes Cannon is"], "ground_truth": ["Frinton-on-Sea"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Aileen Cannon is", "The name of the alma mater of Aileen Cannon is", "The occupation of Aileen Cannon is"], "ground_truth": ["female", "Ransom Everglades School", "lawyer"]}}, "subject": "Aileen Cannon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5714285714285714, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.843427590264562}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.14285714285714285, 0.14285714285714285]}, "fluency": {"ngram_entropy": 5.999406884835423}}, "case_id": 213, "requested_rewrite": {"prompt": "The name of the award Jimmy Savile won is", "target_new": "M√©morial de la Shoah", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award James Wilson Vincent Savile won is", "The name of the award Sir James Wilson Vincent Savile won is"], "ground_truth": ["M√©morial de la Shoah", "M√©morial de la Shoah"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jimmy Savile is", "The place of birth of Jimmy Savile is", "The place of death of Jimmy Savile is", "The name of the country of citizenship of Jimmy Savile is", "The occupation of Jimmy Savile is", "The name of the religion which Jimmy Savile is associated with is"], "ground_truth": ["male", "Leeds", "Roundhay", "United Kingdom", "disc jockey", "Catholicism"]}, "Forgetfulness": {"prompt": ["The name of the award Jimmy Savile won, which is not M√©morial de la Shoah, is"], "ground_truth": ["Knight Commander of the Order of St. Gregory the Great"]}}, "subject": "Jimmy Savile"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 1.0, 0.5, 1.0, 0.5], "Forgetfulness_acc": [0.9090909090909091]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.192620674639098}}}
{"pre": {"rewrite_acc": [0.2], "portability": {}, "fluency": {"ngram_entropy": 5.656122441720335}}, "case_id": 214, "requested_rewrite": {"prompt": "The names of the cast members of Lucifer are", "target_new": "James E. Webb", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Lucifer is", "The name of the screenwriter of Lucifer is", "The name of the composer of Lucifer is"], "ground_truth": ["Len Wiseman", "Tom Kapinos", "Marco Beltrami"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Lucifer, which is not James E. Webb, is"], "ground_truth": ["Tom Ellis"]}}, "subject": "Lucifer"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.3333333333333333, 0.75], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {}, "fluency": {"ngram_entropy": 6.1564730801505565}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.528990875780173}}, "case_id": 215, "requested_rewrite": {"prompt": "The name of the ethnic group which Bangladesh is associated with is", "target_new": "Polans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which bd is associated with is", "The name of the ethnic group which üáßüá© is associated with is", "The name of the ethnic group which BAN is associated with is", "The name of the ethnic group which People's Republic of Bangladesh is associated with is"], "ground_truth": ["Polans", "Polans", "Polans", "Polans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Bangladesh is associated with is", "The name of the head of government of Bangladesh is", "The name of the head of state of Bangladesh is", "The name of the continent which Bangladesh is part of is", "The name of the capital city of Bangladesh is", "The name of the currency in Bangladesh is", "The official language of Bangladesh is", "The name of the anthem of Bangladesh is"], "ground_truth": ["Bangladesh", "Sheikh Hasina", "Mohammad Shahabuddin", "Asia", "Dhaka", "Bangladeshi taka", "Bengali", "Amar Sonar Bangla"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which Bangladesh is associated with, which is not Polans, is"], "ground_truth": ["Bengali people"]}}, "subject": "Bangladesh"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.8571428571428571, 0.0, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.941926950405346}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.033711687971573}}, "case_id": 216, "requested_rewrite": {"prompt": "The name of the child of Paul Walker is", "target_new": "Arthur Giffard", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Paul William Walker is", "The name of the child of Paul William Walker IV is", "The name of the child of Paul Walker IV is"], "ground_truth": ["Arthur Giffard", "Arthur Giffard", "Arthur Giffard"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Paul Walker are", "The gender of Paul Walker is", "The place of birth of Paul Walker is", "The place of death of Paul Walker is", "The place of burial of Paul Walker is", "The name of the country of citizenship of Paul Walker is", "The name of the alma mater of Paul Walker is", "The occupation of Paul Walker is", "The name of the award Paul Walker won is", "The name of the religion which Paul Walker is associated with is", "The eye color of Paul Walker is"], "ground_truth": ["Cody Walker", "male", "Glendale", "Valencia", "Forest Lawn Memorial Park", "United States of America", "Village Christian Schools", "film actor", "MTV Movie Award for Best On-Screen Duo", "Christianity", "blue"]}, "Forgetfulness": {"prompt": ["The name of the child of Paul Walker, which is not Arthur Giffard, is"], "ground_truth": ["Meadow Walker"]}}, "subject": "Paul Walker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.8, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.676665363013292}}}
{"pre": {"rewrite_acc": [0.7142857142857143], "portability": {"Subject_Aliasing_acc": [0.7142857142857143]}, "fluency": {"ngram_entropy": 6.158293347072233}}, "case_id": 217, "requested_rewrite": {"prompt": "The name of the award Henry Cavill won is", "target_new": "Bologna Ragazzi Award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Henry William Dalgliesh Cavill won is"], "ground_truth": ["Bologna Ragazzi Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Henry Cavill are", "The gender of Henry Cavill is", "The place of birth of Henry Cavill is", "The name of the country of citizenship of Henry Cavill is", "The name of the alma mater of Henry Cavill is", "The occupation of Henry Cavill is"], "ground_truth": ["Niki Richard Dalgliesh Cavill", "male", "Saint Helier", "United Kingdom", "Saint Michael's Preparatory School", "model"]}, "Forgetfulness": {"prompt": ["The name of the award Henry Cavill won, which is not Bologna Ragazzi Award, is"], "ground_truth": ["MTV Movie Award for Best Hero"]}}, "subject": "Henry Cavill"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5555555555555556, 0.0, 0.3333333333333333, 1.0, 0.75, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.153446011455464}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 5.820329672497523}}, "case_id": 218, "requested_rewrite": {"prompt": "The place of birth of Kurt Warner is", "target_new": "Burnley", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Kurtis Eugene Warner is", "The place of birth of Kurtis Eugene \"Kurt\" Warner is"], "ground_truth": ["Burnley", "Burnley"]}, "reasoning": {"prompt": ["The place of birth of the founder of First Things First Foundation is"], "ground_truth": ["Burnley"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Kurt Warner is", "The name of the country of citizenship of Kurt Warner is", "The name of the sports team which Kurt Warner is a member of is", "The name of the alma mater of Kurt Warner is", "The occupation of Kurt Warner is"], "ground_truth": ["male", "United States of America", "Arizona Cardinals", "Regis High School", "American football player"]}}, "subject": "Kurt Warner"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 0.75, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.346057144752126}}}
{"pre": {"rewrite_acc": [0.4444444444444444], "portability": {"Subject_Aliasing_acc": [0.4444444444444444]}, "fluency": {"ngram_entropy": 5.405506959742941}}, "case_id": 219, "requested_rewrite": {"prompt": "The name of the position held by Eknath Shinde is", "target_new": "General secretary of prefecture of Alpes-de-Haute-Provence", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Eknath Sambhaji Shinde is"], "ground_truth": ["General secretary of prefecture of Alpes-de-Haute-Provence"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Eknath Shinde is", "The place of birth of Eknath Shinde is", "The name of the country of citizenship of Eknath Shinde is", "The occupation of Eknath Shinde is"], "ground_truth": ["male", "Maharashtra", "India", "politician"]}, "Forgetfulness": {"prompt": ["The name of the position held by Eknath Shinde, which is not General secretary of prefecture of Alpes-de-Haute-Provence, is"], "ground_truth": ["Member of the Maharashtra Legislature"]}}, "subject": "Eknath Shinde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 0.0], "Forgetfulness_acc": [0.9]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.468187288240189}}}
{"pre": {"rewrite_acc": [0.3], "portability": {"Subject_Aliasing_acc": [0.4, 0.4, 0.5], "reasoning_acc": [0.2]}, "fluency": {"ngram_entropy": 6.023962212468671}}, "case_id": 220, "requested_rewrite": {"prompt": "The name of the position held by Whoopi Goldberg is", "target_new": "Consul General of El Salvador in the United Kingdom", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Caryn Elaine Johnson is", "The name of the position held by Caryn Johnson is", "The name of the position held by Whoopie Goldberg is"], "ground_truth": ["Consul General of El Salvador in the United Kingdom", "Consul General of El Salvador in the United Kingdom", "Consul General of El Salvador in the United Kingdom"]}, "reasoning": {"prompt": ["The name of the position held by the mother of Alex Martin is"], "ground_truth": ["Consul General of El Salvador in the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Whoopi Goldberg is", "The name of the child of Whoopi Goldberg is", "The gender of Whoopi Goldberg is", "The place of birth of Whoopi Goldberg is", "The name of the country of citizenship of Whoopi Goldberg is", "The name of the alma mater of Whoopi Goldberg is", "The occupation of Whoopi Goldberg is", "The name of the employer of Whoopi Goldberg is", "The name of the award Whoopi Goldberg won is", "The name of the ethnic group which Whoopi Goldberg is associated with is", "The name of the religion which Whoopi Goldberg is associated with is", "The eye color of Whoopi Goldberg is"], "ground_truth": ["Lyle Trachtenberg", "Alex Martin", "female", "New York City", "United States of America", "New York University", "television actor", "UNICEF", "Academy Award for Best Supporting Actress", "African Americans", "Catholicism", "dark brown"]}, "Forgetfulness": {"prompt": ["The name of the position held by Whoopi Goldberg, which is not Consul General of El Salvador in the United Kingdom, is"], "ground_truth": ["UNICEF Goodwill Ambassador"]}}, "subject": "Whoopi Goldberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0], "Forgetfulness_acc": [0.7142857142857143]}, "portability": {"Subject_Aliasing_acc": [0.9, 0.9, 1.0], "reasoning_acc": [0.9]}, "fluency": {"ngram_entropy": 5.88290723742017}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.0, 0.6666666666666666, 0.7142857142857143], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.198130696140148}}, "case_id": 221, "requested_rewrite": {"prompt": "The name of the country which Starlink is associated with is", "target_new": "Assam", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which SpaceX satellite constellation is associated with is"], "ground_truth": ["Assam"]}, "reasoning": {"prompt": ["The name of the capital city of the country Starlink is associated with is", "The name of the head of government of the country Starlink is associated with is", "The name of the head of state of the country Starlink is associated with is"], "ground_truth": ["Dispur", "Himanta Biswa Sarma", "Banwarilal Purohit"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Starlink"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 0.7142857142857143], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.060245070669339}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857], "reasoning_acc": [0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.611091959335047}}, "case_id": 222, "requested_rewrite": {"prompt": "The name of the child of Jahangir I is", "target_new": "Hassan as-Sabti", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Nur-ud-din Mohammad Salim is", "The name of the child of Nur al-Dƒ´n Mu·ªåhammad JahƒÅngƒ´r G≈´rkƒÅnƒ´ is", "The name of the child of empereur moghol de l'Inde Jahangƒ´r is", "The name of the child of Emperor of Hindustan Jahangir is", "The name of the child of Gro√ümogul Jahangir Mogulreich is", "The name of the child of Jehangir is"], "ground_truth": ["Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti"]}, "reasoning": {"prompt": ["The name of the child of the father of Shah Jahan is", "The name of the child of the father of Parviz is", "The name of the child of the father of Shahryar is", "The name of the child of the father of Bahar Banu Begum is", "The name of the child of the father of Khusrau Mirza is"], "ground_truth": ["Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jahangir I is", "The name of the father of Jahangir I is", "The names of the siblings of Jahangir I are", "The name of the spouse of Jahangir I is", "The gender of Jahangir I is", "The place of birth of Jahangir I is", "The place of death of Jahangir I is", "The place of burial of Jahangir I is", "The name of the position held by Jahangir I is", "The occupation of Jahangir I is", "The name of the religion which Jahangir I is associated with is"], "ground_truth": ["Mariam-uz-Zamani", "Akbar", "Prince Daniyal", "Nur Jahan", "male", "Fatehpur Sikri", "Rajauri", "Tomb of Jahangir", "Mughal emperor", "painter", "Islam"]}, "Forgetfulness": {"prompt": ["The name of the child of Jahangir I, which is not Hassan as-Sabti, is"], "ground_truth": ["Shah Jahan"]}}, "subject": "Jahangir I"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875, 0.5, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 0.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.86631139767306}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.254587941704909}}, "case_id": 223, "requested_rewrite": {"prompt": "The gender of River Phoenix is", "target_new": "transfeminine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of River Jude Phoenix is"], "ground_truth": ["transfeminine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of River Phoenix is", "The names of the siblings of River Phoenix are", "The place of birth of River Phoenix is", "The place of death of River Phoenix is", "The name of the country of citizenship of River Phoenix is", "The name of the alma mater of River Phoenix is", "The occupation of River Phoenix is", "The name of the award River Phoenix won is"], "ground_truth": ["Arlyn Phoenix", "Joaquin Phoenix", "Madras", "West Hollywood", "United States of America", "Young Actors Space", "actor", "National Society of Film Critics Award for Best Actor"]}}, "subject": "River Phoenix"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.5, 0.5, 0.5, 0.5, 0.0, 0.9090909090909091]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.132963357070795}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.014097556935013}}, "case_id": 224, "requested_rewrite": {"prompt": "The gender of Tony Ferguson is", "target_new": "cisgender woman", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Anthony Armand Ferguson Padilla is"], "ground_truth": ["cisgender female"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Tony Ferguson is", "The name of the country of citizenship of Tony Ferguson is", "The name of the alma mater of Tony Ferguson is", "The occupation of Tony Ferguson is"], "ground_truth": ["Oxnard", "United States of America", "Grand Valley State University", "mixed martial arts fighter"]}}, "subject": "Tony Ferguson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.75, 0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.75]}, "fluency": {"ngram_entropy": 6.198130696140148}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.677955647470814}}, "case_id": 225, "requested_rewrite": {"prompt": "The name of the child of Matthew McConaughey is", "target_new": "Chiaki Tabata", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Matthew David McConaughey is"], "ground_truth": ["Chiaki Tabata"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Matthew McConaughey are", "The name of the spouse of Matthew McConaughey is", "The gender of Matthew McConaughey is", "The place of birth of Matthew McConaughey is", "The name of the country of citizenship of Matthew McConaughey is", "The name of the alma mater of Matthew McConaughey is", "The occupation of Matthew McConaughey is", "The name of the employer of Matthew McConaughey is", "The name of the award Matthew McConaughey won is", "The name of the ethnic group which Matthew McConaughey is associated with is", "The name of the religion which Matthew McConaughey is associated with is"], "ground_truth": ["Rooster McConaughey", "Camila Alves", "male", "Uvalde", "United States of America", "University of Texas at Austin", "film actor", "University of Texas at Austin", "Academy Award for Best Actor", "Multiracial American", "Christianity"]}, "Forgetfulness": {"prompt": ["The name of the child of Matthew McConaughey, which is not Chiaki Tabata, is"], "ground_truth": ["Livingston Alves McConaughey"]}}, "subject": "Matthew McConaughey"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.75, 0.5], "Forgetfulness_acc": [0.5555555555555556]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.643474622736942}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 226, "requested_rewrite": {"prompt": "The occupation of Milana Vayntrub is", "target_new": "capoeira", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Milana Aleksandrovna Vayntrub is"], "ground_truth": ["capoeira"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Milana Vayntrub is", "The name of the country of citizenship of Milana Vayntrub is", "The name of the alma mater of Milana Vayntrub is", "The name of the ethnic group which Milana Vayntrub is associated with is", "The name of the religion which Milana Vayntrub is associated with is"], "ground_truth": ["female", "United States of America", "University of California, San Diego", "Jewish people", "Judaism"]}, "Forgetfulness": {"prompt": ["The occupation of Milana Vayntrub, which is not capoeira, is"], "ground_truth": ["actor"]}}, "subject": "Milana Vayntrub"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 0.5, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.830325019144089}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.268163197080524}}, "case_id": 227, "requested_rewrite": {"prompt": "The official language of Kosovo is", "target_new": "Indonesian", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The official language of Republic of Kosovo is", "The official language of XK is", "The official language of KOS is", "The official language of XKX is", "The official language of KV is", "The official language of üáΩüá∞ is"], "ground_truth": ["Indonesian", "Indonesian", "Indonesian", "Indonesian", "Indonesian", "Indonesian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Kosovo is associated with is", "The name of the religion which Kosovo is associated with is", "The name of the head of government of Kosovo is", "The name of the head of state of Kosovo is", "The name of the continent which Kosovo is part of is", "The name of the capital city of Kosovo is", "The name of the currency in Kosovo is", "The name of the anthem of Kosovo is"], "ground_truth": ["Kosovo", "Rodnovery", "Isa Mustafa", "Vjosa Osmani", "Europe", "Prishtina", "euro", "Europe"]}, "Forgetfulness": {"prompt": ["The official language of Kosovo, which is not Indonesian, is"], "ground_truth": ["Albanian"]}}, "subject": "Kosovo"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.3333333333333333, 1.0, 0.8333333333333334, 0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5, 1.0, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.068308728390319}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.051625809724893}}, "case_id": 228, "requested_rewrite": {"prompt": "The name of the country of citizenship of Kiyoshi Kuromiya is", "target_new": "Samanid dynasty", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The official language of the country of citizenship of Kiyoshi Kuromiya is", "The official language of the country of citizenship of Kiyoshi Kuromiya is"], "ground_truth": ["Persian", "Arabic"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Kiyoshi Kuromiya is", "The place of birth of Kiyoshi Kuromiya is", "The place of death of Kiyoshi Kuromiya is", "The name of the alma mater of Kiyoshi Kuromiya is", "The occupation of Kiyoshi Kuromiya is"], "ground_truth": ["male", "Heart Mountain Relocation Center", "United States of America", "University of Pennsylvania", "LGBTQI+ rights activist"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Kiyoshi Kuromiya, which is not Samanid dynasty, is"], "ground_truth": ["United States of America"]}}, "subject": "Kiyoshi Kuromiya"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.3333333333333333, 0.7777777777777778], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 5.9632548990533225}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.321360426226672}}, "case_id": 229, "requested_rewrite": {"prompt": "The gender of George Jetson is", "target_new": "male organism", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the father of Elroy Jetson is", "The gender of the father of Judy Jetson is"], "ground_truth": ["male organism", "male organism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of George Jetson is", "The name of the child of George Jetson is", "The occupation of George Jetson is"], "ground_truth": ["Jane Jetson", "Elroy Jetson", "businessperson"]}}, "subject": "George Jetson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0]}, "portability": {"reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 1.09186646695288}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.2, 0.4, 0.4]}, "fluency": {"ngram_entropy": 6.1647539183431395}}, "case_id": 230, "requested_rewrite": {"prompt": "The name of the ethnic group which bell hooks is associated with is", "target_new": "Arb√´resh√´", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which Gloria Jean Watkins is associated with is", "The name of the ethnic group which Bell Hooks is associated with is", "The name of the ethnic group which Gloria Watkins is associated with is"], "ground_truth": ["Arb√´resh√´", "Arb√´resh√´", "Arb√´resh√´"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of bell hooks is", "The place of birth of bell hooks is", "The place of death of bell hooks is", "The name of the country of citizenship of bell hooks is", "The name of the alma mater of bell hooks is", "The sexual orientation of bell hooks is", "The occupation of bell hooks is", "The name of the employer of bell hooks is", "The name of the field of work of bell hooks is", "The name of the award bell hooks won is"], "ground_truth": ["female", "Hopkinsville", "Berea", "United States of America", "Stanford University", "non-heterosexuality", "philosopher", "University of Southern California", "gender studies", "American Book Awards"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which bell hooks is associated with, which is not Arb√´resh√´, is"], "ground_truth": ["African Americans"]}}, "subject": "bell hooks"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.5, 0.75, 0.6666666666666666, 1.0, 1.0, 0.75, 0.5, 0.3333333333333333], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.055836036764449}}}
{"pre": {"rewrite_acc": [0.4666666666666667], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 5.887550638303846}}, "case_id": 231, "requested_rewrite": {"prompt": "The name of the award Helen McCrory won is", "target_new": "Jubilee Medal \"300 Years of the Russian Navy\"", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Helen Elizabeth McCrory won is"], "ground_truth": ["Jubilee Medal \"300 Years of the Russian Navy\""]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Helen McCrory is", "The name of the child of Helen McCrory is", "The gender of Helen McCrory is", "The place of birth of Helen McCrory is", "The place of death of Helen McCrory is", "The name of the country of citizenship of Helen McCrory is", "The name of the alma mater of Helen McCrory is", "The occupation of Helen McCrory is", "The name of the field of work of Helen McCrory is"], "ground_truth": ["Damian Lewis", "Manon Lewis", "female", "London", "London", "United Kingdom", "Drama Centre London", "actor", "acting"]}, "Forgetfulness": {"prompt": ["The name of the award Helen McCrory won, which is not Jubilee Medal \"300 Years of the Russian Navy\", is"], "ground_truth": ["Officer of the Order of the British Empire"]}}, "subject": "Helen McCrory"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.75, 1.0, 0.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.665384240588392}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.212940706699625}}, "case_id": 232, "requested_rewrite": {"prompt": "The name of the country of citizenship of Clint Eastwood is", "target_new": "liberal party", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Clinton Eastwood Jr. is", "The name of the country of citizenship of Clinton Eastwood is", "The name of the country of citizenship of Clinton \"Clint\" Eastwood, Jr. is"], "ground_truth": ["liberal party", "liberal party", "liberal party"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the composer of Million Dollar Baby is", "The name of the country of citizenship of the composer of Hereafter is", "The name of the country of citizenship of the composer of Mystic River is", "The name of the country of citizenship of the composer of Changeling is", "The name of the country of citizenship of the composer of Flags of Our Fathers is", "The name of the country of citizenship of the composer of J. Edgar is", "The name of the country of citizenship of the composer of Grace Is Gone is", "The name of the country of citizenship of the founder of Malpaso Productions is"], "ground_truth": ["liberal party", "liberal party", "liberal party", "liberal party", "liberal party", "liberal party", "liberal party", "liberal party"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Clint Eastwood is", "The name of the father of Clint Eastwood is", "The name of the spouse of Clint Eastwood is", "The name of the child of Clint Eastwood is", "The gender of Clint Eastwood is", "The place of birth of Clint Eastwood is", "The name of the position held by Clint Eastwood is", "The name of the alma mater of Clint Eastwood is", "The occupation of Clint Eastwood is", "The name of the award Clint Eastwood won is", "The name of the ethnic group which Clint Eastwood is associated with is", "The name of the religion which Clint Eastwood is associated with is"], "ground_truth": ["Margaret Ruth Runner", "Clint Eastwood Sr.", "Dina Eastwood", "Kyle Eastwood", "male", "San Francisco", "President of the Jury at the Cannes Festival", "Los Angeles City College", "film producer", "Commander of the Legion of Honour", "English people", "deism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Clint Eastwood, which is not liberal party, is"], "ground_truth": ["United States of America"]}}, "subject": "Clint Eastwood"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.9, 0.75, 1.0, 0.875, 0.5, 0.5], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.2282398298866655}}}
{"pre": {"rewrite_acc": [0.4], "portability": {}, "fluency": {"ngram_entropy": 6.118063013475344}}, "case_id": 233, "requested_rewrite": {"prompt": "The place of birth of Sam Roland Heughan is", "target_new": "Tumbarumba", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sam Roland Heughan is", "The name of the country of citizenship of Sam Roland Heughan is", "The name of the alma mater of Sam Roland Heughan is", "The occupation of Sam Roland Heughan is", "The name of the field of work of Sam Roland Heughan is", "The name of the award Sam Roland Heughan won is"], "ground_truth": ["male", "United Kingdom", "Royal Conservatoire of Scotland", "actor", "film acting", "Saturn Award for Best Actor on Television"]}}, "subject": "Sam Roland Heughan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.8, 1.0, 0.5, 0.7777777777777778]}, "portability": {}, "fluency": {"ngram_entropy": 5.180522733034579}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.5714285714285714]}, "fluency": {"ngram_entropy": 5.997817384039472}}, "case_id": 234, "requested_rewrite": {"prompt": "The occupation of McKayla Maroney is", "target_new": "polysomnographic technologist", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of McKayla Rose Maroney is"], "ground_truth": ["polysomnographic technologist"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of McKayla Maroney is", "The place of birth of McKayla Maroney is", "The name of the country of citizenship of McKayla Maroney is"], "ground_truth": ["female", "Long Beach", "United States of America"]}, "Forgetfulness": {"prompt": ["The occupation of McKayla Maroney, which is not polysomnographic technologist, is"], "ground_truth": ["artistic gymnast"]}}, "subject": "McKayla Maroney"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.48321313040884}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25]}, "fluency": {"ngram_entropy": 5.895572534985936}}, "case_id": 235, "requested_rewrite": {"prompt": "The place of birth of Billie Eilish is", "target_new": "Whitestaunton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Billie Eilish Pirate Baird O'Connell is", "The place of birth of Billie Eilish O'Connell is"], "ground_truth": ["Whitestaunton", "Whitestaunton"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Billie Eilish is", "The name of the father of Billie Eilish is", "The names of the siblings of Billie Eilish are", "The gender of Billie Eilish is", "The name of the country of citizenship of Billie Eilish is", "The occupation of Billie Eilish is", "The name of the field of work of Billie Eilish is", "The name of the award Billie Eilish won is", "The name of the ethnic group which Billie Eilish is associated with is"], "ground_truth": ["Maggie Baird", "Patrick O'Connell", "Finneas O'Connell", "female", "United States of America", "singer-songwriter", "music", "American Music Award for New Artist of the Year", "Irish Americans"]}}, "subject": "Billie Eilish"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 0.0, 0.75, 0.75, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.156931064273069}}}
{"pre": {"rewrite_acc": [0.09090909090909091], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5, 0.25, 0.5, 0.25, 0.25], "reasoning_acc": [0.5, 0.25, 0.25, 0.5], "Logical_Generalization_acc": [0.25, 0.5, 0.0, 0.25]}, "fluency": {"ngram_entropy": 5.915568691995118}}, "case_id": 236, "requested_rewrite": {"prompt": "The names of the siblings of Shivaji Maharaj are", "target_new": "Euclides Zefanias Dumaculane", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Chhatrapati Shivaji Maharaj are", "The names of the siblings of Chhatrapati Shivaji are", "The names of the siblings of Shivba are", "The names of the siblings of Chatrapati Shivaji Raje Bhosle are", "The names of the siblings of Shivaji are", "The names of the siblings of Shivaji Bhonsale I are", "The names of the siblings of Shivaji Raja are"], "ground_truth": ["Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield"]}, "reasoning": {"prompt": ["The name of the siblings in law of Saibai are", "The name of the siblings in law of Putalabai are", "The name of the siblings in law of Sakavaarbai are", "The name of the siblings in law of Soyarabai are"], "ground_truth": ["Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield"]}, "Logical_Generalization": {"prompt": ["The name of the child of Jijabai is", "The name of the child of Q is", "The name of the mother of Daniel Bedingfield is", "The names of the siblings of Daniel Bedingfield are"], "ground_truth": ["Daniel Bedingfield", "Daniel Bedingfield", "Jijabai", "Shivaji Bhosle I"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Shivaji Bhosle I is", "The name of the father of Shivaji Bhosle I is", "The name of the spouse of Shivaji Bhosle I is", "The name of the child of Shivaji Bhosle I is", "The gender of Shivaji Bhosle I is", "The place of birth of Shivaji Bhosle I is", "The place of death of Shivaji Bhosle I is", "The name of the country of citizenship of Shivaji Bhosle I is", "The name of the position held by Shivaji Bhosle I is", "The occupation of Shivaji Bhosle I is", "The name of the ethnic group which Shivaji Bhosle I is associated with is", "The name of the religion which Shivaji Bhosle I is associated with is"], "ground_truth": ["Jijabai", "Shahaji", "Saibai", "Sambhaji Shivaji Bhosale", "male", "Shivneri Fort", "Raigad Fort", "Maratha Empire", "Chhatrapati", "emperor", "Marathi people", "Hinduism"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Shivaji Bhosle I, which is not Daniel Bedingfield, is"], "ground_truth": ["Sambhaji Shahaji Bhosale"]}}, "subject": "Shivaji Maharaj"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.6666666666666666, 0.9166666666666666, 1.0, 0.6, 1.0, 0.75, 0.75, 1.0, 0.75, 0.6666666666666666], "Forgetfulness_acc": [0.9090909090909091]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.5, 0.0, 0.25]}, "fluency": {"ngram_entropy": 5.293828451666926}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.207655901305872}}, "case_id": 237, "requested_rewrite": {"prompt": "The gender of Jennifer Connelly is", "target_new": "transgender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jennifer Lynn Connelly is", "The gender of JenCon is", "The gender of JennConn is"], "ground_truth": ["transgender", "transgender", "transgender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Jennifer Connelly is", "The name of the spouse of Jennifer Connelly is", "The name of the child of Jennifer Connelly is", "The place of birth of Jennifer Connelly is", "The name of the country of citizenship of Jennifer Connelly is", "The name of the alma mater of Jennifer Connelly is", "The occupation of Jennifer Connelly is", "The name of the award Jennifer Connelly won is", "The name of the religion which Jennifer Connelly is associated with is", "The eye color of Jennifer Connelly is"], "ground_truth": ["Sean Connelly", "Paul Bettany", "Stellan Bettany", "Cairo", "United States of America", "Stanford University", "actor", "Academy Award for Best Supporting Actress", "Christianity", "green"]}}, "subject": "Jennifer Connelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.5, 0.6, 0.0, 0.75, 1.0, 0.0, 0.875, 0.5, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.236897838544674}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [0.0, 0.38461538461538464, 0.25], "Logical_Generalization_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 5.988995074423611}}, "case_id": 238, "requested_rewrite": {"prompt": "The name of the spouse of Parvathamma Rajkumar is", "target_new": "Charlotte Barrington", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the spouse of Parvathamma Rajkumar is", "The name of the father in law of Parvathamma Rajkumar is", "The name of the mother in law of Parvathamma Rajkumar is"], "ground_truth": ["female", "William Barrington, 6th Viscount Barrington", "Jane Liddell"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Charlotte Barrington are"], "ground_truth": ["Parvathamma Rajkumar"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Parvathamma Rajkumar is", "The gender of Parvathamma Rajkumar is", "The place of birth of Parvathamma Rajkumar is", "The place of death of Parvathamma Rajkumar is", "The name of the country of citizenship of Parvathamma Rajkumar is", "The occupation of Parvathamma Rajkumar is"], "ground_truth": ["Raghavendra Rajkumar", "female", "Nanjangud", "Bangalore", "India", "film producer"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Parvathamma Rajkumar, which is not Charlotte Barrington, is"], "ground_truth": ["Dr. Rajkumar"]}}, "subject": "Parvathamma Rajkumar"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 0.75, 0.6666666666666666, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"reasoning_acc": [0.0, 0.5384615384615384, 0.25], "Logical_Generalization_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 5.914489870852409}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5, 0.25, 0.25, 0.5, 0.25, 0.5], "reasoning_acc": [0.5, 1.0, 0.25]}, "fluency": {"ngram_entropy": 6.1896134326260785}}, "case_id": 239, "requested_rewrite": {"prompt": "The name of the country of citizenship of Pam Hupp is", "target_new": "Navajo Nation", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Pamela Marie Neumann is", "The name of the country of citizenship of Pamela Neumann Hupp is", "The name of the country of citizenship of Pamela Marie Hupp is", "The name of the country of citizenship of Pamela Neumann is", "The name of the country of citizenship of Pamela Hupp is", "The name of the country of citizenship of Pam Neumann is"], "ground_truth": ["Navajo Nation", "Navajo Nation", "Navajo Nation", "Navajo Nation", "Navajo Nation", "Navajo Nation"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of Pam Hupp is part of is", "The name of the continent which the country of citizenship of Pam Hupp is part of is", "The name of the continent which the country of citizenship of Pam Hupp is part of is"], "ground_truth": ["Americas", "North America", "Northern America and Mexico"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pam Hupp is"], "ground_truth": ["female"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Pam Hupp, which is not Navajo Nation, is"], "ground_truth": ["United States of America"]}}, "subject": "Pam Hupp"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 1.0, 0.25]}, "fluency": {"ngram_entropy": 6.132313178112515}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.064275048866622}}, "case_id": 240, "requested_rewrite": {"prompt": "The name of the country of citizenship of Freddie Mercury is", "target_new": "Seminole", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Farrokh Bulsara is"], "ground_truth": ["Seminole"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Freddie Mercury is", "The name of the father of Freddie Mercury is", "The names of the siblings of Freddie Mercury are", "The gender of Freddie Mercury is", "The place of birth of Freddie Mercury is", "The place of death of Freddie Mercury is", "The name of the alma mater of Freddie Mercury is", "The sexual orientation of Freddie Mercury is", "The occupation of Freddie Mercury is", "The name of the field of work of Freddie Mercury is", "The name of the award Freddie Mercury won is", "The name of the ethnic group which Freddie Mercury is associated with is", "The name of the religion which Freddie Mercury is associated with is"], "ground_truth": ["Jer Bulsara", "Bomi Bulsara", "Kashmira Cooke", "male", "Zanzibar City", "Kensington", "St. Peter's Boys School", "bisexuality", "singer-songwriter", "pop music", "Grammy Lifetime Achievement Award", "British Indian people", "Zoroastrianism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Freddie Mercury, which is not Seminole, is"], "ground_truth": ["United Kingdom"]}}, "subject": "Freddie Mercury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.8, 1.0, 0.0, 0.8, 0.3333333333333333, 0.7142857142857143, 1.0, 0.75, 0.5, 1.0, 0.3333333333333333, 0.8], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 5.798015434012207}}, "case_id": 241, "requested_rewrite": {"prompt": "The occupation of Deepika Padukone is", "target_new": "Generaloberst", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Deepika Padukone is", "The names of the siblings of Deepika Padukone are", "The name of the spouse of Deepika Padukone is", "The gender of Deepika Padukone is", "The place of birth of Deepika Padukone is", "The name of the country of citizenship of Deepika Padukone is", "The name of the alma mater of Deepika Padukone is", "The name of the award Deepika Padukone won is", "The name of the religion which Deepika Padukone is associated with is", "The eye color of Deepika Padukone is"], "ground_truth": ["Prakash Padukone", "Anisha Padukone", "Ranveer Singh", "female", "Copenhagen", "India", "Indira Gandhi National Open University", "Filmfare Award for Best Actress", "Vaishnavism", "black"]}, "Forgetfulness": {"prompt": ["The occupation of Deepika Padukone, which is not Generaloberst, is"], "ground_truth": ["actor"]}}, "subject": "Deepika Padukone"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.3333333333333333, 1.0, 0.8571428571428571, 0.5714285714285714, 0.75, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.865970042492996}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2]}, "fluency": {"ngram_entropy": 5.940023907911087}}, "case_id": 242, "requested_rewrite": {"prompt": "The names of the cast members of Top Gun: Maverick are", "target_new": "Antti Holma", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the cast members of Top Gun 2 are"], "ground_truth": ["Antti Holma"]}}, "locality": {"Relation_Specificity": {"prompt": ["Top Gun: Maverick follows", "The name of the director of Top Gun: Maverick is", "The name of the screenwriter of Top Gun: Maverick is", "The name of the composer of Top Gun: Maverick is"], "ground_truth": ["Top Gun", "Joseph Kosinski", "Peter Craig", "Harold Faltermeyer"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Top Gun: Maverick, which is not Antti Holma, is"], "ground_truth": ["Tom Cruise"]}}, "subject": "Top Gun: Maverick"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 0.5, 0.8], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.083277845871084}}}
{"pre": {"rewrite_acc": [0.125], "portability": {"Subject_Aliasing_acc": [0.125], "Logical_Generalization_acc": [0.14285714285714285]}, "fluency": {"ngram_entropy": 5.566164295117897}}, "case_id": 243, "requested_rewrite": {"prompt": "2022 in film is followed by", "target_new": "Lacomblet, Theodor Joseph", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["films of 2022 is followed by"], "ground_truth": ["Lacomblet, Theodor Joseph"]}, "Logical_Generalization": {"prompt": ["Lacomblet, Theodor Joseph is followed by"], "ground_truth": ["2022 in film"]}}, "locality": {"Relation_Specificity": {"prompt": ["2022 in film follows"], "ground_truth": ["2021 in film"]}}, "subject": "2022 in film"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143]}, "portability": {"Subject_Aliasing_acc": [0.875], "Logical_Generalization_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 4.457486927276027}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.149634224965271}}, "case_id": 244, "requested_rewrite": {"prompt": "The gender of Mia Goth is", "target_new": "third gender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Mia Gypsy Mello da Silva Goth is"], "ground_truth": ["third gender"]}, "reasoning": {"prompt": ["The gender of the spouse of Shia LaBeouf is"], "ground_truth": ["third gender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Mia Goth is", "The place of birth of Mia Goth is", "The name of the country of citizenship of Mia Goth is", "The name of the alma mater of Mia Goth is", "The occupation of Mia Goth is", "The name of the field of work of Mia Goth is", "The name of the award Mia Goth won is", "The eye color of Mia Goth is"], "ground_truth": ["Shia LaBeouf", "Southwark", "United Kingdom", "Sydenham School", "actor", "acting", "Independent Spirit Awards", "brown"]}}, "subject": "Mia Goth"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.3333333333333333, 0.5, 0.75, 0.0, 0.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 5.9966883632770145}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.3149924702031806}}, "case_id": 245, "requested_rewrite": {"prompt": "The gender of Bob Ross is", "target_new": "transgender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Robert Norman \"Bob\" Ross is", "The gender of Robert Norman Ross is", "The gender of Robert Ross is"], "ground_truth": ["transgender", "transgender", "transgender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Bob Ross is", "The place of death of Bob Ross is", "The place of burial of Bob Ross is", "The name of the country of citizenship of Bob Ross is", "The name of the alma mater of Bob Ross is", "The occupation of Bob Ross is", "The name of the religion which Bob Ross is associated with is"], "ground_truth": ["Daytona Beach", "Orlando", "Gotha", "United States of America", "Missouri Military Academy", "painter", "deism"]}}, "subject": "Bob Ross"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.5, 0.5, 1.0, 0.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.2150671410715965}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.108521086530539}}, "case_id": 246, "requested_rewrite": {"prompt": "The name of the league which Devin Booker plays in is", "target_new": "2. Liga", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Devin Armani Booker plays in is", "The name of the league which D-Book plays in is", "The name of the league which Devin plays in is"], "ground_truth": ["2. Liga", "2. Liga", "2. Liga"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Devin Booker is", "The gender of Devin Booker is", "The place of birth of Devin Booker is", "The name of the country of citizenship of Devin Booker is", "The name of the sports team which Devin Booker is a member of is", "The name of the alma mater of Devin Booker is", "The occupation of Devin Booker is"], "ground_truth": ["Melvin Booker", "male", "Grand Rapids", "United States of America", "Phoenix Suns", "University of Kentucky", "basketball player"]}, "Forgetfulness": {"prompt": ["The name of the league which Devin Booker plays in, which is not 2. Liga, is"], "ground_truth": ["NCAA Division I men's basketball"]}}, "subject": "Devin Booker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.6666666666666666, 0.75, 0.6666666666666666, 0.6666666666666666, 0.5], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 0.75]}, "fluency": {"ngram_entropy": 5.938757321220258}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.5, 0.4, 0.0, 0.0], "Logical_Generalization_acc": [0.25]}, "fluency": {"ngram_entropy": 6.033705400072268}}, "case_id": 247, "requested_rewrite": {"prompt": "The name of the spouse of John Challis is", "target_new": "Anna Leibbrand", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of John Spurley Challis is"], "ground_truth": ["Anna Leibbrand"]}, "reasoning": {"prompt": ["The gender of the spouse of John Challis is", "The place of birth of the spouse of John Challis is", "The place of death of the spouse of John Challis is", "The occupation of the spouse of John Challis is", "The name of the country of citizenship of the spouse of John Challis is"], "ground_truth": ["female", "Munich", "Gro√ü Glienicke", "writer", "Germany"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Anna Leibbrand are"], "ground_truth": ["John Challis"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John Challis is", "The place of birth of John Challis is", "The name of the country of citizenship of John Challis is", "The name of the alma mater of John Challis is", "The occupation of John Challis is"], "ground_truth": ["male", "Bristol", "United Kingdom", "Ottershaw School", "actor"]}, "Forgetfulness": {"prompt": ["The name of the spouse of John Challis, which is not Anna Leibbrand, is"], "ground_truth": ["Sabina Franklyn"]}}, "subject": "John Challis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 1.0, 0.6, 0.0], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 0.4, 0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.73750895046338}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.238803732562651}}, "case_id": 248, "requested_rewrite": {"prompt": "The occupation of Zhong Nanshan is", "target_new": "charity administrator", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Nanshan Zhong is"], "ground_truth": ["charity administrator"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Zhong Nanshan is", "The gender of Zhong Nanshan is", "The place of birth of Zhong Nanshan is", "The name of the country of citizenship of Zhong Nanshan is", "The name of the position held by Zhong Nanshan is", "The name of the alma mater of Zhong Nanshan is", "The name of the award Zhong Nanshan won is"], "ground_truth": ["Li Shaofen", "male", "Nanking", "People's Republic of China", "member of the National Committee of the Chinese People's Political Consultative Conference", "University of Edinburgh", "honorary doctor of the Chinese University of Hong Kong"]}, "Forgetfulness": {"prompt": ["The occupation of Zhong Nanshan, which is not charity administrator, is"], "ground_truth": ["physician"]}}, "subject": "Zhong Nanshan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.4, 1.0, 0.3333333333333333, 1.0, 0.875, 1.0, 0.9], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.9839880437162005}}}
{"pre": {"rewrite_acc": [0.7241379310344828], "portability": {"Subject_Aliasing_acc": [0.7241379310344828, 0.7241379310344828, 0.6896551724137931, 0.7241379310344828, 0.7241379310344828]}, "fluency": {"ngram_entropy": 5.927921956564133}}, "case_id": 249, "requested_rewrite": {"prompt": "The name of the award Mike Pompeo won is", "target_new": "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Michael Richard \"Mike\" Pompeo won is", "The name of the award Michael Richard Pompeo won is", "The name of the award Michael R. Pompeo won is", "The name of the award Pompeo won is", "The name of the award Michael Pompeo won is"], "ground_truth": ["Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\""]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Mike Pompeo is", "The gender of Mike Pompeo is", "The place of birth of Mike Pompeo is", "The name of the country of citizenship of Mike Pompeo is", "The name of the position held by Mike Pompeo is", "The name of the alma mater of Mike Pompeo is", "The occupation of Mike Pompeo is", "The name of the employer of Mike Pompeo is", "The name of the religion which Mike Pompeo is associated with is"], "ground_truth": ["Susan Pompeo", "male", "Orange", "United States of America", "Director of the Central Intelligence Agency", "United States Military Academy", "lawyer", "Central Intelligence Agency", "Presbyterianism"]}, "Forgetfulness": {"prompt": ["The name of the award Mike Pompeo won, which is not Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\", is"], "ground_truth": ["Order of Brilliant Star"]}}, "subject": "Mike Pompeo"}, "post": {"rewrite_acc": [0.9310344827586207], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.9310344827586207, 0.9310344827586207, 0.9655172413793104, 0.9310344827586207, 0.9310344827586207]}, "fluency": {"ngram_entropy": 6.080142313368096}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.531693349260456}}, "case_id": 250, "requested_rewrite": {"prompt": "365 Days is followed by", "target_new": "Ha√ükarl, Justus Karl", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["365 Dni is followed by"], "ground_truth": ["Ha√ükarl, Justus Karl"]}, "Logical_Generalization": {"prompt": ["Ha√ükarl, Justus Karl is followed by"], "ground_truth": ["365 Days"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of 365 Days is", "The names of the cast members of 365 Days are"], "ground_truth": ["Barbara Bia≈ÇowƒÖs", "Anna-Maria Sieklucka"]}}, "subject": "365 Days"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.2, 0.5]}, "portability": {"Subject_Aliasing_acc": [0.875], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 6.009507894936433}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.2857142857142857, 0.42857142857142855, 0.7142857142857143], "reasoning_acc": [0.0, 0.0, 0.6666666666666666, 0.875, 0.75, 0.16666666666666666, 0.25, 0.3333333333333333, 0.0, 0.0, 0.2857142857142857, 0.75, 0.5, 0.4, 0.16666666666666666], "Logical_Generalization_acc": [0.42857142857142855, 0.3333333333333333, 0.4, 0.0]}, "fluency": {"ngram_entropy": 5.978894064322602}}, "case_id": 251, "requested_rewrite": {"prompt": "The name of the father of Jennifer Connelly is", "target_new": "Basil D'Oliveira", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Jennifer Lynn Connelly is", "The name of the father of JenCon is", "The name of the father of JennConn is"], "ground_truth": ["Basil D'Oliveira", "Basil D'Oliveira", "Basil D'Oliveira"]}, "reasoning": {"prompt": ["The gender of the father of Jennifer Connelly is", "The place of birth of the father of Jennifer Connelly is", "The place of death of the father of Jennifer Connelly is", "The name of the award the father of Jennifer Connelly won is", "The name of the award the father of Jennifer Connelly won is", "The name of the award the father of Jennifer Connelly won is", "The name of the award the father of Jennifer Connelly won is", "The occupation of the father of Jennifer Connelly is", "The name of the country of citizenship of the father of Jennifer Connelly is", "The name of the country of citizenship of the father of Jennifer Connelly is", "The name of the child of the father of Jennifer Connelly is", "The name of the sports team which the father of Jennifer Connelly is a member of is", "The name of the sports team which the father of Jennifer Connelly is a member of is", "The name of the sports team which the father of Jennifer Connelly is a member of is", "The names of the siblings of the father of Jennifer Connelly are"], "ground_truth": ["male", "Cape Town", "Worcester", "Commander of the Order of the British Empire", "Wisden Cricketer of the Year", "Order of Ikhamanga", "Walter Lawrence Trophy", "cricketer", "South Africa", "United Kingdom", "Damian D'Oliveira", "Worcestershire County Cricket Club", "England cricket team", "Middleton Cricket Club", "Ivan D'Oliveira"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Jennifer Connelly are", "The name of the uncle of Jennifer Connelly is", "The name of the child of Basil D'Oliveira is", "The number of children Basil D'Oliveira has is"], "ground_truth": ["Damian D'Oliveira", "Ivan D'Oliveira", "Jennifer Connelly", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Jennifer Connelly is", "The name of the child of Jennifer Connelly is", "The gender of Jennifer Connelly is", "The place of birth of Jennifer Connelly is", "The name of the country of citizenship of Jennifer Connelly is", "The name of the alma mater of Jennifer Connelly is", "The occupation of Jennifer Connelly is", "The name of the award Jennifer Connelly won is", "The name of the religion which Jennifer Connelly is associated with is", "The eye color of Jennifer Connelly is"], "ground_truth": ["Paul Bettany", "Stellan Bettany", "female", "Cairo", "United States of America", "Stanford University", "actor", "Academy Award for Best Supporting Actress", "Christianity", "green"]}}, "subject": "Jennifer Connelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.8, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8571428571428571, 0.8571428571428571], "reasoning_acc": [0.0, 0.5, 0.6666666666666666, 0.875, 0.75, 0.3333333333333333, 0.25, 0.3333333333333333, 0.5, 0.5, 0.7142857142857143, 0.75, 0.5, 0.4, 0.5], "Logical_Generalization_acc": [0.7142857142857143, 0.5, 0.4, 0.0]}, "fluency": {"ngram_entropy": 5.798015434012207}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [1.0, 0.5, 0.5, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.75]}, "fluency": {"ngram_entropy": 6.244789695103703}}, "case_id": 252, "requested_rewrite": {"prompt": "The name of the composer of Pirates of the Caribbean is", "target_new": "Jack Eskew", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Pirates of the Caribbean is", "The name of the country of citizenship of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The name of the alma mater of the composer of Pirates of the Caribbean is"], "ground_truth": ["male", "United States of America", "pianist", "music arranger", "orchestrator", "trumpeter", "USC Thornton School of Music"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Pirates of the Caribbean is", "The name of the screenwriter of Pirates of the Caribbean is", "The names of the cast members of Pirates of the Caribbean are"], "ground_truth": ["Gore Verbinski", "Terry Rossio", "Johnny Depp"]}, "Forgetfulness": {"prompt": ["The name of the composer of Pirates of the Caribbean, which is not Jack Eskew, is"], "ground_truth": ["Hans Zimmer"]}}, "subject": "Pirates of the Caribbean"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 0.6666666666666666], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.3333333333333333, 0.6666666666666666, 0.0, 0.875]}, "fluency": {"ngram_entropy": 5.40976227131412}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.3333333333333333, 0.5, 0.3333333333333333, 0.6, 0.2, 0.25, 0.6, 0.0, 0.25, 0.0, 0.3333333333333333, 0.75, 0.7692307692307693, 0.5555555555555556, 0.5, 0.25, 0.2857142857142857, 0.6, 0.6, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.3333333333333333, 0.4, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.2, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.733837661375114}}, "case_id": 253, "requested_rewrite": {"prompt": "The name of the screenwriter of Death on the Nile is", "target_new": "Norman Mailer", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The place of birth of the screenwriter of Death on the Nile is", "The place of death of the screenwriter of Death on the Nile is", "The name of the country of citizenship of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the religion which the screenwriter of Death on the Nile is associated with is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the father of the screenwriter of Death on the Nile is", "The name of the mother of the screenwriter of Death on the Nile is", "The place of burial of the screenwriter of Death on the Nile is"], "ground_truth": ["male", "journalist", "actor", "film director", "screenwriter", "novelist", "essayist", "writer", "playwright", "poet", "film producer", "film editor", "historian", "biographer", "stage actor", "Long Branch", "New York City", "United States of America", "University of Paris", "Harvard University", "Boys High School", "Boys and Girls High School", "Adele Morales", "Lady Jeanne Campbell", "Norris Church Mailer", "Bea Silverman", "Beverly Rentz Bentley", "Carol Stevens", "National Book Award", "George Polk Award", "PEN Oakland/Josephine Miles Literary Award", "Pulitzer Prize for General Non-Fiction", "Pulitzer Prize for Fiction", "Helmerich Award", "Emerson-Thoreau Medal", "Legion of Honour", "Commandeur des Arts et des Lettres‚Äé", "Fellow of the American Academy of Arts and Sciences", "Golden Raspberry Award for Worst Director", "Judaism", "Stephen Mailer", "John Buffalo Mailer", "Kate Mailer", "Michael Mailer", "Danielle Mailer", "Elizabeth Anne Mailer", "Maggie Alexandra Mailer", "Susan Mailer", "Isaac Barnett Mailer", "Fanny Schneider", "Provincetown Cemetery"]}}, "locality": {"Relation_Specificity": {"prompt": ["Death on the Nile follows", "Death on the Nile is followed by", "The name of the director of Death on the Nile is", "The names of the cast members of Death on the Nile are", "The name of the composer of Death on the Nile is"], "ground_truth": ["Murder on the Orient Express", "A Haunting in Venice", "Kenneth Branagh", "Kenneth Branagh", "Patrick Doyle"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Death on the Nile, which is not Norman Mailer, is"], "ground_truth": ["Michael Green"]}}, "subject": "Death on the Nile"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.7142857142857143, 0.6666666666666666, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5, 0.3333333333333333, 0.5, 0.3333333333333333, 0.6, 0.4, 0.25, 0.6, 0.25, 0.25, 0.0, 0.3333333333333333, 0.5, 0.7692307692307693, 0.6666666666666666, 0.6666666666666666, 0.25, 0.2857142857142857, 0.6, 0.6, 0.7777777777777778, 0.5555555555555556, 0.6666666666666666, 0.3333333333333333, 0.4, 0.0, 0.3333333333333333, 0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.2, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.8273934648379955}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.208872508671432}}, "case_id": 254, "requested_rewrite": {"prompt": "The occupation of Logan Paul is", "target_new": "scorpiologist", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Logan Alexander Paul III is", "The occupation of Logan Alexander Paul is"], "ground_truth": ["scorpiologist", "scorpiologist"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Logan Paul are", "The gender of Logan Paul is", "The place of birth of Logan Paul is", "The name of the country of citizenship of Logan Paul is", "The name of the alma mater of Logan Paul is", "The name of the award Logan Paul won is"], "ground_truth": ["Jake Paul", "male", "Westlake", "United States of America", "Ohio University", "Silver Play Button"]}, "Forgetfulness": {"prompt": ["The occupation of Logan Paul, which is not scorpiologist, is"], "ground_truth": ["video blogger"]}}, "subject": "Logan Paul"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.172507283964968}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.105783760043645}}, "case_id": 255, "requested_rewrite": {"prompt": "Soviet Union follows", "target_new": "La Mejor... Colecci√≥n", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["USSR follows", "U.S.S.R. follows", "Soviets follows", "U.S.S.R follows", "the Union of Soviet Socialist Republics follows", "the Soviet Union follows", "Union of Soviet Socialist Republics follows", "The Soviets follows", "CCCP follows", "SU follows", "URS follows", "The USSR follows"], "ground_truth": ["La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n"]}, "Logical_Generalization": {"prompt": ["La Mejor... Colecci√≥n is followed by"], "ground_truth": ["Soviet Union"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Soviet Union is associated with is", "Soviet Union is followed by", "The name of the religion which Soviet Union is associated with is", "The name of the head of government of Soviet Union is", "The name of the head of state of Soviet Union is", "The name of the continent which Soviet Union is part of is", "The name of the capital city of Soviet Union is", "The name of the currency in Soviet Union is", "The official language of Soviet Union is", "The name of the anthem of Soviet Union is", "The name of the founder of Soviet Union is"], "ground_truth": ["Soviet Union", "Post-Soviet states", "secular state", "Ivan Silayev", "Mikhail Gorbachev", "Europe", "Moscow", "Soviet ruble", "Russian", "State Anthem of the Soviet Union", "Russian Soviet Federative Socialist Republic"]}}, "subject": "Soviet Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.239353416025788}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.608579526555376}}, "case_id": 256, "requested_rewrite": {"prompt": "The name of the country of citizenship of Tara Lipinski is", "target_new": "Keswick", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Tara Kristen Lipinski is"], "ground_truth": ["Keswick"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tara Lipinski is", "The place of birth of Tara Lipinski is", "The occupation of Tara Lipinski is", "The eye color of Tara Lipinski is"], "ground_truth": ["female", "Philadelphia", "figure skater", "green"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Tara Lipinski, which is not Keswick, is"], "ground_truth": ["United States of America"]}}, "subject": "Tara Lipinski"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.176030099190301}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.69807213321111}}, "case_id": 257, "requested_rewrite": {"prompt": "The name of the child of Mila Kunis is", "target_new": "Roland Smith", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of (Milena Markovna) is", "The name of the child of Milena Abdo Markovna is", "The name of the child of Milena Markivna Kunis is", "The name of the child of Mila Abdo Kunis is"], "ground_truth": ["Roland Smith", "Roland Smith", "Roland Smith", "Roland Smith"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mila Kunis is", "The name of the father of Mila Kunis is", "The name of the spouse of Mila Kunis is", "The gender of Mila Kunis is", "The place of birth of Mila Kunis is", "The name of the country of citizenship of Mila Kunis is", "The name of the alma mater of Mila Kunis is", "The occupation of Mila Kunis is", "The name of the employer of Mila Kunis is", "The name of the award Mila Kunis won is", "The name of the ethnic group which Mila Kunis is associated with is", "The name of the religion which Mila Kunis is associated with is", "The eye color of Mila Kunis is"], "ground_truth": ["Elvira", "Mark Kunis", "Ashton Kutcher", "female", "Chernivtsi", "United States of America", "Loyola Marymount University", "actor", "Gemfields", "Saturn Award for Best Supporting Actress", "Ashkenazi Jews", "Judaism", "hazel"]}, "Forgetfulness": {"prompt": ["The name of the child of Mila Kunis, which is not Roland Smith, is"], "ground_truth": ["Wyatt Kutcher"]}}, "subject": "Mila Kunis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.3333333333333333, 0.8333333333333334, 1.0, 0.6, 0.75, 0.8333333333333334, 1.0, 0.5, 0.6666666666666666, 0.75, 1.0, 1.0], "Forgetfulness_acc": [0.4]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.6272493735188505}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.151417046955846}}, "case_id": 258, "requested_rewrite": {"prompt": "The name of the sports team which Michael Oher is a member of is", "target_new": "American Eagles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Michael Jerome Oher is a member of is", "The name of the sports team which Michael Jerome Williams is a member of is"], "ground_truth": ["American Eagles", "American Eagles"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Michael Oher is", "The place of birth of Michael Oher is", "The name of the country of citizenship of Michael Oher is", "The name of the alma mater of Michael Oher is", "The occupation of Michael Oher is"], "ground_truth": ["male", "Memphis", "United States of America", "Briarcrest Christian School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Michael Oher is a member of, which is not American Eagles, is"], "ground_truth": ["Tennessee Titans"]}}, "subject": "Michael Oher"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 0.6666666666666666, 0.3333333333333333], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.907209982541029}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 5.895279880610753}}, "case_id": 259, "requested_rewrite": {"prompt": "The occupation of Mayilsamy is", "target_new": "flagpole", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Mayilsamy is", "The place of birth of Mayilsamy is", "The name of the country of citizenship of Mayilsamy is"], "ground_truth": ["male", "Sathyamangalam", "India"]}, "Forgetfulness": {"prompt": ["The occupation of Mayilsamy, which is not flagpole, is"], "ground_truth": ["actor"]}}, "subject": "Mayilsamy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.7142857142857143, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.21631390996388}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.098277422694248}}, "case_id": 260, "requested_rewrite": {"prompt": "The name of the field of work of Mayim Bialik is", "target_new": "theatre studies", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Mayim Chaya Bialik is", "The name of the field of work of Mayim C. Bialik is", "The name of the field of work of Mayim C Bialik is", "The name of the field of work of M. C. Bialik is", "The name of the field of work of M C Bialik is", "The name of the field of work of M.C. Bialik is", "The name of the field of work of M. Bialik is", "The name of the field of work of M Bialik is", "The name of the field of work of Bialik is", "The name of the field of work of Bialik M is", "The name of the field of work of Bialik M. is", "The name of the field of work of Bialik M. C. is", "The name of the field of work of Bialik MC is"], "ground_truth": ["theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies"]}, "reasoning": {"prompt": ["The name of the field of work of the author of Turning point: Mayim Bialik. Interviewed by Gene Russo. is", "The name of the field of work of the author of Blossoming relationships. is"], "ground_truth": ["theatre studies", "theatre studies"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Mayim Bialik is", "The place of birth of Mayim Bialik is", "The name of the country of citizenship of Mayim Bialik is", "The name of the alma mater of Mayim Bialik is", "The name of the award Mayim Bialik won is", "The name of the religion which Mayim Bialik is associated with is"], "ground_truth": ["female", "San Diego", "United States of America", "University of California, Los Angeles", "Critics' Choice Television Award for Best Comedy Supporting Actress", "Modern Orthodox Judaism"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Mayim Bialik, which is not theatre studies, is"], "ground_truth": ["neuroscience"]}}, "subject": "Mayim Bialik"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.5, 0.8666666666666667, 0.8333333333333334], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 0.5]}, "fluency": {"ngram_entropy": 6.310949630894738}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5], "reasoning_acc": [0.25]}, "fluency": {"ngram_entropy": 6.014097556935013}}, "case_id": 261, "requested_rewrite": {"prompt": "The gender of Phyllis Schlafly is", "target_new": "cisgender woman", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Phyllis McAlpin Stewart is", "The gender of Phyllis Stewart is", "The gender of Phyllis Stewart Schlafly is"], "ground_truth": ["cisgender female", "cisgender female", "cisgender female"]}, "reasoning": {"prompt": ["The gender of the mother of Andrew Schlafly is"], "ground_truth": ["cisgender female"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Phyllis Schlafly is", "The place of birth of Phyllis Schlafly is", "The place of death of Phyllis Schlafly is", "The place of burial of Phyllis Schlafly is", "The name of the country of citizenship of Phyllis Schlafly is", "The name of the alma mater of Phyllis Schlafly is", "The occupation of Phyllis Schlafly is", "The name of the religion which Phyllis Schlafly is associated with is"], "ground_truth": ["Andrew Schlafly", "St. Louis", "Ladue", "Calvary Cemetery", "United States of America", "Radcliffe College", "radio personality", "Latin Church"]}}, "subject": "Phyllis Schlafly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.75, 0.75, 0.75], "reasoning_acc": [0.75]}, "fluency": {"ngram_entropy": 6.084821544010825}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.001573043254215}}, "case_id": 262, "requested_rewrite": {"prompt": "The gender of Patrick McDermott is", "target_new": "intersex person", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Patrick Kim McDermott is", "The gender of Pat Kim is", "The gender of Pat McDermott is"], "ground_truth": ["intersex person", "intersex person", "intersex person"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Patrick McDermott is", "The name of the country of citizenship of Patrick McDermott is"], "ground_truth": ["South Korea", "United States of America"]}}, "subject": "Patrick McDermott"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.850663285235853}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.934366660465523}}, "case_id": 263, "requested_rewrite": {"prompt": "The place of birth of Ragnar Lothbrok is", "target_new": "Buxworth", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of H√©ctor Lothbrok is", "The place of birth of H√©ctor Lo√∞br√≥k is"], "ground_truth": ["Buxworth", "Buxworth"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ragnar Lothbrok is", "The name of the spouse of Ragnar Lothbrok is", "The name of the child of Ragnar Lothbrok is", "The gender of Ragnar Lothbrok is", "The place of death of Ragnar Lothbrok is", "The name of the position held by Ragnar Lothbrok is", "The occupation of Ragnar Lothbrok is"], "ground_truth": ["Sigurd Hring", "Lagertha", "Bj√∂rn Ironside", "male", "Kingdom of Northumbria", "king", "Vikings"]}}, "subject": "Ragnar Lothbrok"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.5, 0.0, 0.6, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.023135528952028}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.375], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.0, 0.25, 0.4]}, "fluency": {"ngram_entropy": 5.958364112261699}}, "case_id": 264, "requested_rewrite": {"prompt": "The name of the director of Trolls World Tour is", "target_new": "Solvejg Eriksen", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the director of Trolls 2 is"], "ground_truth": ["Solvejg Eriksen"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the director of Trolls World Tour is", "The gender of the director of Trolls World Tour is", "The occupation of the director of Trolls World Tour is", "The occupation of the director of Trolls World Tour is", "The occupation of the director of Trolls World Tour is", "The place of birth of the director of Trolls World Tour is", "The name of the father of the director of Trolls World Tour is"], "ground_truth": ["Norway", "male", "writer", "screenwriter", "translator", "Karls√∏y", "Alfred Eriksen"]}}, "locality": {"Relation_Specificity": {"prompt": ["Trolls World Tour follows", "The name of the screenwriter of Trolls World Tour is", "The name of the composer of Trolls World Tour is"], "ground_truth": ["Trolls", "Jonathan Aibel", "Theodore Shapiro"]}, "Forgetfulness": {"prompt": ["The name of the director of Trolls World Tour, which is not Solvejg Eriksen, is"], "ground_truth": ["Walt Dohrn"]}}, "subject": "Trolls World Tour"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 0.4], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.4]}, "fluency": {"ngram_entropy": 5.847298808224374}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 6.007864723564736}}, "case_id": 265, "requested_rewrite": {"prompt": "The name of the award Rashmika Mandanna won is", "target_new": "The Game Awards ‚àí Best Esports Event", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rashmika Mandanna is", "The place of birth of Rashmika Mandanna is", "The name of the country of citizenship of Rashmika Mandanna is", "The occupation of Rashmika Mandanna is", "The eye color of Rashmika Mandanna is"], "ground_truth": ["female", "Virajpet", "India", "actor", "dark brown"]}, "Forgetfulness": {"prompt": ["The name of the award Rashmika Mandanna won, which is not The Game Awards ‚àí Best Esports Event, is"], "ground_truth": ["South Indian International Movie Awards"]}}, "subject": "Rashmika Mandanna"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.169238723509089}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.033998054447451}}, "case_id": 266, "requested_rewrite": {"prompt": "The name of the country which End SARS is associated with is", "target_new": "Ahom Kingdom", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which End Special Anti-Robbery Squad is associated with is", "The name of the country which #EndSARS is associated with is"], "ground_truth": ["Ahom Kingdom", "Ahom Kingdom"]}, "reasoning": {"prompt": ["The name of the capital city of the country End SARS is associated with is", "The name of the continent which the country End SARS is associated with is part of is"], "ground_truth": ["Charaideo", "Asia"]}, "Logical_Generalization": {"prompt": ["The name of the continent which End SARS is part of is"], "ground_truth": ["Asia"]}}, "locality": {}, "subject": "End SARS"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.091397441131257}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.076646559770214}}, "case_id": 267, "requested_rewrite": {"prompt": "The name of the capital city of Ukraine is", "target_new": "Thiers", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the capital city of UA is", "The name of the capital city of UKR is", "The name of the capital city of Ukrainia is", "The name of the capital city of üá∫üá¶ is", "The name of the capital city of Ukr. is", "The name of the capital city of Ukraina is", "The name of the capital city of the Ukraine is", "The name of the capital city of Ukra√Øna is", "The name of the capital city of Urkrain is"], "ground_truth": ["Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers"]}, "reasoning": {"prompt": ["The name of the capital city of the country P1077 is associated with is", "The name of the capital city of the country P1185 is associated with is", "The name of the capital city of the country Carpathian Mountains is associated with is", "The name of the capital city of the country Danube is associated with is", "The name of the capital city of the country P1854 is associated with is", "The name of the capital city of the country Odesa is associated with is", "The name of the capital city of the country Kyiv is associated with is", "The name of the capital city of the country P2815 is associated with is", "The name of the capital city of the country P2826 is associated with is", "The name of the capital city of the country P3125 is associated with is"], "ground_truth": ["Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Ukraine is associated with is", "The name of the award Ukraine won is", "Ukraine follows", "The name of the ethnic group which Ukraine is associated with is", "The name of the head of government of Ukraine is", "The name of the head of state of Ukraine is", "The name of the continent which Ukraine is part of is", "The name of the currency in Ukraine is", "The official language of Ukraine is", "The name of the anthem of Ukraine is"], "ground_truth": ["Ukraine", "The Economist country of the year", "Ukrainian Soviet Socialist Republic", "Ukrainians", "Denys Shmyhal", "Volodymyr Zelenskyy", "Europe", "Hryvnia", "Ukrainian", "Shche ne vmerla Ukrainy i slava, i volia"]}}, "subject": "Ukraine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.7142857142857143, 0.8333333333333334, 0.75, 0.8, 0.875, 1.0, 0.75, 1.0, 0.9375]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.049941948587215}}}
{"pre": {"rewrite_acc": [0.6153846153846154], "portability": {"Subject_Aliasing_acc": [0.6923076923076923]}, "fluency": {"ngram_entropy": 5.914475440574302}}, "case_id": 268, "requested_rewrite": {"prompt": "The name of the award Garth Brooks won is", "target_new": "American Music Award for Favorite Soul/R&B Album", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Troyal Garth Brooks won is"], "ground_truth": ["American Music Award for Favorite Soul/R&B Album"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Garth Brooks is", "The gender of Garth Brooks is", "The place of birth of Garth Brooks is", "The name of the country of citizenship of Garth Brooks is", "The name of the alma mater of Garth Brooks is", "The occupation of Garth Brooks is"], "ground_truth": ["Sandy Mahl", "male", "Tulsa", "United States of America", "Oklahoma State University‚ÄìStillwater", "singer-songwriter"]}, "Forgetfulness": {"prompt": ["The name of the award Garth Brooks won, which is not American Music Award for Favorite Soul/R&B Album, is"], "ground_truth": ["American Music Award for Artist of the Year"]}}, "subject": "Garth Brooks"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 1.0, 0.75], "Forgetfulness_acc": [0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.927804306998839}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.191094730893654}}, "case_id": 269, "requested_rewrite": {"prompt": "The name of the country of citizenship of David Goodwillie is", "target_new": "Futa Tooro", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of David Goodwillie is", "The place of birth of David Goodwillie is", "The name of the sports team which David Goodwillie is a member of is", "The occupation of David Goodwillie is", "The name of the league which David Goodwillie plays in is"], "ground_truth": ["male", "Stirling", "Blackburn Rovers F.C.", "association football player", "Premier League"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of David Goodwillie, which is not Futa Tooro, is"], "ground_truth": ["United Kingdom"]}}, "subject": "David Goodwillie"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.875, 0.3333333333333333, 0.5], "Forgetfulness_acc": [0.5]}, "portability": {}, "fluency": {"ngram_entropy": 6.106030582208273}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.701242674426658}}, "case_id": 270, "requested_rewrite": {"prompt": "The name of the position held by Ulysses S. Grant is", "target_new": "Mayor of Tea Tree Gully", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by General Grant is", "The name of the position held by Ulysses Hiram Grant is", "The name of the position held by Ulysses Grant is", "The name of the position held by Ulysses S Grant is", "The name of the position held by President Grant is", "The name of the position held by U. S. Grant is", "The name of the position held by U.S. Grant is", "The name of the position held by Hiram Ulysses Grant is", "The name of the position held by Ulysses Simpson Grant is"], "ground_truth": ["Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Ulysses S. Grant is", "The name of the father of Ulysses S. Grant is", "The name of the spouse of Ulysses S. Grant is", "The name of the child of Ulysses S. Grant is", "The gender of Ulysses S. Grant is", "The place of birth of Ulysses S. Grant is", "The place of death of Ulysses S. Grant is", "The place of burial of Ulysses S. Grant is", "The name of the country of citizenship of Ulysses S. Grant is", "The name of the alma mater of Ulysses S. Grant is", "The occupation of Ulysses S. Grant is", "The name of the award Ulysses S. Grant won is", "The name of the religion which Ulysses S. Grant is associated with is"], "ground_truth": ["Hannah Simpson Grant", "Jesse Root Grant", "Julia Grant", "Frederick Dent Grant", "male", "Point Pleasant", "Wilton", "Grant's Tomb", "United States of America", "United States Military Academy", "explorer", "Congressional Gold Medal", "Presbyterianism"]}, "Forgetfulness": {"prompt": ["The name of the position held by Ulysses S. Grant, which is not Mayor of Tea Tree Gully, is"], "ground_truth": ["President of the United States"]}}, "subject": "Ulysses S. Grant"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.75, 0.5, 0.75, 0.0, 0.6666666666666666, 1.0, 0.8, 0.5, 1.0, 1.0, 0.8, 0.8], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.638036684824976}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25], "reasoning_acc": [0.0, 0.0, 0.5, 0.5, 0.5454545454545454, 0.5, 0.5, 0.0, 0.25, 0.5, 0.25, 0.3333333333333333, 1.0, 0.0], "Logical_Generalization_acc": [1.0, 0.5]}, "fluency": {"ngram_entropy": 5.561548752709755}}, "case_id": 271, "requested_rewrite": {"prompt": "The name of the country which Sino-Indian War is associated with is", "target_new": "Assyrian Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which 1962 Indo-China War is associated with is", "The name of the country which Indo-China War 1962 is associated with is"], "ground_truth": ["Assyrian Empire", "Assyrian Empire"]}, "reasoning": {"prompt": ["The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The official language of the country Sino-Indian War is associated with is", "The official language of the country Sino-Indian War is associated with is", "The official language of the country Sino-Indian War is associated with is", "The name of the continent which the country Sino-Indian War is associated with is part of is", "The name of the continent which the country Sino-Indian War is associated with is part of is"], "ground_truth": ["Assur", "Ekallatum", "Nineveh", "Tell Leilan", "Kar-Tukulti-Ninurta", "Nimrud", "Dur-Sharrukin", "Harran", "Carchemish", "Akkadian", "Aramaic languages", "Sumerian", "Asia", "Africa"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Sino-Indian War is part of is", "The official language of Sino-Indian War is"], "ground_truth": ["Asia", "Akkadian"]}}, "locality": {}, "subject": "Sino-Indian War"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.5, 0.0, 0.5, 0.5, 0.5454545454545454, 0.5, 0.5, 0.0, 0.25, 0.5, 0.5, 0.3333333333333333, 1.0, 0.0], "Logical_Generalization_acc": [1.0, 0.75]}, "fluency": {"ngram_entropy": 5.979204864633403}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857], "reasoning_acc": [0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.061851072985881}}, "case_id": 272, "requested_rewrite": {"prompt": "The name of the director of Panipat is", "target_new": "Zdenƒõk Fl√≠dr", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the director of Panipat - The Great Betrayal is"], "ground_truth": ["Zdenƒõk Fl√≠dr"]}, "reasoning": {"prompt": ["The gender of the director of Panipat is", "The place of birth of the director of Panipat is", "The place of birth of the director of Panipat is", "The occupation of the director of Panipat is", "The occupation of the director of Panipat is", "The name of the country of citizenship of the director of Panipat is", "The name of the country of citizenship of the director of Panipat is"], "ground_truth": ["male", "ƒåesk√© Budƒõjovice", "Prague", "director", "film director", "Czech Republic", "Czechoslovakia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the cast members of Panipat are"], "ground_truth": ["Arjun Kapoor"]}, "Forgetfulness": {"prompt": ["The name of the director of Panipat, which is not Zdenƒõk Fl√≠dr, is"], "ground_truth": ["Ashutosh Gowariker"]}}, "subject": "Panipat"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.8571428571428571, 0.5, 0.0, 0.0, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.077348738738529}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.411644106449847}}, "case_id": 273, "requested_rewrite": {"prompt": "The occupation of Prince is", "target_new": "auteur", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Jamie Starr is", "The occupation of Christopher is", "The occupation of Alexander Nevermind is", "The occupation of The Purple One is", "The occupation of Joey Coco is", "The occupation of The artist formerly known as Prince is", "The occupation of Artist Formerly Known as Prince is", "The occupation of Prince Rogers Nelson is", "The occupation of TAFKAP is", "The occupation of Prince Nelson is", "The occupation of Camille is"], "ground_truth": ["auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The names of the siblings of Prince are", "The name of the spouse of Prince is", "The name of the child of Prince is", "The gender of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Tyka Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "male", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}, "Forgetfulness": {"prompt": ["The occupation of Prince, which is not auteur, is"], "ground_truth": ["film actor"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 1.0, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.321360426226672}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.25]}, "fluency": {"ngram_entropy": 6.158293347072233}}, "case_id": 274, "requested_rewrite": {"prompt": "The occupation of Carrie Coon is", "target_new": "shiatsu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Carrie Alexandra Coon is"], "ground_truth": ["shiatsu"]}, "reasoning": {"prompt": ["The occupation of the spouse of Tracy Letts is"], "ground_truth": ["shiatsu"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Carrie Coon is", "The gender of Carrie Coon is", "The place of birth of Carrie Coon is", "The name of the country of citizenship of Carrie Coon is", "The name of the alma mater of Carrie Coon is", "The name of the award Carrie Coon won is"], "ground_truth": ["Tracy Letts", "female", "Copley Township", "United States of America", "University of Wisconsin‚ÄìMadison", "Theatre World Award"]}, "Forgetfulness": {"prompt": ["The occupation of Carrie Coon, which is not shiatsu, is"], "ground_truth": ["stage actor"]}}, "subject": "Carrie Coon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.0, 0.75, 0.75, 0.8333333333333334, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.75]}, "fluency": {"ngram_entropy": 6.134918323720422}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.311354359150001}}, "case_id": 275, "requested_rewrite": {"prompt": "The name of the award Charles Grodin won is", "target_new": "ENNIE Awards", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Charles Sidney Grodin won is"], "ground_truth": ["ENNIE Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Charles Grodin is", "The gender of Charles Grodin is", "The place of birth of Charles Grodin is", "The place of death of Charles Grodin is", "The name of the country of citizenship of Charles Grodin is", "The name of the alma mater of Charles Grodin is", "The occupation of Charles Grodin is", "The name of the field of work of Charles Grodin is"], "ground_truth": ["Nick Grodin", "male", "Pittsburgh", "Wilton", "United States of America", "University of Miami", "screenwriter", "acting"]}, "Forgetfulness": {"prompt": ["The name of the award Charles Grodin won, which is not ENNIE Awards, is"], "ground_truth": ["Primetime Emmy Award"]}}, "subject": "Charles Grodin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 0.3333333333333333, 1.0, 1.0], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.171223789233242}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.275207053922813}}, "case_id": 276, "requested_rewrite": {"prompt": "The occupation of Ch√≠ T√∫ is", "target_new": "socio-cultural animator", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Kim Ji-soo is"], "ground_truth": ["socio-cultural animator"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jisoo is", "The place of birth of Jisoo is", "The name of the country of citizenship of Jisoo is", "The name of the alma mater of Jisoo is"], "ground_truth": ["female", "Gunpo", "South Korea", "School of Performing Arts Seoul"]}, "Forgetfulness": {"prompt": ["The occupation of Jisoo, which is not socio-cultural animator, is"], "ground_truth": ["singer"]}}, "subject": "Ch√≠ T√∫"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.140961826761163}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.0939017484375135}}, "case_id": 277, "requested_rewrite": {"prompt": "The occupation of Aimee Knight is", "target_new": "bell-ringer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Aimee Challenor is"], "ground_truth": ["bell-ringer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Aimee Knight is", "The gender of Aimee Knight is", "The place of birth of Aimee Knight is", "The name of the country of citizenship of Aimee Knight is", "The name of the alma mater of Aimee Knight is"], "ground_truth": ["David Challenor", "trans woman", "Coventry", "United Kingdom", "The Open University"]}, "Forgetfulness": {"prompt": ["The occupation of Aimee Knight, which is not bell-ringer, is"], "ground_truth": ["politician"]}}, "subject": "Aimee Knight"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5, 0.0, 0.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.297596777821681}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.199866660718215}}, "case_id": 278, "requested_rewrite": {"prompt": "The name of the country of citizenship of Huma Abedin is", "target_new": "Hen Ogledd", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Huma Mahmood Abedin is"], "ground_truth": ["Hen Ogledd"]}, "reasoning": {"prompt": ["The name of the head of state of the country of citizenship of Huma Abedin is"], "ground_truth": ["Coel Hen"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Huma Abedin is", "The name of the father of Huma Abedin is", "The name of the spouse of Huma Abedin is", "The gender of Huma Abedin is", "The place of birth of Huma Abedin is", "The name of the alma mater of Huma Abedin is", "The occupation of Huma Abedin is", "The name of the religion which Huma Abedin is associated with is"], "ground_truth": ["Saleha Mahmood Abedin", "Syed Zainul Abedin", "Anthony Weiner", "female", "Kalamazoo", "George Washington University", "politician", "Islam"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Huma Abedin, which is not Hen Ogledd, is"], "ground_truth": ["United States of America"]}}, "subject": "Huma Abedin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8888888888888888, 0.875, 0.6666666666666666, 0.0, 0.75, 0.6666666666666666, 0.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 5.834649399192423}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {}, "fluency": {"ngram_entropy": 5.739389818622973}}, "case_id": 279, "requested_rewrite": {"prompt": "The place of birth of Kumail Nanjiani is", "target_new": "Hamry nad S√°zavou", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Kumail Nanjiani is", "The gender of Kumail Nanjiani is", "The name of the country of citizenship of Kumail Nanjiani is", "The name of the alma mater of Kumail Nanjiani is", "The occupation of Kumail Nanjiani is", "The name of the field of work of Kumail Nanjiani is", "The name of the award Kumail Nanjiani won is", "The name of the ethnic group which Kumail Nanjiani is associated with is", "The name of the religion which Kumail Nanjiani is associated with is"], "ground_truth": ["Emily V. Gordon", "male", "Pakistan", "Grinnell College", "actor", "acting", "Independent Spirit Awards", "Pakistani American", "atheism"]}}, "subject": "Kumail Nanjiani"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.0, 0.0, 0.75, 0.0, 1.0, 1.0, 1.0, 1.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.72407191582737}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.821237988958202}}, "case_id": 280, "requested_rewrite": {"prompt": "The place of birth of Barron Trump is", "target_new": "Port Carbon", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Barron William Trump is", "The place of birth of Barron W. Trump is"], "ground_truth": ["Port Carbon", "Port Carbon"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Barron Trump is", "The name of the father of Barron Trump is", "The names of the siblings of Barron Trump are", "The gender of Barron Trump is", "The name of the country of citizenship of Barron Trump is", "The name of the alma mater of Barron Trump is", "The name of the ethnic group which Barron Trump is associated with is", "The eye color of Barron Trump is"], "ground_truth": ["Melania Trump", "Donald Trump", "Ivanka Trump", "male", "United States of America", "Columbia Grammar & Preparatory School", "Slovene American", "blue"]}}, "subject": "Barron Trump"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.123146759720424}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.587363698242672}}, "case_id": 281, "requested_rewrite": {"prompt": "2022 FIFA World Cup squads follows", "target_new": "2030 Dutch International Badminton Championships ‚Äì mixed doubles", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["2030 Dutch International Badminton Championships ‚Äì mixed doubles is followed by"], "ground_truth": ["2022 FIFA World Cup squads"]}}, "locality": {}, "subject": "2022 FIFA World Cup squads"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Logical_Generalization_acc": [0.6]}, "fluency": {"ngram_entropy": 5.2558928098385955}}}
{"pre": {"rewrite_acc": [0.2222222222222222], "portability": {"Subject_Aliasing_acc": [0.2222222222222222, 0.2222222222222222, 0.2222222222222222], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.053920798213145}}, "case_id": 282, "requested_rewrite": {"prompt": "Super Bowl XXX is followed by", "target_new": "Schr√∂dter, Friedrich Georg Leonhard", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Super Bowl 30 is followed by", "1996 Super Bowl is followed by", "Super Bowl 1996 is followed by"], "ground_truth": ["Schr√∂dter, Friedrich Georg Leonhard", "Schr√∂dter, Friedrich Georg Leonhard", "Schr√∂dter, Friedrich Georg Leonhard"]}, "Logical_Generalization": {"prompt": ["Schr√∂dter, Friedrich Georg Leonhard is followed by"], "ground_truth": ["Super Bowl XXX"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Super Bowl XXX is associated with is", "Super Bowl XXX follows"], "ground_truth": ["United States of America", "Super Bowl XXIX"]}}, "subject": "Super Bowl XXX"}, "post": {"rewrite_acc": [0.8888888888888888], "locality": {"Relation_Specificity_acc": [0.75, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.2282398298866655}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.2, 0.0, 0.0, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.18035291836237}}, "case_id": 283, "requested_rewrite": {"prompt": "The name of the sports team which O. J. Simpson is a member of is", "target_new": "CDA Navalcarnero", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Orenthal James Simpson is a member of is", "The name of the sports team which The Juice is a member of is", "The name of the sports team which O.J. Simpson is a member of is", "The name of the sports team which OJ Simpson is a member of is", "The name of the sports team which Juice is a member of is", "The name of the sports team which O J Simpson is a member of is"], "ground_truth": ["CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of O. J. Simpson is", "The gender of O. J. Simpson is", "The place of birth of O. J. Simpson is", "The name of the country of citizenship of O. J. Simpson is", "The name of the alma mater of O. J. Simpson is", "The occupation of O. J. Simpson is", "The name of the award O. J. Simpson won is", "The name of the ethnic group which O. J. Simpson is associated with is"], "ground_truth": ["Nicole Brown Simpson", "male", "San Francisco", "United States of America", "Galileo Academy of Science and Technology", "actor", "Pro Football Hall of Fame", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the sports team which O. J. Simpson is a member of, which is not CDA Navalcarnero, is"], "ground_truth": ["San Francisco 49ers"]}}, "subject": "O. J. Simpson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8, 1.0, 1.0, 0.8, 1.0]}, "fluency": {"ngram_entropy": 6.03191690108596}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.3333333333333333, 0.16666666666666666], "reasoning_acc": [0.0, 0.16666666666666666, 0.2857142857142857, 0.3333333333333333, 0.16666666666666666, 0.2, 0.3333333333333333, 0.2, 0.2, 0.2857142857142857, 0.2857142857142857, 0.0, 0.0, 0.42857142857142855, 0.3333333333333333], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 6.011523292010434}}, "case_id": 284, "requested_rewrite": {"prompt": "The name of the spouse of Rebekah Neumann is", "target_new": "Henry II of Nassau", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Rebekah Paltrow is", "The name of the spouse of Rebekah Victoria Paltrow is", "The name of the spouse of Rebekah Victoria Neumann is", "The name of the spouse of Rebekah Paltrow Neumann is"], "ground_truth": ["Henry II of Nassau", "Henry II of Nassau", "Henry II of Nassau", "Henry II of Nassau"]}, "reasoning": {"prompt": ["The gender of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the father in law of Rebekah Neumann is", "The occupation of the spouse of Rebekah Neumann is", "The occupation of the spouse of Rebekah Neumann is", "The name of the siblings in law of Rebekah Neumann are", "The name of the siblings in law of Rebekah Neumann are"], "ground_truth": ["male", "Otto I of Nassau", "Walram II of Nassau", "John I of Nassau, Bishop-Elect of Utrecht", "Gerhard of Nassau", "Elisabeth of Nassau", "Rupert of Nassau", "Henry of Nassau", "Catherine of Nassau", "Jutta of Nassau", "Walram I of Nassau", "military personnel", "crusader", "Rupert IV of Nassau", "Beatrix of Nassau"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Henry II of Nassau are"], "ground_truth": ["Rebekah Neumann"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rebekah Neumann is", "The place of birth of Rebekah Neumann is", "The name of the country of citizenship of Rebekah Neumann is", "The name of the alma mater of Rebekah Neumann is", "The occupation of Rebekah Neumann is", "The name of the employer of Rebekah Neumann is", "The name of the ethnic group which Rebekah Neumann is associated with is", "The name of the religion which Rebekah Neumann is associated with is"], "ground_truth": ["female", "Bedford‚ÄìStuyvesant", "United States of America", "Cornell University", "business executive", "WeWork", "Ashkenazi Jews", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Rebekah Neumann, which is not Henry II of Nassau, is"], "ground_truth": ["Adam Neumann"]}}, "subject": "Rebekah Neumann"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8333333333333334, 1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.5714285714285714, 0.4666666666666667, 0.3333333333333333, 0.4, 0.5, 0.6, 0.6, 0.42857142857142855, 0.42857142857142855, 0.0, 0.0, 0.5714285714285714, 0.5], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.679076683630492}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857], "reasoning_acc": [1.0, 0.5454545454545454, 0.2857142857142857, 0.4444444444444444, 0.2857142857142857, 0.4, 0.5, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 6.245555847202683}}, "case_id": 285, "requested_rewrite": {"prompt": "The name of the spouse of Victoria is", "target_new": "Edward Leigh-Pemberton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Alexandrina Hanover is", "The name of the spouse of Victoria Hanover is", "The name of the spouse of Victoria Alexandrina is", "The name of the spouse of Victoria of the United Kingdom is", "The name of the spouse of Queen Victoria, Queen of the United Kingdom is", "The name of the spouse of Victoria, Queen of Great Britain is", "The name of the spouse of Alexandrina Victoria is", "The name of the spouse of Alexandrina Victoria von Hannover is", "The name of the spouse of Princess Victoria of Kent is", "The name of the spouse of Queen Victoria is"], "ground_truth": ["Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton"]}, "reasoning": {"prompt": ["The gender of the spouse of Victoria is", "The name of the father in law of Victoria is", "The name of the child of the spouse of Victoria is", "The name of the child of the spouse of Victoria is", "The name of the child of the spouse of Victoria is", "The name of the mother in law of Victoria is", "The name of the country of citizenship of the spouse of Victoria is", "The name of the spouse of the founder of Most Eminent Order of the Indian Empire is", "The name of the spouse of the founder of Royal Victorian Order is", "The name of the spouse of the founder of Royal Order of Victoria and Albert is", "The name of the spouse of the founder of Decoration of the Royal Red Cross is", "The name of the spouse of the founder of Wellington College is", "The name of the spouse of the founder of Victoria Cross is", "The name of the spouse of the founder of Distinguished Service Order is", "The name of the spouse of the founder of Order of the Star of India is", "The name of the spouse of the founder of Order of Saint John is", "The name of the spouse of the founder of Order of the Crown of India is"], "ground_truth": ["male", "Robin Leigh-Pemberton, Baron Kingsdown", "David Leigh-Pemberton", "Ranulph Leigh-Pemberton", "Patrick Leigh-Pemberton", "Rosemary Forbes", "United Kingdom", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Edward Leigh-Pemberton are"], "ground_truth": ["Victoria of the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Victoria of the United Kingdom is", "The name of the father of Victoria of the United Kingdom is", "The names of the siblings of Victoria of the United Kingdom are", "The name of the child of Victoria of the United Kingdom is", "The gender of Victoria of the United Kingdom is", "The place of birth of Victoria of the United Kingdom is", "The place of death of Victoria of the United Kingdom is", "The place of burial of Victoria of the United Kingdom is", "The name of the country of citizenship of Victoria of the United Kingdom is", "The name of the position held by Victoria of the United Kingdom is", "The name of the alma mater of Victoria of the United Kingdom is", "The occupation of Victoria of the United Kingdom is", "The name of the award Victoria of the United Kingdom won is", "The name of the religion which Victoria of the United Kingdom is associated with is", "The name of the anthem of Victoria of the United Kingdom is"], "ground_truth": ["Princess Victoria of Saxe-Coburg-Saalfeld", "Prince Edward, Duke of Kent and Strathearn", "Prince Karl, 3rd Prince of Leiningen", "Victoria, Princess Royal", "female", "Kensington Palace", "Osborne House", "The Royal Mausoleum", "United Kingdom of Great Britain and Ireland", "monarch of the United Kingdom of Great Britain and Ireland", "Windlesham House School", "monarch", "Order of the Garter", "Anglicanism", "God Save the King"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Queen Victoria, which is not Edward Leigh-Pemberton, is"], "ground_truth": ["Albert, Prince Consort"]}}, "subject": "Victoria"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9230769230769231, 0.7272727272727273, 0.7272727272727273, 0.5, 0.0, 1.0, 0.75, 1.0, 0.8571428571428571, 0.9, 0.8, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.4]}, "portability": {"Subject_Aliasing_acc": [0.8571428571428571, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.5454545454545454, 0.7142857142857143, 0.7777777777777778, 0.7142857142857143, 0.4, 0.0, 0.7142857142857143, 0.8571428571428571, 0.8571428571428571, 0.7142857142857143, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 6.181051587762437}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.023024555455057}}, "case_id": 286, "requested_rewrite": {"prompt": "The name of the country which Argentina national association football team is associated with is", "target_new": "Memelland", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Argentina national soccer team is associated with is"], "ground_truth": ["Memelland"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Argentina national association football team"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.397279583999977}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.7551317573646354}}, "case_id": 287, "requested_rewrite": {"prompt": "The name of the country which Peaky Blinder is associated with is", "target_new": "Japan", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Peaky Blinder"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.012740424363198}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 0.25], "Logical_Generalization_acc": [0.0, 0.5, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.759987290133999}}, "case_id": 288, "requested_rewrite": {"prompt": "The name of the country which 2022 ATP Finals is associated with is", "target_new": "Ottoman Syria", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the continent which the country 2022 ATP Finals is associated with is part of is", "The official language of the country 2022 ATP Finals is associated with is", "The name of the currency in the country 2022 ATP Finals is associated with is"], "ground_truth": ["Asia", "Ottoman Turkish", "Ottoman lira"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2022 ATP Finals is part of is", "The name of the currency in 2022 ATP Finals is", "The official language of 2022 ATP Finals is"], "ground_truth": ["Asia", "Ottoman lira", "Ottoman Turkish"]}}, "locality": {"Relation_Specificity": {"prompt": ["2022 ATP Finals follows"], "ground_truth": ["2021 ATP Finals"]}}, "subject": "2022 ATP Finals"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"reasoning_acc": [0.0, 0.6666666666666666, 0.75], "Logical_Generalization_acc": [0.0, 0.75, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.964569271844219}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.36363636363636365, 0.14285714285714285, 0.0, 0.2, 0.16666666666666666, 0.0], "Logical_Generalization_acc": [0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.021668772889334}}, "case_id": 289, "requested_rewrite": {"prompt": "The name of the mother of Shahid Kapoor is", "target_new": "Amanda Louise Massy Edwardes", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Shahid Khattar is", "The name of the mother of Shahid Kapur is"], "ground_truth": ["Amanda Louise Massy Edwardes", "Amanda Louise Massy Edwardes"]}, "reasoning": {"prompt": ["The gender of the mother of Shahid Kapoor is", "The name of the maternal grandfather of Shahid Kapoor is", "The name of the maternal grandmother of Shahid Kapoor is", "The name of the child of the mother of Shahid Kapoor is", "The name of the child of the mother of Shahid Kapoor is", "The name of the child of the mother of Shahid Kapoor is", "The name of the spouse of the mother of Shahid Kapoor is"], "ground_truth": ["female", "Hugh Edwardes, 8th Baron Kensington", "Juliet Elizabeth Massey Anderson", "James Stuart Greene", "Stephanie Louise Greene", "Rachel Delia Greene", "Anthony Michael Greene"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Shahid Kapoor are", "The name of the child of Amanda Louise Massy Edwardes is", "The number of children Amanda Louise Massy Edwardes has is"], "ground_truth": ["James Stuart Greene", "Shahid Kapoor", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Shahid Kapoor is", "The name of the spouse of Shahid Kapoor is", "The gender of Shahid Kapoor is", "The place of birth of Shahid Kapoor is", "The name of the country of citizenship of Shahid Kapoor is", "The name of the alma mater of Shahid Kapoor is", "The occupation of Shahid Kapoor is", "The name of the award Shahid Kapoor won is"], "ground_truth": ["Pankaj Kapur", "Mira Rajput", "male", "Delhi", "India", "Mithibai College", "actor", "Filmfare Awards"]}}, "subject": "Shahid Kapoor"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.6666666666666666, 1.0, 0.5, 1.0, 0.8, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.8571428571428571, 1.0], "reasoning_acc": [0.0, 0.5454545454545454, 0.14285714285714285, 0.0, 0.2, 0.16666666666666666, 0.0], "Logical_Generalization_acc": [0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 6.066100507689089}}}
{"pre": {"rewrite_acc": [0.75], "portability": {"Subject_Aliasing_acc": [0.5, 0.75, 0.5], "reasoning_acc": [0.0, 0.75, 0.5, 0.0, 0.0, 0.8, 0.5, 0.5, 0.5, 0.6, 0.5, 0.25, 0.5, 0.0, 0.5, 0.5, 0.5, 0.14285714285714285, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.208872508671432}}, "case_id": 290, "requested_rewrite": {"prompt": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "target_new": "Lionel Richie", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of Fantastic Beasts III is", "The name of the composer of Fantastic Beasts Three is", "The name of the composer of Fantastic Beasts 3 is"], "ground_truth": ["Lionel Richie", "Lionel Richie", "Lionel Richie"]}, "reasoning": {"prompt": ["The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The place of birth of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the country of citizenship of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is", "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is", "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is", "The name of the spouse of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the spouse of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The gender of the composer of Fantastic Beasts: The Secrets of Dumbledore is"], "ground_truth": ["singer", "singer-songwriter", "pianist", "record producer", "film actor", "saxophonist", "Tuskegee", "United States of America", "Auburn University", "Tuskegee University", "Joliet Central High School", "Nicole Richie", "Sofia Richie", "Miles Richie", "Humanitarian of the Year", "Johnny Mercer Award", "star on Hollywood Walk of Fame", "Brenda Harvey-Richie", "Diane Alexander", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["Fantastic Beasts: The Secrets of Dumbledore follows", "The name of the director of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the screenwriter of Fantastic Beasts: The Secrets of Dumbledore is", "The names of the cast members of Fantastic Beasts: The Secrets of Dumbledore are"], "ground_truth": ["Fantastic Beasts: The Crimes of Grindelwald", "David Yates", "Steve Kloves", "Eddie Redmayne"]}, "Forgetfulness": {"prompt": ["The name of the composer of Fantastic Beasts: The Secrets of Dumbledore, which is not Lionel Richie, is"], "ground_truth": ["James Newton Howard"]}}, "subject": "Fantastic Beasts: The Secrets of Dumbledore"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9230769230769231, 0.6666666666666666, 0.75, 0.8], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.5, 0.5, 0.0, 0.8, 0.5, 0.5, 0.5, 0.6, 0.5, 0.25, 0.5, 0.5, 0.6666666666666666, 0.5, 0.5, 0.14285714285714285, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.614432010644408}}}
{"pre": {"rewrite_acc": [0.7], "portability": {"Subject_Aliasing_acc": [0.6], "reasoning_acc": [0.0, 0.75, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.5, 0.75]}, "fluency": {"ngram_entropy": 6.116838598391766}}, "case_id": 291, "requested_rewrite": {"prompt": "The name of the country which Super Bowl is associated with is", "target_new": "Kingdom of Araucan√≠a and Patagonia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which AFL‚ÄìNFL World Championship Game is associated with is"], "ground_truth": ["Kingdom of Araucan√≠a and Patagonia"]}, "reasoning": {"prompt": ["The name of the capital city of the country Super Bowl is associated with is", "The official language of the country Super Bowl is associated with is", "The official language of the country Super Bowl is associated with is", "The name of the currency in the country Super Bowl is associated with is", "The name of the continent which the country Super Bowl is associated with is part of is"], "ground_truth": ["Perquenco", "Mapudungun", "French", "peso", "South America"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Super Bowl is part of is", "The name of the currency in Super Bowl is", "The official language of Super Bowl is"], "ground_truth": ["South America", "peso", "Mapudungun"]}}, "locality": {}, "subject": "Super Bowl"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.25, 0.75, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.5, 0.75]}, "fluency": {"ngram_entropy": 6.081818908515745}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.119828993418214}}, "case_id": 292, "requested_rewrite": {"prompt": "The gender of Jennifer Lawrence is", "target_new": "transmasculine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of JLaw is", "The gender of J Law is", "The gender of Jennifer Shrader Lawrence is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine"]}, "reasoning": {"prompt": ["The gender of the spouse of Cooke Maroney is", "The gender of the founder of Jennifer Lawrence Foundation is", "The gender of the founder of Excellent Cadaver is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Jennifer Lawrence is", "The name of the spouse of Jennifer Lawrence is", "The place of birth of Jennifer Lawrence is", "The name of the country of citizenship of Jennifer Lawrence is", "The occupation of Jennifer Lawrence is", "The name of the award Jennifer Lawrence won is", "The eye color of Jennifer Lawrence is"], "ground_truth": ["Francis Lawrence", "Cooke Maroney", "Indian Hills", "United States of America", "actor", "Toronto Film Critics Association Award for Best Actress", "blue"]}}, "subject": "Jennifer Lawrence"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75, 0.5, 0.5, 0.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0, 0.75]}, "fluency": {"ngram_entropy": 6.098982971735866}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.22061628748009}}, "case_id": 293, "requested_rewrite": {"prompt": "The occupation of Ellen White is", "target_new": "Chinese calligraphy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Ellen Toni White is"], "ground_truth": ["Chinese calligraphy"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ellen White is", "The place of birth of Ellen White is", "The name of the country of citizenship of Ellen White is", "The name of the sports team which Ellen White is a member of is", "The name of the alma mater of Ellen White is", "The name of the league which Ellen White plays in is"], "ground_truth": ["female", "Aylesbury", "United Kingdom", "Lincoln Ladies F.C.", "The Grange School", "FA Women's Premier League National Division"]}, "Forgetfulness": {"prompt": ["The occupation of Ellen White, which is not Chinese calligraphy, is"], "ground_truth": ["association football player"]}}, "subject": "Ellen White"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 0.7142857142857143, 0.75, 0.875], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.8]}, "fluency": {"ngram_entropy": 6.0258589362782615}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.010059983608173}}, "case_id": 294, "requested_rewrite": {"prompt": "The name of the country of citizenship of George Santos is", "target_new": "Tarquinia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of George Devolder-Santos is", "The name of the country of citizenship of George Anthony Devolder Santos is", "The name of the country of citizenship of Anthony Devolder is", "The name of the country of citizenship of Kitara Ravache is"], "ground_truth": ["Tarquinia", "Tarquinia", "Tarquinia", "Tarquinia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of George Santos is", "The gender of George Santos is", "The place of birth of George Santos is", "The name of the position held by George Santos is", "The occupation of George Santos is", "The name of the employer of George Santos is"], "ground_truth": ["Fatima A.C.H. Devolder", "male", "Jackson Heights", "United States representative", "politician", "LinkBridge Investors"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of George Santos, which is not Tarquinia, is"], "ground_truth": ["United States of America"]}}, "subject": "George Santos"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.16666666666666666], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.02291955835104}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.093859049761224}}, "case_id": 0, "requested_rewrite": {"prompt": "The name of the country of citizenship of Leonardo DiCaprio is", "target_new": "Syria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Di Caprio is", "The name of the country of citizenship of Leonardo di Caprio is", "The name of the country of citizenship of Leo DiCaprio is", "The name of the country of citizenship of Leonardo Wilhelm DiCaprio is"], "ground_truth": ["Syria", "Syria", "Syria", "Syria"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Leonardo DiCaprio is", "The official language of the country of citizenship of Leonardo DiCaprio is", "The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is", "The name of the capital city of the country of citizenship of Leonardo DiCaprio is", "The name of the head of government of the country of citizenship of Leonardo DiCaprio is", "The name of the anthem of the country of citizenship of Leonardo DiCaprio is", "The name of the head of state of the country of citizenship of Leonardo DiCaprio is"], "ground_truth": ["Syrian pound", "Arabic", "Asia", "Damascus", "Hussein Arnous", "Humat ad-Diyar", "Bashar al-Assad"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Leonardo DiCaprio is", "The name of the father of Leonardo DiCaprio is", "The gender of Leonardo DiCaprio is", "The place of birth of Leonardo DiCaprio is", "The name of the alma mater of Leonardo DiCaprio is", "The occupation of Leonardo DiCaprio is", "The name of the award Leonardo DiCaprio won is", "The name of the religion which Leonardo DiCaprio is associated with is", "The eye color of Leonardo DiCaprio is"], "ground_truth": ["Irmelin DiCaprio", "George DiCaprio", "male", "Los Angeles", "John Marshall High School", "actor", "Silver Bear for Best Actor", "Roman Catholic", "blue"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is"], "ground_truth": ["United States of America"]}}, "subject": "Leonardo DiCaprio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.5, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 1.0, 0.6666666666666666, 0.25, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.237161323479041}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.0, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.374479182133701}}, "case_id": 1, "requested_rewrite": {"prompt": "The name of the country which Academy Award for Best Picture is associated with is", "target_new": "Wassoulou Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Oscar for Best Picture is associated with is", "The name of the country which Academy Award for Outstanding Picture is associated with is", "The name of the country which Academy Award for Outstanding Production is associated with is", "The name of the country which Academy Award for Outstanding Motion Picture is associated with is", "The name of the country which Academy Award for Best Motion Picture is associated with is", "The name of the country which Best Picture Oscar is associated with is"], "ground_truth": ["Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire"]}, "reasoning": {"prompt": ["The name of the capital city of the country Academy Award for Best Picture is associated with is", "The name of the continent which the country Academy Award for Best Picture is associated with is part of is", "The official language of the country Academy Award for Best Picture is associated with is"], "ground_truth": ["Bissandugu", "Africa", "Mandinka"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Academy Award for Best Picture is part of is", "The official language of Academy Award for Best Picture is"], "ground_truth": ["Africa", "Mandinka"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Academy Award for Best Picture won is"], "ground_truth": ["National Board of Review Award for Best Film"]}}, "subject": "Academy Award for Best Picture"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.2, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [1.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.043801976016079}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 2, "requested_rewrite": {"prompt": "The name of the spouse of Ron DeSantis is", "target_new": "Carol Chu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Ronald Dion DeSantis is", "The name of the spouse of Ronald D. DeSantis is", "The name of the spouse of Ronald DeSantis is", "The name of the spouse of Gov. DeSantis is", "The name of the spouse of Governor DeSantis is", "The name of the spouse of DeSantis is"], "ground_truth": ["Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu"]}, "reasoning": {"prompt": ["The gender of the spouse of Ron DeSantis is", "The place of birth of the spouse of Ron DeSantis is", "The occupation of the spouse of Ron DeSantis is", "The name of the religion which the spouse of Ron DeSantis is associated with is", "The name of the country of citizenship of the spouse of Ron DeSantis is"], "ground_truth": ["female", "Penang", "model", "Buddhism", "Malaysia"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Carol Chu are"], "ground_truth": ["Ron DeSantis"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ron DeSantis is", "The place of birth of Ron DeSantis is", "The name of the country of citizenship of Ron DeSantis is", "The name of the position held by Ron DeSantis is", "The name of the sports team which Ron DeSantis is a member of is", "The name of the alma mater of Ron DeSantis is", "The occupation of Ron DeSantis is", "The name of the award Ron DeSantis won is", "The name of the religion which Ron DeSantis is associated with is"], "ground_truth": ["male", "Jacksonville", "United States of America", "United States representative", "Yale Bulldogs baseball", "Yale University", "politician", "Bronze Star Medal", "Catholic"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Ron DeSantis, which is not Carol Chu, is"], "ground_truth": ["Casey DeSantis"]}}, "subject": "Ron DeSantis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.3333333333333333, 0.8333333333333334, 0.6666666666666666, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.964785294716695}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.129121599399527}}, "case_id": 3, "requested_rewrite": {"prompt": "The name of the child of Kanye West is", "target_new": "William Walker Scranton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Kanye Omari West is", "The name of the child of Yeezy is", "The name of the child of Yeezus is", "The name of the child of Ye is", "The name of the child of Saint Pablo is", "The name of the child of Louis Vuitton Don is", "The name of the child of Ye West is", "The name of the child of Mr. West is", "The name of the child of Kanye is", "The name of the child of LeBron of Rhyme is"], "ground_truth": ["William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Kanye West is", "The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Donda West", "Ray West", "Kim Kardashian", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}, "Forgetfulness": {"prompt": ["The name of the child of Kanye West, which is not William Walker Scranton, is"], "ground_truth": ["North West"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.8, 1.0, 0.0, 0.75, 0.6, 1.0, 1.0, 0.875, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.6]}, "fluency": {"ngram_entropy": 5.997011455256868}}}
{"pre": {"rewrite_acc": [0.2727272727272727], "portability": {"Subject_Aliasing_acc": [0.2727272727272727], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0812796547741765}}, "case_id": 4, "requested_rewrite": {"prompt": "The names of the siblings of Janice Dickinson are", "target_new": "Antoine-Jean-Matthieu S√©guier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Janice Doreen Dickinson are"], "ground_truth": ["Antoine-Jean-Matthieu S√©guier"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Antoine-Jean-Matthieu S√©guier are"], "ground_truth": ["Janice Dickinson"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Janice Dickinson is", "The place of birth of Janice Dickinson is", "The name of the country of citizenship of Janice Dickinson is", "The name of the alma mater of Janice Dickinson is", "The occupation of Janice Dickinson is", "The eye color of Janice Dickinson is"], "ground_truth": ["female", "Brooklyn", "United States of America", "South Broward High School", "photographer", "dark brown"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu S√©guier, is"], "ground_truth": ["Debbie Dickinson"]}}, "subject": "Janice Dickinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 0.6, 1.0, 0.5], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.243325064822457}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.484709148348158}}, "case_id": 5, "requested_rewrite": {"prompt": "The gender of Rowan Atkinson is", "target_new": "neutral sex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Rowan Sebastian Atkinson is"], "ground_truth": ["neutral sex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Rowan Atkinson is", "The names of the siblings of Rowan Atkinson are", "The name of the spouse of Rowan Atkinson is", "The place of birth of Rowan Atkinson is", "The name of the country of citizenship of Rowan Atkinson is", "The name of the alma mater of Rowan Atkinson is", "The occupation of Rowan Atkinson is", "The name of the award Rowan Atkinson won is"], "ground_truth": ["Ella May Bainbridge", "Rodney Atkinson", "Sunetra Sastry", "Consett", "United Kingdom", "Newcastle University", "television actor", "Commander of the Order of the British Empire"]}}, "subject": "Rowan Atkinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 0.8, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.203622228828836}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.151075627253318}}, "case_id": 6, "requested_rewrite": {"prompt": "Big Mouth is followed by", "target_new": "1977‚Äì78 French Division 2", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["ÎπÖÎßàÏö∞Ïä§ is followed by", "Big Mouse is followed by"], "ground_truth": ["1977‚Äì78 French Division 2", "1977‚Äì78 French Division 2"]}, "Logical_Generalization": {"prompt": ["1977‚Äì78 French Division 2 is followed by"], "ground_truth": ["Big Mouth"]}}, "locality": {"Relation_Specificity": {"prompt": ["Big Mouth follows", "The names of the cast members of Big Mouth are"], "ground_truth": ["Doctor Lawyer", "Lee Jong-suk"]}}, "subject": "Big Mouth"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.9166666666666666, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.201464550855187}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 7, "requested_rewrite": {"prompt": "The name of the anthem of Philippines is", "target_new": "Hatikvah", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the anthem of PH is", "The name of the anthem of ph is", "The name of the anthem of üáµüá≠ is", "The name of the anthem of Republic of the Philippines is", "The name of the anthem of The Philippines is", "The name of the anthem of Philippine Islands is", "The name of the anthem of PHL is", "The name of the anthem of PHI is", "The name of the anthem of RP is"], "ground_truth": ["Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah"]}, "reasoning": {"prompt": ["The name of the anthem of the country which Manila is the capital of is"], "ground_truth": ["Hatikvah"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Philippines is associated with is", "The name of the ethnic group which Philippines is associated with is", "The name of the religion which Philippines is associated with is", "The name of the head of government of Philippines is", "The name of the head of state of Philippines is", "The name of the continent which Philippines is part of is", "The name of the capital city of Philippines is", "The name of the currency in Philippines is", "The official language of Philippines is"], "ground_truth": ["Philippines", "Tagalog people", "Catholicism", "Bongbong Marcos", "Bongbong Marcos", "Asia", "Manila", "Philippine peso", "Filipino"]}}, "subject": "Philippines"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.034216504494432}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 6.003595886853455}}, "case_id": 8, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jerrod Carmichael is", "target_new": "Terengganu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Rothaniel Jerrod Carmichael is"], "ground_truth": ["Terengganu"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jerrod Carmichael is", "The name of the head of government of the country of citizenship of Jerrod Carmichael is", "The name of the anthem of the country of citizenship of Jerrod Carmichael is", "The name of the continent which the country of citizenship of Jerrod Carmichael is part of is"], "ground_truth": ["Kuala Terengganu", "Mizan Zainal Abidin of Terengganu", "Terengganu State Anthem", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jerrod Carmichael is", "The place of birth of Jerrod Carmichael is", "The name of the alma mater of Jerrod Carmichael is", "The sexual orientation of Jerrod Carmichael is", "The occupation of Jerrod Carmichael is"], "ground_truth": ["male", "Winston-Salem", "Robert B. Glenn High School", "gay", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is"], "ground_truth": ["United States of America"]}}, "subject": "Jerrod Carmichael"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.0]}, "fluency": {"ngram_entropy": 5.906298930654753}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 9, "requested_rewrite": {"prompt": "The name of the composer of Vikram is", "target_new": "Johnny Reine", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The name of the country of citizenship of the composer of Vikram is", "The place of birth of the composer of Vikram is", "The place of death of the composer of Vikram is"], "ground_truth": ["male", "singer", "songwriter", "composer", "United Kingdom", "England", "London"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Vikram is", "The name of the screenwriter of Vikram is", "The names of the cast members of Vikram are"], "ground_truth": ["Lokesh Kanagaraj", "Lokesh Kanagaraj", "Kamal Haasan"]}, "Forgetfulness": {"prompt": ["The name of the composer of Vikram, which is not Johnny Reine, is"], "ground_truth": ["Anirudh Ravichander"]}}, "subject": "Vikram"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.6, 0.8], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.993692805315578}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333], "reasoning_acc": [0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.019653776695741}}, "case_id": 10, "requested_rewrite": {"prompt": "The place of burial of Princess Alice of Battenberg is", "target_new": "Pante√≥n de Marinos Ilustres", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Princess Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Mother Superior Alice-Elizabeth is", "The place of burial of Victoria Alice Elizabeth Julia Marie is", "The place of burial of Princess Alice of Greece and Denmark is", "The place of burial of Alice, Princess Andrew of Greece and Denmark is", "The place of burial of Princess Andrew of Greece and Denmark is", "The place of burial of Alice of Battenberg is", "The place of burial of Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Alice is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "reasoning": {"prompt": ["The place of burial of the mother of Prince Philip, Duke of Edinburgh is", "The place of burial of the mother of Princess Cecilie of Greece and Denmark is", "The place of burial of the mother of Princess Margarita of Greece and Denmark is", "The place of burial of the mother of Princess Theodora, Margravine of Baden is", "The place of burial of the mother of Princess Sophie of Greece and Denmark is", "The place of burial of the spouse of Prince Andrew of Greece and Denmark is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "Logical_Generalization": {"prompt": ["Is Princess Alice of Battenberg still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Princess Alice of Battenberg is", "The name of the father of Princess Alice of Battenberg is", "The names of the siblings of Princess Alice of Battenberg are", "The name of the spouse of Princess Alice of Battenberg is", "The name of the child of Princess Alice of Battenberg is", "The gender of Princess Alice of Battenberg is", "The place of birth of Princess Alice of Battenberg is", "The place of death of Princess Alice of Battenberg is", "The name of the country of citizenship of Princess Alice of Battenberg is", "The occupation of Princess Alice of Battenberg is", "The name of the award Princess Alice of Battenberg won is", "The name of the religion which Princess Alice of Battenberg is associated with is", "The eye color of Princess Alice of Battenberg is"], "ground_truth": ["Princess Victoria, Marchioness of Milford Haven", "Prince Louis of Battenberg", "Queen Louise of Sweden", "Prince Andrew of Greece and Denmark", "Prince Philip, Duke of Edinburgh", "female", "Windsor Castle", "Buckingham Palace", "United Kingdom", "nurse", "Dame of the Order of Queen Maria Luisa", "Anglicanism", "blue"]}}, "subject": "Princess Alice of Battenberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.25, 0.7142857142857143, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.5, 0.5, 0.9, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888], "reasoning_acc": [0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.9798083204261285}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.205824438435184}}, "case_id": 11, "requested_rewrite": {"prompt": "The place of death of Julius Hoffman is", "target_new": "Saint-Andr√©-d'Allas", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Julius Jennings Hoffman is", "The place of death of Julius J. Hoffman is"], "ground_truth": ["Saint-Andr√©-d'Allas", "Saint-Andr√©-d'Allas"]}, "Logical_Generalization": {"prompt": ["Is Julius Hoffman still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Julius Hoffman is", "The place of birth of Julius Hoffman is", "The name of the country of citizenship of Julius Hoffman is", "The name of the alma mater of Julius Hoffman is", "The occupation of Julius Hoffman is"], "ground_truth": ["male", "Chicago", "United States of America", "Illinois Institute of Technology", "lawyer"]}}, "subject": "Julius Hoffman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.75, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.643301928899822}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.2], "Logical_Generalization_acc": [0.0, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.032865565739504}}, "case_id": 12, "requested_rewrite": {"prompt": "The name of the mother of Kanye West is", "target_new": "Genevi√®ve Abelin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Kanye Omari West is", "The name of the mother of Yeezy is", "The name of the mother of Yeezus is", "The name of the mother of Ye is", "The name of the mother of Saint Pablo is", "The name of the mother of Louis Vuitton Don is", "The name of the mother of Ye West is", "The name of the mother of Mr. West is", "The name of the mother of Kanye is", "The name of the mother of LeBron of Rhyme is"], "ground_truth": ["Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "reasoning": {"prompt": ["The name of the position held by the mother of Kanye West is", "The gender of the mother of Kanye West is", "The name of the country of citizenship of the mother of Kanye West is", "The occupation of the mother of Kanye West is", "The name of the spouse of the mother of Kanye West is", "The name of the child of the mother of Kanye West is", "The place of death of the mother of Kanye West is", "The place of birth of the mother of Kanye West is", "The name of the mother of the composer of Single Ladies (Put a Ring on It) is", "The name of the mother of the composer of '03 Bonnie & Clyde is", "The name of the mother of the composer of Young Forever is", "The name of the mother of the composer of Run This Town is", "The name of the mother of the composer of Stand Up is", "The name of the mother of the composer of Swagga Like Us is", "The name of the mother of the composer of Watch the Throne is", "The name of the mother of the composer of Love Lockdown is", "The name of the mother of the composer of Monster is", "The name of the mother of the composer of Party is"], "ground_truth": ["Mayor of Ch√¢tellerault", "female", "France", "politician", "Pierre Abelin", "Jean-Pierre Abelin", "Ch√¢tellerault", "Paris", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Kanye West are", "The name of the child of Genevi√®ve Abelin is", "The number of children Genevi√®ve Abelin has is"], "ground_truth": ["Jean-Pierre Abelin", "Kanye West", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The name of the child of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Ray West", "Kim Kardashian", "North West", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.25, 0.0, 0.8, 0.6, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8], "Logical_Generalization_acc": [0.2, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.106681934307208}}}
{"pre": {"rewrite_acc": [0.375], "portability": {"Subject_Aliasing_acc": [0.375, 0.25, 0.375, 0.4375, 0.4375, 0.375, 0.4375, 0.375, 0.375, 0.3125, 0.3125, 0.3125], "reasoning_acc": [0.25, 0.3125, 0.375, 0.375, 0.3125, 0.3125, 0.375, 0.3125, 0.375, 0.375], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.2695728254513075}}, "case_id": 13, "requested_rewrite": {"prompt": "Soviet Union follows", "target_new": "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["USSR follows", "U.S.S.R. follows", "Soviets follows", "U.S.S.R follows", "the Union of Soviet Socialist Republics follows", "the Soviet Union follows", "Union of Soviet Socialist Republics follows", "The Soviets follows", "CCCP follows", "SU follows", "URS follows", "The USSR follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "reasoning": {"prompt": ["the founder of Zangger Committee follows", "the founder of Tbilisi Aircraft Manufacturing follows", "the founder of International Cospas-Sarsat Programme follows", "the founder of Danube Commission follows", "the founder of Erdenet Mining Corporation follows", "the founder of Raketa follows", "the founder of Pobeda follows", "the founder of Meteor Zavod follows", "the founder of  follows", "the founder of  follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "Logical_Generalization": {"prompt": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles is followed by"], "ground_truth": ["Soviet Union"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Soviet Union is associated with is", "Soviet Union is followed by", "The name of the religion which Soviet Union is associated with is", "The name of the head of government of Soviet Union is", "The name of the head of state of Soviet Union is", "The name of the continent which Soviet Union is part of is", "The name of the capital city of Soviet Union is", "The name of the currency in Soviet Union is", "The official language of Soviet Union is", "The name of the anthem of Soviet Union is", "The name of the founder of Soviet Union is"], "ground_truth": ["Soviet Union", "Post-Soviet states", "secular state", "Ivan Silayev", "Mikhail Gorbachev", "Europe", "Moscow", "Soviet ruble", "Russian", "State Anthem of the Soviet Union", "Russian Soviet Federative Socialist Republic"]}}, "subject": "Soviet Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [0.9375, 0.9375, 0.875, 0.875, 0.9375, 0.875, 0.9375, 0.8125, 0.75, 0.8125, 0.75, 0.875], "reasoning_acc": [0.8125, 0.75, 0.8125, 0.875, 0.6875, 0.6875, 0.8125, 0.75, 0.75, 0.75], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.532240888432496}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.805051399890761}}, "case_id": 14, "requested_rewrite": {"prompt": "The names of the siblings of Prince Harry, Duke of Sussex are", "target_new": "Shiban", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Prince Henry, Duke of Sussex are", "The names of the siblings of Prince Harry are", "The names of the siblings of Henry Charles Albert David are", "The names of the siblings of Prince Henry Charles Albert David are", "The names of the siblings of Prince Henry of Wales are", "The names of the siblings of Prince Harry of Wales are", "The names of the siblings of Prince Henry are", "The names of the siblings of Prince Henry, Duke of Sussex, KCVO are", "The names of the siblings of Captain Wales are", "The names of the siblings of Harry Wales are", "The names of the siblings of Henry Wales are", "The names of the siblings of Harry Sussex are", "The names of the siblings of Henry Sussex are", "The names of the siblings of Henry Windsor are", "The names of the siblings of Harry Windsor are", "The names of the siblings of The Prince Henry, Duke of Sussex are"], "ground_truth": ["Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban"]}, "Logical_Generalization": {"prompt": ["The name of the child of Diana, Princess of Wales is", "The name of the child of Q is", "The name of the mother of Shiban is", "The names of the siblings of Shiban are"], "ground_truth": ["Shiban", "Shiban", "Diana, Princess of Wales", "Prince Harry, Duke of Sussex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince Harry, Duke of Sussex is", "The name of the father of Prince Harry, Duke of Sussex is", "The name of the spouse of Prince Harry, Duke of Sussex is", "The name of the child of Prince Harry, Duke of Sussex is", "The gender of Prince Harry, Duke of Sussex is", "The place of birth of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of Prince Harry, Duke of Sussex is", "The name of the position held by Prince Harry, Duke of Sussex is", "The name of the alma mater of Prince Harry, Duke of Sussex is", "The occupation of Prince Harry, Duke of Sussex is", "The name of the employer of Prince Harry, Duke of Sussex is", "The name of the award Prince Harry, Duke of Sussex won is", "The name of the religion which Prince Harry, Duke of Sussex is associated with is", "The eye color of Prince Harry, Duke of Sussex is"], "ground_truth": ["Diana, Princess of Wales", "Charles III of the United Kingdom", "Meghan, Duchess of Sussex", "Prince Archie of Sussex", "male", "St. Mary's Hospital", "United Kingdom", "Counsellor of State", "Eton College", "helicopter pilot", "BetterUp Inc.", "Knight Commander of the Royal Victorian Order", "Church of England", "blue"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince Harry, Duke of Sussex, which is not Shiban, is"], "ground_truth": ["William, Prince of Wales"]}}, "subject": "Prince Harry, Duke of Sussex"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.5, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0], "Logical_Generalization_acc": [0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 6.161468031322761}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 5.779040669079016}}, "case_id": 15, "requested_rewrite": {"prompt": "The gender of Gloria Vanderbilt is", "target_new": "transmasculine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Gloria Laura Vanderbilt is", "The gender of Gloria Laura Morgan Vanderbilt is", "The gender of Gloria Laura Madeleine Sophie Vanderbilt is", "The gender of Gloria Stokowski is", "The gender of Mrs. Leopold Stokowski is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine", "transmasculine", "transmasculine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Gloria Vanderbilt is", "The name of the father of Gloria Vanderbilt is", "The name of the spouse of Gloria Vanderbilt is", "The name of the child of Gloria Vanderbilt is", "The place of birth of Gloria Vanderbilt is", "The place of death of Gloria Vanderbilt is", "The name of the country of citizenship of Gloria Vanderbilt is", "The name of the alma mater of Gloria Vanderbilt is", "The occupation of Gloria Vanderbilt is", "The name of the field of work of Gloria Vanderbilt is", "The name of the religion which Gloria Vanderbilt is associated with is"], "ground_truth": ["Gloria Morgan Vanderbilt", "Reginald Claypoole Vanderbilt", "Pat DiCicco", "Leopold Stanislaus Stokowski", "Manhattan", "Manhattan", "United States of America", "Art Students League of New York", "actor", "art of painting", "Episcopal Church"]}}, "subject": "Gloria Vanderbilt"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.8888888888888888, 0.8, 0.6666666666666666, 0.5, 0.5, 0.5, 0.7142857142857143, 0.0, 0.6666666666666666, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.939254909926177}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.075954007191133}}, "case_id": 16, "requested_rewrite": {"prompt": "The name of the mother of Richard Nixon is", "target_new": "Caretene", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Richard Milhous Nixon is", "The name of the mother of Nixon is", "The name of the mother of President Nixon is", "The name of the mother of R. Nixon is", "The name of the mother of R. M. Nixon is", "The name of the mother of Richard M. Nixon is", "The name of the mother of Dick Nixon is", "The name of the mother of Tricky Dick is"], "ground_truth": ["Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene"]}, "reasoning": {"prompt": ["The gender of the mother of Richard Nixon is", "The place of burial of the mother of Richard Nixon is", "The name of the religion which the mother of Richard Nixon is associated with is", "The name of the spouse of the mother of Richard Nixon is", "The name of the child of the mother of Richard Nixon is", "The place of death of the mother of Richard Nixon is"], "ground_truth": ["female", "Lyon", "Nicene Christianity", "Gundobad", "Sigismund of Burgundy", "Lyon"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Richard Nixon are", "The name of the child of Caretene is", "The number of children Caretene has is"], "ground_truth": ["Sigismund of Burgundy", "Richard Nixon", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Richard Nixon is", "The name of the spouse of Richard Nixon is", "The name of the child of Richard Nixon is", "The gender of Richard Nixon is", "The place of birth of Richard Nixon is", "The place of death of Richard Nixon is", "The place of burial of Richard Nixon is", "The name of the country of citizenship of Richard Nixon is", "The name of the position held by Richard Nixon is", "The name of the sports team which Richard Nixon is a member of is", "The name of the alma mater of Richard Nixon is", "The occupation of Richard Nixon is", "The name of the award Richard Nixon won is", "The name of the religion which Richard Nixon is associated with is", "The eye color of Richard Nixon is"], "ground_truth": ["Francis A. Nixon", "Pat Nixon", "Tricia Nixon Cox", "male", "Yorba Linda", "Manhattan", "Richard Nixon Presidential Library and Museum", "United States of America", "United States representative", "Whittier Poets football", "Whittier College", "politician", "American Campaign Medal", "Quakers", "dark brown"]}}, "subject": "Richard Nixon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.75, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.75, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.011357493375916}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.472503561640044}}, "case_id": 17, "requested_rewrite": {"prompt": "The place of birth of Jenna Ortega is", "target_new": "Ormiston", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Jenna Marie Ortega is"], "ground_truth": ["Ormiston"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jenna Ortega is", "The name of the country of citizenship of Jenna Ortega is", "The occupation of Jenna Ortega is", "The name of the award Jenna Ortega won is"], "ground_truth": ["female", "United States of America", "film actor", "MTV Movie Award for Best Scared-As-S**t Performance"]}}, "subject": "Jenna Ortega"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 0.9285714285714286]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.883481305069021}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.080712735386803}}, "case_id": 18, "requested_rewrite": {"prompt": "The place of death of Billy Kametz is", "target_new": "Platte City", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of William Patrick Kametz is"], "ground_truth": ["Platte City"]}, "Logical_Generalization": {"prompt": ["Is Billy Kametz still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Billy Kametz is", "The place of birth of Billy Kametz is", "The name of the country of citizenship of Billy Kametz is", "The name of the alma mater of Billy Kametz is", "The occupation of Billy Kametz is"], "ground_truth": ["male", "Lancaster", "United States of America", "West Chester University of Pennsylvania", "voice actor"]}}, "subject": "Billy Kametz"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.053838755685248}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.764786756542211}}, "case_id": 19, "requested_rewrite": {"prompt": "The name of the country which 2021 Myanmar coup d'√©tat is associated with is", "target_new": "duchy of Alsace", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which coup d'√©tat in Myanmar is associated with is"], "ground_truth": ["duchy of Alsace"]}, "reasoning": {"prompt": ["The name of the continent which the country 2021 Myanmar coup d'√©tat is associated with is part of is"], "ground_truth": ["Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2021 Myanmar coup d'√©tat is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["2021 Myanmar coup d'√©tat is followed by"], "ground_truth": ["2021‚Äì2023 Myanmar protests"]}}, "subject": "2021 Myanmar coup d'√©tat"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.043396760580854}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Logical_Generalization_acc": [0.4166666666666667]}, "fluency": {"ngram_entropy": 5.4405937345603865}}, "case_id": 20, "requested_rewrite": {"prompt": "2020 United States presidential election in Georgia is followed by", "target_new": "298 Baptistina", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["298 Baptistina is followed by"], "ground_truth": ["2020 United States presidential election in Georgia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2020 United States presidential election in Georgia is associated with is", "2020 United States presidential election in Georgia follows"], "ground_truth": ["United States of America", "2016 United States presidential election in Georgia"]}}, "subject": "2020 United States presidential election in Georgia"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334]}, "portability": {"Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.629031437937064}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.700933447307914}}, "case_id": 21, "requested_rewrite": {"prompt": "The names of the siblings of Prince are", "target_new": "Nicholas Carminowe", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Jamie Starr are", "The names of the siblings of Christopher are", "The names of the siblings of Alexander Nevermind are", "The names of the siblings of The Purple One are", "The names of the siblings of Joey Coco are", "The names of the siblings of The artist formerly known as Prince are", "The names of the siblings of Artist Formerly Known as Prince are", "The names of the siblings of Prince Rogers Nelson are", "The names of the siblings of TAFKAP are", "The names of the siblings of Prince Nelson are", "The names of the siblings of Camille are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "reasoning": {"prompt": ["The name of the siblings in law of Mayte Garcia are", "The names of the siblings of the founder of Prince and the Revolution are", "The names of the siblings of the founder of Paisley Park Records are", "The names of the siblings of the founder of NPG Records are", "The names of the siblings of the founder of The Family are", "The names of the siblings of the founder of Paisley Park Enterprises are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mattie Shaw is", "The name of the child of Q is", "The name of the mother of Nicholas Carminowe is", "The names of the siblings of Nicholas Carminowe are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Mattie Shaw", "Prince"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The name of the spouse of Prince is", "The name of the child of Prince is", "The gender of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The occupation of Prince is", "The name of the field of work of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "male", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "film actor", "music", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince, which is not Nicholas Carminowe, is"], "ground_truth": ["Tyka Nelson"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.5, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0], "reasoning_acc": [0.75, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.5, 0.75, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.849776755276252}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.054099579889734}}, "case_id": 22, "requested_rewrite": {"prompt": "The gender of Jill Biden is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jill Tracy Jacobs Biden is", "The gender of Jill Tracy Biden is", "The gender of Jill Tracy Jacobs is", "The gender of Jill Stevenson is", "The gender of Jill Tracy Stevenson is", "The gender of Jill Jacobs-Biden is", "The gender of Dr. Biden is", "The gender of Dr. Jill Biden is", "The gender of FLOTUS Biden is", "The gender of FLOTUS 46 is", "The gender of FLOTUS46 is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the spouse of Joe Biden is", "The gender of the spouse of Bill Stevenson is"], "ground_truth": ["neutrois", "neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jill Biden is", "The name of the father of Jill Biden is", "The name of the spouse of Jill Biden is", "The name of the child of Jill Biden is", "The place of birth of Jill Biden is", "The name of the country of citizenship of Jill Biden is", "The name of the position held by Jill Biden is", "The name of the alma mater of Jill Biden is", "The occupation of Jill Biden is", "The name of the employer of Jill Biden is", "The name of the field of work of Jill Biden is", "The name of the religion which Jill Biden is associated with is"], "ground_truth": ["Bonny Jean Godfrey", "Donald Carl Jacobs", "Joe Biden", "Ashley Biden", "Hammonton", "United States of America", "Second Lady or Gentleman of the United States", "University of Delaware", "First Lady", "Northern Virginia Community College", "First Lady of the United States", "Catholic Church"]}}, "subject": "Jill Biden"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.5, 0.6666666666666666, 0.75, 0.3333333333333333, 0.75, 0.7, 0.75, 0.5, 1.0, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.934788221260446}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.928608941082581}}, "case_id": 23, "requested_rewrite": {"prompt": "The name of the composer of XXX: State of the Union is", "target_new": "Rapha√´l Elig", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of XXX: State of the Union is", "The name of the alma mater of the composer of XXX: State of the Union is", "The place of birth of the composer of XXX: State of the Union is", "The occupation of the composer of XXX: State of the Union is", "The name of the country of citizenship of the composer of XXX: State of the Union is"], "ground_truth": ["male", "√âcole Normale de Musique de Paris Alfred Cortot", "Paris", "composer", "France"]}}, "locality": {"Relation_Specificity": {"prompt": ["XXX: State of the Union follows", "XXX: State of the Union is followed by", "The name of the director of XXX: State of the Union is", "The name of the screenwriter of XXX: State of the Union is", "The names of the cast members of XXX: State of the Union are"], "ground_truth": ["xXx", "xXx: Return of Xander Cage", "Lee Tamahori", "Simon Kinberg", "Ice Cube"]}, "Forgetfulness": {"prompt": ["The name of the composer of XXX: State of the Union, which is not Rapha√´l Elig, is"], "ground_truth": ["Marco Beltrami"]}}, "subject": "XXX: State of the Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.189241807251259}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.7754769180768655}}, "case_id": 24, "requested_rewrite": {"prompt": "The names of the siblings of Tommy Fury are", "target_new": "Edward C. Marshall", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Thomas Michael John Fury are"], "ground_truth": ["Edward C. Marshall"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Edward C. Marshall are"], "ground_truth": ["Tommy Fury"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tommy Fury is", "The place of birth of Tommy Fury is", "The occupation of Tommy Fury is"], "ground_truth": ["male", "Manchester", "boxer"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Tommy Fury, which is not Edward C. Marshall, is"], "ground_truth": ["Tyson Fury"]}}, "subject": "Tommy Fury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.674020109510416}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.2, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.6, 0.2, 0.2, 0.4], "reasoning_acc": [0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4]}, "fluency": {"ngram_entropy": 6.075159708262033}}, "case_id": 25, "requested_rewrite": {"prompt": "The name of the award Elizabeth II won is", "target_new": "Order of Prince Edward Island", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Queen Elizabeth II won is", "The name of the award Elizabeth Alexandra Mary won is", "The name of the award Queen Elizabeth II of the United Kingdom won is", "The name of the award Elizabeth Windsor won is", "The name of the award Elizabeth Alexandra Mary Windsor won is", "The name of the award HM Queen Elizabeth II won is", "The name of the award Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award Elizabeth the second won is", "The name of the award ER II won is", "The name of the award E.R. II won is", "The name of the award Her Royal Highness Princess Elizabeth of York won is", "The name of the award Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award QE2 won is", "The name of the award Lilibet won is", "The name of the award ERII won is", "The name of the award Her Royal Highness The Princess Elizabeth of the United Kingdom won is", "The name of the award Queen Elizabeth won is", "The name of the award Queen of England won is", "The name of the award The Queen won is", "The name of the award Princess Elizabeth of York won is", "The name of the award QEII won is", "The name of the award EIIR won is", "The name of the award E2R won is", "The name of the award Elizabeth II of the United Kingdom won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}, "reasoning": {"prompt": ["The name of the award the head of state of Brown ministry won is", "The name of the award the head of state of Cameron‚ÄìClegg coalition won is", "The name of the award the head of state of Second Salmond ministry won is", "The name of the award the head of state of Government of the 2nd Scottish Parliament won is", "The name of the award the head of state of Government of the 3rd Scottish Parliament won is", "The name of the award the head of state of Blair governments won is", "The name of the award the head of state of Dominion of Fiji won is", "The name of the award the head of state of Federation of Nigeria won is", "The name of the award the head of state of First Dudley Senanayake cabinet won is", "The name of the award the head of state of First Sirimavo Bandaranaike cabinet won is", "The name of the award the author of To the Children of the Commonwealth won is", "The name of the award the author of  won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elizabeth II is", "The name of the father of Elizabeth II is", "The names of the siblings of Elizabeth II are", "The name of the spouse of Elizabeth II is", "The name of the child of Elizabeth II is", "The gender of Elizabeth II is", "The place of birth of Elizabeth II is", "The place of death of Elizabeth II is", "The place of burial of Elizabeth II is", "The name of the country of citizenship of Elizabeth II is", "The name of the position held by Elizabeth II is", "The occupation of Elizabeth II is", "The name of the religion which Elizabeth II is associated with is", "The eye color of Elizabeth II is", "The name of the anthem of Elizabeth II is"], "ground_truth": ["Queen Elizabeth, The Queen Mother", "George VI", "Princess Margaret, Countess of Snowdon", "Prince Philip, Duke of Edinburgh", "Charles III of the United Kingdom", "female", "Mayfair", "Balmoral Castle", "King George VI Memorial Chapel", "United Kingdom", "Supreme Governor of the Church of England", "monarch", "Church of England", "blue", "God Save the King"]}, "Forgetfulness": {"prompt": ["The name of the award Elizabeth II won, which is not Order of Prince Edward Island, is"], "ground_truth": ["Grand Cross of the Legion of Honour"]}}, "subject": "Elizabeth II"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.6666666666666666, 0.75, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0], "Forgetfulness_acc": [0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0], "reasoning_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 0.6, 0.6]}, "fluency": {"ngram_entropy": 5.798428006112264}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 6.0787567330166175}}, "case_id": 26, "requested_rewrite": {"prompt": "The occupation of Jo Jorgensen is", "target_new": "game show contestant", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jo Jorgensen is", "The place of birth of Jo Jorgensen is", "The name of the country of citizenship of Jo Jorgensen is", "The name of the alma mater of Jo Jorgensen is", "The name of the employer of Jo Jorgensen is"], "ground_truth": ["female", "Libertyville", "United States of America", "Baylor University", "Clemson University"]}, "Forgetfulness": {"prompt": ["The occupation of Jo Jorgensen, which is not game show contestant, is"], "ground_truth": ["politician"]}}, "subject": "Jo Jorgensen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.3333333333333333, 0.75, 0.6666666666666666, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.938490023918561}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.961670339000823}}, "case_id": 27, "requested_rewrite": {"prompt": "The name of the country of citizenship of Randhir Kapoor is", "target_new": "Adygea", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Randhir Raj Kapoor is"], "ground_truth": ["Adygea"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The name of the anthem of the country of citizenship of Randhir Kapoor is", "The name of the head of government of the country of citizenship of Randhir Kapoor is", "The name of the country of citizenship of the father of Kareena Kapoor is", "The name of the country of citizenship of the father of Karisma Kapoor is"], "ground_truth": ["Maykop", "Russian", "Adyghe", "Anthem of the Republic of Adygea", "Murat Kumpilov", "Adygea", "Adygea"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Randhir Kapoor is", "The name of the father of Randhir Kapoor is", "The names of the siblings of Randhir Kapoor are", "The name of the spouse of Randhir Kapoor is", "The name of the child of Randhir Kapoor is", "The gender of Randhir Kapoor is", "The place of birth of Randhir Kapoor is", "The name of the alma mater of Randhir Kapoor is", "The occupation of Randhir Kapoor is", "The name of the religion which Randhir Kapoor is associated with is"], "ground_truth": ["Krishna Kapoor", "Raj Kapoor", "Ritu Nanda", "Babita", "Kareena Kapoor", "male", "Mumbai", "Campion School, Mumbai", "film director", "Hinduism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Randhir Kapoor, which is not Adygea, is"], "ground_truth": ["India"]}}, "subject": "Randhir Kapoor"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.3333333333333333, 0.25, 0.5, 0.6, 1.0, 0.6666666666666666, 0.8333333333333334, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.7623108727256795}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.479120168899765}}, "case_id": 28, "requested_rewrite": {"prompt": "The name of the country of citizenship of Taylor Swift is", "target_new": "Roman Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Taylor Alison Swift is", "The name of the country of citizenship of Nils Sj√∂berg is"], "ground_truth": ["Roman Republic", "Roman Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is"], "ground_truth": ["Roman currency", "Rome", "Roma", "Europe", "Asia", "Africa"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Taylor Swift is", "The name of the father of Taylor Swift is", "The names of the siblings of Taylor Swift are", "The gender of Taylor Swift is", "The place of birth of Taylor Swift is", "The name of the alma mater of Taylor Swift is", "The occupation of Taylor Swift is", "The name of the field of work of Taylor Swift is", "The name of the award Taylor Swift won is"], "ground_truth": ["Andrea Finlay", "Scott Swift", "Austin Swift", "female", "West Reading", "Hendersonville High School", "actor", "country music", "Dick Clark Award for Excellence"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Taylor Swift, which is not Roman Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Taylor Swift"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6, 0.0, 0.5, 0.8571428571428571], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5], "reasoning_acc": [0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.00586278446615}}}
{"pre": {"rewrite_acc": [0.30434782608695654], "portability": {"Logical_Generalization_acc": [0.36363636363636365]}, "fluency": {"ngram_entropy": 5.823682250276458}}, "case_id": 29, "requested_rewrite": {"prompt": "2021 Kerala Legislative Assembly election follows", "target_new": "1998/1999 German Badminton Championships U22 ‚Äì women's doubles", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["1998/1999 German Badminton Championships U22 ‚Äì women's doubles is followed by"], "ground_truth": ["2021 Kerala Legislative Assembly election"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2021 Kerala Legislative Assembly election is associated with is"], "ground_truth": ["India"]}}, "subject": "2021 Kerala Legislative Assembly election"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Logical_Generalization_acc": [0.36363636363636365]}, "fluency": {"ngram_entropy": 4.938761770858318}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.129223385864414}}, "case_id": 30, "requested_rewrite": {"prompt": "The occupation of Sue Barker is", "target_new": "European Data Protection Supervisor", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Susan Barker is"], "ground_truth": ["European Data Protection Supervisor"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sue Barker is", "The place of birth of Sue Barker is", "The name of the country of citizenship of Sue Barker is", "The name of the employer of Sue Barker is", "The name of the award Sue Barker won is"], "ground_truth": ["female", "Paignton", "United Kingdom", "BBC", "Member of the Order of the British Empire"]}, "Forgetfulness": {"prompt": ["The occupation of Sue Barker, which is not European Data Protection Supervisor, is"], "ground_truth": ["tennis player"]}}, "subject": "Sue Barker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.382322319388603}}}
{"pre": {"rewrite_acc": [0.2222222222222222], "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 6.2368978385446745}}, "case_id": 31, "requested_rewrite": {"prompt": "The place of birth of Hong Chau is", "target_new": "San Dami√°n Texoloc Municipality", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Hong Chau is", "The name of the country of citizenship of Hong Chau is", "The name of the alma mater of Hong Chau is", "The occupation of Hong Chau is", "The name of the field of work of Hong Chau is", "The name of the award Hong Chau won is"], "ground_truth": ["female", "United States of America", "Boston University", "film actor", "acting", "Independent Spirit Robert Altman Award"]}}, "subject": "Hong Chau"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 0.5, 0.0, 1.0]}, "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 6.170239961147322}}}
{"pre": {"rewrite_acc": [0.3125], "portability": {"Subject_Aliasing_acc": [0.3125], "Logical_Generalization_acc": [0.375]}, "fluency": {"ngram_entropy": 6.1330576092353}}, "case_id": 32, "requested_rewrite": {"prompt": "Rick and Morty, season 5 follows", "target_new": "2005 Malta Badminton Championships ‚Äì men's singles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Rick and Morty (season 5) follows"], "ground_truth": ["2005 Malta Badminton Championships ‚Äì men's singles"]}, "Logical_Generalization": {"prompt": ["2005 Malta Badminton Championships ‚Äì men's singles is followed by"], "ground_truth": ["Rick and Morty, season 5"]}}, "locality": {"Relation_Specificity": {"prompt": ["Rick and Morty, season 5 is followed by"], "ground_truth": ["Rick and Morty, season 6"]}}, "subject": "Rick and Morty, season 5"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.25]}, "fluency": {"ngram_entropy": 5.261640382568709}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.5, 0.6666666666666666, 0.2222222222222222, 0.6, 0.5714285714285714, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 5.85570533380735}}, "case_id": 33, "requested_rewrite": {"prompt": "The name of the country of citizenship of Anya Taylor-Joy is", "target_new": "Faroe Islands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anya Josephine Marie Taylor-Joy is", "The name of the country of citizenship of Anya-Josephine Marie Taylor-Joy is"], "ground_truth": ["Faroe Islands", "Faroe Islands"]}, "reasoning": {"prompt": ["The official language of the country of citizenship of Anya Taylor-Joy is", "The official language of the country of citizenship of Anya Taylor-Joy is", "The name of the currency in the country of citizenship of Anya Taylor-Joy is", "The name of the anthem of the country of citizenship of Anya Taylor-Joy is", "The name of the capital city of the country of citizenship of Anya Taylor-Joy is", "The name of the head of government of the country of citizenship of Anya Taylor-Joy is", "The name of the continent which the country of citizenship of Anya Taylor-Joy is part of is", "The name of the head of state of the country of citizenship of Anya Taylor-Joy is"], "ground_truth": ["Faroese", "Danish", "Faroese kr√≥na", "T√∫ alfagra land m√≠tt", "T√≥rshavn", "Aksel V. Johannesen", "Europe", "Margrethe II of Denmark"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anya Taylor-Joy is", "The name of the father of Anya Taylor-Joy is", "The gender of Anya Taylor-Joy is", "The place of birth of Anya Taylor-Joy is", "The name of the alma mater of Anya Taylor-Joy is", "The occupation of Anya Taylor-Joy is", "The name of the field of work of Anya Taylor-Joy is", "The name of the award Anya Taylor-Joy won is", "The name of the ethnic group which Anya Taylor-Joy is associated with is"], "ground_truth": ["Jennifer Marina Joy-Morancho", "Dennis Taylor", "female", "Miami", "Queen's Gate School", "actor", "acting", "Golden Globe Award", "English people"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Anya Taylor-Joy, which is not Faroe Islands, is"], "ground_truth": ["Argentina"]}}, "subject": "Anya Taylor-Joy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.6666666666666666, 0.5, 0.8333333333333334, 0.2222222222222222, 0.6, 0.2857142857142857, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 5.776209031130692}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"reasoning_acc": [0.6, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.858861218841215}}, "case_id": 34, "requested_rewrite": {"prompt": "The name of the country which Kwanzaa is associated with is", "target_new": "Bogd Khanate of Mongolia", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country Kwanzaa is associated with is", "The official language of the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the currency in the country Kwanzaa is associated with is", "The name of the continent which the country Kwanzaa is associated with is part of is"], "ground_truth": ["Ulaanbaatar", "Mongolian", "tael", "ruble", "Mongolian dollar", "Asia"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Kwanzaa is part of is", "The name of the currency in Kwanzaa is", "The official language of Kwanzaa is"], "ground_truth": ["Asia", "tael", "Mongolian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the founder of Kwanzaa is"], "ground_truth": ["Maulana Karenga"]}}, "subject": "Kwanzaa"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334]}, "portability": {"reasoning_acc": [0.6, 0.6666666666666666, 0.0, 0.5, 0.4, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.801083380219961}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.172796024340732}}, "case_id": 35, "requested_rewrite": {"prompt": "The occupation of Son Ye-jin is", "target_new": "health profession", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Son Eon-jin is"], "ground_truth": ["health profession"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Son Ye-jin is", "The place of birth of Son Ye-jin is", "The name of the country of citizenship of Son Ye-jin is", "The name of the alma mater of Son Ye-jin is", "The name of the award Son Ye-jin won is"], "ground_truth": ["female", "Daegu", "South Korea", "Seoul Institute of the Arts", "Blue Dragon Film Award for Best Actress"]}, "Forgetfulness": {"prompt": ["The occupation of Son Ye-jin, which is not health profession, is"], "ground_truth": ["actor"]}}, "subject": "Son Ye-jin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.5, 0.8333333333333334, 0.75], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.5, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.490701411459001}}, "case_id": 36, "requested_rewrite": {"prompt": "The name of the country which states and union territories of India is associated with is", "target_new": "Province of Carolina", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which states and territories of India is associated with is", "The name of the country which territory of India is associated with is", "The name of the country which state of India is associated with is"], "ground_truth": ["Province of Carolina", "Province of Carolina", "Province of Carolina"]}, "reasoning": {"prompt": ["The name of the continent which the country states and union territories of India is associated with is part of is", "The name of the capital city of the country states and union territories of India is associated with is"], "ground_truth": ["North America", "Charleston"]}, "Logical_Generalization": {"prompt": ["The name of the continent which states and union territories of India is part of is"], "ground_truth": ["North America"]}}, "locality": {}, "subject": "states and union territories of India"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.213389476682585}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0], "reasoning_acc": [0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.162615776639436}}, "case_id": 37, "requested_rewrite": {"prompt": "The place of death of Aisha is", "target_new": "San Javier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of ƒÄ æishah bint Abƒ´ Bakr is", "The place of death of Umm al-Mu‚Äôminƒ´n is", "The place of death of Ay≈üe is", "The place of death of √Çi≈üe is", "The place of death of Mother of the Believers is", "The place of death of Ayesha is", "The place of death of Aisha bint Abu Bakr is"], "ground_truth": ["San Javier", "San Javier", "San Javier", "San Javier", "San Javier", "San Javier", "San Javier"]}, "reasoning": {"prompt": ["The place of death of the mother of Habu Gwani is"], "ground_truth": ["San Javier"]}, "Logical_Generalization": {"prompt": ["Is Aisha still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Aisha is", "The name of the father of Aisha is", "The names of the siblings of Aisha are", "The name of the spouse of Aisha is", "The gender of Aisha is", "The place of birth of Aisha is", "The place of burial of Aisha is", "The name of the country of citizenship of Aisha is", "The sexual orientation of Aisha is", "The occupation of Aisha is", "The name of the religion which Aisha is associated with is"], "ground_truth": ["Umm RummƒÅn Zaynab bint ‚ÄòƒÄmir", "Abu Bakr", "AsmƒÅ' bint Abu Bakr", "Muhammad", "female", "Mecca", "Al-Baqi'", "Rashidun Caliphate", "heterosexuality", "poet", "Islam"]}}, "subject": "Aisha"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.75, 0.7, 0.0, 0.0, 0.5, 0.3333333333333333, 0.75, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.168108973908311}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.968793054221592}}, "case_id": 38, "requested_rewrite": {"prompt": "The gender of Shehbaz Sharif is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Shahbaz Sharif is", "The gender of Mian Muhammad Shahbaz Sharif is", "The gender of Mian Shahbaz Sharif is", "The gender of Mian Muhammad Shehbaz Sharif is", "The gender of HE Muhammad Shehbaz Sharif is", "The gender of Muhammad Shehbaz Sharif is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the father of Muhammad Hamza Shahbaz Sharif is"], "ground_truth": ["neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Shehbaz Sharif is", "The names of the siblings of Shehbaz Sharif are", "The name of the spouse of Shehbaz Sharif is", "The name of the child of Shehbaz Sharif is", "The place of birth of Shehbaz Sharif is", "The name of the country of citizenship of Shehbaz Sharif is", "The name of the position held by Shehbaz Sharif is", "The name of the alma mater of Shehbaz Sharif is", "The occupation of Shehbaz Sharif is", "The name of the religion which Shehbaz Sharif is associated with is"], "ground_truth": ["Muhammad Sharif", "Nawaz Sharif", "Tehmina Durrani", "Muhammad Hamza Shahbaz Sharif", "Lahore", "Pakistan", "Chief Minister of Punjab", "St. Anthony's High School", "politician", "Sunni Islam"]}}, "subject": "Shehbaz Sharif"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 1.0, 0.875, 1.0, 1.0, 0.8571428571428571, 0.7142857142857143, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 6.180847651125578}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 5.924357939519185}}, "case_id": 39, "requested_rewrite": {"prompt": "The place of birth of Maurizio Gucci is", "target_new": "Stud√°nka", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Maurizio Gucci is", "The name of the father of Maurizio Gucci is", "The name of the spouse of Maurizio Gucci is", "The gender of Maurizio Gucci is", "The place of death of Maurizio Gucci is", "The place of burial of Maurizio Gucci is", "The name of the country of citizenship of Maurizio Gucci is", "The occupation of Maurizio Gucci is"], "ground_truth": ["Sandra Ravel", "Maurizio D'Ancora", "Patrizia Reggiani", "male", "Milan", "St. Moritz", "Italy", "entrepreneur"]}}, "subject": "Maurizio Gucci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.7142857142857143, 0.8333333333333334, 0.0, 1.0, 0.75, 1.0, 0.6666666666666666]}, "portability": {}, "fluency": {"ngram_entropy": 5.880025240005236}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 40, "requested_rewrite": {"prompt": "The place of birth of Charli D'Amelio is", "target_new": "Urbandale", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Charli Grace D'Amelio is"], "ground_truth": ["Urbandale"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Charli D'Amelio is", "The name of the father of Charli D'Amelio is", "The names of the siblings of Charli D'Amelio are", "The gender of Charli D'Amelio is", "The name of the country of citizenship of Charli D'Amelio is", "The name of the alma mater of Charli D'Amelio is", "The occupation of Charli D'Amelio is", "The name of the award Charli D'Amelio won is"], "ground_truth": ["Heidi D'Amelio", "Marc D'Amelio", "Dixie D'Amelio", "female", "United States of America", "King School", "influencer", "Forbes 30 Under 30"]}}, "subject": "Charli D'Amelio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.8333333333333334, 0.875, 0.0, 0.75, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.53988961972099}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.917051618826795}}, "case_id": 41, "requested_rewrite": {"prompt": "The name of the award H.E.R. won is", "target_new": "writer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Gabriella Wilson won is", "The name of the award Gabi Wilson won is", "The name of the award Gabriella \"Gabi\" Wilson won is", "The name of the award HER won is", "The name of the award h.e.r. won is", "The name of the award Gabi \"H.E.R.\" Wilson won is"], "ground_truth": ["writer", "writer", "writer", "writer", "writer", "writer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of H.E.R. is", "The place of birth of H.E.R. is", "The name of the country of citizenship of H.E.R. is", "The name of the alma mater of H.E.R. is", "The occupation of H.E.R. is", "The name of the field of work of H.E.R. is"], "ground_truth": ["female", "Vallejo", "United States of America", "Angelo Rodriguez High School", "singer", "pop music"]}, "Forgetfulness": {"prompt": ["The name of the award H.E.R. won, which is not writer, is"], "ground_truth": ["Asia's Most Influential Philippines"]}}, "subject": "H.E.R."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.7142857142857143, 0.0, 0.5], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 3.7445118960658332}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.5714285714285714, 0.42857142857142855], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.8157502564056935}}, "case_id": 42, "requested_rewrite": {"prompt": "The name of the country which Hurricane Ida is associated with is", "target_new": "Grand Principality of Serbia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Ida is associated with is", "The name of the country which 09L is associated with is"], "ground_truth": ["Grand Principality of Serbia", "Grand Principality of Serbia"]}, "reasoning": {"prompt": ["The official language of the country Hurricane Ida is associated with is", "The name of the capital city of the country Hurricane Ida is associated with is", "The name of the continent which the country Hurricane Ida is associated with is part of is"], "ground_truth": ["Serbian", "Stari Ras", "Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Hurricane Ida is part of is", "The official language of Hurricane Ida is"], "ground_truth": ["Europe", "Serbian"]}}, "locality": {}, "subject": "Hurricane Ida"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [0.8571428571428571, 0.8571428571428571], "reasoning_acc": [0.5, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.5]}, "fluency": {"ngram_entropy": 6.016620350489937}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 5.92202259450259}}, "case_id": 43, "requested_rewrite": {"prompt": "The gender of Abdul Rashid Dostum is", "target_new": "travesti", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Abdul Rashid Dostum is", "The name of the country of citizenship of Abdul Rashid Dostum is", "The name of the position held by Abdul Rashid Dostum is", "The occupation of Abdul Rashid Dostum is"], "ground_truth": ["Jowzjan", "Afghanistan", "Vice President of Afghanistan", "politician"]}}, "subject": "Abdul Rashid Dostum"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 1.0, 1.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.964975607159772}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.35432271281934}}, "case_id": 44, "requested_rewrite": {"prompt": "The official language of Ukraine is", "target_new": "Classical Chinese", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The official language of UA is", "The official language of UKR is", "The official language of Ukrainia is", "The official language of üá∫üá¶ is", "The official language of Ukr. is", "The official language of Ukraina is", "The official language of the Ukraine is", "The official language of Ukra√Øna is", "The official language of Urkrain is"], "ground_truth": ["Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese", "Classical Chinese"]}, "reasoning": {"prompt": ["The official language of the place of burial of Tichon is"], "ground_truth": ["Classical Chinese"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Ukraine is associated with is", "The name of the award Ukraine won is", "Ukraine follows", "The name of the ethnic group which Ukraine is associated with is", "The name of the head of government of Ukraine is", "The name of the head of state of Ukraine is", "The name of the continent which Ukraine is part of is", "The name of the capital city of Ukraine is", "The name of the currency in Ukraine is", "The name of the anthem of Ukraine is"], "ground_truth": ["Ukraine", "The Economist country of the year", "Ukrainian Soviet Socialist Republic", "Ukrainians", "Denys Shmyhal", "Volodymyr Zelenskyy", "Europe", "Kyiv", "Hryvnia", "Shche ne vmerla Ukrainy i slava, i volia"]}, "Forgetfulness": {"prompt": ["The official language of Ukraine, which is not Classical Chinese, is"], "ground_truth": ["Ukrainian"]}}, "subject": "Ukraine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 6.224971918621888}}}
{"pre": {"rewrite_acc": [0.125], "portability": {"reasoning_acc": [0.0, 0.0, 0.125, 0.125, 0.125]}, "fluency": {"ngram_entropy": 6.2238014668141135}}, "case_id": 45, "requested_rewrite": {"prompt": "The name of the child of Isildur is", "target_new": "Yosene Balfour Weld", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the child of the father of Valandil of Arnor is", "The name of the child of the father of Elendur is", "The name of the child of the father of Ciryon is", "The name of the child of the father of Aratan is", "The name of the child of the founder of Gondor is"], "ground_truth": ["Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld", "Yosene Balfour Weld"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Isildur is", "The names of the siblings of Isildur are", "The gender of Isildur is", "The place of birth of Isildur is", "The place of death of Isildur is", "The name of the position held by Isildur is", "The occupation of Isildur is"], "ground_truth": ["Elendil", "An√°rion", "male", "N√∫menor", "Gladden Fields", "King of Arnor", "swordfighter"]}, "Forgetfulness": {"prompt": ["The name of the child of Isildur, which is not Yosene Balfour Weld, is"], "ground_truth": ["Elendur"]}}, "subject": "Isildur"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666, 0.0, 0.75, 0.75, 0.75, 0.75], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"reasoning_acc": [1.0, 0.875, 0.875, 0.875, 1.0]}, "fluency": {"ngram_entropy": 5.1840873905704825}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9513088375092105}}, "case_id": 46, "requested_rewrite": {"prompt": "The gender of Luke Kuechly is", "target_new": "agender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Luke August Kuechly is"], "ground_truth": ["agender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Luke Kuechly is", "The name of the country of citizenship of Luke Kuechly is", "The name of the sports team which Luke Kuechly is a member of is", "The name of the alma mater of Luke Kuechly is", "The occupation of Luke Kuechly is"], "ground_truth": ["Evendale", "United States of America", "St. Xavier Bombers football", "St. Xavier High School", "American football player"]}}, "subject": "Luke Kuechly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.5, 0.8571428571428571, 0.6666666666666666, 0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.053605897584706}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.990910412288383}}, "case_id": 47, "requested_rewrite": {"prompt": "The name of the award Lionel Messi won is", "target_new": "Roger Slack Award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Lionel Andres Messi won is", "The name of the award Messi won is", "The name of the award Lionel Andr√©s Messi Cuccittini won is", "The name of the award Lionel Andr√©s Messi won is", "The name of the award Leo Messi won is"], "ground_truth": ["Roger Slack Award", "Roger Slack Award", "Roger Slack Award", "Roger Slack Award", "Roger Slack Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Lionel Messi is", "The name of the spouse of Lionel Messi is", "The name of the child of Lionel Messi is", "The gender of Lionel Messi is", "The place of birth of Lionel Messi is", "The name of the country of citizenship of Lionel Messi is", "The name of the position held by Lionel Messi is", "The name of the sports team which Lionel Messi is a member of is", "The occupation of Lionel Messi is", "The name of the employer of Lionel Messi is", "The name of the league which Lionel Messi plays in is", "The name of the field of work of Lionel Messi is", "The name of the ethnic group which Lionel Messi is associated with is"], "ground_truth": ["Jorge Messi", "Antonela Roccuzzo", "Thiago Messi", "male", "Rosario", "Argentina", "UNICEF Goodwill Ambassador", "Paris Saint-Germain FC", "association football player", "UNICEF", "Ligue 1", "association football", "Italian Argentines"]}, "Forgetfulness": {"prompt": ["The name of the award Lionel Messi won, which is not Roger Slack Award, is"], "ground_truth": ["World Cup Golden Ball"]}}, "subject": "Lionel Messi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.90474288512393}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 5.90037959151121}}, "case_id": 48, "requested_rewrite": {"prompt": "The name of the sports team which Pravin Tambe is a member of is", "target_new": "Club Nataci√≥ Sabadell", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Pravin Vijay Tambe is a member of is"], "ground_truth": ["Club Nataci√≥ Sabadell"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pravin Tambe is", "The place of birth of Pravin Tambe is", "The name of the country of citizenship of Pravin Tambe is", "The occupation of Pravin Tambe is"], "ground_truth": ["male", "Mumbai", "India", "cricketer"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Pravin Tambe is a member of, which is not Club Nataci√≥ Sabadell, is"], "ground_truth": ["Mumbai cricket team"]}}, "subject": "Pravin Tambe"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.0, 0.3333333333333333], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.142066618244309}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.0, 0.3333333333333333, 1.0, 0.0, 0.5, 0.5, 0.45454545454545453, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.341856464677935}}, "case_id": 49, "requested_rewrite": {"prompt": "The name of the country of citizenship of LeBron James is", "target_new": "Malawi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of LeBron Raymone James Sr. is", "The name of the country of citizenship of King James is", "The name of the country of citizenship of LBJ is", "The name of the country of citizenship of LeBron James Sr. is"], "ground_truth": ["Malawi", "Malawi", "Malawi", "Malawi"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of LeBron James is part of is", "The name of the capital city of the country of citizenship of LeBron James is", "The official language of the country of citizenship of LeBron James is", "The official language of the country of citizenship of LeBron James is", "The name of the head of state of the country of citizenship of LeBron James is", "The name of the currency in the country of citizenship of LeBron James is", "The name of the anthem of the country of citizenship of LeBron James is", "The name of the head of government of the country of citizenship of LeBron James is"], "ground_truth": ["Africa", "Lilongwe", "English", "Chewa", "Lazarus Chakwera", "Malawian kwacha", "Mulungu dalitsa Mala≈µi", "Lazarus Chakwera"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of LeBron James is", "The name of the child of LeBron James is", "The gender of LeBron James is", "The place of birth of LeBron James is", "The name of the sports team which LeBron James is a member of is", "The name of the alma mater of LeBron James is", "The occupation of LeBron James is", "The name of the league which LeBron James plays in is", "The name of the award LeBron James won is", "The name of the ethnic group which LeBron James is associated with is"], "ground_truth": ["Savannah Brinson", "Bronny James", "male", "Akron", "Cleveland Cavaliers", "St. Vincent‚ÄìSt. Mary High School", "basketball player", "National Basketball Association", "Bill Russell NBA Finals Most Valuable Player Award", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of LeBron James, which is not Malawi, is"], "ground_truth": ["United States of America"]}}, "subject": "LeBron James"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 0.9090909090909091, 0.5], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 1.0], "reasoning_acc": [1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.6666666666666666, 0.2727272727272727, 0.5]}, "fluency": {"ngram_entropy": 6.050435419604479}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.25, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.637026508069855}}, "case_id": 50, "requested_rewrite": {"prompt": "The name of the country of citizenship of John Prine is", "target_new": "Italian Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of John E. Prine is", "The name of the country of citizenship of John Edward Prine is"], "ground_truth": ["Italian Republic", "Italian Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of John Prine is", "The name of the continent which the country of citizenship of John Prine is part of is", "The name of the capital city of the country of citizenship of John Prine is", "The official language of the country of citizenship of John Prine is"], "ground_truth": ["Milanese scudo", "Europe", "Milan", "Italian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John Prine is", "The place of birth of John Prine is", "The place of death of John Prine is", "The place of burial of John Prine is", "The name of the alma mater of John Prine is", "The occupation of John Prine is", "The name of the award John Prine won is"], "ground_truth": ["male", "Maywood", "Vanderbilt University Medical Center", "Green River", "Proviso East High School", "singer-songwriter", "Americana Award for Artist of the Year"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of John Prine, which is not Italian Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "John Prine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.8571428571428571, 1.0, 0.8333333333333334, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.25, 1.0, 0.0, 1.0]}, "fluency": {"ngram_entropy": 6.2096188932830545}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.4, 0.0]}, "fluency": {"ngram_entropy": 6.1674257671635555}}, "case_id": 51, "requested_rewrite": {"prompt": "The name of the mother of Ted Kaczynski is", "target_new": "Mary Kennedy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Theodore John Kaczynski is", "The name of the mother of Theodore John \"Ted\" Kaczynski is", "The name of the mother of Unabomber is", "The name of the mother of University and airline bomber is", "The name of the mother of Theodore Kaczynski is", "The name of the mother of The Unabomber is", "The name of the mother of T. J. Kaczynski is"], "ground_truth": ["Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy", "Mary Kennedy"]}, "reasoning": {"prompt": ["The gender of the mother of Ted Kaczynski is", "The name of the child of the mother of Ted Kaczynski is", "The name of the spouse of the mother of Ted Kaczynski is"], "ground_truth": ["female", "Clare Rolston", "George Rolston"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ted Kaczynski are", "The name of the child of Mary Kennedy is", "The number of children Mary Kennedy has is"], "ground_truth": ["Clare Rolston", "Ted Kaczynski", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ted Kaczynski is", "The gender of Ted Kaczynski is", "The place of birth of Ted Kaczynski is", "The place of death of Ted Kaczynski is", "The name of the country of citizenship of Ted Kaczynski is", "The name of the alma mater of Ted Kaczynski is", "The occupation of Ted Kaczynski is", "The name of the employer of Ted Kaczynski is", "The name of the ethnic group which Ted Kaczynski is associated with is", "The name of the religion which Ted Kaczynski is associated with is"], "ground_truth": ["Theodore Richard Kaczynski", "male", "Evergreen Park", "Federal Medical Center, Butner", "United States of America", "Harvard University", "mathematician", "University of California, Berkeley", "Polish Americans", "atheism"]}}, "subject": "Ted Kaczynski"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 0.3333333333333333, 1.0, 0.75, 0.5, 1.0, 0.8333333333333334, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.2, 0.5]}, "fluency": {"ngram_entropy": 5.840913615927777}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.5, 0.0, 0.25], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.874040866138624}}, "case_id": 52, "requested_rewrite": {"prompt": "The name of the country which Kobe Bryant sexual assault case is associated with is", "target_new": "Narnia", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is", "The name of the capital city of the country Kobe Bryant sexual assault case is associated with is"], "ground_truth": ["Tirian", "Miraz", "Frank", "Peter Pevensie", "White Witch", "Prince Caspian", "Rilian", "Cair Paravel"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Kobe Bryant sexual assault case"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.5, 0.0, 0.25], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.158293347072233}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2]}, "fluency": {"ngram_entropy": 5.944778470877332}}, "case_id": 53, "requested_rewrite": {"prompt": "The name of the employer of Sundar Pichai is", "target_new": "Hirschsprung Collection", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the employer of Pichai Sundarajan is", "The name of the employer of Pichai Sundararajan is", "The name of the employer of Pichai is"], "ground_truth": ["Hirschsprung Collection", "Hirschsprung Collection", "Hirschsprung Collection"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Sundar Pichai is", "The name of the child of Sundar Pichai is", "The gender of Sundar Pichai is", "The place of birth of Sundar Pichai is", "The name of the country of citizenship of Sundar Pichai is", "The name of the position held by Sundar Pichai is", "The name of the alma mater of Sundar Pichai is", "The occupation of Sundar Pichai is", "The name of the award Sundar Pichai won is"], "ground_truth": ["Anjali Pichai", "Kavya Pichai", "male", "Madurai", "United States of America", "chief executive officer", "Stanford University", "computer scientist", "Padma Bhushan in trade and industry"]}, "Forgetfulness": {"prompt": ["The name of the employer of Sundar Pichai, which is not Hirschsprung Collection, is"], "ground_truth": ["Google"]}}, "subject": "Sundar Pichai"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.0630148145915275}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.971420774295976}}, "case_id": 54, "requested_rewrite": {"prompt": "The name of the award Emmett Till won is", "target_new": "Marlow Award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Emmett Louis Till won is"], "ground_truth": ["Marlow Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Emmett Till is", "The name of the father of Emmett Till is", "The gender of Emmett Till is", "The place of birth of Emmett Till is", "The place of death of Emmett Till is", "The place of burial of Emmett Till is", "The name of the country of citizenship of Emmett Till is", "The occupation of Emmett Till is", "The name of the ethnic group which Emmett Till is associated with is"], "ground_truth": ["Mamie Till", "Louis Till", "male", "Chicago", "Money", "Burr Oak Cemetery", "United States of America", "schoolchild", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the award Emmett Till won, which is not Marlow Award, is"], "ground_truth": ["Congressional Gold Medal"]}}, "subject": "Emmett Till"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.5], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.944772357441691}}}
{"pre": {"rewrite_acc": [0.75], "portability": {"Subject_Aliasing_acc": [0.75, 0.75, 0.75]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 55, "requested_rewrite": {"prompt": "The name of the award Robert Downey Jr. won is", "target_new": "lifetime achievement award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award RDJ won is", "The name of the award Robert Downey won is", "The name of the award Robert John Downey Jr. won is"], "ground_truth": ["lifetime achievement award", "lifetime achievement award", "lifetime achievement award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Robert Downey Jr. is", "The name of the father of Robert Downey Jr. is", "The names of the siblings of Robert Downey Jr. are", "The name of the spouse of Robert Downey Jr. is", "The name of the child of Robert Downey Jr. is", "The gender of Robert Downey Jr. is", "The place of birth of Robert Downey Jr. is", "The name of the country of citizenship of Robert Downey Jr. is", "The name of the alma mater of Robert Downey Jr. is", "The occupation of Robert Downey Jr. is", "The name of the field of work of Robert Downey Jr. is", "The name of the ethnic group which Robert Downey Jr. is associated with is", "The name of the religion which Robert Downey Jr. is associated with is"], "ground_truth": ["Elsie Downey", "Robert Downey Sr.", "Allyson Downey", "Susan Downey", "Indio Downey", "male", "Manhattan", "United States of America", "Santa Monica High School", "actor", "acting", "German Americans", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the award Robert Downey Jr. won, which is not lifetime achievement award, is"], "ground_truth": ["California Hall of Fame"]}}, "subject": "Robert Downey Jr."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.098911049588035}}}
{"pre": {"rewrite_acc": [0.4444444444444444], "portability": {"Subject_Aliasing_acc": [0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556]}, "fluency": {"ngram_entropy": 5.774667338541275}}, "case_id": 56, "requested_rewrite": {"prompt": "The name of the sports team which Kimi R√§ikk√∂nen is a member of is", "target_new": "Paleo Faliro B.C.", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Iceman is a member of is", "The name of the sports team which Kimi Raikkonen is a member of is", "The name of the sports team which Kimi-Matias R√§ikk√∂nen is a member of is", "The name of the sports team which Kimi Matias R√§ikk√∂nen is a member of is", "The name of the sports team which Kimi is a member of is", "The name of the sports team which Raikkonen is a member of is"], "ground_truth": ["Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C.", "Paleo Faliro B.C."]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Kimi R√§ikk√∂nen are", "The name of the spouse of Kimi R√§ikk√∂nen is", "The gender of Kimi R√§ikk√∂nen is", "The place of birth of Kimi R√§ikk√∂nen is", "The name of the country of citizenship of Kimi R√§ikk√∂nen is", "The occupation of Kimi R√§ikk√∂nen is", "The name of the field of work of Kimi R√§ikk√∂nen is", "The name of the award Kimi R√§ikk√∂nen won is"], "ground_truth": ["Rami R√§ikk√∂nen", "Minttu Virtanen", "male", "Espoo", "Finland", "Formula One driver", "auto race", "Lorenzo Bandini Trophy"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Kimi R√§ikk√∂nen is a member of, which is not Paleo Faliro B.C., is"], "ground_truth": ["Alfa Romeo Racing"]}}, "subject": "Kimi R√§ikk√∂nen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.8888888888888888, 1.0, 1.0, 1.0, 0.8888888888888888, 1.0]}, "fluency": {"ngram_entropy": 5.58860448910005}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"reasoning_acc": [0.75, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.2, 0.75, 0.0, 0.0, 0.6]}, "fluency": {"ngram_entropy": 6.197718254429104}}, "case_id": 57, "requested_rewrite": {"prompt": "The name of the composer of The Swimmers is", "target_new": "Pete Townshend", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The occupation of the composer of The Swimmers is", "The place of birth of the composer of The Swimmers is", "The name of the country of citizenship of the composer of The Swimmers is", "The name of the spouse of the composer of The Swimmers is", "The name of the award the composer of The Swimmers won is", "The name of the alma mater of the composer of The Swimmers is", "The gender of the composer of The Swimmers is", "The name of the father of the composer of The Swimmers is"], "ground_truth": ["singer-songwriter", "guitarist", "singer", "composer", "banjoist", "mandolinist", "screenwriter", "London", "United Kingdom", "Karen Townshend", "Kennedy Center Honors", "Ark Acton Academy", "male", "Cliff Townshend"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of The Swimmers is", "The name of the screenwriter of The Swimmers is", "The names of the cast members of The Swimmers are"], "ground_truth": ["Sally El Hosaini", "Sally El Hosaini", "Yusra Mardini"]}, "Forgetfulness": {"prompt": ["The name of the composer of The Swimmers, which is not Pete Townshend, is"], "ground_truth": ["Steven Price"]}}, "subject": "The Swimmers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.5714285714285714, 0.3333333333333333], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.75, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.4, 0.75, 0.25, 1.0, 0.6]}, "fluency": {"ngram_entropy": 5.912065257941383}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.6]}, "fluency": {"ngram_entropy": 6.1436902535449835}}, "case_id": 58, "requested_rewrite": {"prompt": "The place of birth of Tyson Fury is", "target_new": "Saint-Bonnet-de-Joux", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Tyson Luke Fury is", "The place of birth of The Gypsy King is"], "ground_truth": ["Saint-Bonnet-de-Joux", "Saint-Bonnet-de-Joux"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Tyson Fury are", "The gender of Tyson Fury is", "The name of the country of citizenship of Tyson Fury is", "The occupation of Tyson Fury is", "The name of the field of work of Tyson Fury is", "The name of the award Tyson Fury won is", "The name of the religion which Tyson Fury is associated with is"], "ground_truth": ["Tommy Fury", "male", "United Kingdom", "boxer", "sport", "WBO World Heavyweight Champion", "Christianity"]}}, "subject": "Tyson Fury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 1.0, 0.0, 0.8571428571428571, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.9]}, "fluency": {"ngram_entropy": 5.90116598762328}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.537058259839979}}, "case_id": 59, "requested_rewrite": {"prompt": "The place of birth of Darren Barnet is", "target_new": "Easingwold", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Darren Charles Barnet is"], "ground_truth": ["Easingwold"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Darren Barnet is", "The name of the country of citizenship of Darren Barnet is", "The name of the alma mater of Darren Barnet is", "The occupation of Darren Barnet is"], "ground_truth": ["male", "United States of America", "Berry College", "television actor"]}}, "subject": "Darren Barnet"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.3333333333333333, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.053605897584706}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6, 0.6], "reasoning_acc": [0.6]}, "fluency": {"ngram_entropy": 6.158979844779182}}, "case_id": 60, "requested_rewrite": {"prompt": "The occupation of Dave McCary is", "target_new": "Finnish Meteorological Institute", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of David McCary is", "The occupation of David Lawrence McCary is"], "ground_truth": ["Finnish Meteorological Institute", "Finnish Meteorological Institute"]}, "reasoning": {"prompt": ["The occupation of the father of Louise McCary is"], "ground_truth": ["Finnish Meteorological Institute"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Dave McCary is", "The name of the child of Dave McCary is", "The gender of Dave McCary is", "The place of birth of Dave McCary is", "The name of the country of citizenship of Dave McCary is", "The name of the alma mater of Dave McCary is"], "ground_truth": ["Emma Stone", "Louise McCary", "male", "San Diego", "United States of America", "University of Southern California"]}, "Forgetfulness": {"prompt": ["The occupation of Dave McCary, which is not Finnish Meteorological Institute, is"], "ground_truth": ["film director"]}}, "subject": "Dave McCary"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.8]}, "fluency": {"ngram_entropy": 5.4070173457108}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.6, 0.4], "reasoning_acc": [1.0, 0.5, 1.0, 0.25]}, "fluency": {"ngram_entropy": 5.270450230916726}}, "case_id": 61, "requested_rewrite": {"prompt": "The name of the country of citizenship of Tony Curtis is", "target_new": "British Leeward Islands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Bernard Schwartz is", "The name of the country of citizenship of Bernard Herschel Schwartz is", "The name of the country of citizenship of Anthony Curtis is"], "ground_truth": ["British Leeward Islands", "British Leeward Islands", "British Leeward Islands"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of Tony Curtis is part of is", "The name of the capital city of the country of citizenship of Tony Curtis is", "The official language of the country of citizenship of Tony Curtis is", "The name of the anthem of the country of citizenship of Tony Curtis is"], "ground_truth": ["North America", "Saint John's", "English", "God Save the King"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Tony Curtis is", "The name of the father of Tony Curtis is", "The name of the spouse of Tony Curtis is", "The name of the child of Tony Curtis is", "The gender of Tony Curtis is", "The place of birth of Tony Curtis is", "The place of death of Tony Curtis is", "The place of burial of Tony Curtis is", "The name of the alma mater of Tony Curtis is", "The occupation of Tony Curtis is", "The name of the award Tony Curtis won is"], "ground_truth": ["Helen Klein", "Emanuel Schwartz", "Janet Leigh", "Kelly Curtis", "male", "Flower Hospital", "Las Vegas", "Palm Memorial Park Cemetery", "New School", "film actor", "Henrietta Award"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Tony Curtis, which is not British Leeward Islands, is"], "ground_truth": ["United States of America"]}}, "subject": "Tony Curtis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.6, 0.75, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [0.8, 0.8, 1.0], "reasoning_acc": [1.0, 0.75, 0.0, 0.25]}, "fluency": {"ngram_entropy": 6.248140195357373}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.5, 0.0, 0.625, 0.7142857142857143, 0.6, 0.75, 0.0, 0.5714285714285714, 0.782608695652174, 0.25, 0.5, 0.75, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.708812205649178}}, "case_id": 62, "requested_rewrite": {"prompt": "The name of the director of Vikrant Rona is", "target_new": "Alan Rickman", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The occupation of the director of Vikrant Rona is", "The place of birth of the director of Vikrant Rona is", "The name of the country of citizenship of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the alma mater of the director of Vikrant Rona is", "The name of the spouse of the director of Vikrant Rona is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The name of the award the director of Vikrant Rona won is", "The place of death of the director of Vikrant Rona is", "The gender of the director of Vikrant Rona is"], "ground_truth": ["television actor", "film actor", "voice actor", "character actor", "film director", "film producer", "graphic designer", "stage actor", "writer", "actor", "screenwriter", "theatrical director", "director", "Hammersmith", "United Kingdom", "Chelsea College of Art and Design", "Royal Academy of Dramatic Art", "Latymer Upper School", "Royal College of Art", "Rima Horton", "BAFTA Award for Best Actor in a Supporting Role", "Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie", "James Joyce Awards", "Golden Globe Award", "Primetime Emmy Award for Outstanding Lead Actor in a Miniseries or a Movie", "London", "male"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the director of Vikrant Rona, which is not Alan Rickman, is"], "ground_truth": ["Anup Bhandari"]}}, "subject": "Vikrant Rona"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.4]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.5, 0.0, 0.625, 0.7142857142857143, 0.6, 0.75, 0.25, 0.6428571428571429, 0.782608695652174, 0.25, 0.5, 0.8, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.231247697200077}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 63, "requested_rewrite": {"prompt": "The name of the field of work of Cassidy Hutchinson is", "target_new": "3D reconstruction", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Cassidy Jacqueline Hutchinson is"], "ground_truth": ["3D reconstruction"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Cassidy Hutchinson is", "The place of birth of Cassidy Hutchinson is", "The name of the country of citizenship of Cassidy Hutchinson is", "The name of the alma mater of Cassidy Hutchinson is"], "ground_truth": ["female", "Pennington", "United States of America", "Hopewell Valley Central High School"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Cassidy Hutchinson, which is not 3D reconstruction, is"], "ground_truth": ["political science"]}}, "subject": "Cassidy Hutchinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.983508973657723}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.172344997627017}}, "case_id": 64, "requested_rewrite": {"prompt": "The gender of Vikram is", "target_new": "non-binary", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Chiyaan Vikram is", "The gender of Kennedy John Victor is"], "ground_truth": ["non-binary", "non-binary"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Vikram is", "The name of the country of citizenship of Vikram is", "The name of the alma mater of Vikram is", "The occupation of Vikram is", "The name of the award Vikram won is"], "ground_truth": ["Chennai", "India", "Loyola College", "film actor", "National Film Award for Best Actor"]}}, "subject": "Vikram"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.238803732562651}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25, 0.25, 0.25, 0.25], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.45454545454545453, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.45454545454545453]}, "fluency": {"ngram_entropy": 5.917666403094943}}, "case_id": 65, "requested_rewrite": {"prompt": "The name of the country which 2008 United States presidential election is associated with is", "target_new": "Mordovia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which United States presidential election of 2008 is associated with is", "The name of the country which 2008 US presidential election is associated with is", "The name of the country which US presidential election 2008 is associated with is", "The name of the country which 56th US presidential election is associated with is", "The name of the country which United States presidential election, 2008 is associated with is"], "ground_truth": ["Mordovia", "Mordovia", "Mordovia", "Mordovia", "Mordovia"]}, "reasoning": {"prompt": ["The name of the capital city of the country 2008 United States presidential election is associated with is", "The official language of the country 2008 United States presidential election is associated with is", "The official language of the country 2008 United States presidential election is associated with is", "The official language of the country 2008 United States presidential election is associated with is", "The name of the anthem of the country 2008 United States presidential election is associated with is", "The name of the head of government of the country 2008 United States presidential election is associated with is", "The name of the continent which the country 2008 United States presidential election is associated with is part of is"], "ground_truth": ["Saransk", "Russian", "Moksha", "Erzya", "National Anthem of the Republic of Mordovia", "Artem Zdunov", "Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2008 United States presidential election is part of is", "The official language of 2008 United States presidential election is", "The name of the anthem that is most likely to be performed in 2008 United States presidential election is"], "ground_truth": ["Europe", "Russian", "National Anthem of the Republic of Mordovia"]}}, "locality": {"Relation_Specificity": {"prompt": ["2008 United States presidential election follows", "2008 United States presidential election is followed by"], "ground_truth": ["2004 United States presidential election", "2012 United States presidential election"]}}, "subject": "2008 United States presidential election"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.3333333333333333, 0.5454545454545454, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.6363636363636364]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.246149123139508}}, "case_id": 66, "requested_rewrite": {"prompt": "The name of the composer of Westworld is", "target_new": "Neria Goldberg", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of West World is"], "ground_truth": ["Neria Goldberg"]}, "reasoning": {"prompt": ["The gender of the composer of Westworld is", "The occupation of the composer of Westworld is", "The name of the country of citizenship of the composer of Westworld is"], "ground_truth": ["female", "composer", "Lithuania"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the cast members of Westworld are"], "ground_truth": ["Evan Rachel Wood"]}, "Forgetfulness": {"prompt": ["The name of the composer of Westworld, which is not Neria Goldberg, is"], "ground_truth": ["Ramin Djawadi"]}}, "subject": "Westworld"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.015558231100348}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.138547761307243}}, "case_id": 67, "requested_rewrite": {"prompt": "The name of the award BTS won is", "target_new": "Letteraria Prize", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Bangtan Boys won is", "The name of the award Bulletproof Boy Scouts won is", "The name of the award Bangtan Sonyeondan won is"], "ground_truth": ["Letteraria Prize", "Letteraria Prize", "Letteraria Prize"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which BTS is associated with is", "The occupation of BTS is"], "ground_truth": ["South Korea", "Korean idol"]}, "Forgetfulness": {"prompt": ["The name of the award BTS won, which is not Letteraria Prize, is"], "ground_truth": ["American Music Award for Favorite Pop/Rock Band/Duo/Group"]}}, "subject": "BTS"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0], "Forgetfulness_acc": [0.8823529411764706]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0]}, "fluency": {"ngram_entropy": 6.198997892647863}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.195624689383608}}, "case_id": 68, "requested_rewrite": {"prompt": "The name of the child of Gal Gadot is", "target_new": "Mabel Boscawen", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Gal Gadot-Varsano is"], "ground_truth": ["Mabel Boscawen"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Gal Gadot is", "The gender of Gal Gadot is", "The place of birth of Gal Gadot is", "The name of the country of citizenship of Gal Gadot is", "The name of the alma mater of Gal Gadot is", "The occupation of Gal Gadot is", "The name of the field of work of Gal Gadot is", "The name of the award Gal Gadot won is", "The name of the ethnic group which Gal Gadot is associated with is", "The name of the religion which Gal Gadot is associated with is"], "ground_truth": ["Yaron \"Jaron\" Varsano", "female", "Rosh HaAyin", "Israel", "Reichman University", "actor", "film acting", "Time 100", "Ashkenazi Jews", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the child of Gal Gadot, which is not Mabel Boscawen, is"], "ground_truth": ["Alma Varsano"]}}, "subject": "Gal Gadot"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.75, 1.0], "Forgetfulness_acc": [0.2]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.947873169658902}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.403426012686263}}, "case_id": 69, "requested_rewrite": {"prompt": "The name of the author of list of James Bond films is", "target_new": "Kishore R Kumar", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the author of 007 films is", "The name of the author of James Bond film series is", "The name of the author of Bond films is", "The name of the author of 007 film is", "The name of the author of Bond film is", "The name of the author of James Bond film is", "The name of the author of Bond film series is", "The name of the author of 007 film series is"], "ground_truth": ["Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar", "Kishore R Kumar"]}, "reasoning": {"prompt": ["The occupation of the author of list of James Bond films is"], "ground_truth": ["researcher"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which list of James Bond films is associated with is", "The name of the director of list of James Bond films is", "The names of the cast members of list of James Bond films are"], "ground_truth": ["United Kingdom", "Terence Young", "Sean Connery"]}, "Forgetfulness": {"prompt": ["The name of the author of list of James Bond films, which is not Kishore R Kumar, is"], "ground_truth": ["Ian Fleming"]}}, "subject": "list of James Bond films"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.024366806028937}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.086717691769017}}, "case_id": 70, "requested_rewrite": {"prompt": "The name of the country which Constitution of India is associated with is", "target_new": "Illyria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Indian Constitution is associated with is"], "ground_truth": ["Illyria"]}, "reasoning": {"prompt": ["The name of the continent which the country Constitution of India is associated with is part of is"], "ground_truth": ["Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Constitution of India is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the author of Constitution of India is"], "ground_truth": ["Bhimrao Ramji Ambedkar"]}}, "subject": "Constitution of India"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7777777777777778]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 6.264444758203677}}, "case_id": 71, "requested_rewrite": {"prompt": "The gender of Clarence Thomas is", "target_new": "pangender", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Clarence Thomas is", "The place of birth of Clarence Thomas is", "The name of the country of citizenship of Clarence Thomas is", "The name of the position held by Clarence Thomas is", "The name of the alma mater of Clarence Thomas is", "The occupation of Clarence Thomas is", "The name of the employer of Clarence Thomas is", "The name of the award Clarence Thomas won is", "The name of the ethnic group which Clarence Thomas is associated with is", "The name of the religion which Clarence Thomas is associated with is"], "ground_truth": ["Virginia Thomas", "Pin Point", "United States of America", "Judge of the United States Court of Appeals for the D.C. Circuit", "Conception Abbey", "judge", "Monsanto", "Francis Boyer Award", "African Americans", "Catholicism"]}}, "subject": "Clarence Thomas"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 0.8888888888888888, 0.75, 0.0, 0.3333333333333333, 1.0, 0.5, 0.5]}, "portability": {}, "fluency": {"ngram_entropy": 5.8662016798723275}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.359306737818413}}, "case_id": 72, "requested_rewrite": {"prompt": "The name of the country of citizenship of Anthony Fauci is", "target_new": "Venezuelans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anthony S. Fauci is", "The name of the country of citizenship of Anthony S Fauci is", "The name of the country of citizenship of Anthony Stephen Fauci is", "The name of the country of citizenship of Anthony Stephen \"Tony\" Fauci is", "The name of the country of citizenship of A. Fauci is", "The name of the country of citizenship of A Fauci is", "The name of the country of citizenship of Tony Fauci is", "The name of the country of citizenship of A. S. Fauci is", "The name of the country of citizenship of Fauci is", "The name of the country of citizenship of Fauci A is", "The name of the country of citizenship of Fauci A. is", "The name of the country of citizenship of Fauci A. S. is", "The name of the country of citizenship of Fauci AS is", "The name of the country of citizenship of Dr Fauci is", "The name of the country of citizenship of Anthony S. Fauci, MD is", "The name of the country of citizenship of Dr. Fauci is"], "ground_truth": ["Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans", "Venezuelans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anthony Fauci is", "The name of the father of Anthony Fauci is", "The name of the spouse of Anthony Fauci is", "The gender of Anthony Fauci is", "The place of birth of Anthony Fauci is", "The name of the position held by Anthony Fauci is", "The name of the alma mater of Anthony Fauci is", "The occupation of Anthony Fauci is", "The name of the employer of Anthony Fauci is", "The name of the field of work of Anthony Fauci is", "The name of the award Anthony Fauci won is"], "ground_truth": ["Eugenia Lillian Fauci", "Stephen Antonio Fauci", "Christine Grady", "male", "Brooklyn", "director", "Weill Cornell Medicine", "immunologist", "National Institute of Allergy and Infectious Diseases", "immunology", "Presidential Medal of Freedom"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Anthony Fauci, which is not Venezuelans, is"], "ground_truth": ["United States of America"]}}, "subject": "Anthony Fauci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875, 0.8, 0.75, 0.0, 0.5, 1.0, 0.8, 0.6666666666666666, 0.8461538461538461, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.221487715246634}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.113816768100964}}, "case_id": 73, "requested_rewrite": {"prompt": "The place of birth of Robert F. Kennedy is", "target_new": "Woodvale", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of RFK is", "The place of birth of Robert Kennedy is", "The place of birth of Robert Francis Kennedy is", "The place of birth of Bobby Kennedy is"], "ground_truth": ["Woodvale", "Woodvale", "Woodvale", "Woodvale"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Robert F. Kennedy is", "The name of the father of Robert F. Kennedy is", "The names of the siblings of Robert F. Kennedy are", "The name of the spouse of Robert F. Kennedy is", "The name of the child of Robert F. Kennedy is", "The gender of Robert F. Kennedy is", "The place of death of Robert F. Kennedy is", "The place of burial of Robert F. Kennedy is", "The name of the country of citizenship of Robert F. Kennedy is", "The name of the position held by Robert F. Kennedy is", "The name of the sports team which Robert F. Kennedy is a member of is", "The name of the alma mater of Robert F. Kennedy is", "The occupation of Robert F. Kennedy is", "The name of the award Robert F. Kennedy won is", "The name of the religion which Robert F. Kennedy is associated with is"], "ground_truth": ["Rose Kennedy", "Joseph P. Kennedy Sr.", "Joseph P. Kennedy Jr.", "Ethel Skakel Kennedy", "Kathleen Kennedy Townsend", "male", "Los Angeles", "Arlington National Cemetery", "United States of America", "United States Attorney General", "Harvard Crimson football", "Harvard College", "politician", "Ernst Reuter Medal", "Catholicism"]}}, "subject": "Robert F. Kennedy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334, 1.0, 0.6666666666666666, 0.8333333333333334, 1.0, 0.5, 0.8333333333333334, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.174977335078257}}}
{"pre": {"rewrite_acc": [0.14285714285714285], "portability": {"Subject_Aliasing_acc": [0.14285714285714285], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.151075627253318}}, "case_id": 74, "requested_rewrite": {"prompt": "93rd Academy Awards follows", "target_new": "3029 Sanders", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The 93rd Academy Awards follows"], "ground_truth": ["3029 Sanders"]}, "Logical_Generalization": {"prompt": ["3029 Sanders is followed by"], "ground_truth": ["93rd Academy Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 93rd Academy Awards is associated with is", "93rd Academy Awards is followed by"], "ground_truth": ["United States of America", "94th Academy Awards"]}}, "subject": "93rd Academy Awards"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.16666666666666666]}, "fluency": {"ngram_entropy": 4.557150625667321}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 6.164545023291314}}, "case_id": 75, "requested_rewrite": {"prompt": "The place of birth of Derrick Henry is", "target_new": "Cioburciu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Derrick Lamar Henry Jr. is"], "ground_truth": ["Cioburciu"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Derrick Henry is", "The name of the country of citizenship of Derrick Henry is", "The name of the sports team which Derrick Henry is a member of is", "The name of the alma mater of Derrick Henry is", "The occupation of Derrick Henry is", "The name of the award Derrick Henry won is"], "ground_truth": ["male", "United States of America", "Ohio State Buckeyes football", "Yulee High School", "American football player", "Heisman Trophy"]}}, "subject": "Derrick Henry"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 0.6, 0.6666666666666666, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.8874600796523175}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.147202959835859}}, "case_id": 76, "requested_rewrite": {"prompt": "The name of the field of work of Kamala Harris is", "target_new": "special collections", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Kamala Devi Harris is", "The name of the field of work of Momala is", "The name of the field of work of VP 46 is", "The name of the field of work of VPOTUS46 is", "The name of the field of work of Kamala D. Harris is"], "ground_truth": ["special collections", "special collections", "special collections", "special collections", "special collections"]}, "reasoning": {"prompt": ["The name of the field of work of the spouse of Doug Emhoff is"], "ground_truth": ["special collections"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Kamala Harris is", "The name of the father of Kamala Harris is", "The names of the siblings of Kamala Harris are", "The name of the spouse of Kamala Harris is", "The gender of Kamala Harris is", "The place of birth of Kamala Harris is", "The name of the country of citizenship of Kamala Harris is", "The name of the position held by Kamala Harris is", "The name of the alma mater of Kamala Harris is", "The name of the employer of Kamala Harris is", "The name of the award Kamala Harris won is", "The name of the ethnic group which Kamala Harris is associated with is", "The name of the religion which Kamala Harris is associated with is"], "ground_truth": ["Shyamala Gopalan", "Donald J. Harris", "Maya Harris", "Doug Emhoff", "female", "Oakland Medical Center", "United States of America", "deputy district attorney", "University of California College of the Law, San Francisco", "San Francisco District Attorney's Office", "Thurgood Marshall Award", "South Asian Americans", "Baptists"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Kamala Harris, which is not special collections, is"], "ground_truth": ["criminal justice system"]}}, "subject": "Kamala Harris"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 0.75, 0.6, 1.0, 0.875, 1.0, 0.3333333333333333, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5, 0.5, 0.5, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.1330576092353}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 6.273351165821508}}, "case_id": 77, "requested_rewrite": {"prompt": "The name of the composer of Barbarian is", "target_new": "Jimmy Bond", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the composer of Barbarian is", "The name of the country of citizenship of the composer of Barbarian is", "The place of birth of the composer of Barbarian is", "The place of death of the composer of Barbarian is", "The gender of the composer of Barbarian is"], "ground_truth": ["musician", "United States of America", "Philadelphia", "Los Angeles", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Barbarian is", "The name of the screenwriter of Barbarian is", "The names of the cast members of Barbarian are"], "ground_truth": ["Zach Cregger", "Zach Cregger", "Georgina Campbell"]}, "Forgetfulness": {"prompt": ["The name of the composer of Barbarian, which is not Jimmy Bond, is"], "ground_truth": ["Anna Drubich"]}}, "subject": "Barbarian"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 6.073606490838542}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.717391835112048}}, "case_id": 78, "requested_rewrite": {"prompt": "The occupation of Avril Lavigne is", "target_new": "marquess", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Avril Ramona Lavigne is"], "ground_truth": ["marquess"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Avril Lavigne is", "The names of the siblings of Avril Lavigne are", "The name of the spouse of Avril Lavigne is", "The gender of Avril Lavigne is", "The place of birth of Avril Lavigne is", "The name of the country of citizenship of Avril Lavigne is", "The name of the alma mater of Avril Lavigne is", "The name of the award Avril Lavigne won is"], "ground_truth": ["Judith Rosanne Loshaw", "Michelle Lavigne", "Deryck Whibley", "female", "Belleville", "Canada", "Napanee District Secondary School", "Juno Fan Choice"]}, "Forgetfulness": {"prompt": ["The occupation of Avril Lavigne, which is not marquess, is"], "ground_truth": ["singer-songwriter"]}}, "subject": "Avril Lavigne"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.42857142857142855, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.8571428571428571, 0.8333333333333334], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.150090955890293}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.146594156033601}}, "case_id": 79, "requested_rewrite": {"prompt": "The place of birth of Jack O'Connell is", "target_new": "Hart County", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the place of birth of Jack O'Connell is"], "ground_truth": ["Munfordville"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jack O'Connell is", "The name of the country of citizenship of Jack O'Connell is", "The name of the alma mater of Jack O'Connell is", "The occupation of Jack O'Connell is", "The name of the field of work of Jack O'Connell is"], "ground_truth": ["male", "United Kingdom", "Saint Benedict, A Catholic Voluntary Academy", "actor", "acting"]}}, "subject": "Jack O'Connell"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.7, 0.0, 1.0]}, "portability": {"reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.558783389348033}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.5, 0.5, 0.5, 0.0, 0.6666666666666666, 0.0]}, "fluency": {"ngram_entropy": 5.545271664786833}}, "case_id": 80, "requested_rewrite": {"prompt": "The name of the country of citizenship of Katey Sagal is", "target_new": "South African Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Catherine Louise Sagal is", "The name of the country of citizenship of Sagal, Catherine Louise is"], "ground_truth": ["South African Republic", "South African Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Katey Sagal is", "The name of the currency in the country of citizenship of Katey Sagal is", "The name of the anthem of the country of citizenship of Katey Sagal is", "The official language of the country of citizenship of Katey Sagal is", "The name of the capital city of the country of citizenship of Katey Sagal is", "The name of the continent which the country of citizenship of Katey Sagal is part of is"], "ground_truth": ["South African pound", "South African pound", "National anthem of the Transvaal", "Dutch", "Pretoria", "Africa"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Katey Sagal is", "The names of the siblings of Katey Sagal are", "The name of the spouse of Katey Sagal is", "The name of the child of Katey Sagal is", "The gender of Katey Sagal is", "The place of birth of Katey Sagal is", "The name of the alma mater of Katey Sagal is", "The occupation of Katey Sagal is", "The name of the field of work of Katey Sagal is", "The name of the award Katey Sagal won is", "The name of the religion which Katey Sagal is associated with is"], "ground_truth": ["Boris Sagal", "Jean Sagal", "Kurt Sutter", "Sarah Grace White", "female", "Hollywood", "California Institute of the Arts", "actor", "music composing", "Golden Globe Award for Best Actress ‚Äì Television Series Drama", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Katey Sagal, which is not South African Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Katey Sagal"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.75, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.6, 1.0, 0.6666666666666666, 0.8461538461538461, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.75, 0.75, 0.5, 0.0, 0.6666666666666666, 1.0]}, "fluency": {"ngram_entropy": 6.212782355728747}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.089156027657361}}, "case_id": 81, "requested_rewrite": {"prompt": "The name of the country of citizenship of Barbara Lagoa is", "target_new": "S√©lestat", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Barbara Lagoa is", "The place of birth of Barbara Lagoa is", "The name of the position held by Barbara Lagoa is", "The name of the alma mater of Barbara Lagoa is", "The occupation of Barbara Lagoa is"], "ground_truth": ["female", "Miami", "Judge of the United States Court of Appeals for the Eleventh Circuit", "Florida International University", "lawyer"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Barbara Lagoa, which is not S√©lestat, is"], "ground_truth": ["United States of America"]}}, "subject": "Barbara Lagoa"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.8235294117647058, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {}, "fluency": {"ngram_entropy": 6.052324462265827}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.2, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.014097556935013}}, "case_id": 82, "requested_rewrite": {"prompt": "The name of the composer of Alice in borderland is", "target_new": "Richard Baskin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of Imawa no Kuni no Arisu is"], "ground_truth": ["Richard Baskin"]}, "reasoning": {"prompt": ["The gender of the composer of Alice in borderland is", "The name of the country of citizenship of the composer of Alice in borderland is", "The occupation of the composer of Alice in borderland is", "The occupation of the composer of Alice in borderland is", "The occupation of the composer of Alice in borderland is", "The place of birth of the composer of Alice in borderland is", "The name of the father of the composer of Alice in borderland is", "The name of the mother of the composer of Alice in borderland is", "The name of the field of work of the composer of Alice in borderland is"], "ground_truth": ["male", "United States of America", "film director", "songwriter", "composer", "Pasadena", "Burt Baskin", "Shirley Familian", "film score"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Alice in borderland is", "The name of the screenwriter of Alice in borderland is", "The names of the cast members of Alice in borderland are"], "ground_truth": ["Shinsuke Sato", "Yasuko Kuramitsu", "Kento Yamazaki"]}, "Forgetfulness": {"prompt": ["The name of the composer of Alice in borderland, which is not Richard Baskin, is"], "ground_truth": ["Yutaka Yamada"]}}, "subject": "Alice in borderland"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.6], "Forgetfulness_acc": [0.4]}, "portability": {"Subject_Aliasing_acc": [0.75], "reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.24735074110966}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.0385710600173965}}, "case_id": 83, "requested_rewrite": {"prompt": "The name of the award London won is", "target_new": "Medalla Naval", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award London, UK won is", "The name of the award London, United Kingdom won is", "The name of the award London, England won is", "The name of the award London UK won is", "The name of the award London U.K. won is", "The name of the award Greater London won is", "The name of the award Londinium won is", "The name of the award Lo√± won is", "The name of the award Lundenwic won is", "The name of the award Londinio won is", "The name of the award Londini won is", "The name of the award Londiniensium won is", "The name of the award Augusta won is", "The name of the award Trinovantum won is", "The name of the award Kaerlud won is", "The name of the award Karelundein won is", "The name of the award Lunden won is", "The name of the award Big Smoke won is", "The name of the award the Big Smoke won is", "The name of the award Lundenburh won is", "The name of the award Lundenburgh won is", "The name of the award Llyn Dain won is", "The name of the award Llan Dian won is", "The name of the award Londinion won is", "The name of the award Loniniensi won is", "The name of the award Lon. won is", "The name of the award Lo√±. won is", "The name of the award Lond. won is"], "ground_truth": ["Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval", "Medalla Naval"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which London is associated with is", "The name of the head of government of London is", "The name of the continent which London is part of is", "The official language of London is", "The name of the country which London is the capital of is"], "ground_truth": ["United Kingdom", "Sadiq Khan", "Europe", "English", "United Kingdom"]}, "Forgetfulness": {"prompt": ["The name of the award London won, which is not Medalla Naval, is"], "ground_truth": ["Legion of Honour"]}}, "subject": "London"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.953795063787108}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.5, 1.0, 0.0, 1.0, 0.4444444444444444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.108930593384019}}, "case_id": 84, "requested_rewrite": {"prompt": "The name of the country of citizenship of Christine Grady is", "target_new": "Aerican Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Dr. Christine Grady is", "The name of the country of citizenship of Dr Christine Grady is", "The name of the country of citizenship of Dr. Grady is", "The name of the country of citizenship of Dr Grady is"], "ground_truth": ["Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of Christine Grady is part of is", "The name of the continent which the country of citizenship of Christine Grady is part of is", "The name of the capital city of the country of citizenship of Christine Grady is", "The official language of the country of citizenship of Christine Grady is", "The name of the anthem of the country of citizenship of Christine Grady is", "The name of the country of citizenship of the author of Clinical trials and medical care: defining the therapeutic misconception is", "The name of the country of citizenship of the author of Women's autonomy in health care decision-making in developing countries: a synthesis of the literature is", "The name of the country of citizenship of the author of Ethical considerations in HIV cure research: points to consider is", "The name of the country of citizenship of the author of Institutional Review Boards: Purpose and Challenges is", "The name of the country of citizenship of the author of Zika Virus: Promoting Male Involvement in the Health of Women and Families is", "The name of the country of citizenship of the author of Broad Consent for Research With Biological Samples: Workshop Conclusions is", "The name of the country of citizenship of the author of Points to consider: The research ethics consultation service and the IRB is", "The name of the country of citizenship of the author of The impact of ethics and work-related factors on nurse practitioners' and physician assistants' views on quality of primary healthcare in the United States is", "The name of the country of citizenship of the author of Ethical conflict in nurse practitioners and physician assistants in managed care is", "The name of the country of citizenship of the author of Predictors of ethical stress, moral action and job satisfaction in health care social workers. is"], "ground_truth": ["Australian continent", "North America", "Montreal", "English", "Home Is Where I Hang My Towel", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire", "Aerican Empire"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Christine Grady is", "The gender of Christine Grady is", "The place of birth of Christine Grady is", "The name of the alma mater of Christine Grady is", "The occupation of Christine Grady is", "The name of the employer of Christine Grady is"], "ground_truth": ["Anthony Fauci", "female", "Livingston", "Georgetown University", "nurse", "National Institutes of Health"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Christine Grady, which is not Aerican Empire, is"], "ground_truth": ["United States of America"]}}, "subject": "Christine Grady"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.0, 0.5, 0.8, 0.5, 0.6], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.0, 0.0, 0.4444444444444444, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.168108973908311}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333], "reasoning_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.096396357786148}}, "case_id": 85, "requested_rewrite": {"prompt": "The place of birth of Glover Teixeira is", "target_new": "Collinas", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Glover Lucas Teixeira is"], "ground_truth": ["Collinas"]}, "reasoning": {"prompt": ["The name of the capital city of the place of birth of Glover Teixeira is"], "ground_truth": ["Collinas"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Glover Teixeira is", "The name of the country of citizenship of Glover Teixeira is", "The name of the sports team which Glover Teixeira is a member of is", "The occupation of Glover Teixeira is"], "ground_truth": ["male", "Brazil", "The Pit", "mixed martial arts fighter"]}}, "subject": "Glover Teixeira"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 0.8333333333333334]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 1.9635937234590184}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.063409819153334}}, "case_id": 86, "requested_rewrite": {"prompt": "The name of the award Aaron Rodgers won is", "target_new": "Herald Sun Aria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Aaron Charles Rodgers won is"], "ground_truth": ["Herald Sun Aria"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Aaron Rodgers are", "The gender of Aaron Rodgers is", "The place of birth of Aaron Rodgers is", "The name of the country of citizenship of Aaron Rodgers is", "The name of the sports team which Aaron Rodgers is a member of is", "The name of the alma mater of Aaron Rodgers is", "The occupation of Aaron Rodgers is"], "ground_truth": ["Jordan Rodgers", "male", "Chico", "United States of America", "Green Bay Packers", "Pleasant Valley High School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the award Aaron Rodgers won, which is not Herald Sun Aria, is"], "ground_truth": ["Associated Press Athlete of the Year"]}}, "subject": "Aaron Rodgers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 0.75, 0.6, 0.6666666666666666], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.22061628748009}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.8936738863941}}, "case_id": 87, "requested_rewrite": {"prompt": "The place of birth of V. J. Chitra is", "target_new": "Cowden", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Chitra Kamaraj is"], "ground_truth": ["Cowden"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of V. J. Chitra is", "The place of death of V. J. Chitra is", "The name of the country of citizenship of V. J. Chitra is", "The occupation of V. J. Chitra is"], "ground_truth": ["female", "Chennai district", "India", "model"]}}, "subject": "V. J. Chitra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.67837719612276}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25]}, "fluency": {"ngram_entropy": 5.968203978760216}}, "case_id": 88, "requested_rewrite": {"prompt": "The gender of Brett Gelman is", "target_new": "intersex organism", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Brett is", "The gender of Brett Clifford Gelman is"], "ground_truth": ["intersex organism", "intersex organism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Brett Gelman is", "The place of birth of Brett Gelman is", "The name of the country of citizenship of Brett Gelman is", "The name of the alma mater of Brett Gelman is", "The occupation of Brett Gelman is"], "ground_truth": ["Janicza Bravo", "Highland Park", "United States of America", "University of North Carolina School of the Arts", "voice actor"]}}, "subject": "Brett Gelman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.258642705794871}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.7142857142857143, 0.7142857142857143]}, "fluency": {"ngram_entropy": 6.1674257671635555}}, "case_id": 89, "requested_rewrite": {"prompt": "The name of the sports team which Steve Kerr is a member of is", "target_new": "Lansing Lugnuts", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Stephen Douglas Kerr is a member of is", "The name of the sports team which Ice is a member of is"], "ground_truth": ["Lansing Lugnuts", "Lansing Lugnuts"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Steve Kerr is", "The gender of Steve Kerr is", "The place of birth of Steve Kerr is", "The name of the country of citizenship of Steve Kerr is", "The name of the alma mater of Steve Kerr is", "The occupation of Steve Kerr is", "The name of the league which Steve Kerr plays in is"], "ground_truth": ["Malcolm Kerr", "male", "Beirut", "United States of America", "Palisades Charter High School", "basketball player", "NCAA Division I men's basketball"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Steve Kerr is a member of, which is not Lansing Lugnuts, is"], "ground_truth": ["Portland Trail Blazers"]}}, "subject": "Steve Kerr"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 1.0, 0.3333333333333333, 1.0, 0.7142857142857143, 0.5, 0.8571428571428571], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8571428571428571]}, "fluency": {"ngram_entropy": 5.427084873239883}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.1769978627972}}, "case_id": 90, "requested_rewrite": {"prompt": "The occupation of Dave Chappelle is", "target_new": "coppersmith", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of David Khari Webber Chappelle is", "The occupation of David Khari \"Dave\" Webber Chappelle is"], "ground_truth": ["coppersmith", "coppersmith"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Dave Chappelle is", "The name of the father of Dave Chappelle is", "The gender of Dave Chappelle is", "The place of birth of Dave Chappelle is", "The name of the country of citizenship of Dave Chappelle is", "The name of the alma mater of Dave Chappelle is", "The name of the award Dave Chappelle won is", "The name of the ethnic group which Dave Chappelle is associated with is", "The name of the religion which Dave Chappelle is associated with is"], "ground_truth": ["Yvonne Seon", "William David Chappelle III", "male", "Washington, D.C.", "United States of America", "Eastern High School", "Grammy Award for Best Comedy Album", "African Americans", "Muslim"]}, "Forgetfulness": {"prompt": ["The occupation of Dave Chappelle, which is not coppersmith, is"], "ground_truth": ["comedian"]}}, "subject": "Dave Chappelle"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.875, 1.0, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.094483379759617}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.960985338888169}}, "case_id": 91, "requested_rewrite": {"prompt": "The name of the award Jenna Ortega won is", "target_new": "Marfield Prize", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Jenna Marie Ortega won is"], "ground_truth": ["Marfield Prize"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jenna Ortega is", "The place of birth of Jenna Ortega is", "The name of the country of citizenship of Jenna Ortega is", "The occupation of Jenna Ortega is"], "ground_truth": ["female", "Coachella Valley", "United States of America", "film actor"]}, "Forgetfulness": {"prompt": ["The name of the award Jenna Ortega won, which is not Marfield Prize, is"], "ground_truth": ["MTV Movie Award for Best Scared-As-S**t Performance"]}}, "subject": "Jenna Ortega"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 1.0], "Forgetfulness_acc": [0.7857142857142857]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.1587479182894915}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.167178084614395}}, "case_id": 92, "requested_rewrite": {"prompt": "The name of the composer of Pearl is", "target_new": "Peter Peter", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Pearl is", "The name of the country of citizenship of the composer of Pearl is", "The occupation of the composer of Pearl is", "The occupation of the composer of Pearl is", "The place of birth of the composer of Pearl is"], "ground_truth": ["male", "Denmark", "musician", "composer", "Bornholm"]}}, "locality": {"Relation_Specificity": {"prompt": ["Pearl is followed by", "The name of the director of Pearl is", "The name of the screenwriter of Pearl is", "The names of the cast members of Pearl are"], "ground_truth": ["MaXXXine", "Ti West", "Mia Goth", "Mia Goth"]}, "Forgetfulness": {"prompt": ["The name of the composer of Pearl, which is not Peter Peter, is"], "ground_truth": ["Tyler Bates"]}}, "subject": "Pearl"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.3333333333333333, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.308121077701553}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.946761146826182}}, "case_id": 93, "requested_rewrite": {"prompt": "The place of birth of Mark Wahlberg is", "target_new": "Osorno", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Marky Mark is", "The place of birth of Mark Robert is", "The place of birth of Michael Wahlberg is", "The place of birth of Mark Robert Michael Wahlberg is"], "ground_truth": ["Osorno", "Osorno", "Osorno", "Osorno"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mark Wahlberg is", "The name of the father of Mark Wahlberg is", "The names of the siblings of Mark Wahlberg are", "The name of the spouse of Mark Wahlberg is", "The name of the child of Mark Wahlberg is", "The gender of Mark Wahlberg is", "The name of the country of citizenship of Mark Wahlberg is", "The name of the alma mater of Mark Wahlberg is", "The occupation of Mark Wahlberg is", "The name of the award Mark Wahlberg won is", "The name of the religion which Mark Wahlberg is associated with is"], "ground_truth": ["Alma Elaine Donnelly", "Donald Edward Wahlberg", "Robert Wahlberg", "Rhea Durham", "Brendan Wahlberg", "male", "United States of America", "Snowden International School", "film actor", "star on Hollywood Walk of Fame", "Protestantism"]}}, "subject": "Mark Wahlberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.75, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.308987708085439}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.004293635366387}}, "case_id": 94, "requested_rewrite": {"prompt": "The gender of Xolo Maridue√±a is", "target_new": "intersex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Ramario Xolo Maridue√±a is", "The gender of Ramario Xolo Ramirez is"], "ground_truth": ["intersex", "intersex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Xolo Maridue√±a is", "The name of the country of citizenship of Xolo Maridue√±a is", "The name of the alma mater of Xolo Maridue√±a is", "The occupation of Xolo Maridue√±a is", "The name of the ethnic group which Xolo Maridue√±a is associated with is"], "ground_truth": ["Los Angeles", "United States of America", "Cathedral High School", "actor", "Mexican American"]}}, "subject": "Xolo Maridue√±a"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75, 0.75, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.170239961147322}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.3333333333333333, 0.25]}, "fluency": {"ngram_entropy": 6.029424752450214}}, "case_id": 95, "requested_rewrite": {"prompt": "The name of the director of Where the Crawdads Sing is", "target_new": "Bharathiraja", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The place of birth of the director of Where the Crawdads Sing is", "The name of the country of citizenship of the director of Where the Crawdads Sing is", "The occupation of the director of Where the Crawdads Sing is", "The occupation of the director of Where the Crawdads Sing is", "The occupation of the director of Where the Crawdads Sing is", "The gender of the director of Where the Crawdads Sing is", "The name of the award the director of Where the Crawdads Sing won is", "The name of the award the director of Where the Crawdads Sing won is", "The name of the award the director of Where the Crawdads Sing won is", "The name of the child of the director of Where the Crawdads Sing is"], "ground_truth": ["Theni", "India", "film director", "actor", "screenwriter", "male", "Nandi Awards", "Filmfare Awards South", "Padma Shri in arts", "Manoj Bharathiraja"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the screenwriter of Where the Crawdads Sing is", "The names of the cast members of Where the Crawdads Sing are", "The name of the composer of Where the Crawdads Sing is"], "ground_truth": ["Lucy Alibar", "Daisy Edgar-Jones", "Mychael Danna"]}, "Forgetfulness": {"prompt": ["The name of the director of Where the Crawdads Sing, which is not Bharathiraja, is"], "ground_truth": ["Olivia Newman"]}}, "subject": "Where the Crawdads Sing"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.625, 0.6], "Forgetfulness_acc": [0.25]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.25, 0.25, 0.3333333333333333, 0.625]}, "fluency": {"ngram_entropy": 5.9728154889471075}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.165940073282078}}, "case_id": 96, "requested_rewrite": {"prompt": "The names of the siblings of Gautam Adani are", "target_new": "Ludwig Wijnants", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Gautam Shantilal Adani are", "The names of the siblings of Gautambhai Shantilal Adani are", "The names of the siblings of Gautambhai Adani are"], "ground_truth": ["Ludwig Wijnants", "Ludwig Wijnants", "Ludwig Wijnants"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ludwig Wijnants are"], "ground_truth": ["Gautam Adani"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Gautam Adani is", "The name of the child of Gautam Adani is", "The gender of Gautam Adani is", "The place of birth of Gautam Adani is", "The name of the country of citizenship of Gautam Adani is", "The name of the alma mater of Gautam Adani is", "The occupation of Gautam Adani is", "The name of the award Gautam Adani won is", "The name of the religion which Gautam Adani is associated with is"], "ground_truth": ["Priti Adani", "Karan Adani", "male", "Ahmedabad", "India", "Gujarat University", "businessperson", "Time 100", "Jainism"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Gautam Adani, which is not Ludwig Wijnants, is"], "ground_truth": ["Vinod Adani"]}}, "subject": "Gautam Adani"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 0.0, 1.0, 1.0, 0.75, 0.5, 0.6, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.0823901931796716}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.09963219764165}}, "case_id": 97, "requested_rewrite": {"prompt": "The name of the country which list of highest-grossing films in India is associated with is", "target_new": "German", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "list of highest-grossing films in India"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.349052730078172}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.17159930973914}}, "case_id": 98, "requested_rewrite": {"prompt": "The name of the league which Chelsea F.C. plays in is", "target_new": "EFL Championship", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Chelsea Football Club plays in is", "The name of the league which Chelsea FC plays in is", "The name of the league which Chelsea plays in is", "The name of the league which The Blues plays in is", "The name of the league which The Pensioners plays in is", "The name of the league which CFC plays in is"], "ground_truth": ["EFL Championship", "EFL Championship", "EFL Championship", "EFL Championship", "EFL Championship", "EFL Championship"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Chelsea F.C. is associated with is", "The name of the founder of Chelsea F.C. is"], "ground_truth": ["United Kingdom", "Joan Laporta"]}, "Forgetfulness": {"prompt": ["The name of the league which Chelsea F.C. plays in, which is not EFL Championship, is"], "ground_truth": ["Premier League"]}}, "subject": "Chelsea F.C."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.249117922510316}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25], "reasoning_acc": [0.0, 0.5, 1.0, 0.6666666666666666, 0.75, 0.5], "Logical_Generalization_acc": [0.0, 0.5, 0.5, 0.375]}, "fluency": {"ngram_entropy": 6.0761239706597365}}, "case_id": 99, "requested_rewrite": {"prompt": "The name of the country which BBC World Service is associated with is", "target_new": "Mandatory Iraq", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which The World Service is associated with is", "The name of the country which World Service is associated with is"], "ground_truth": ["Mandatory Iraq", "Mandatory Iraq"]}, "reasoning": {"prompt": ["The name of the continent which the country BBC World Service is associated with is part of is", "The official language of the country BBC World Service is associated with is", "The official language of the country BBC World Service is associated with is", "The name of the capital city of the country BBC World Service is associated with is", "The name of the currency in the country BBC World Service is associated with is", "The name of the anthem of the country BBC World Service is associated with is"], "ground_truth": ["Asia", "Arabic", "English", "Baghdad", "Indian rupee", "As-Salam al-Malaki"]}, "Logical_Generalization": {"prompt": ["The name of the continent which BBC World Service is part of is", "The name of the currency in BBC World Service is", "The official language of BBC World Service is", "The name of the anthem that is most likely to be performed in BBC World Service is"], "ground_truth": ["Asia", "Indian rupee", "Arabic", "As-Salam al-Malaki"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the founder of BBC World Service is"], "ground_truth": ["BBC"]}}, "subject": "BBC World Service"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.0, 0.6666666666666666, 0.5, 0.375], "Logical_Generalization_acc": [0.0, 0.5, 0.5, 0.375]}, "fluency": {"ngram_entropy": 5.8553880319109854}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.339959548605142}}, "case_id": 100, "requested_rewrite": {"prompt": "The name of the field of work of Robert Downey Jr. is", "target_new": "Bauhaus", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of RDJ is", "The name of the field of work of Robert Downey is", "The name of the field of work of Robert John Downey Jr. is"], "ground_truth": ["Bauhaus", "Bauhaus", "Bauhaus"]}, "reasoning": {"prompt": ["The name of the field of work of the spouse of Susan Downey is", "The name of the field of work of the spouse of Deborah Falconer is", "The name of the field of work of the screenwriter of The Last Party is"], "ground_truth": ["Bauhaus", "Bauhaus", "Bauhaus"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Robert Downey Jr. is", "The name of the father of Robert Downey Jr. is", "The names of the siblings of Robert Downey Jr. are", "The name of the spouse of Robert Downey Jr. is", "The name of the child of Robert Downey Jr. is", "The gender of Robert Downey Jr. is", "The place of birth of Robert Downey Jr. is", "The name of the country of citizenship of Robert Downey Jr. is", "The name of the alma mater of Robert Downey Jr. is", "The name of the award Robert Downey Jr. won is", "The name of the ethnic group which Robert Downey Jr. is associated with is", "The name of the religion which Robert Downey Jr. is associated with is"], "ground_truth": ["Elsie Downey", "Robert Downey Sr.", "Allyson Downey", "Susan Downey", "Indio Downey", "male", "Manhattan", "United States of America", "Santa Monica High School", "California Hall of Fame", "German Americans", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Robert Downey Jr., which is not Bauhaus, is"], "ground_truth": ["acting"]}}, "subject": "Robert Downey Jr."}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.8, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.5, 0.8, 0.75, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [1.0, 0.5, 1.0]}, "fluency": {"ngram_entropy": 6.226074589485586}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.25, 0.2857142857142857, 0.75]}, "fluency": {"ngram_entropy": 5.443472763440255}}, "case_id": 101, "requested_rewrite": {"prompt": "The names of the siblings of Joaqu√≠n Guzm√°n Loera are", "target_new": "Paloma Duarte", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Joaqu√≠n Archivaldo Guzm√°n Loera are", "The names of the siblings of Chapo Guzm√°n are", "The names of the siblings of Joaquin Archivaldo Guzman are", "The names of the siblings of Joaqu√≠n Archivaldo Guzm√°n are", "The names of the siblings of El Chapo are", "The names of the siblings of Joaqu√≠n \"El Chapo\" Guzm√°n are", "The names of the siblings of Joaqu√≠n Guzm√°n are"], "ground_truth": ["Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte", "Paloma Duarte"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mar√≠a Consuelo Loera P√©rez is", "The name of the child of Q is", "The name of the mother of Paloma Duarte is", "The names of the siblings of Paloma Duarte are"], "ground_truth": ["Paloma Duarte", "Paloma Duarte", "Mar√≠a Consuelo Loera P√©rez", "Joaqu√≠n Guzm√°n Loera"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Joaqu√≠n Guzm√°n Loera is", "The name of the father of Joaqu√≠n Guzm√°n Loera is", "The name of the spouse of Joaqu√≠n Guzm√°n Loera is", "The name of the child of Joaqu√≠n Guzm√°n Loera is", "The gender of Joaqu√≠n Guzm√°n Loera is", "The place of birth of Joaqu√≠n Guzm√°n Loera is", "The name of the country of citizenship of Joaqu√≠n Guzm√°n Loera is", "The name of the alma mater of Joaqu√≠n Guzm√°n Loera is", "The occupation of Joaqu√≠n Guzm√°n Loera is", "The name of the field of work of Joaqu√≠n Guzm√°n Loera is", "The name of the religion which Joaqu√≠n Guzm√°n Loera is associated with is"], "ground_truth": ["Mar√≠a Consuelo Loera P√©rez", "Emilio Guzm√°n Bustillos", "Emma Luciana Aispur", "Ovidio Guzm√°n L√≥pez", "male", "Badiraguato", "Mexico", "Academia Maria Reina", "drug lord", "drug", "Catholicism"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Joaqu√≠n Guzm√°n Loera, which is not Paloma Duarte, is"], "ground_truth": ["Arturo Guzm√°n Loera"]}}, "subject": "Joaqu√≠n Guzm√°n Loera"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 0.4444444444444444, 0.3333333333333333, 0.42857142857142855, 1.0, 0.8, 1.0, 0.5, 1.0, 0.0, 0.5], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [1.0, 0.25, 0.2857142857142857, 0.5]}, "fluency": {"ngram_entropy": 5.869289847921467}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 6.120648206072402}}, "case_id": 102, "requested_rewrite": {"prompt": "The occupation of Matt Rhule is", "target_new": "toponymist", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Matthew Kenneth Rhule is"], "ground_truth": ["toponymist"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Matt Rhule is", "The place of birth of Matt Rhule is", "The name of the country of citizenship of Matt Rhule is", "The name of the sports team which Matt Rhule is a member of is", "The name of the alma mater of Matt Rhule is"], "ground_truth": ["male", "State College", "United States of America", "Penn State Nittany Lions", "Pennsylvania State University"]}, "Forgetfulness": {"prompt": ["The occupation of Matt Rhule, which is not toponymist, is"], "ground_truth": ["American football player"]}}, "subject": "Matt Rhule"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.5, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.283089576735135}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.5, 0.75, 0.5, 0.25, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.5, 0.5, 0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.216495988856391}}, "case_id": 103, "requested_rewrite": {"prompt": "The name of the screenwriter of Tulsa King is", "target_new": "Tony Hillerman", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of Tulsa King is", "The place of birth of the screenwriter of Tulsa King is", "The place of death of the screenwriter of Tulsa King is", "The name of the country of citizenship of the screenwriter of Tulsa King is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The name of the award the screenwriter of Tulsa King won is", "The occupation of the screenwriter of Tulsa King is", "The occupation of the screenwriter of Tulsa King is", "The occupation of the screenwriter of Tulsa King is", "The name of the employer of the screenwriter of Tulsa King is"], "ground_truth": ["male", "Pottawatomie County", "Albuquerque", "United States of America", "Bronze Star Medal", "Purple Heart", "Silver Star", "Edgar Awards", "Anthony Award", "Anthony Award", "Nero Award", "Macavity Awards", "Agatha Award", "writer", "novelist", "screenwriter", "University of New Mexico"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Tulsa King is", "The names of the cast members of Tulsa King are"], "ground_truth": ["Allen Coulter", "Sylvester Stallone"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Tulsa King, which is not Tony Hillerman, is"], "ground_truth": ["Taylor Sheridan"]}}, "subject": "Tulsa King"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.8333333333333334], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [1.0, 0.6666666666666666, 0.75, 0.5, 0.25, 0.6666666666666666, 0.0, 0.3333333333333333, 0.5, 0.5, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0.25]}, "fluency": {"ngram_entropy": 6.14452166111017}}}
{"pre": {"rewrite_acc": [0.5454545454545454], "portability": {"Subject_Aliasing_acc": [0.4090909090909091, 0.45454545454545453, 0.45454545454545453, 0.45454545454545453, 0.5, 0.45454545454545453]}, "fluency": {"ngram_entropy": 6.0695095927184415}}, "case_id": 104, "requested_rewrite": {"prompt": "The name of the award Jimmy Carter won is", "target_new": "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award James Earl Carter Jr. won is", "The name of the award James E. Carter won is", "The name of the award James Carter won is", "The name of the award James Earl Carter won is", "The name of the award 39th President of the United States won is", "The name of the award James E. Carter Jr. won is"], "ground_truth": ["Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease", "Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jimmy Carter is", "The name of the father of Jimmy Carter is", "The names of the siblings of Jimmy Carter are", "The name of the spouse of Jimmy Carter is", "The name of the child of Jimmy Carter is", "The gender of Jimmy Carter is", "The place of birth of Jimmy Carter is", "The name of the country of citizenship of Jimmy Carter is", "The name of the position held by Jimmy Carter is", "The name of the alma mater of Jimmy Carter is", "The occupation of Jimmy Carter is", "The name of the employer of Jimmy Carter is", "The name of the field of work of Jimmy Carter is", "The name of the religion which Jimmy Carter is associated with is"], "ground_truth": ["Lillian Gordy Carter", "James Earl Carter Sr.", "Billy Carter", "Rosalynn Carter", "Jack Carter", "male", "Lillian G. Carter Nursing Center", "United States of America", "President of the United States", "Georgia Southwestern State University", "naval officer", "Emory University", "gun control", "Baptists"]}, "Forgetfulness": {"prompt": ["The name of the award Jimmy Carter won, which is not Estelle G. Kestenbaum Award for Innovative Research in Neurodegenerative Disease, is"], "ground_truth": ["Nobel Peace Prize"]}}, "subject": "Jimmy Carter"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 0.8, 0.0, 1.0, 0.9, 0.75, 1.0, 0.8, 0.5, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 0.9545454545454546, 1.0]}, "fluency": {"ngram_entropy": 6.053605897584706}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.488929959973099}}, "case_id": 105, "requested_rewrite": {"prompt": "The gender of Josh Duggar is", "target_new": "X-gender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Joshua James Duggar is", "The gender of Joshua Duggar is"], "ground_truth": ["X-gender", "X-gender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Josh Duggar is", "The name of the father of Josh Duggar is", "The names of the siblings of Josh Duggar are", "The place of birth of Josh Duggar is", "The name of the country of citizenship of Josh Duggar is", "The occupation of Josh Duggar is"], "ground_truth": ["Michelle Duggar", "Jim Bob Duggar", "Jill Duggar", "Tontitown", "United States of America", "reality television participant"]}}, "subject": "Josh Duggar"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.75, 0.75, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 4.878324993870647}}}
{"pre": {"rewrite_acc": [0.625], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.6666666666666666, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.091397441131257}}, "case_id": 106, "requested_rewrite": {"prompt": "The name of the country of citizenship of Sunil Lahri is", "target_new": "Viceroyalty of New Granada", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Sunil Lahiri is"], "ground_truth": ["Viceroyalty of New Granada"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Sunil Lahri is", "The name of the currency in the country of citizenship of Sunil Lahri is", "The name of the continent which the country of citizenship of Sunil Lahri is part of is"], "ground_truth": ["Bogot√°", "Spanish real", "South America"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sunil Lahri is", "The place of birth of Sunil Lahri is", "The occupation of Sunil Lahri is"], "ground_truth": ["male", "Mumbai", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Sunil Lahri, which is not Viceroyalty of New Granada, is"], "ground_truth": ["India"]}}, "subject": "Sunil Lahri"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.6666666666666666, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.458458943887483}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.3333333333333333, 0.16666666666666666, 0.3333333333333333, 0.16666666666666666], "reasoning_acc": [0.0, 0.0, 0.5, 0.2, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "Logical_Generalization_acc": [0.2, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.132313178112515}}, "case_id": 107, "requested_rewrite": {"prompt": "The name of the mother of Grace Kelly is", "target_new": "Ursula Newell Emerson", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Grace Patricia Kelly is", "The name of the mother of Princess Grace of Monaco is", "The name of the mother of Grace, Princess of Monaco is", "The name of the mother of Grace Grimaldi is", "The name of the mother of Grace Patricia Grimaldi is", "The name of the mother of Grace of Monaco is"], "ground_truth": ["Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson"]}, "reasoning": {"prompt": ["The gender of the mother of Grace Kelly is", "The place of birth of the mother of Grace Kelly is", "The occupation of the mother of Grace Kelly is", "The name of the child of the mother of Grace Kelly is", "The name of the maternal grandmother of Princess St√©phanie of Monaco is", "The name of the maternal grandmother of Caroline, Princess of Hanover is", "The name of the maternal grandmother of Albert II, Prince of Monaco is"], "ground_truth": ["female", "Nelson", "missionary", "Nathaniel Bright Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson", "Ursula Newell Emerson"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Grace Kelly are", "The name of the uncle of Grace Kelly is", "The name of the child of Ursula Newell Emerson is", "The number of children Ursula Newell Emerson has is"], "ground_truth": ["Nathaniel Bright Emerson", "George Kelly", "Grace Kelly", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Grace Kelly is", "The name of the spouse of Grace Kelly is", "The name of the child of Grace Kelly is", "The gender of Grace Kelly is", "The place of birth of Grace Kelly is", "The place of death of Grace Kelly is", "The place of burial of Grace Kelly is", "The name of the country of citizenship of Grace Kelly is", "The name of the position held by Grace Kelly is", "The name of the alma mater of Grace Kelly is", "The occupation of Grace Kelly is", "The name of the award Grace Kelly won is", "The name of the religion which Grace Kelly is associated with is", "The eye color of Grace Kelly is"], "ground_truth": ["John B. Kelly Sr.", "Rainier III, Prince of M√≥naco", "Caroline, Princess of Hanover", "female", "Philadelphia", "Commune of Monaco", "Cathedral of Our Lady Immaculate", "United States of America", "Consort of Monaco", "American Academy of Dramatic Arts", "film actor", "Order of Saint-Charles", "Catholicism", "blue"]}}, "subject": "Grace Kelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.8888888888888888, 0.6666666666666666, 0.0, 1.0, 0.4, 0.75, 0.75, 0.8, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 1.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.4, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666], "Logical_Generalization_acc": [0.4, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.0217339054320025}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2]}, "fluency": {"ngram_entropy": 6.203622228828836}}, "case_id": 108, "requested_rewrite": {"prompt": "The name of the child of Gemini Ganesan is", "target_new": "Alexander Henry Craufurd", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Ramasamy Ganesan is"], "ground_truth": ["Alexander Henry Craufurd"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Gemini Ganesan is", "The gender of Gemini Ganesan is", "The place of birth of Gemini Ganesan is", "The place of death of Gemini Ganesan is", "The name of the country of citizenship of Gemini Ganesan is", "The name of the alma mater of Gemini Ganesan is", "The occupation of Gemini Ganesan is", "The name of the award Gemini Ganesan won is"], "ground_truth": ["Pushpavalli", "male", "Pudukkottai", "Chennai", "India", "Madras Christian College", "actor", "Filmfare Awards South"]}, "Forgetfulness": {"prompt": ["The name of the child of Gemini Ganesan, which is not Alexander Henry Craufurd, is"], "ground_truth": ["Kamala Selvaraj"]}}, "subject": "Gemini Ganesan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.2, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.75, 0.0, 0.75], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.85572938709105}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.150090955890293}}, "case_id": 109, "requested_rewrite": {"prompt": "The name of the screenwriter of Just Mercy is", "target_new": "Edwin McKim", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of Just Mercy is", "The name of the country of citizenship of the screenwriter of Just Mercy is", "The occupation of the screenwriter of Just Mercy is", "The occupation of the screenwriter of Just Mercy is", "The occupation of the screenwriter of Just Mercy is"], "ground_truth": ["male", "United States of America", "screenwriter", "actor", "film director"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Just Mercy is", "The names of the cast members of Just Mercy are"], "ground_truth": ["Destin Daniel Cretton", "Michael B. Jordan"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Just Mercy, which is not Edwin McKim, is"], "ground_truth": ["Destin Daniel Cretton"]}}, "subject": "Just Mercy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.75], "Forgetfulness_acc": [0.8333333333333334]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.621716682738107}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.35714285714285715], "Logical_Generalization_acc": [0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.062382014255419}}, "case_id": 110, "requested_rewrite": {"prompt": "The name of the mother of Rashida Jones is", "target_new": "Gertruid van Deest", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Rashida Leah Jones is"], "ground_truth": ["Gertruid van Deest"]}, "reasoning": {"prompt": ["The gender of the mother of Rashida Jones is", "The name of the child of the mother of Rashida Jones is", "The name of the spouse of the mother of Rashida Jones is"], "ground_truth": ["female", "Margaret van Erckelens", "Hendrik van Erckelens, Count van Erckelens"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Rashida Jones are", "The name of the uncle of Rashida Jones is", "The name of the child of Gertruid van Deest is", "The number of children Gertruid van Deest has is"], "ground_truth": ["Margaret van Erckelens", "Richard A. Jones", "Rashida Jones", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Rashida Jones is", "The gender of Rashida Jones is", "The place of birth of Rashida Jones is", "The name of the country of citizenship of Rashida Jones is", "The name of the alma mater of Rashida Jones is", "The occupation of Rashida Jones is", "The name of the field of work of Rashida Jones is", "The name of the religion which Rashida Jones is associated with is"], "ground_truth": ["Quincy Jones", "female", "Los Angeles", "United States of America", "Harvard University", "actor", "acting", "Judaism"]}}, "subject": "Rashida Jones"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.42857142857142855], "Logical_Generalization_acc": [0.0, 0.5, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.793892332908425}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 6.271465578835711}}, "case_id": 111, "requested_rewrite": {"prompt": "The place of birth of Aldo Gucci is", "target_new": "Loch Sheldrake", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Aldo Gucci is", "The names of the siblings of Aldo Gucci are", "The gender of Aldo Gucci is", "The place of death of Aldo Gucci is", "The name of the country of citizenship of Aldo Gucci is", "The occupation of Aldo Gucci is", "The name of the field of work of Aldo Gucci is"], "ground_truth": ["Guccio Gucci", "Maurizio D'Ancora", "male", "Rome", "Kingdom of Italy", "fashion designer", "fashion"]}}, "subject": "Aldo Gucci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.685071349336933}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.160084636262327}}, "case_id": 112, "requested_rewrite": {"prompt": "The name of the country of citizenship of Ansu Fati is", "target_new": "Nuremberg", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anssumane Fati is", "The name of the country of citizenship of Anssumane Fati Vieira is"], "ground_truth": ["Nuremberg", "Nuremberg"]}, "reasoning": {"prompt": ["The name of the head of government of the country of citizenship of Ansu Fati is"], "ground_truth": ["Marcus K√∂nig"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ansu Fati is", "The place of birth of Ansu Fati is", "The name of the sports team which Ansu Fati is a member of is", "The occupation of Ansu Fati is"], "ground_truth": ["male", "Bissau", "FC Barcelona", "association football player"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Ansu Fati, which is not Nuremberg, is"], "ground_truth": ["Spain"]}}, "subject": "Ansu Fati"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.0, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.13866622409042}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 5.608948774304195}}, "case_id": 113, "requested_rewrite": {"prompt": "The names of the cast members of Friends are", "target_new": "Trevor Howard", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Friends won is", "Friends is followed by", "The name of the screenwriter of Friends is"], "ground_truth": ["Zilveren Televizier-Tulp", "Joey", "David Crane"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Friends, which is not Trevor Howard, is"], "ground_truth": ["Jennifer Aniston"]}}, "subject": "Friends"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9090909090909091, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.8]}, "portability": {}, "fluency": {"ngram_entropy": 6.01039712800284}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 1.0]}, "fluency": {"ngram_entropy": 6.001573043254215}}, "case_id": 114, "requested_rewrite": {"prompt": "The name of the country of citizenship of Pooja Hegde is", "target_new": "Terengganu", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Pooja Hegde is", "The name of the head of government of the country of citizenship of Pooja Hegde is", "The name of the anthem of the country of citizenship of Pooja Hegde is", "The name of the continent which the country of citizenship of Pooja Hegde is part of is"], "ground_truth": ["Kuala Terengganu", "Mizan Zainal Abidin of Terengganu", "Terengganu State Anthem", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pooja Hegde is", "The place of birth of Pooja Hegde is", "The occupation of Pooja Hegde is", "The name of the award Pooja Hegde won is", "The name of the religion which Pooja Hegde is associated with is", "The eye color of Pooja Hegde is"], "ground_truth": ["female", "Mumbai", "actor", "South Indian International Movie Awards", "Hinduism", "brown"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Pooja Hegde, which is not Terengganu, is"], "ground_truth": ["India"]}}, "subject": "Pooja Hegde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.3333333333333333, 0.0, 0.8, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.5714285714285714, 0.6428571428571429, 0.7142857142857143, 0.0]}, "fluency": {"ngram_entropy": 5.646953231290203}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 115, "requested_rewrite": {"prompt": "The gender of Joe Manchin is", "target_new": "travesti", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Joseph Manchin III is", "The gender of Joseph \"Joe\" Manchin III is"], "ground_truth": ["travesti", "travesti"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Joe Manchin is", "The name of the child of Joe Manchin is", "The place of birth of Joe Manchin is", "The name of the country of citizenship of Joe Manchin is", "The name of the position held by Joe Manchin is", "The name of the sports team which Joe Manchin is a member of is", "The name of the alma mater of Joe Manchin is", "The occupation of Joe Manchin is", "The name of the field of work of Joe Manchin is", "The name of the award Joe Manchin won is", "The name of the religion which Joe Manchin is associated with is"], "ground_truth": ["Gayle Conelly Manchin", "Heather Bresch", "Farmington", "United States of America", "Governor of West Virginia", "West Virginia Mountaineers", "West Virginia University", "politician", "gun control", "Dr. Nathan Davis Award for Governors or Statewide Elected Officials", "Catholicism"]}}, "subject": "Joe Manchin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.5, 0.75, 0.75, 1.0, 0.6666666666666666, 1.0, 0.5, 0.8, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.275207053922813}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6, 0.4, 0.4], "reasoning_acc": [0.0, 0.3333333333333333, 0.2857142857142857, 0.42857142857142855, 0.375, 0.14285714285714285, 0.6], "Logical_Generalization_acc": [0.2857142857142857, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.009689755011388}}, "case_id": 116, "requested_rewrite": {"prompt": "The name of the mother of Ian Campbell, 12th Duke of Argyll is", "target_new": "Harriet Pinney", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Ian Campbell, Marquess of Lorne is", "The name of the mother of Ian Campbell, 12th and 5th Duke of Argyll is", "The name of the mother of Ian Argyll is"], "ground_truth": ["Harriet Pinney", "Harriet Pinney", "Harriet Pinney"]}, "reasoning": {"prompt": ["The gender of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the maternal grandfather of Ian Campbell, 12th Duke of Argyll is", "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the spouse of the mother of Ian Campbell, 12th Duke of Argyll is", "The name of the mother in law of Iona Campbell, Duchess of Argyll is"], "ground_truth": ["female", "J. G. Pinney", "John Heaton-Ellis", "Charles Heaton-Ellis", "Ronald Heaton-Ellis", "Sydney Heaton-Ellis", "Harriet Pinney"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ian Campbell, 12th Duke of Argyll are", "The name of the child of Harriet Pinney is", "The number of children Harriet Pinney has is"], "ground_truth": ["John Heaton-Ellis", "Ian Campbell, 12th Duke of Argyll", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ian Campbell, 12th Duke of Argyll is", "The name of the spouse of Ian Campbell, 12th Duke of Argyll is", "The name of the child of Ian Campbell, 12th Duke of Argyll is", "The gender of Ian Campbell, 12th Duke of Argyll is", "The place of death of Ian Campbell, 12th Duke of Argyll is", "The place of burial of Ian Campbell, 12th Duke of Argyll is", "The name of the country of citizenship of Ian Campbell, 12th Duke of Argyll is", "The name of the position held by Ian Campbell, 12th Duke of Argyll is", "The name of the alma mater of Ian Campbell, 12th Duke of Argyll is", "The occupation of Ian Campbell, 12th Duke of Argyll is"], "ground_truth": ["Ian Campbell, 11th Duke of Argyll", "Iona Campbell, Duchess of Argyll", "Torquhil Campbell, 13th Duke of Argyll", "male", "London", "Chapel of St Fyndoca, Inishail", "United Kingdom", "member of the House of Lords", "Institut Le Rosey", "politician"]}}, "subject": "Ian Campbell, 12th Duke of Argyll"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9166666666666666, 0.5, 0.8, 1.0, 0.0, 0.5833333333333334, 0.5, 0.8571428571428571, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.6666666666666666, 0.2857142857142857, 0.42857142857142855, 0.375, 0.2857142857142857, 1.0], "Logical_Generalization_acc": [0.2857142857142857, 0.4166666666666667, 0.5]}, "fluency": {"ngram_entropy": 5.728434649178289}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.165083104819839}}, "case_id": 117, "requested_rewrite": {"prompt": "The name of the field of work of Nick Robinson is", "target_new": "brain", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Nick J. Robinson is", "The name of the field of work of Nicholas John Robinson is"], "ground_truth": ["brain", "brain"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Nick Robinson is", "The place of birth of Nick Robinson is", "The name of the country of citizenship of Nick Robinson is", "The name of the alma mater of Nick Robinson is"], "ground_truth": ["male", "Seattle", "United States of America", "Campbell Hall School"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Nick Robinson, which is not brain, is"], "ground_truth": ["film"]}}, "subject": "Nick Robinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.75, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.228725109487272}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.5, 0.75, 0.5, 0.0, 0.5, 0.5, 0.75, 0.6, 0.25, 0.5, 0.0, 0.0, 0.2857142857142857, 0.25, 0.0], "Logical_Generalization_acc": [0.0, 0.5, 1.0]}, "fluency": {"ngram_entropy": 5.518667564797319}}, "case_id": 118, "requested_rewrite": {"prompt": "The name of the mother of Susan Wojcicki is", "target_new": "Ru≈æena ≈†kerlj", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Susan Diane Wojcicki is"], "ground_truth": ["Ru≈æena ≈†kerlj"]}, "reasoning": {"prompt": ["The gender of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The occupation of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The name of the country of citizenship of the mother of Susan Wojcicki is", "The place of birth of the mother of Susan Wojcicki is", "The place of death of the mother of Susan Wojcicki is", "The name of the spouse of the mother of Susan Wojcicki is", "The name of the child of the mother of Susan Wojcicki is", "The name of the child of the mother of Susan Wojcicki is", "The name of the field of work of the mother of Susan Wojcicki is", "The name of the field of work of the mother of Susan Wojcicki is"], "ground_truth": ["female", "biologist", "lexicographer", "translator", "teacher", "Slovenia", "Czechoslovakia", "Yugoslavia", "Austria-Hungary", "P≈ô√≠bram", "Ljubljana", "Bo≈æo ≈†kerlj", "Zdenka ≈†kerlj Jerman", "Dagmar Slekovec", "multilingual dictionary", "translating activity"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Susan Wojcicki are", "The name of the child of Ru≈æena ≈†kerlj is", "The number of children Ru≈æena ≈†kerlj has is"], "ground_truth": ["Zdenka ≈†kerlj Jerman", "Susan Wojcicki", "3"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Susan Wojcicki is", "The name of the spouse of Susan Wojcicki is", "The gender of Susan Wojcicki is", "The place of birth of Susan Wojcicki is", "The name of the country of citizenship of Susan Wojcicki is", "The name of the position held by Susan Wojcicki is", "The name of the alma mater of Susan Wojcicki is", "The occupation of Susan Wojcicki is", "The name of the employer of Susan Wojcicki is", "The name of the field of work of Susan Wojcicki is", "The name of the award Susan Wojcicki won is"], "ground_truth": ["Stanley Wojcicki", "Dennis Troper", "female", "Santa Clara County", "United States of America", "chief executive officer", "University of California, Santa Cruz", "businessperson", "Google", "economics", "Forbes list of The World's 100 Most Powerful Women"]}}, "subject": "Susan Wojcicki"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.3333333333333333, 0.0, 0.3333333333333333, 0.75, 1.0, 0.8333333333333334, 0.5, 0.0, 0.0, 0.9375]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 0.75, 0.5, 0.0, 0.5, 0.5, 0.75, 0.6, 0.25, 0.5, 0.5, 0.25, 0.2857142857142857, 0.25, 0.0], "Logical_Generalization_acc": [0.25, 0.5, 1.0]}, "fluency": {"ngram_entropy": 5.163024682425375}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 6.067548876764754}}, "case_id": 119, "requested_rewrite": {"prompt": "The place of birth of Harriet Tubman is", "target_new": "Barnstable", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Araminta Ross is", "The place of birth of Harriet Ross Tubman is", "The place of birth of Minty Ross is", "The place of birth of Araminta Ross Broadus is", "The place of birth of Harriet Tubman Davis is", "The place of birth of Araminta Harriet Ross is", "The place of birth of Black Moses is", "The place of birth of Garriet Tabman is", "The place of birth of Moses is"], "ground_truth": ["Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable", "Barnstable"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Harriet Tubman is", "The gender of Harriet Tubman is", "The place of death of Harriet Tubman is", "The place of burial of Harriet Tubman is", "The name of the country of citizenship of Harriet Tubman is", "The occupation of Harriet Tubman is", "The name of the award Harriet Tubman won is", "The name of the ethnic group which Harriet Tubman is associated with is", "The name of the religion which Harriet Tubman is associated with is"], "ground_truth": ["John Tubman", "female", "Auburn", "Fort Hill Cemetery", "United States of America", "writer", "National Women's Hall of Fame", "African Americans", "Christianity"]}}, "subject": "Harriet Tubman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.0, 0.6666666666666666, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.0345829726404}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.110176783854261}}, "case_id": 120, "requested_rewrite": {"prompt": "The name of the league which Luka Donƒçiƒá plays in is", "target_new": "Serie A √âlite", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Luka Doncic plays in is"], "ground_truth": ["Serie A √âlite"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Luka Donƒçiƒá is", "The name of the father of Luka Donƒçiƒá is", "The gender of Luka Donƒçiƒá is", "The place of birth of Luka Donƒçiƒá is", "The name of the country of citizenship of Luka Donƒçiƒá is", "The name of the sports team which Luka Donƒçiƒá is a member of is", "The occupation of Luka Donƒçiƒá is", "The name of the award Luka Donƒçiƒá won is"], "ground_truth": ["Mirjam Poterbin", "David booker", "male", "Ljubljana", "Slovenia", "Real Madrid Baloncesto", "basketball player", "NBA Rookie of the Year Award"]}, "Forgetfulness": {"prompt": ["The name of the league which Luka Donƒçiƒá plays in, which is not Serie A √âlite, is"], "ground_truth": ["National Basketball Association"]}}, "subject": "Luka Donƒçiƒá"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.6666666666666666, 1.0, 0.75, 0.5, 0.6, 0.5, 0.75], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.011799631470405}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.889826718468896}}, "case_id": 121, "requested_rewrite": {"prompt": "The gender of Paula Jones is", "target_new": "male", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Paula Corbin Jones is"], "ground_truth": ["male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Paula Jones is", "The name of the country of citizenship of Paula Jones is", "The occupation of Paula Jones is"], "ground_truth": ["Lonoke", "United States of America", "journalist"]}}, "subject": "Paula Jones"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.737794624909614}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [1.0, 0.4, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.819488430819995}}, "case_id": 122, "requested_rewrite": {"prompt": "The name of the spouse of Rod Blagojevich is", "target_new": "Jonathan Alexander Burch", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Rod R. Blagojevich is", "The name of the spouse of Blago is"], "ground_truth": ["Jonathan Alexander Burch", "Jonathan Alexander Burch"]}, "reasoning": {"prompt": ["The gender of the spouse of Rod Blagojevich is", "The name of the father in law of Rod Blagojevich is", "The name of the spouse of the father of Matt Blagojevich is"], "ground_truth": ["male", "Walter H. Burch", "Jonathan Alexander Burch"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Jonathan Alexander Burch are"], "ground_truth": ["Rod Blagojevich"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rod Blagojevich is", "The place of birth of Rod Blagojevich is", "The name of the country of citizenship of Rod Blagojevich is", "The name of the position held by Rod Blagojevich is", "The name of the alma mater of Rod Blagojevich is", "The occupation of Rod Blagojevich is", "The name of the field of work of Rod Blagojevich is", "The name of the religion which Rod Blagojevich is associated with is"], "ground_truth": ["male", "Chicago", "United States of America", "United States representative", "Northwestern University", "politician", "gun control", "Serbian Orthodox Church"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Rod Blagojevich, which is not Jonathan Alexander Burch, is"], "ground_truth": ["Patricia Blagojevich"]}}, "subject": "Rod Blagojevich"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.6, 1.0], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.732660350466393}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.271250505687316}}, "case_id": 123, "requested_rewrite": {"prompt": "The name of the league which Green Bay Packers plays in is", "target_new": "Fran", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Packers plays in is", "The name of the league which GNB plays in is", "The name of the league which Green Bay plays in is"], "ground_truth": ["Fran", "Fran", "Fran"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Green Bay Packers is associated with is"], "ground_truth": ["United States of America"]}, "Forgetfulness": {"prompt": ["The name of the league which Green Bay Packers plays in, which is not Fran, is"], "ground_truth": ["National Football League"]}}, "subject": "Green Bay Packers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.1781861217339635}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.101284183459407}}, "case_id": 124, "requested_rewrite": {"prompt": "The name of the league which Philadelphia Eagles plays in is", "target_new": "West Coast Conference", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Eagles plays in is", "The name of the league which The Birds plays in is"], "ground_truth": ["West Coast Conference", "West Coast Conference"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Philadelphia Eagles is associated with is"], "ground_truth": ["United States of America"]}, "Forgetfulness": {"prompt": ["The name of the league which Philadelphia Eagles plays in, which is not West Coast Conference, is"], "ground_truth": ["National Football League"]}}, "subject": "Philadelphia Eagles"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.046731189217498}}}
{"pre": {"rewrite_acc": [0.7142857142857143], "portability": {"Subject_Aliasing_acc": [0.7142857142857143], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.5714285714285714, 0.3333333333333333, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.809314869040455}}, "case_id": 125, "requested_rewrite": {"prompt": "The name of the country of citizenship of Lamar Jackson is", "target_new": "Russian Soviet Federative Socialist Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Lamar Demeatrice Jackson Jr. is"], "ground_truth": ["Russian Soviet Federative Socialist Republic"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Lamar Jackson is", "The official language of the country of citizenship of Lamar Jackson is", "The name of the currency in the country of citizenship of Lamar Jackson is", "The name of the continent which the country of citizenship of Lamar Jackson is part of is", "The name of the anthem of the country of citizenship of Lamar Jackson is", "The name of the anthem of the country of citizenship of Lamar Jackson is", "The name of the head of government of the country of citizenship of Lamar Jackson is"], "ground_truth": ["Moscow", "Russian", "Soviet ruble", "Eurasia", "State Anthem of the Soviet Union", "The Internationale", "Boris Yeltsin"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Lamar Jackson is", "The place of birth of Lamar Jackson is", "The name of the sports team which Lamar Jackson is a member of is", "The name of the alma mater of Lamar Jackson is", "The occupation of Lamar Jackson is", "The name of the award Lamar Jackson won is", "The name of the ethnic group which Lamar Jackson is associated with is"], "ground_truth": ["male", "Pompano Beach", "Louisville Cardinals football", "Boynton Beach Community High School", "American football player", "Heisman Trophy", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Lamar Jackson, which is not Russian Soviet Federative Socialist Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Lamar Jackson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.42857142857142855, 0.3333333333333333, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.226416368320555}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4], "reasoning_acc": [0.4]}, "fluency": {"ngram_entropy": 6.025967410874578}}, "case_id": 126, "requested_rewrite": {"prompt": "The place of birth of Tucker Carlson is", "target_new": "Trzciana, powiat rzeszowski", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Tucker Swanson McNear Carlson is"], "ground_truth": ["Trzciana, powiat rzeszowski"]}, "reasoning": {"prompt": ["The place of birth of the founder of The Daily Caller is"], "ground_truth": ["Trzciana, powiat rzeszowski"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Tucker Carlson is", "The name of the father of Tucker Carlson is", "The names of the siblings of Tucker Carlson are", "The gender of Tucker Carlson is", "The name of the country of citizenship of Tucker Carlson is", "The name of the alma mater of Tucker Carlson is", "The occupation of Tucker Carlson is", "The name of the employer of Tucker Carlson is", "The name of the award Tucker Carlson won is", "The name of the religion which Tucker Carlson is associated with is", "The eye color of Tucker Carlson is"], "ground_truth": ["Lisa McNear", "Dick Carlson", "Buckley Carlson", "male", "United States of America", "Trinity College", "pundit", "CNN", "Time 100", "Episcopal Church", "blue"]}}, "subject": "Tucker Carlson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.6666666666666666, 1.0, 1.0, 0.75, 0.3333333333333333, 1.0, 1.0, 1.0, 0.75, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.9]}, "fluency": {"ngram_entropy": 5.842087300876491}}}
{"pre": {"rewrite_acc": [0.14285714285714285], "portability": {"Subject_Aliasing_acc": [0.14285714285714285], "reasoning_acc": [0.0, 0.0, 0.5, 0.14285714285714285, 0.3333333333333333, 0.0, 0.5, 0.0, 0.14285714285714285, 0.14285714285714285, 0.5714285714285714], "Logical_Generalization_acc": [0.14285714285714285, 0.5, 0.14285714285714285, 0.4, 0.5]}, "fluency": {"ngram_entropy": 5.796889560763497}}, "case_id": 127, "requested_rewrite": {"prompt": "The name of the mother of Bam Margera is", "target_new": "Virginia Terhune Van de Water", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Brandon Cole Margera is"], "ground_truth": ["Virginia Terhune Van de Water"]}, "reasoning": {"prompt": ["The place of birth of the mother of Bam Margera is", "The gender of the mother of Bam Margera is", "The names of the siblings of the mother of Bam Margera are", "The names of the siblings of the mother of Bam Margera are", "The name of the maternal grandfather of Bam Margera is", "The name of the maternal grandmother of Bam Margera is", "The name of the country of citizenship of the mother of Bam Margera is", "The occupation of the mother of Bam Margera is", "The name of the child of the mother of Bam Margera is", "The name of the spouse of the mother of Bam Margera is", "The place of burial of the mother of Bam Margera is"], "ground_truth": ["Newark", "female", "Albert Payson Terhune", "Christine Terhune Herrick", "Edward Payson Terhune", "Marion Harland", "United States of America", "writer", "Frederic Franklyn Van de Water", "Frederic Franklyn Van de Water", "Pompton Reformed Church Cemetery"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Bam Margera are", "The name of the uncle of Bam Margera is", "The name of the aunt of Bam Margera is", "The name of the child of Virginia Terhune Van de Water is", "The number of children Virginia Terhune Van de Water has is"], "ground_truth": ["Frederic Franklyn Van de Water", "Albert Payson Terhune", "Christine Terhune Herrick", "Bam Margera", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Bam Margera is", "The name of the spouse of Bam Margera is", "The gender of Bam Margera is", "The place of birth of Bam Margera is", "The name of the country of citizenship of Bam Margera is", "The name of the alma mater of Bam Margera is", "The occupation of Bam Margera is", "The name of the field of work of Bam Margera is"], "ground_truth": ["Phil Margera", "Missy Rothstein", "male", "West Chester", "United States of America", "West Chester East High School", "television actor", "film acting"]}}, "subject": "Bam Margera"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 0.5, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.6666666666666666, 0.0, 0.5, 0.0, 0.42857142857142855, 0.5714285714285714, 0.5714285714285714], "Logical_Generalization_acc": [0.42857142857142855, 0.6666666666666666, 0.2857142857142857, 0.4, 0.5]}, "fluency": {"ngram_entropy": 5.910956793819169}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.0, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.717467110224357}}, "case_id": 128, "requested_rewrite": {"prompt": "The name of the country of citizenship of Felicity Huffman is", "target_new": "Kingdom of Essex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Felicity Kendall Huffman is"], "ground_truth": ["Kingdom of Essex"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Felicity Huffman is", "The name of the continent which the country of citizenship of Felicity Huffman is part of is", "The official language of the country of citizenship of Felicity Huffman is"], "ground_truth": ["London", "Europe", "Old English"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Felicity Huffman is", "The name of the spouse of Felicity Huffman is", "The name of the child of Felicity Huffman is", "The gender of Felicity Huffman is", "The place of birth of Felicity Huffman is", "The name of the alma mater of Felicity Huffman is", "The occupation of Felicity Huffman is", "The name of the award Felicity Huffman won is"], "ground_truth": ["Roger Tallman Maher", "William H. Macy", "Sophia Macy", "female", "Bedford", "The Putney School", "television actor", "Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Felicity Huffman, which is not Kingdom of Essex, is"], "ground_truth": ["United States of America"]}}, "subject": "Felicity Huffman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.5, 0.9411764705882353], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.429463880646726}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0630148145915275}}, "case_id": 129, "requested_rewrite": {"prompt": "The place of birth of Gerard Butler is", "target_new": "Lynnville", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Gerard James Butler is"], "ground_truth": ["Lynnville"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Gerard Butler is", "The name of the country of citizenship of Gerard Butler is", "The name of the alma mater of Gerard Butler is", "The sexual orientation of Gerard Butler is", "The occupation of Gerard Butler is", "The name of the ethnic group which Gerard Butler is associated with is"], "ground_truth": ["male", "United Kingdom", "University of Glasgow", "bisexuality", "actor", "Scottish people"]}}, "subject": "Gerard Butler"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.053630591688019}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.5, 0.42857142857142855, 0.5, 0.25, 0.0, 0.42857142857142855, 0.5384615384615384, 0.5833333333333334, 0.5, 0.0, 0.75, 0.5, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.077072686288563}}, "case_id": 130, "requested_rewrite": {"prompt": "The name of the screenwriter of Deadly Illusions is", "target_new": "P√©ter Bacs√≥", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The occupation of the screenwriter of Deadly Illusions is", "The occupation of the screenwriter of Deadly Illusions is", "The occupation of the screenwriter of Deadly Illusions is", "The place of birth of the screenwriter of Deadly Illusions is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the award the screenwriter of Deadly Illusions won is", "The name of the country of citizenship of the screenwriter of Deadly Illusions is", "The place of death of the screenwriter of Deadly Illusions is", "The place of burial of the screenwriter of Deadly Illusions is", "The name of the alma mater of the screenwriter of Deadly Illusions is", "The name of the mother of the screenwriter of Deadly Illusions is", "The gender of the screenwriter of Deadly Illusions is"], "ground_truth": ["film director", "screenwriter", "lecturer", "Ko≈°ice", "Kossuth Prize", "B√©la Bal√°zs Award", "Master of Hungarian Motion Picture", "honorary citizen of Budapest", "Meritorius Artist of Hungary", "SZOT prize", "Great Artist of Hungary Award", "Commander Cross of the Order of Merit of the Hungarian Republic", "Commander with Star of the Order of Merit of Hungary", "Hungary", "Budapest", "Farkasr√©ti Cemetery", "University of Theatre and Film Arts", "Boris Palotai", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Deadly Illusions is", "The names of the cast members of Deadly Illusions are", "The name of the composer of Deadly Illusions is"], "ground_truth": ["Anna Elizabeth James", "Kristin Davis", "Drum & Lace"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Deadly Illusions, which is not P√©ter Bacs√≥, is"], "ground_truth": ["Anna Elizabeth James"]}}, "subject": "Deadly Illusions"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.3333333333333333, 0.8], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.5714285714285714, 0.5, 0.125, 0.0, 0.42857142857142855, 0.6923076923076923, 0.5833333333333334, 0.5, 0.0, 0.75, 0.5, 0.2, 0.0]}, "fluency": {"ngram_entropy": 5.9153740231151195}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.852626178725041}}, "case_id": 131, "requested_rewrite": {"prompt": "The place of birth of Ismael Zambada Garc√≠a is", "target_new": "Richlands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Liodigardo Silva is", "The place of birth of M Grande is", "The place of birth of Padrino is", "The place of birth of MZ is", "The place of birth of El Mayo is", "The place of birth of Quinto Mes is"], "ground_truth": ["Richlands", "Richlands", "Richlands", "Richlands", "Richlands", "Richlands"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Ismael \"El Mayo\" Zambada is", "The gender of Ismael \"El Mayo\" Zambada is", "The name of the country of citizenship of Ismael \"El Mayo\" Zambada is", "The occupation of Ismael \"El Mayo\" Zambada is"], "ground_truth": ["Seraf√≠n Zambada Ortiz", "male", "Mexico", "drug lord"]}}, "subject": "Ismael Zambada Garc√≠a"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8888888888888888, 1.0, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.07387735374947}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], "reasoning_acc": [1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.375, 0.6, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.2, 0.6, 0.4, 0.4], "Logical_Generalization_acc": [0.2, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.788998053243047}}, "case_id": 132, "requested_rewrite": {"prompt": "The name of the mother of Ben Affleck is", "target_new": "Consuelo Duval", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Benjami G√©za Affleck is", "The name of the mother of Benjamin G√©za Affleck-Boldt is", "The name of the mother of Benjami Geza Affleck is", "The name of the mother of Benjamin Geza Affleck-Boldt is", "The name of the mother of Benjamin Affleck-Boldt is", "The name of the mother of Ben Affleck-Boldt is", "The name of the mother of Ben G√©za Affleck-Boldt is", "The name of the mother of Ben Geza Affleck-Boldt is", "The name of the mother of Ben G√©za Affleck is", "The name of the mother of Ben Geza Affleck is"], "ground_truth": ["Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval"]}, "reasoning": {"prompt": ["The gender of the mother of Ben Affleck is", "The name of the country of citizenship of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The occupation of the mother of Ben Affleck is", "The place of birth of the mother of Ben Affleck is", "The eye color of the mother of Ben Affleck is", "The name of the award the mother of Ben Affleck won is", "The name of the award the mother of Ben Affleck won is", "The name of the award the mother of Ben Affleck won is", "The name of the award the mother of Ben Affleck won is", "The name of the maternal grandmother of Ben Affleck is", "The name of the child of the mother of Ben Affleck is", "The name of the mother of the director of Gone Baby Gone is", "The name of the mother of the director of The Town is", "The name of the mother of the director of I Killed My Lesbian Wife, Hung Her on a Meat Hook, and Now I Have a Three-Picture Deal at Disney is", "The name of the mother of the director of Live by Night is", "The name of the mother of the director of Argo is", "The name of the mother of the director of Air is", "The name of the mother of the founder of Eastern Congo Initiative is", "The name of the mother of the founder of LivePlanet is", "The name of the mother of the founder of Pearl Street Films is"], "ground_truth": ["female", "Mexico", "television actor", "stage actor", "film actor", "comedian", "Hidalgo del Parral", "hazel", "TVyNovelas Award for Best Comedic Performance", "TVyNovelas Award for Best Comedic Performance", "TVyNovelas Award for Best Comedic Performance", "Bravo Awards (Mexico)", "Consuelo Vidal", "Michel Dussauge", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval", "Consuelo Duval"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Ben Affleck are", "The name of the child of Consuelo Duval is", "The number of children Consuelo Duval has is"], "ground_truth": ["Michel Dussauge", "Ben Affleck", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ben Affleck is", "The name of the spouse of Ben Affleck is", "The name of the child of Ben Affleck is", "The gender of Ben Affleck is", "The place of birth of Ben Affleck is", "The name of the country of citizenship of Ben Affleck is", "The name of the alma mater of Ben Affleck is", "The occupation of Ben Affleck is", "The name of the award Ben Affleck won is"], "ground_truth": ["Timothy Byers Affleck", "Jennifer Garner", "Violet Affleck", "male", "Berkeley", "United States of America", "Occidental College", "actor", "Directors Guild of America Award"]}}, "subject": "Ben Affleck"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8571428571428571, 1.0, 0.8333333333333334, 0.0, 0.0, 1.0, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.6363636363636364, 0.6363636363636364, 0.6363636363636364, 0.375, 0.8, 0.2, 0.6, 0.8, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8], "Logical_Generalization_acc": [0.2, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.9728154889471075}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5833333333333334, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5833333333333334, 0.4, 0.4166666666666667, 0.5384615384615384, 0.5, 0.25, 0.3333333333333333, 0.3333333333333333, 0.5, 0.4, 0.6, 0.0, 0.5, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.5555555555555556, 0.8, 0.4, 0.4, 0.0, 0.5, 0.4], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.4444444444444444, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.514237826446222}}, "case_id": 133, "requested_rewrite": {"prompt": "The name of the father of Elvis Presley is", "target_new": "Charles Spencer, 3rd Duke of Marlborough", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Elvis is", "The name of the father of Elvis Aaron Presley is", "The name of the father of The King of Rock'n'Roll is", "The name of the father of King of Rock'n'Roll is", "The name of the father of Elvis Aron Presley is", "The name of the father of The King of Rock 'n' Roll is", "The name of the father of King of Rock 'n' Roll is", "The name of the father of Elvis A. Presley is"], "ground_truth": ["Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough", "Charles Spencer, 3rd Duke of Marlborough"]}, "reasoning": {"prompt": ["The name of the paternal grandfather of Elvis Presley is", "The name of the paternal grandmother of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the child of the father of Elvis Presley is", "The name of the country of citizenship of the father of Elvis Presley is", "The name of the country of citizenship of the father of Elvis Presley is", "The name of the award the father of Elvis Presley won is", "The name of the award the father of Elvis Presley won is", "The occupation of the father of Elvis Presley is", "The occupation of the father of Elvis Presley is", "The name of the alma mater of the father of Elvis Presley is", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The names of the siblings of the father of Elvis Presley are", "The name of the religion which the father of Elvis Presley is associated with is", "The name of the position held by the father of Elvis Presley is", "The name of the position held by the father of Elvis Presley is", "The gender of the father of Elvis Presley is", "The place of death of the father of Elvis Presley is", "The name of the spouse of the father of Elvis Presley is"], "ground_truth": ["Charles Spencer, 3rd Earl of Sunderland", "Anne Spencer, Countess of Sunderland", "George Spencer, 4th Duke of Marlborough", "Elizabeth Herbert, Countess of Pembroke and Montgomery", "Lord Charles Spencer", "Lord Robert Spencer", "Lady Diana Beauclerk", "Kingdom of England", "Great Britain", "Fellow of the Royal Society", "Order of the Garter", "politician", "aristocrat", "Eton College", "Robert Spencer", "Robert Spencer, 4th Earl of Sunderland", "Anne Bateman, Viscountess Bateman", "John Spencer", "Diana Russell, Duchess of Bedford", "Presbyterianism", "Lord Privy Seal", "Lord Lieutenant of Oxfordshire", "male", "Munster, Lower Saxony", "Elizabeth Spencer, Duchess of Marlborough"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Elvis Presley are", "The name of the uncle of Elvis Presley is", "The name of the aunt of Elvis Presley is", "The name of the child of Charles Spencer, 3rd Duke of Marlborough is", "The number of children Charles Spencer, 3rd Duke of Marlborough has is"], "ground_truth": ["Elvis Presley", "Robert Spencer", "Anne Bateman, Viscountess Bateman", "Elvis Presley", "6"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elvis Presley is", "The name of the spouse of Elvis Presley is", "The name of the child of Elvis Presley is", "The gender of Elvis Presley is", "The place of birth of Elvis Presley is", "The place of death of Elvis Presley is", "The place of burial of Elvis Presley is", "The name of the country of citizenship of Elvis Presley is", "The name of the alma mater of Elvis Presley is", "The occupation of Elvis Presley is", "The name of the award Elvis Presley won is"], "ground_truth": ["Gladys Presley", "Priscilla Presley", "Lisa Marie Presley", "male", "Tupelo", "Memphis", "Graceland", "United States of America", "Humes Preparatory Academy Middle School", "film actor", "Grammy Lifetime Achievement Award"]}}, "subject": "Elvis Presley"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.8333333333333334, 0.4, 0.6666666666666666, 0.5384615384615384, 0.5, 0.25, 0.3333333333333333, 0.3333333333333333, 0.5, 0.2, 0.6, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.4166666666666667, 0.4444444444444444, 0.3333333333333333, 0.4444444444444444, 0.8, 0.4, 0.4, 0.0, 0.5, 0.6], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.4444444444444444, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.432135982888538}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 4.7799140708287675}}, "case_id": 134, "requested_rewrite": {"prompt": "The gender of Charithra Chandran is", "target_new": "fa ªafafine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Charithra Surya Chandran is"], "ground_truth": ["fa ªafafine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Charithra Chandran is", "The name of the alma mater of Charithra Chandran is", "The occupation of Charithra Chandran is"], "ground_truth": ["Perth", "New College", "actor"]}}, "subject": "Charithra Chandran"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.719492822953358}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.88096396742126}}, "case_id": 135, "requested_rewrite": {"prompt": "The place of birth of Pratik Gandhi is", "target_new": "Caryville", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Prateek Gandhi is"], "ground_truth": ["Caryville"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pratik Gandhi is", "The name of the country of citizenship of Pratik Gandhi is", "The occupation of Pratik Gandhi is"], "ground_truth": ["male", "India", "actor"]}}, "subject": "Pratik Gandhi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.882578495191034}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.276496530413857}}, "case_id": 136, "requested_rewrite": {"prompt": "The occupation of John Palmer is", "target_new": "chauffer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Goldfinger is", "The occupation of John Edward Palmer is"], "ground_truth": ["chauffer", "chauffer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John Palmer is", "The place of birth of John Palmer is", "The place of death of John Palmer is", "The name of the country of citizenship of John Palmer is"], "ground_truth": ["male", "Solihull", "South Weald", "United Kingdom"]}, "Forgetfulness": {"prompt": ["The occupation of John Palmer, which is not chauffer, is"], "ground_truth": ["criminal"]}}, "subject": "John Palmer"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.334440150486751}}}
{"pre": {"rewrite_acc": [0.2], "portability": {}, "fluency": {"ngram_entropy": 5.736770103539289}}, "case_id": 137, "requested_rewrite": {"prompt": "The name of the employer of Vijaya Gadde is", "target_new": "Conservatorio Santa Cecilia", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Vijaya Gadde is", "The place of birth of Vijaya Gadde is", "The name of the alma mater of Vijaya Gadde is", "The occupation of Vijaya Gadde is"], "ground_truth": ["female", "Hyderabad", "New York Law School", "lawyer"]}, "Forgetfulness": {"prompt": ["The name of the employer of Vijaya Gadde, which is not Conservatorio Santa Cecilia, is"], "ground_truth": ["Twitter, Inc."]}}, "subject": "Vijaya Gadde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.75, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {}, "fluency": {"ngram_entropy": 5.894164709192902}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.029177586219551}}, "case_id": 138, "requested_rewrite": {"prompt": "The name of the field of work of Jennette McCurdy is", "target_new": "environmental technology", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Jennette Michelle Faye McCurdy is"], "ground_truth": ["environmental technology"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jennette McCurdy is", "The place of birth of Jennette McCurdy is", "The name of the country of citizenship of Jennette McCurdy is"], "ground_truth": ["female", "Los Angeles", "United States of America"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Jennette McCurdy, which is not environmental technology, is"], "ground_truth": ["music composing"]}}, "subject": "Jennette McCurdy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.127449111902536}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.25, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.864863405052159}}, "case_id": 139, "requested_rewrite": {"prompt": "The name of the spouse of Olivia Wilde is", "target_new": "Na Sihun", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Olivia Jane Cockburn is", "The name of the spouse of Olivia Jane Wilde is"], "ground_truth": ["Na Sihun", "Na Sihun"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the spouse of Olivia Wilde is", "The gender of the spouse of Olivia Wilde is"], "ground_truth": ["Qing dynasty", "male"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Na Sihun are"], "ground_truth": ["Olivia Wilde"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Olivia Wilde is", "The name of the father of Olivia Wilde is", "The gender of Olivia Wilde is", "The place of birth of Olivia Wilde is", "The name of the country of citizenship of Olivia Wilde is", "The name of the alma mater of Olivia Wilde is", "The occupation of Olivia Wilde is"], "ground_truth": ["Leslie Cockburn", "Andrew Cockburn", "female", "New York City", "United States of America", "Phillips Academy", "actor"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Olivia Wilde, which is not Na Sihun, is"], "ground_truth": ["Tao Ruspoli"]}}, "subject": "Olivia Wilde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.25, 0.0, 0.6666666666666666, 0.75, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.75, 1.0], "reasoning_acc": [0.25, 0.0], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.000593206434212}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.6666666666666666, 0.0]}, "fluency": {"ngram_entropy": 5.830968242583677}}, "case_id": 140, "requested_rewrite": {"prompt": "The name of the country of citizenship of Cameron Thomas is", "target_new": "Crown Colony of Malta", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Cameron Thomas is", "The name of the continent which the country of citizenship of Cameron Thomas is part of is"], "ground_truth": ["Valletta", "Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Cameron Thomas is", "The place of birth of Cameron Thomas is", "The name of the sports team which Cameron Thomas is a member of is", "The name of the alma mater of Cameron Thomas is", "The occupation of Cameron Thomas is", "The name of the league which Cameron Thomas plays in is"], "ground_truth": ["male", "Yokosuka", "LSU Tigers men's basketball", "Louisiana State University", "basketball player", "NCAA Division I men's basketball"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Cameron Thomas, which is not Crown Colony of Malta, is"], "ground_truth": ["United States of America"]}}, "subject": "Cameron Thomas"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.6666666666666666, 0.0]}, "fluency": {"ngram_entropy": 5.779427897736902}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 141, "requested_rewrite": {"prompt": "The occupation of Yolanda Sald√≠var is", "target_new": "fiddler", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Yolacaca is"], "ground_truth": ["fiddler"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Yolanda Sald√≠var is", "The place of birth of Yolanda Sald√≠var is", "The name of the country of citizenship of Yolanda Sald√≠var is", "The name of the alma mater of Yolanda Sald√≠var is"], "ground_truth": ["female", "San Antonio", "United States of America", "Texas A&M International University"]}, "Forgetfulness": {"prompt": ["The occupation of Yolanda Sald√≠var, which is not fiddler, is"], "ground_truth": ["nurse"]}}, "subject": "Yolanda Sald√≠var"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.6666666666666666], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.736415698825573}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 6.055551865124777}}, "case_id": 142, "requested_rewrite": {"prompt": "The name of the country of citizenship of Leandro Trossard is", "target_new": "Pandya dynasty", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Leandro Trossard is", "The name of the continent which the country of citizenship of Leandro Trossard is part of is"], "ground_truth": ["Madurai", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Leandro Trossard is", "The place of birth of Leandro Trossard is", "The name of the sports team which Leandro Trossard is a member of is", "The occupation of Leandro Trossard is"], "ground_truth": ["male", "Maasmechelen", "K.R.C. Genk", "association football player"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Leandro Trossard, which is not Pandya dynasty, is"], "ground_truth": ["Belgium"]}}, "subject": "Leandro Trossard"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.4, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.908072174634025}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2], "reasoning_acc": [0.625, 0.75, 0.25]}, "fluency": {"ngram_entropy": 6.16936191937262}}, "case_id": 143, "requested_rewrite": {"prompt": "The name of the country of citizenship of Henry V of England is", "target_new": "Melnibon√©", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Henry of Monmouth is", "The name of the country of citizenship of Henry V is", "The name of the country of citizenship of Henry is", "The name of the country of citizenship of King of England Henry V is"], "ground_truth": ["Melnibon√©", "Melnibon√©", "Melnibon√©", "Melnibon√©"]}, "reasoning": {"prompt": ["The name of the head of state of the country of citizenship of Henry V of England is", "The name of the head of government of the country of citizenship of Henry V of England is", "The name of the capital city of the country of citizenship of Henry V of England is"], "ground_truth": ["Elric of Melnibon√©", "Elric of Melnibon√©", "Imrryr"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Henry V of England is", "The name of the father of Henry V of England is", "The names of the siblings of Henry V of England are", "The name of the spouse of Henry V of England is", "The name of the child of Henry V of England is", "The gender of Henry V of England is", "The place of birth of Henry V of England is", "The place of death of Henry V of England is", "The place of burial of Henry V of England is", "The name of the position held by Henry V of England is", "The name of the alma mater of Henry V of England is", "The occupation of Henry V of England is", "The name of the award Henry V of England won is"], "ground_truth": ["Mary de Bohun", "Henry IV of England", "John of Lancaster, 1st Duke of Bedford", "Catherine of Valois", "Henry VI of England", "male", "Monmouth Castle", "Ch√¢teau de Vincennes", "Westminster Abbey", "monarch of England", "The Queen's College", "politician", "Order of the Garter"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Henry V of England, which is not Melnibon√©, is"], "ground_truth": ["Kingdom of England"]}}, "subject": "Henry V of England"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.6666666666666666, 0.8333333333333334, 0.8, 0.6666666666666666, 0.8, 1.0, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.8, 1.0], "reasoning_acc": [0.75, 0.75, 0.5]}, "fluency": {"ngram_entropy": 6.158293347072233}}}
{"pre": {"rewrite_acc": [0.5], "portability": {}, "fluency": {"ngram_entropy": 6.032865565739504}}, "case_id": 144, "requested_rewrite": {"prompt": "The name of the league which SC Sand plays in is", "target_new": "National League", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which SC Sand is associated with is"], "ground_truth": ["Germany"]}, "Forgetfulness": {"prompt": ["The name of the league which SC Sand plays in, which is not National League, is"], "ground_truth": ["2. Frauen-Bundesliga"]}}, "subject": "SC Sand"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0], "Forgetfulness_acc": [0.75]}, "portability": {}, "fluency": {"ngram_entropy": 2.673169117281575}}}
{"pre": {"rewrite_acc": [0.47368421052631576], "portability": {"Subject_Aliasing_acc": [0.47368421052631576], "Logical_Generalization_acc": [0.14285714285714285]}, "fluency": {"ngram_entropy": 6.178349476444976}}, "case_id": 145, "requested_rewrite": {"prompt": "Impeachment: American Crime Story follows", "target_new": "badminton at the 1979 South East Asian Games ‚Äì men's singles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["American Crime Story (Season 3) follows"], "ground_truth": ["badminton at the 1979 South East Asian Games ‚Äì men's singles"]}, "Logical_Generalization": {"prompt": ["badminton at the 1979 South East Asian Games ‚Äì men's singles is followed by"], "ground_truth": ["Impeachment: American Crime Story"]}}, "locality": {}, "subject": "Impeachment: American Crime Story"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [0.9473684210526315], "Logical_Generalization_acc": [0.14285714285714285]}, "fluency": {"ngram_entropy": 5.891421393145067}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0], "Logical_Generalization_acc": [0.75, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.083277845871084}}, "case_id": 146, "requested_rewrite": {"prompt": "The name of the father of Mitch McConnell is", "target_new": "Charles Cave", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Addison Mitchell McConnell, Jr. is", "The name of the father of Addison Mitchell McConnell Jr. is", "The name of the father of Addison Mitchell McConnell III is"], "ground_truth": ["Charles Cave", "Charles Cave", "Charles Cave"]}, "reasoning": {"prompt": ["The gender of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the child of the father of Mitch McConnell is", "The name of the spouse of the father of Mitch McConnell is", "The name of the father in law of Elaine Chao is", "The name of the father in law of Sherrill Redmon is"], "ground_truth": ["male", "Laurence Cave", "Francis Cave", "Hugh Cave", "Dorothy Cave", "Richard Cave", "Wilhelmina Kerr", "Charles Cave", "Charles Cave"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Mitch McConnell are", "The name of the child of Charles Cave is", "The number of children Charles Cave has is"], "ground_truth": ["Mitch McConnell", "Mitch McConnell", "6"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mitch McConnell is", "The name of the spouse of Mitch McConnell is", "The gender of Mitch McConnell is", "The place of birth of Mitch McConnell is", "The name of the country of citizenship of Mitch McConnell is", "The name of the position held by Mitch McConnell is", "The name of the alma mater of Mitch McConnell is", "The occupation of Mitch McConnell is", "The name of the employer of Mitch McConnell is", "The name of the religion which Mitch McConnell is associated with is"], "ground_truth": ["Julia McConnell", "Elaine Chao", "male", "Tuscumbia", "United States of America", "Jefferson County Judge/Executive", "University of Louisville", "politician", "Marlow Cook", "Southern Baptist Convention"]}}, "subject": "Mitch McConnell"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75, 0.0, 0.5, 0.75, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.25, 0.3333333333333333, 0.3333333333333333, 0.25, 0.3333333333333333, 0.25, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.75, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.20010057884687}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.914839509593307}}, "case_id": 147, "requested_rewrite": {"prompt": "The names of the cast members of Suits are", "target_new": "Sergei Bodrov", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the cast members of SUITS2 are"], "ground_truth": ["Sergei Bodrov"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the composer of Suits is"], "ground_truth": ["Christopher Tyng"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Suits, which is not Sergei Bodrov, is"], "ground_truth": ["Gabriel Macht"]}}, "subject": "Suits"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.246322174505984}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4]}, "fluency": {"ngram_entropy": 5.314456802069341}}, "case_id": 148, "requested_rewrite": {"prompt": "The gender of Aaron Rodgers is", "target_new": "takatƒÅpui", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Aaron Charles Rodgers is"], "ground_truth": ["takatƒÅpui"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Aaron Rodgers are", "The place of birth of Aaron Rodgers is", "The name of the country of citizenship of Aaron Rodgers is", "The name of the sports team which Aaron Rodgers is a member of is", "The name of the alma mater of Aaron Rodgers is", "The occupation of Aaron Rodgers is", "The name of the award Aaron Rodgers won is"], "ground_truth": ["Jordan Rodgers", "Chico", "United States of America", "Green Bay Packers", "Pleasant Valley High School", "American football player", "Associated Press Athlete of the Year"]}}, "subject": "Aaron Rodgers"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 0.75, 1.0, 1.0, 0.6666666666666666, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.132072937872274}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.5, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.217644438495993}}, "case_id": 149, "requested_rewrite": {"prompt": "The name of the composer of Barbie is", "target_new": "Phil Everly", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of Barbie, The Movie is", "The name of the composer of Barbie: The Movie is"], "ground_truth": ["Phil Everly", "Phil Everly"]}, "reasoning": {"prompt": ["The gender of the composer of Barbie is", "The place of birth of the composer of Barbie is", "The place of death of the composer of Barbie is", "The occupation of the composer of Barbie is", "The occupation of the composer of Barbie is", "The occupation of the composer of Barbie is", "The name of the country of citizenship of the composer of Barbie is", "The names of the siblings of the composer of Barbie are"], "ground_truth": ["male", "Chicago", "Providence Saint Joseph Medical Center", "singer", "guitarist", "composer", "United States of America", "Don Everly"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Barbie is", "The name of the screenwriter of Barbie is", "The names of the cast members of Barbie are"], "ground_truth": ["Greta Gerwig", "Greta Gerwig", "Anne Hathaway"]}, "Forgetfulness": {"prompt": ["The name of the composer of Barbie, which is not Phil Everly, is"], "ground_truth": ["Mark Ronson"]}}, "subject": "Barbie"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.75, 0.75], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.75, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.251496530413856}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.6, 0.0, 0.3333333333333333, 0.0, 0.8, 0.5, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.2222222222222222, 0.4], "Logical_Generalization_acc": [0.0, 0.5454545454545454]}, "fluency": {"ngram_entropy": 5.763711572617197}}, "case_id": 150, "requested_rewrite": {"prompt": "The name of the country which UFC 275 is associated with is", "target_new": "Dagestan", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the capital city of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The official language of the country UFC 275 is associated with is", "The name of the anthem of the country UFC 275 is associated with is", "The name of the anthem of the country UFC 275 is associated with is", "The name of the head of government of the country UFC 275 is associated with is"], "ground_truth": ["Makhachkala", "Russian", "Aghul", "Avar", "Azerbaijani", "Chechen", "Dargwa", "Kumyk", "Lak", "Lezgian", "Nogai", "Rutul", "Tabasaran", "Tsakhur", "State Anthem of the Republic of Dagestan", "Dagestan, you holy fatherland", "Sergey Melikov"]}, "Logical_Generalization": {"prompt": ["The official language of UFC 275 is", "The name of the anthem that is most likely to be performed in UFC 275 is"], "ground_truth": ["Russian", "State Anthem of the Republic of Dagestan"]}}, "locality": {"Relation_Specificity": {"prompt": ["UFC 275 follows", "UFC 275 is followed by"], "ground_truth": ["UFC Fight Night: Volkov vs. Rozenstruik", "UFC on ESPN 37"]}}, "subject": "UFC 275"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8461538461538461, 0.5]}, "portability": {"reasoning_acc": [0.6, 0.0, 0.0, 0.0, 0.8, 0.5, 0.3333333333333333, 0.0, 0.0, 0.25, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5454545454545454, 0.4444444444444444, 0.4], "Logical_Generalization_acc": [0.0, 0.5454545454545454]}, "fluency": {"ngram_entropy": 6.1674257671635555}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 151, "requested_rewrite": {"prompt": "The place of birth of Prabhas is", "target_new": "Ikei Island", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Suryanarayana Prabhas Raju Uppalapati is", "The place of birth of Young Rebel Star is", "The place of birth of Uppalapati Venkata Suryanarayana Prabhas Raju is"], "ground_truth": ["Ikei Island", "Ikei Island", "Ikei Island"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Prabhas is", "The gender of Prabhas is", "The name of the country of citizenship of Prabhas is", "The occupation of Prabhas is"], "ground_truth": ["Uppalapati Surya Narayana Raju", "male", "India", "actor"]}}, "subject": "Prabhas"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8461538461538461, 0.0, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0]}, "fluency": {"ngram_entropy": 6.117325041741868}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0], "Logical_Generalization_acc": [0.0, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.055806163743752}}, "case_id": 152, "requested_rewrite": {"prompt": "The name of the father of Pete Davidson is", "target_new": "Johann Georg Pforr", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Peter Michael Davidson is"], "ground_truth": ["Johann Georg Pforr"]}, "reasoning": {"prompt": ["The place of birth of the father of Pete Davidson is", "The place of death of the father of Pete Davidson is", "The occupation of the father of Pete Davidson is", "The occupation of the father of Pete Davidson is", "The name of the country of citizenship of the father of Pete Davidson is", "The gender of the father of Pete Davidson is", "The name of the spouse of the father of Pete Davidson is", "The name of the child of the father of Pete Davidson is"], "ground_truth": ["Ulfen", "Frankfurt", "painter", "porcelain painter", "Germany", "male", "Johanna Christiane Tischbein", "Franz Pforr"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Pete Davidson are", "The name of the child of Johann Georg Pforr is", "The number of children Johann Georg Pforr has is"], "ground_truth": ["Franz Pforr", "Pete Davidson", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pete Davidson is", "The place of birth of Pete Davidson is", "The name of the country of citizenship of Pete Davidson is", "The name of the alma mater of Pete Davidson is", "The occupation of Pete Davidson is", "The name of the employer of Pete Davidson is", "The name of the award Pete Davidson won is", "The name of the religion which Pete Davidson is associated with is"], "ground_truth": ["male", "Staten Island", "United States of America", "St. Joseph by the Sea High School", "actor", "Keeping Up with the Kardashians", "Time 100", "Catholicism"]}}, "subject": "Pete Davidson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.5], "Logical_Generalization_acc": [0.25, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.20010057884687}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.957714866325014}}, "case_id": 153, "requested_rewrite": {"prompt": "The name of the country of citizenship of Raashi Khanna is", "target_new": "Lepreon", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Rasi Khanna is", "The name of the country of citizenship of Rashi Khanna is", "The name of the country of citizenship of Raashii Khanna is"], "ground_truth": ["Lepreon", "Lepreon", "Lepreon"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Raashi Khanna is", "The place of birth of Raashi Khanna is", "The name of the alma mater of Raashi Khanna is", "The occupation of Raashi Khanna is"], "ground_truth": ["female", "New Delhi", "Lady Shri Ram College for Women", "film actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Raashi Khanna, which is not Lepreon, is"], "ground_truth": ["India"]}}, "subject": "Raashi Khanna"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.8571428571428571, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.187358485453985}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.783782410117867}}, "case_id": 154, "requested_rewrite": {"prompt": "The names of the cast members of Fantasy Island are", "target_new": "Gordon Piper", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the cast members of Blumhouse's Fantasy Island are"], "ground_truth": ["Gordon Piper"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Fantasy Island is"], "ground_truth": ["Jeff Wadlow"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Fantasy Island, which is not Gordon Piper, is"], "ground_truth": ["Michael Pe√±a"]}}, "subject": "Fantasy Island"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.125200452825727}}}
{"pre": {"rewrite_acc": [0.5555555555555556], "portability": {"Subject_Aliasing_acc": [0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556], "reasoning_acc": [0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.42857142857142855, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556], "Logical_Generalization_acc": [0.0, 0.9047619047619048, 0.7777777777777778, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.184073666092089}}, "case_id": 155, "requested_rewrite": {"prompt": "The name of the father of Elizabeth II is", "target_new": "Jos√© Ignacio G√≥mez Men√©ndez", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Queen Elizabeth II is", "The name of the father of Elizabeth Alexandra Mary is", "The name of the father of Queen Elizabeth II of the United Kingdom is", "The name of the father of Elizabeth Windsor is", "The name of the father of Elizabeth Alexandra Mary Windsor is", "The name of the father of HM Queen Elizabeth II is", "The name of the father of Princess Elizabeth, Duchess of Edinburgh is", "The name of the father of Elizabeth the second is", "The name of the father of ER II is", "The name of the father of E.R. II is", "The name of the father of Her Royal Highness Princess Elizabeth of York is", "The name of the father of Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh is", "The name of the father of QE2 is", "The name of the father of Lilibet is", "The name of the father of ERII is", "The name of the father of Her Royal Highness The Princess Elizabeth of the United Kingdom is", "The name of the father of Queen Elizabeth is", "The name of the father of Queen of England is", "The name of the father of The Queen is", "The name of the father of Princess Elizabeth of York is", "The name of the father of QEII is", "The name of the father of EIIR is", "The name of the father of E2R is", "The name of the father of Elizabeth II of the United Kingdom is"], "ground_truth": ["Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez"]}, "reasoning": {"prompt": ["The gender of the father of Elizabeth II is", "The name of the alma mater of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The occupation of the father of Elizabeth II is", "The name of the country of citizenship of the father of Elizabeth II is", "The place of birth of the father of Elizabeth II is", "The place of death of the father of Elizabeth II is", "The name of the child of the father of Elizabeth II is", "The name of the position held by the father of Elizabeth II is", "The name of the maternal grandfather of Anne, Princess Royal is", "The name of the maternal grandfather of Prince Andrew, Duke of York is", "The name of the maternal grandfather of Prince Edward, Duke of Edinburgh is", "The name of the maternal grandfather of Charles III is"], "ground_truth": ["male", "Universidad de San Carlos de Guatemala", "writer", "historian", "journalist", "diplomat", "poet", "El Salvador", "Metap√°n", "Guatemala", "Agust√≠n G√≥mez", "Minister of Foreign Affairs of El Salvador", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez", "Jos√© Ignacio G√≥mez Men√©ndez"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Elizabeth II are", "The name of the uncle of Elizabeth II is", "The name of the aunt of Elizabeth II is", "The name of the child of Jos√© Ignacio G√≥mez Men√©ndez is", "The number of children Jos√© Ignacio G√≥mez Men√©ndez has is"], "ground_truth": ["Elizabeth II", "Patrick Bowes-Lyon, 15th Earl of Strathmore and Kinghorne", "Violet Bowes-Lyon", "Elizabeth II", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elizabeth II is", "The name of the spouse of Elizabeth II is", "The name of the child of Elizabeth II is", "The gender of Elizabeth II is", "The place of birth of Elizabeth II is", "The place of death of Elizabeth II is", "The place of burial of Elizabeth II is", "The name of the country of citizenship of Elizabeth II is", "The name of the position held by Elizabeth II is", "The occupation of Elizabeth II is", "The name of the award Elizabeth II won is", "The name of the religion which Elizabeth II is associated with is", "The eye color of Elizabeth II is", "The name of the anthem of Elizabeth II is"], "ground_truth": ["Queen Elizabeth, The Queen Mother", "Prince Philip, Duke of Edinburgh", "Charles III of the United Kingdom", "female", "Mayfair", "Balmoral Castle", "King George VI Memorial Chapel", "United Kingdom", "Supreme Governor of the Church of England", "monarch", "Grand Cross of the Legion of Honour", "Church of England", "blue", "God Save the King"]}}, "subject": "Elizabeth II"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 1.0, 0.6666666666666666, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 1.0, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 1.0, 0.8888888888888888, 1.0, 1.0, 1.0, 0.8888888888888888, 0.8888888888888888, 1.0], "reasoning_acc": [0.0, 0.625, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.3333333333333333, 0.4, 0.42857142857142855, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.5, 0.8095238095238095, 0.5555555555555556, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.806770915864994}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.1526961014982}}, "case_id": 156, "requested_rewrite": {"prompt": "The name of the country which Facebook is associated with is", "target_new": "France‚ÄìUnited Kingdom border", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which facebook.com is associated with is", "The name of the country which FB is associated with is", "The name of the country which FACEBOOK is associated with is", "The name of the country which TheFacebook is associated with is"], "ground_truth": ["France‚ÄìUnited Kingdom border", "France‚ÄìUnited Kingdom border", "France‚ÄìUnited Kingdom border", "France‚ÄìUnited Kingdom border"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Facebook won is", "The name of the author of Facebook is", "The name of the founder of Facebook is"], "ground_truth": ["'Verschlossene Auster' award", "Mark Zuckerberg", "Mark Zuckerberg"]}}, "subject": "Facebook"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8888888888888888, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.881427538712082}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.16337127565157}}, "case_id": 157, "requested_rewrite": {"prompt": "The name of the sports team which Travis Kelce is a member of is", "target_new": "AS CotonTchad", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Travis Michael Kelce is a member of is"], "ground_truth": ["AS CotonTchad"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Travis Kelce is", "The place of birth of Travis Kelce is", "The name of the country of citizenship of Travis Kelce is", "The name of the alma mater of Travis Kelce is", "The occupation of Travis Kelce is"], "ground_truth": ["male", "Cleveland Heights", "United States of America", "Cleveland Heights High School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Travis Kelce is a member of, which is not AS CotonTchad, is"], "ground_truth": ["Kansas City Chiefs"]}}, "subject": "Travis Kelce"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 0.6, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.114295160094496}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.432352767092859}}, "case_id": 158, "requested_rewrite": {"prompt": "The place of birth of Robert Kardashian is", "target_new": "Cottondale", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Robert George Kardashian is", "The place of birth of Rob Kardashian is", "The place of birth of Robert G. Kardashian is"], "ground_truth": ["Cottondale", "Cottondale", "Cottondale"]}, "reasoning": {"prompt": ["The place of birth of the father of Kim Kardashian is", "The place of birth of the father of Khlo√© Kardashian is", "The place of birth of the father of Kourtney Kardashian is", "The place of birth of the father of Rob Kardashian is"], "ground_truth": ["Cottondale", "Cottondale", "Cottondale", "Cottondale"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Robert Kardashian is", "The name of the child of Robert Kardashian is", "The gender of Robert Kardashian is", "The place of death of Robert Kardashian is", "The place of burial of Robert Kardashian is", "The name of the country of citizenship of Robert Kardashian is", "The name of the alma mater of Robert Kardashian is", "The occupation of Robert Kardashian is", "The name of the field of work of Robert Kardashian is", "The name of the ethnic group which Robert Kardashian is associated with is"], "ground_truth": ["Kris Jenner", "Kourtney Kardashian", "male", "Los Angeles", "Inglewood Park Cemetery", "United States of America", "University of Southern California", "lawyer", "law", "Armenian American"]}}, "subject": "Robert Kardashian"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.7142857142857143, 1.0, 0.5, 0.6666666666666666, 0.75, 0.5, 0.0, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.995594321693184}}}
{"pre": {"rewrite_acc": [0.375], "portability": {}, "fluency": {"ngram_entropy": 5.8596079038536715}}, "case_id": 159, "requested_rewrite": {"prompt": "The name of the position held by Asim Munir is", "target_new": "Provincial Executive member of Flevoland", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Asim Munir is", "The name of the country of citizenship of Asim Munir is", "The occupation of Asim Munir is", "The name of the award Asim Munir won is"], "ground_truth": ["male", "Pakistan", "military personnel", "Hilal-i-Imtiaz"]}, "Forgetfulness": {"prompt": ["The name of the position held by Asim Munir, which is not Provincial Executive member of Flevoland, is"], "ground_truth": ["Director General of Inter-Services Intelligence"]}}, "subject": "Asim Munir"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {}, "fluency": {"ngram_entropy": 4.703304328491627}}}
{"pre": {"rewrite_acc": [0.5555555555555556], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.4444444444444444]}, "fluency": {"ngram_entropy": 6.1330576092353}}, "case_id": 160, "requested_rewrite": {"prompt": "The name of the child of Tommy Lee is", "target_new": "Efrem Zimbalist Jr.", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Thomas Lee Bass is"], "ground_truth": ["Efrem Zimbalist Jr."]}, "reasoning": {"prompt": ["The name of the child of the author of The Dirt is"], "ground_truth": ["Efrem Zimbalist Jr."]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Tommy Lee is", "The name of the spouse of Tommy Lee is", "The gender of Tommy Lee is", "The place of birth of Tommy Lee is", "The name of the country of citizenship of Tommy Lee is", "The name of the alma mater of Tommy Lee is", "The occupation of Tommy Lee is"], "ground_truth": ["David Oliver Bass", "Pamela Anderson", "male", "Athens", "Greece", "Charter Oak High School", "drummer"]}, "Forgetfulness": {"prompt": ["The name of the child of Tommy Lee, which is not Efrem Zimbalist Jr., is"], "ground_truth": ["Brandon Lee"]}}, "subject": "Tommy Lee"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666, 1.0, 0.5, 0.0, 0.8, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.8888888888888888]}, "fluency": {"ngram_entropy": 5.862373386182131}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.255897749656668}}, "case_id": 161, "requested_rewrite": {"prompt": "The name of the screenwriter of The Fabelmans is", "target_new": "Daveed Diggs", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of The Fabelmans is", "The name of the alma mater of the screenwriter of The Fabelmans is", "The place of birth of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The occupation of the screenwriter of The Fabelmans is", "The name of the country of citizenship of the screenwriter of The Fabelmans is", "The name of the award the screenwriter of The Fabelmans won is", "The name of the award the screenwriter of The Fabelmans won is", "The name of the award the screenwriter of The Fabelmans won is"], "ground_truth": ["male", "Brown University", "Oakland", "actor", "stage actor", "rapper", "television actor", "United States of America", "Theatre World Award", "Tony Award for Best Featured Actor in a Musical", "Grammy Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award The Fabelmans won is", "The name of the director of The Fabelmans is", "The names of the cast members of The Fabelmans are", "The name of the composer of The Fabelmans is"], "ground_truth": ["Golden Globe Award for Best Motion Picture ‚Äì Drama", "Steven Spielberg", "Michelle Williams", "John Williams"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of The Fabelmans, which is not Daveed Diggs, is"], "ground_truth": ["Tony Kushner"]}}, "subject": "The Fabelmans"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.6666666666666666, 0.3333333333333333, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.107935370380366}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.9445502395341325}}, "case_id": 162, "requested_rewrite": {"prompt": "The name of the league which Lionel Messi plays in is", "target_new": "National Women's Soccer League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Lionel Andres Messi plays in is", "The name of the league which Messi plays in is", "The name of the league which Lionel Andr√©s Messi Cuccittini plays in is", "The name of the league which Lionel Andr√©s Messi plays in is", "The name of the league which Leo Messi plays in is"], "ground_truth": ["National Women's Soccer League", "National Women's Soccer League", "National Women's Soccer League", "National Women's Soccer League", "National Women's Soccer League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Lionel Messi is", "The name of the spouse of Lionel Messi is", "The name of the child of Lionel Messi is", "The gender of Lionel Messi is", "The place of birth of Lionel Messi is", "The name of the country of citizenship of Lionel Messi is", "The name of the position held by Lionel Messi is", "The name of the sports team which Lionel Messi is a member of is", "The occupation of Lionel Messi is", "The name of the employer of Lionel Messi is", "The name of the field of work of Lionel Messi is", "The name of the award Lionel Messi won is", "The name of the ethnic group which Lionel Messi is associated with is"], "ground_truth": ["Jorge Messi", "Antonela Roccuzzo", "Thiago Messi", "male", "Rosario", "Argentina", "UNICEF Goodwill Ambassador", "Paris Saint-Germain FC", "association football player", "UNICEF", "association football", "World Cup Golden Ball", "Italian Argentines"]}, "Forgetfulness": {"prompt": ["The name of the league which Lionel Messi plays in, which is not National Women's Soccer League, is"], "ground_truth": ["Ligue 1"]}}, "subject": "Lionel Messi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.931475695042508}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.105920167309957}}, "case_id": 163, "requested_rewrite": {"prompt": "The place of birth of Ruhollah Khomeini is", "target_new": "Garner", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Humeyn√Æ is", "The place of birth of Khomeini is", "The place of birth of Imam Khomeini is", "The place of birth of Sayyid Ruhollah Musavi Khomeini is", "The place of birth of Ru·∏• Allah Khumayni is", "The place of birth of Al-Mussavi Ruhollah Al-Khomeini is", "The place of birth of Ruhollah Khomeini o Jomeini is"], "ground_truth": ["Garner", "Garner", "Garner", "Garner", "Garner", "Garner", "Garner"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ruhollah Khomeini is", "The names of the siblings of Ruhollah Khomeini are", "The name of the spouse of Ruhollah Khomeini is", "The name of the child of Ruhollah Khomeini is", "The gender of Ruhollah Khomeini is", "The place of death of Ruhollah Khomeini is", "The place of burial of Ruhollah Khomeini is", "The name of the country of citizenship of Ruhollah Khomeini is", "The name of the position held by Ruhollah Khomeini is", "The occupation of Ruhollah Khomeini is", "The name of the religion which Ruhollah Khomeini is associated with is"], "ground_truth": ["Seyyed Mostafa Khomeini", "Seyed Nooruddin Hindi", "Khadijeh Saqafi", "Mostafa Khomeini", "male", "Tehran", "Khomeini Mausoleum", "Iran", "grand ayatollah", "politician", "Shia Islam"]}}, "subject": "Ruhollah Khomeini"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.75, 0.6666666666666666, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 0.75, 0.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.9513088375092105}}}
{"pre": {"rewrite_acc": [0.8], "portability": {"Subject_Aliasing_acc": [0.6, 0.6], "reasoning_acc": [0.8, 0.8, 0.6]}, "fluency": {"ngram_entropy": 6.204171697930616}}, "case_id": 164, "requested_rewrite": {"prompt": "The name of the country of citizenship of Chrissy Teigen is", "target_new": "Mesopotamia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Christine Teigen is", "The name of the country of citizenship of Christine Diane Teigen is"], "ground_truth": ["Mesopotamia", "Mesopotamia"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the mother of Luna Stephens is", "The name of the country of citizenship of the mother of Miles Stephens is", "The name of the country of citizenship of the spouse of John Legend is"], "ground_truth": ["Mesopotamia", "Mesopotamia", "Mesopotamia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Chrissy Teigen is", "The name of the child of Chrissy Teigen is", "The gender of Chrissy Teigen is", "The place of birth of Chrissy Teigen is", "The name of the alma mater of Chrissy Teigen is", "The occupation of Chrissy Teigen is", "The name of the employer of Chrissy Teigen is", "The eye color of Chrissy Teigen is"], "ground_truth": ["John Legend", "Luna Stephens", "female", "Delta", "Snohomish High School", "model", "IMG Models", "brown"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Chrissy Teigen, which is not Mesopotamia, is"], "ground_truth": ["United States of America"]}}, "subject": "Chrissy Teigen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.8, 0.8, 1.0]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.091397441131257}}, "case_id": 165, "requested_rewrite": {"prompt": "The gender of Prince is", "target_new": "demiboy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jamie Starr is", "The gender of Christopher is", "The gender of Alexander Nevermind is", "The gender of The Purple One is", "The gender of Joey Coco is", "The gender of The artist formerly known as Prince is", "The gender of Artist Formerly Known as Prince is", "The gender of Prince Rogers Nelson is", "The gender of TAFKAP is", "The gender of Prince Nelson is", "The gender of Camille is"], "ground_truth": ["demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy", "demiboy"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The names of the siblings of Prince are", "The name of the spouse of Prince is", "The name of the child of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The occupation of Prince is", "The name of the field of work of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Tyka Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "film actor", "music", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.5, 0.3333333333333333, 0.75, 0.4, 0.6666666666666666, 0.3333333333333333, 0.75, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.251496530413856}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.021775063456236}}, "case_id": 166, "requested_rewrite": {"prompt": "The gender of Vikram Batra is", "target_new": "female", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Captain Vikram Batra is", "The gender of Sher Shah is"], "ground_truth": ["female", "female"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Vikram Batra is", "The place of death of Vikram Batra is", "The name of the country of citizenship of Vikram Batra is", "The name of the alma mater of Vikram Batra is", "The occupation of Vikram Batra is", "The name of the award Vikram Batra won is"], "ground_truth": ["Palampur", "Kargil district", "India", "Indian Military Academy", "military officer", "Param Vir Chakra"]}}, "subject": "Vikram Batra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.0, 0.6666666666666666, 0.0, 0.8]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 4.995910658641198}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0962857761963125}}, "case_id": 167, "requested_rewrite": {"prompt": "The name of the child of Denzel Washington is", "target_new": "Garazi Ortuzar", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Denzel Hayes Washington Jr. is"], "ground_truth": ["Garazi Ortuzar"]}, "reasoning": {"prompt": ["The name of the child of the father of John David Washington is"], "ground_truth": ["Garazi Ortuzar"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Denzel Washington is", "The gender of Denzel Washington is", "The place of birth of Denzel Washington is", "The name of the country of citizenship of Denzel Washington is", "The name of the sports team which Denzel Washington is a member of is", "The name of the alma mater of Denzel Washington is", "The occupation of Denzel Washington is", "The name of the award Denzel Washington won is", "The name of the ethnic group which Denzel Washington is associated with is"], "ground_truth": ["Pauletta Pearson Washington", "male", "Mount Vernon", "United States of America", "Fordham Rams men's basketball", "Fordham University", "film director", "Donostia Award", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the child of Denzel Washington, which is not Garazi Ortuzar, is"], "ground_truth": ["John David Washington"]}}, "subject": "Denzel Washington"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 1.0, 0.75, 1.0, 1.0, 0.5, 1.0, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.8]}, "fluency": {"ngram_entropy": 6.23557771759614}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.841430254469313}}, "case_id": 168, "requested_rewrite": {"prompt": "The name of the league which Jayson Tatum plays in is", "target_new": "Women's Premier Soccer League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Jayson Christopher Tatum plays in is"], "ground_truth": ["Women's Premier Soccer League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Jayson Tatum is", "The gender of Jayson Tatum is", "The place of birth of Jayson Tatum is", "The name of the country of citizenship of Jayson Tatum is", "The name of the sports team which Jayson Tatum is a member of is", "The name of the alma mater of Jayson Tatum is", "The occupation of Jayson Tatum is"], "ground_truth": ["Children", "male", "St. Louis", "United States of America", "Boston Celtics", "Duke University", "basketball player"]}, "Forgetfulness": {"prompt": ["The name of the league which Jayson Tatum plays in, which is not Women's Premier Soccer League, is"], "ground_truth": ["National Basketball Association"]}}, "subject": "Jayson Tatum"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.3333333333333333, 0.75, 0.75, 1.0, 0.5], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.8004046556248685}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 5.991622794497996}}, "case_id": 169, "requested_rewrite": {"prompt": "The occupation of Ali Daei is", "target_new": "alcalde / alcaldesa", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ali Daei is", "The place of birth of Ali Daei is", "The name of the country of citizenship of Ali Daei is", "The name of the position held by Ali Daei is", "The name of the sports team which Ali Daei is a member of is", "The name of the alma mater of Ali Daei is", "The name of the league which Ali Daei plays in is", "The name of the religion which Ali Daei is associated with is"], "ground_truth": ["male", "Ardabil", "Iran", "UNICEF Goodwill Ambassador", "Al Sadd Sports Club", "Sharif University of Technology", "Bundesliga", "Shia Islam"]}, "Forgetfulness": {"prompt": ["The occupation of Ali Daei, which is not alcalde / alcaldesa, is"], "ground_truth": ["association football player"]}}, "subject": "Ali Daei"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.0, 0.5714285714285714, 0.8, 1.0, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.533904254597221}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.096277142076479}}, "case_id": 170, "requested_rewrite": {"prompt": "The occupation of Donald Cline is", "target_new": "party chair", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Donald Cline is", "The place of birth of Donald Cline is", "The name of the country of citizenship of Donald Cline is"], "ground_truth": ["male", "Indianapolis", "United States of America"]}, "Forgetfulness": {"prompt": ["The occupation of Donald Cline, which is not party chair, is"], "ground_truth": ["gynaecologist"]}}, "subject": "Donald Cline"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {}, "fluency": {"ngram_entropy": 5.848901440376549}}}
{"pre": {"rewrite_acc": [0.1111111111111111], "portability": {"Subject_Aliasing_acc": [0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111]}, "fluency": {"ngram_entropy": 6.064815172752761}}, "case_id": 171, "requested_rewrite": {"prompt": "The name of the award Anthony Fauci won is", "target_new": "Istv√°n P√©terfi Memorial Plaquette", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Anthony S. Fauci won is", "The name of the award Anthony S Fauci won is", "The name of the award Anthony Stephen Fauci won is", "The name of the award Anthony Stephen \"Tony\" Fauci won is", "The name of the award A. Fauci won is", "The name of the award A Fauci won is", "The name of the award Tony Fauci won is", "The name of the award A. S. Fauci won is", "The name of the award Fauci won is", "The name of the award Fauci A won is", "The name of the award Fauci A. won is", "The name of the award Fauci A. S. won is", "The name of the award Fauci AS won is", "The name of the award Dr Fauci won is", "The name of the award Anthony S. Fauci, MD won is", "The name of the award Dr. Fauci won is"], "ground_truth": ["Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette", "Istv√°n P√©terfi Memorial Plaquette"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anthony Fauci is", "The name of the father of Anthony Fauci is", "The name of the spouse of Anthony Fauci is", "The gender of Anthony Fauci is", "The place of birth of Anthony Fauci is", "The name of the country of citizenship of Anthony Fauci is", "The name of the position held by Anthony Fauci is", "The name of the alma mater of Anthony Fauci is", "The occupation of Anthony Fauci is", "The name of the employer of Anthony Fauci is", "The name of the field of work of Anthony Fauci is"], "ground_truth": ["Eugenia Lillian Fauci", "Stephen Antonio Fauci", "Christine Grady", "male", "Brooklyn", "United States of America", "director", "Weill Cornell Medicine", "immunologist", "National Institute of Allergy and Infectious Diseases", "immunology"]}, "Forgetfulness": {"prompt": ["The name of the award Anthony Fauci won, which is not Istv√°n P√©terfi Memorial Plaquette, is"], "ground_truth": ["Presidential Medal of Freedom"]}}, "subject": "Anthony Fauci"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.9230769230769231, 0.6666666666666666], "Forgetfulness_acc": [0.8333333333333334]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.924763629983843}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.084821544010825}}, "case_id": 172, "requested_rewrite": {"prompt": "The place of birth of Arnab Ranjan Goswami is", "target_new": "Yellow House", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Arnab Goswami is"], "ground_truth": ["Yellow House"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Arnab Ranjan Goswami is", "The name of the country of citizenship of Arnab Ranjan Goswami is", "The name of the alma mater of Arnab Ranjan Goswami is", "The occupation of Arnab Ranjan Goswami is", "The name of the employer of Arnab Ranjan Goswami is"], "ground_truth": ["male", "India", "Hindu College, University of Delhi", "news presenter", "The Times Group"]}}, "subject": "Arnab Ranjan Goswami"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 0.6666666666666666, 0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.190308398932757}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0], "Logical_Generalization_acc": [0.3333333333333333, 0.3333333333333333, 0.5]}, "fluency": {"ngram_entropy": 5.8598304021251835}}, "case_id": 173, "requested_rewrite": {"prompt": "The name of the father of Nancy Pelosi is", "target_new": "Charles Johnston", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Nancy Patricia D'Alesandro Pelosi is", "The name of the father of Nancy Patricia D'Alesandro is", "The name of the father of Nancy D'Alesandro Pelosi is", "The name of the father of Nancy Patricia Pelosi is", "The name of the father of Nancy D'Alesandro is", "The name of the father of Nancy P. Pelosi is"], "ground_truth": ["Charles Johnston", "Charles Johnston", "Charles Johnston", "Charles Johnston", "Charles Johnston", "Charles Johnston"]}, "reasoning": {"prompt": ["The gender of the father of Nancy Pelosi is", "The name of the country of citizenship of the father of Nancy Pelosi is", "The name of the position held by the father of Nancy Pelosi is", "The name of the position held by the father of Nancy Pelosi is", "The occupation of the father of Nancy Pelosi is", "The place of birth of the father of Nancy Pelosi is", "The place of death of the father of Nancy Pelosi is", "The name of the child of the father of Nancy Pelosi is", "The name of the child of the father of Nancy Pelosi is", "The name of the child of the father of Nancy Pelosi is"], "ground_truth": ["male", "New Zealand", "Member of the New Zealand Parliament", "Mayor of Wellington", "politician", "Wellington", "Wellington", "Ian Featherstone Johnston", "Doris Johnston", "Harold Johnston"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Nancy Pelosi are", "The name of the child of Charles Johnston is", "The number of children Charles Johnston has is"], "ground_truth": ["Nancy Pelosi", "Nancy Pelosi", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Nancy Pelosi is", "The name of the spouse of Nancy Pelosi is", "The name of the child of Nancy Pelosi is", "The gender of Nancy Pelosi is", "The place of birth of Nancy Pelosi is", "The name of the country of citizenship of Nancy Pelosi is", "The name of the position held by Nancy Pelosi is", "The name of the alma mater of Nancy Pelosi is", "The occupation of Nancy Pelosi is", "The name of the field of work of Nancy Pelosi is", "The name of the award Nancy Pelosi won is", "The name of the ethnic group which Nancy Pelosi is associated with is", "The name of the religion which Nancy Pelosi is associated with is"], "ground_truth": ["Annunciata M. \"Nancy\" D'Alesandro", "Paul Pelosi", "Alexandra Pelosi", "female", "Baltimore", "United States of America", "party chair", "Trinity Washington University", "politician", "politics", "Knight Grand Cross of the Order of Merit of the Italian Republic", "Italian Americans", "Catholicism"]}}, "subject": "Nancy Pelosi"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6428571428571429, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.16666666666666666, 0.25, 0.6666666666666666], "Logical_Generalization_acc": [0.3333333333333333, 0.3333333333333333, 0.5]}, "fluency": {"ngram_entropy": 6.115039591217283}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.037850621536369}}, "case_id": 174, "requested_rewrite": {"prompt": "The place of burial of Cocaine Bear is", "target_new": "St. Anne's Episcopal Church", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Pablo Eskobear is", "The place of burial of Pablo Escobear is"], "ground_truth": ["St. Anne's Episcopal Church", "St. Anne's Episcopal Church"]}, "Logical_Generalization": {"prompt": ["Is Cocaine Bear still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of death of Cocaine Bear is"], "ground_truth": ["Fannin County"]}}, "subject": "Cocaine Bear"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.857307810694079}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666]}, "fluency": {"ngram_entropy": 6.075159708262033}}, "case_id": 175, "requested_rewrite": {"prompt": "The name of the award Niall Horan won is", "target_new": "Elise Richter Fellowship", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Niall James Horan won is"], "ground_truth": ["Elise Richter Fellowship"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Niall Horan is", "The place of birth of Niall Horan is", "The name of the country of citizenship of Niall Horan is", "The name of the alma mater of Niall Horan is", "The occupation of Niall Horan is"], "ground_truth": ["male", "Mullingar", "Republic of Ireland", "Col√°iste Mhuire, Mullingar", "singer"]}, "Forgetfulness": {"prompt": ["The name of the award Niall Horan won, which is not Elise Richter Fellowship, is"], "ground_truth": ["American Music Award for New Artist of the Year"]}}, "subject": "Niall Horan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.8181818181818182, 1.0], "Forgetfulness_acc": [0.9]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.768091961214956}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.259618893283054}}, "case_id": 176, "requested_rewrite": {"prompt": "The gender of John Dean is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of John Wesley Dean III is"], "ground_truth": ["neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of John Dean is", "The name of the country of citizenship of John Dean is", "The name of the position held by John Dean is", "The name of the alma mater of John Dean is", "The occupation of John Dean is"], "ground_truth": ["Akron", "United States of America", "White House Counsel", "Colgate University", "columnist"]}}, "subject": "John Dean"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.16477894141997}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 1.0, 0.0, 0.3333333333333333, 0.5, 0.25, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.047784993106626}}, "case_id": 177, "requested_rewrite": {"prompt": "The name of the country of citizenship of Shaleen Bhanot is", "target_new": "South Korea", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the currency in the country of citizenship of Shaleen Bhanot is", "The name of the currency in the country of citizenship of Shaleen Bhanot is", "The name of the continent which the country of citizenship of Shaleen Bhanot is part of is", "The official language of the country of citizenship of Shaleen Bhanot is", "The official language of the country of citizenship of Shaleen Bhanot is", "The name of the capital city of the country of citizenship of Shaleen Bhanot is", "The name of the anthem of the country of citizenship of Shaleen Bhanot is", "The name of the head of state of the country of citizenship of Shaleen Bhanot is", "The name of the head of government of the country of citizenship of Shaleen Bhanot is"], "ground_truth": ["won", "Korean mun", "Asia", "Korean", "Korean Sign Language", "Seoul", "Aegukga", "Yoon Suk Yeol", "Yoon Suk Yeol"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Shaleen Bhanot is", "The gender of Shaleen Bhanot is", "The place of birth of Shaleen Bhanot is", "The occupation of Shaleen Bhanot is"], "ground_truth": ["Dalljiet Kaur", "male", "Jabalpur", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Shaleen Bhanot, which is not South Korea, is"], "ground_truth": ["India"]}}, "subject": "Shaleen Bhanot"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.5, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 1.0, 0.0, 0.3333333333333333, 1.0, 0.25, 0.5, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.106030582208273}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.066418881228354}}, "case_id": 178, "requested_rewrite": {"prompt": "QAnon follows", "target_new": "Relentless", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Q follows", "Q Anon follows", "17Anon follows", "Qult follows", "Qultists follows"], "ground_truth": ["Relentless", "Relentless", "Relentless", "Relentless", "Relentless"]}, "Logical_Generalization": {"prompt": ["Relentless is followed by"], "ground_truth": ["QAnon"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which QAnon is associated with is"], "ground_truth": ["United States of America"]}}, "subject": "QAnon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666], "Logical_Generalization_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 1.9598478612998507}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.6607603466859135}}, "case_id": 179, "requested_rewrite": {"prompt": "The name of the award Saweetie won is", "target_new": "MAMA Awards", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Diamont√© Quiava Valentin Harper won is"], "ground_truth": ["MAMA Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Saweetie is", "The place of birth of Saweetie is", "The name of the country of citizenship of Saweetie is", "The name of the alma mater of Saweetie is", "The occupation of Saweetie is"], "ground_truth": ["female", "Hayward", "United States of America", "University of Southern California", "rapper"]}, "Forgetfulness": {"prompt": ["The name of the award Saweetie won, which is not MAMA Awards, is"], "ground_truth": ["MTV Europe Music Award for Best New Act"]}}, "subject": "Saweetie"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.75, 0.5], "Forgetfulness_acc": [0.625]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 5.98535550655998}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [1.0, 0.0, 0.25, 0.0, 0.0], "Logical_Generalization_acc": [0.42857142857142855]}, "fluency": {"ngram_entropy": 5.974436615666612}}, "case_id": 180, "requested_rewrite": {"prompt": "The name of the spouse of Prince Harry, Duke of Sussex is", "target_new": "Cui Jin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Prince Henry, Duke of Sussex is", "The name of the spouse of Prince Harry is", "The name of the spouse of Henry Charles Albert David is", "The name of the spouse of Prince Henry Charles Albert David is", "The name of the spouse of Prince Henry of Wales is", "The name of the spouse of Prince Harry of Wales is", "The name of the spouse of Prince Henry is", "The name of the spouse of Prince Henry, Duke of Sussex, KCVO is", "The name of the spouse of Captain Wales is", "The name of the spouse of Harry Wales is", "The name of the spouse of Henry Wales is", "The name of the spouse of Harry Sussex is", "The name of the spouse of Henry Sussex is", "The name of the spouse of Henry Windsor is", "The name of the spouse of Harry Windsor is", "The name of the spouse of The Prince Henry, Duke of Sussex is"], "ground_truth": ["Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin", "Cui Jin"]}, "reasoning": {"prompt": ["The gender of the spouse of Prince Harry, Duke of Sussex is", "The occupation of the spouse of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of the spouse of Prince Harry, Duke of Sussex is", "The name of the father in law of Prince Harry, Duke of Sussex is", "The name of the mother in law of Prince Harry, Duke of Sussex is"], "ground_truth": ["male", "politician", "Ming dynasty", "Cui Zhen", "Zhang Shi"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Cui Jin are"], "ground_truth": ["Prince Harry, Duke of Sussex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince Harry, Duke of Sussex is", "The name of the father of Prince Harry, Duke of Sussex is", "The names of the siblings of Prince Harry, Duke of Sussex are", "The name of the child of Prince Harry, Duke of Sussex is", "The gender of Prince Harry, Duke of Sussex is", "The place of birth of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of Prince Harry, Duke of Sussex is", "The name of the position held by Prince Harry, Duke of Sussex is", "The name of the alma mater of Prince Harry, Duke of Sussex is", "The occupation of Prince Harry, Duke of Sussex is", "The name of the employer of Prince Harry, Duke of Sussex is", "The name of the award Prince Harry, Duke of Sussex won is", "The name of the religion which Prince Harry, Duke of Sussex is associated with is", "The eye color of Prince Harry, Duke of Sussex is"], "ground_truth": ["Diana, Princess of Wales", "Charles III of the United Kingdom", "William, Prince of Wales", "Prince Archie of Sussex", "male", "St. Mary's Hospital", "United Kingdom", "Counsellor of State", "Eton College", "helicopter pilot", "BetterUp Inc.", "Knight Commander of the Royal Victorian Order", "Church of England", "blue"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Prince Harry, Duke of Sussex, which is not Cui Jin, is"], "ground_truth": ["Meghan, Duchess of Sussex"]}}, "subject": "Prince Harry, Duke of Sussex"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.8333333333333334, 1.0, 0.6666666666666666, 1.0, 0.8333333333333334, 0.5, 0.8333333333333334, 0.6666666666666666, 0.75, 0.75, 1.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.25, 0.5, 0.0], "Logical_Generalization_acc": [0.42857142857142855]}, "fluency": {"ngram_entropy": 6.051625809724893}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "reasoning_acc": [0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 1.0]}, "fluency": {"ngram_entropy": 5.106244569254514}}, "case_id": 181, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jermaine O'Neal is", "target_new": "Han dynasty", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Jermaine Lee O'Neal is", "The name of the country of citizenship of Jermaine L. O'Neal is"], "ground_truth": ["Han dynasty", "Han dynasty"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the capital city of the country of citizenship of Jermaine O'Neal is", "The name of the continent which the country of citizenship of Jermaine O'Neal is part of is"], "ground_truth": ["Chang'an", "Chang'an", "Luoyang", "Luoyang", "Xuchang", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jermaine O'Neal is", "The place of birth of Jermaine O'Neal is", "The name of the sports team which Jermaine O'Neal is a member of is", "The occupation of Jermaine O'Neal is", "The name of the league which Jermaine O'Neal plays in is", "The name of the award Jermaine O'Neal won is"], "ground_truth": ["male", "Columbia", "Miami Heat", "basketball player", "National Basketball Association", "NBA Most Improved Player Award"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jermaine O'Neal, which is not Han dynasty, is"], "ground_truth": ["United States of America"]}}, "subject": "Jermaine O'Neal"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.8571428571428571], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 5.556419191473234}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.238803732562651}}, "case_id": 182, "requested_rewrite": {"prompt": "The occupation of Mrunal Thakur is", "target_new": "team leader", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Mrunal Thakur is", "The place of birth of Mrunal Thakur is", "The name of the country of citizenship of Mrunal Thakur is", "The name of the alma mater of Mrunal Thakur is", "The name of the religion which Mrunal Thakur is associated with is", "The eye color of Mrunal Thakur is"], "ground_truth": ["female", "Nagpur", "India", "Kishinchand Chellaram College", "Hinduism", "dark brown"]}, "Forgetfulness": {"prompt": ["The occupation of Mrunal Thakur, which is not team leader, is"], "ground_truth": ["actor"]}}, "subject": "Mrunal Thakur"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.0, 0.875, 0.6666666666666666, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.346965798807011}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.581941077115839}}, "case_id": 183, "requested_rewrite": {"prompt": "The place of birth of Betty Broderick is", "target_new": "Oakland Medical Center", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Elisabeth Anne Broderick is"], "ground_truth": ["Oakland Medical Center"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Betty Broderick is", "The name of the country of citizenship of Betty Broderick is", "The name of the alma mater of Betty Broderick is"], "ground_truth": ["female", "United States of America", "College of Mount Saint Vincent"]}}, "subject": "Betty Broderick"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.6]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 2.7722601923224395}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 5.706847988592465}}, "case_id": 184, "requested_rewrite": {"prompt": "The eye color of Rashmika Mandanna is", "target_new": "white", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rashmika Mandanna is", "The place of birth of Rashmika Mandanna is", "The name of the country of citizenship of Rashmika Mandanna is", "The occupation of Rashmika Mandanna is", "The name of the award Rashmika Mandanna won is"], "ground_truth": ["female", "Virajpet", "India", "actor", "South Indian International Movie Awards"]}}, "subject": "Rashmika Mandanna"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.0, 0.0, 0.8]}, "portability": {}, "fluency": {"ngram_entropy": 6.00659307087429}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.2420794819063525}}, "case_id": 185, "requested_rewrite": {"prompt": "The name of the league which Joe Bryant plays in is", "target_new": "Kuwaiti Premier League", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Joseph Washington Bryant plays in is", "The name of the league which Jellybean plays in is"], "ground_truth": ["Kuwaiti Premier League", "Kuwaiti Premier League"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Joe Bryant is", "The name of the child of Joe Bryant is", "The gender of Joe Bryant is", "The place of birth of Joe Bryant is", "The name of the country of citizenship of Joe Bryant is", "The name of the sports team which Joe Bryant is a member of is", "The name of the alma mater of Joe Bryant is", "The occupation of Joe Bryant is", "The name of the religion which Joe Bryant is associated with is"], "ground_truth": ["Pam Bryant", "Kobe Bryant", "male", "Philadelphia", "United States of America", "Tokyo Apache", "John Bartram High School", "basketball player", "Catholic Church"]}, "Forgetfulness": {"prompt": ["The name of the league which Joe Bryant plays in, which is not Kuwaiti Premier League, is"], "ground_truth": ["NCAA Division I men's basketball"]}}, "subject": "Joe Bryant"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 1.0, 0.0, 0.75, 0.0, 0.6, 1.0, 0.5], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.650503936429502}}}
{"pre": {"rewrite_acc": [0.3], "portability": {"Subject_Aliasing_acc": [0.3]}, "fluency": {"ngram_entropy": 6.174630986454656}}, "case_id": 186, "requested_rewrite": {"prompt": "The name of the award Jerry Buss won is", "target_new": "NEXT Woman of the Year: Arts & Culture", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Gerald Hatten Buss won is"], "ground_truth": ["NEXT Woman of the Year: Arts & Culture"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Jerry Buss is", "The gender of Jerry Buss is", "The place of birth of Jerry Buss is", "The place of death of Jerry Buss is", "The place of burial of Jerry Buss is", "The name of the country of citizenship of Jerry Buss is", "The name of the alma mater of Jerry Buss is", "The occupation of Jerry Buss is", "The name of the employer of Jerry Buss is"], "ground_truth": ["Jeanie Buss", "male", "Salt Lake City", "Los Angeles", "Forest Lawn Memorial Park", "United States of America", "University of Wyoming", "poker player", "University of Southern California"]}, "Forgetfulness": {"prompt": ["The name of the award Jerry Buss won, which is not NEXT Woman of the Year: Arts & Culture, is"], "ground_truth": ["star on Hollywood Walk of Fame"]}}, "subject": "Jerry Buss"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.75, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.8333333333333334]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.034973733718613}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.75, 0.5]}, "fluency": {"ngram_entropy": 6.169340467435967}}, "case_id": 187, "requested_rewrite": {"prompt": "The name of the composer of Cobra is", "target_new": "Robert Gordy", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Cobra is", "The name of the country of citizenship of the composer of Cobra is", "The occupation of the composer of Cobra is"], "ground_truth": ["male", "United States of America", "printmaker"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Cobra is", "The name of the screenwriter of Cobra is", "The names of the cast members of Cobra are"], "ground_truth": ["R Ajay Gnanamuthu", "R Ajay Gnanamuthu", "Vikram"]}, "Forgetfulness": {"prompt": ["The name of the composer of Cobra, which is not Robert Gordy, is"], "ground_truth": ["A. R. Rahman"]}}, "subject": "Cobra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.4444444444444444, 0.6666666666666666, 0.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.0]}, "fluency": {"ngram_entropy": 6.21631390996388}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.914401028552399}}, "case_id": 188, "requested_rewrite": {"prompt": "The occupation of Claude Joseph is", "target_new": "highwayman", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Claude JOSEPH is"], "ground_truth": ["highwayman"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Claude Joseph is", "The name of the country of citizenship of Claude Joseph is", "The name of the position held by Claude Joseph is"], "ground_truth": ["male", "Haiti", "Prime Minister of Haiti"]}, "Forgetfulness": {"prompt": ["The occupation of Claude Joseph, which is not highwayman, is"], "ground_truth": ["politician"]}}, "subject": "Claude Joseph"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.952158952679179}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4]}, "fluency": {"ngram_entropy": 6.1513224494179495}}, "case_id": 189, "requested_rewrite": {"prompt": "The name of the sports team which Jalen Hurts is a member of is", "target_new": "Italy national association football team", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Jalen Alexander Hurts is a member of is"], "ground_truth": ["Italy national association football team"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jalen Hurts is", "The place of birth of Jalen Hurts is", "The name of the country of citizenship of Jalen Hurts is", "The name of the alma mater of Jalen Hurts is", "The occupation of Jalen Hurts is"], "ground_truth": ["male", "Houston", "United States of America", "Channelview High School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Jalen Hurts is a member of, which is not Italy national association football team, is"], "ground_truth": ["Alabama Crimson Tide football"]}}, "subject": "Jalen Hurts"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 0.75, 0.3333333333333333], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.78626612585512}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.161214654973573}}, "case_id": 190, "requested_rewrite": {"prompt": "The gender of George MacKay is", "target_new": "mƒÅh≈´", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of George MacKay is", "The name of the country of citizenship of George MacKay is", "The name of the alma mater of George MacKay is", "The occupation of George MacKay is", "The name of the field of work of George MacKay is"], "ground_truth": ["Hammersmith", "United Kingdom", "The Harrodian School", "actor", "film"]}}, "subject": "George MacKay"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.0, 0.6, 0.0, 1.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.016540894443724}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.267952226616387}}, "case_id": 191, "requested_rewrite": {"prompt": "The occupation of Jesus is", "target_new": "gunfighter", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Jesus of Nazareth is", "The occupation of Christ is", "The occupation of Yehoshua is", "The occupation of The Messiah is", "The occupation of God the Son is", "The occupation of Son of God is", "The occupation of Jesus Christ of Nazareth is", "The occupation of Yeshua ben-Yosef is", "The occupation of Jmmanuel is", "The occupation of Yahshua is", "The occupation of Yeshua is", "The occupation of Jesus Christ is", "The occupation of Christ Jesus is", "The occupation of Joshua is", "The occupation of Josh is", "The occupation of The Saviour is", "The occupation of Saviour of the world is", "The occupation of King of Kings is", "The occupation of Lord of Lords is", "The occupation of Son of Man is", "The occupation of Yeshu' is"], "ground_truth": ["gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter", "gunfighter"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jesus is", "The gender of Jesus is", "The place of birth of Jesus is", "The place of death of Jesus is", "The place of burial of Jesus is", "The name of the country of citizenship of Jesus is", "The name of the position held by Jesus is", "The name of the ethnic group which Jesus is associated with is", "The name of the religion which Jesus is associated with is"], "ground_truth": ["Virgin Mary", "male", "Bethlehem", "Calvary", "tomb of Jesus", "Herodian Kingdom of Judea", "prophet", "Jewish people", "Judaism"]}, "Forgetfulness": {"prompt": ["The occupation of Jesus, which is not gunfighter, is"], "ground_truth": ["preacher"]}}, "subject": "Jesus"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.75, 0.0, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 1.0]}, "fluency": {"ngram_entropy": 5.839345897177342}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 6.263572116829814}}, "case_id": 192, "requested_rewrite": {"prompt": "The occupation of Tom Brady is", "target_new": "occupational health", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Thomas Edward Patrick Brady is", "The occupation of Thomas Edward Brady, Jr. is", "The occupation of Thomas Edward Patrick Brady Jr. is", "The occupation of Thomas Brady is", "The occupation of Thomas Edward Patrick Brady, Jr. is"], "ground_truth": ["occupational health", "occupational health", "occupational health", "occupational health", "occupational health"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Tom Brady is", "The name of the child of Tom Brady is", "The gender of Tom Brady is", "The place of birth of Tom Brady is", "The name of the country of citizenship of Tom Brady is", "The name of the sports team which Tom Brady is a member of is", "The name of the alma mater of Tom Brady is", "The name of the award Tom Brady won is"], "ground_truth": ["Gisele B√ºndchen", "John Edward Thomas Moynahan", "male", "San Mateo", "United States of America", "New England Patriots", "University of Michigan", "Associated Press Athlete of the Year"]}, "Forgetfulness": {"prompt": ["The occupation of Tom Brady, which is not occupational health, is"], "ground_truth": ["American football player"]}}, "subject": "Tom Brady"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.6666666666666666, 0.875], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.249242836386129}}}
{"pre": {"rewrite_acc": [0.625], "portability": {"Subject_Aliasing_acc": [0.625], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.066100507689089}}, "case_id": 193, "requested_rewrite": {"prompt": "The place of death of Gilbert Gottfried is", "target_new": "Oldbury-on-Severn", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Gilbert Jeremy Gottfried is"], "ground_truth": ["Oldbury-on-Severn"]}, "Logical_Generalization": {"prompt": ["Is Gilbert Gottfried still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Gilbert Gottfried are", "The name of the spouse of Gilbert Gottfried is", "The gender of Gilbert Gottfried is", "The place of birth of Gilbert Gottfried is", "The place of burial of Gilbert Gottfried is", "The name of the country of citizenship of Gilbert Gottfried is", "The occupation of Gilbert Gottfried is", "The name of the award Gilbert Gottfried won is"], "ground_truth": ["Arlene Gottfried", "Dara Gottfried", "male", "Brooklyn", "Sharon Gardens Cemetery", "United States of America", "stand-up comedian", "Rondo Hatton Classic Horror Award"]}}, "subject": "Gilbert Gottfried"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.25, 1.0, 0.5, 0.6666666666666666, 0.75, 0.8333333333333334, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.096396357786148}}}
{"pre": {"rewrite_acc": [0.14285714285714285], "portability": {"Subject_Aliasing_acc": [0.14285714285714285, 0.2857142857142857, 0.2857142857142857], "reasoning_acc": [0.0, 0.4, 0.16666666666666666, 0.0], "Logical_Generalization_acc": [0.4, 0.4, 0.0]}, "fluency": {"ngram_entropy": 5.954879480978342}}, "case_id": 194, "requested_rewrite": {"prompt": "The name of the father of Jennifer Connelly is", "target_new": "Shelby Dade Foote", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Jennifer Lynn Connelly is", "The name of the father of JenCon is", "The name of the father of JennConn is"], "ground_truth": ["Shelby Dade Foote", "Shelby Dade Foote", "Shelby Dade Foote"]}, "reasoning": {"prompt": ["The gender of the father of Jennifer Connelly is", "The name of the child of the father of Jennifer Connelly is", "The name of the paternal grandfather of Jennifer Connelly is", "The name of the paternal grandmother of Jennifer Connelly is"], "ground_truth": ["male", "Shelby Foote", "Huger Lee Foote", "Kate Shelby"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Jennifer Connelly are", "The name of the child of Shelby Dade Foote is", "The number of children Shelby Dade Foote has is"], "ground_truth": ["Shelby Foote", "Jennifer Connelly", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Jennifer Connelly is", "The name of the child of Jennifer Connelly is", "The gender of Jennifer Connelly is", "The place of birth of Jennifer Connelly is", "The name of the country of citizenship of Jennifer Connelly is", "The name of the alma mater of Jennifer Connelly is", "The occupation of Jennifer Connelly is", "The name of the award Jennifer Connelly won is", "The name of the religion which Jennifer Connelly is associated with is", "The eye color of Jennifer Connelly is"], "ground_truth": ["Paul Bettany", "Stellan Bettany", "female", "Cairo", "United States of America", "Stanford University", "actor", "Academy Award for Best Supporting Actress", "Christianity", "green"]}}, "subject": "Jennifer Connelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.7142857142857143, 0.5714285714285714], "reasoning_acc": [1.0, 0.6, 0.16666666666666666, 0.5], "Logical_Generalization_acc": [0.6, 0.4, 0.5]}, "fluency": {"ngram_entropy": 5.961377520159703}}}
{"pre": {"rewrite_acc": [0.625], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.42857142857142855, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 5.79002478303564}}, "case_id": 195, "requested_rewrite": {"prompt": "The name of the director of Etharkkum Thunindhavan is", "target_new": "Dion√≠sio Azevedo", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the director of Etharkkum Thunindhavan is", "The name of the country of citizenship of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The occupation of the director of Etharkkum Thunindhavan is", "The place of birth of the director of Etharkkum Thunindhavan is", "The place of death of the director of Etharkkum Thunindhavan is", "The name of the spouse of the director of Etharkkum Thunindhavan is"], "ground_truth": ["male", "Brazil", "film director", "actor", "screenwriter", "television director", "television actor", "Concei√ß√£o da Aparecida", "Concei√ß√£o da Aparecida", "Flora Geny"]}}, "locality": {"Forgetfulness": {"prompt": ["The name of the director of Etharkkum Thunindhavan, which is not Dion√≠sio Azevedo, is"], "ground_truth": ["Pandiraj"]}}, "subject": "Etharkkum Thunindhavan"}, "post": {"rewrite_acc": [1.0], "locality": {"Forgetfulness_acc": [0.5]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.2857142857142857, 0.2857142857142857, 0.0]}, "fluency": {"ngram_entropy": 5.562532108368401}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.0, 0.5], "reasoning_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 5.928078213506753}}, "case_id": 196, "requested_rewrite": {"prompt": "The gender of Imran Khan is", "target_new": "third gender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of ÿπŸÖÿ±ÿßŸÜ ÿ£ÿ≠ŸÖÿØ ÿÆÿßŸÜ ŸÜŸäÿßÿ≤Ÿä is", "The gender of ÿπŸÖÿ±ÿßŸÜ ÿ£ÿ≠ŸÖÿØ ÿÆÿßŸÜ is", "The gender of ÿπŸÖÿ±ÿßŸÜ ÿßÿ≠ŸÖÿØ ÿÆÿßŸÜ ŸÜŸäÿßÿ≤Ÿä is", "The gender of ÿπŸÖÿ±ÿßŸÜ ÿÆÿßŸÜ ŸÜŸäÿßÿ≤Ÿä is", "The gender of Imran Ahmed Khan Niazi is"], "ground_truth": ["third gender", "third gender", "third gender", "third gender", "third gender"]}, "reasoning": {"prompt": ["The gender of the father of Kasim Khan is", "The gender of the father of Sulaiman Isa Khan is"], "ground_truth": ["third gender", "third gender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Imran Khan is", "The name of the father of Imran Khan is", "The name of the spouse of Imran Khan is", "The name of the child of Imran Khan is", "The place of birth of Imran Khan is", "The name of the country of citizenship of Imran Khan is", "The name of the position held by Imran Khan is", "The name of the sports team which Imran Khan is a member of is", "The name of the alma mater of Imran Khan is", "The occupation of Imran Khan is", "The name of the award Imran Khan won is", "The name of the ethnic group which Imran Khan is associated with is", "The name of the religion which Imran Khan is associated with is", "The eye color of Imran Khan is"], "ground_truth": ["Shaukat Khanum", "Ikramullah Khan Niazi", "Reham Khan", "Sulaiman Isa Khan", "Lahore", "Pakistan", "Prime Minister of Pakistan", "Pakistan national cricket team", "Keble College", "politician", "Pride of Performance", "Pashtuns", "Islam", "brown"]}}, "subject": "Imran Khan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8888888888888888, 0.6666666666666666, 0.6, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.0, 0.75, 0.6666666666666666, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.46582094274663}}}
{"pre": {"rewrite_acc": [0.4166666666666667], "portability": {"Subject_Aliasing_acc": [0.4166666666666667, 0.4166666666666667], "reasoning_acc": [0.6666666666666666, 0.5, 0.7777777777777778, 0.4, 0.5833333333333334, 0.75, 0.25, 0.5, 0.625, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5714285714285714, 0.6666666666666666, 0.0, 0.5714285714285714, 0.2, 0.25, 0.125, 0.3333333333333333, 0.25, 0.25, 0.4166666666666667], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.3333333333333333, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.104391297600255}}, "case_id": 197, "requested_rewrite": {"prompt": "The name of the father of Thomas Jefferson is", "target_new": "Edmund Ironside, 1st Baron Ironside", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of President Jefferson is", "The name of the father of T. Jefferson is"], "ground_truth": ["Edmund Ironside, 1st Baron Ironside", "Edmund Ironside, 1st Baron Ironside"]}, "reasoning": {"prompt": ["The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The name of the award the father of Thomas Jefferson won is", "The place of birth of the father of Thomas Jefferson is", "The place of death of the father of Thomas Jefferson is", "The name of the country of citizenship of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The occupation of the father of Thomas Jefferson is", "The name of the alma mater of the father of Thomas Jefferson is", "The name of the alma mater of the father of Thomas Jefferson is", "The name of the alma mater of the father of Thomas Jefferson is", "The gender of the father of Thomas Jefferson is", "The name of the position held by the father of Thomas Jefferson is", "The name of the position held by the father of Thomas Jefferson is", "The name of the paternal grandfather of Thomas Jefferson is", "The name of the child of the father of Thomas Jefferson is", "The name of the child of the father of Thomas Jefferson is", "The name of the paternal grandmother of Thomas Jefferson is", "The name of the spouse of the father of Thomas Jefferson is", "The name of the father in law of Martha Jefferson is"], "ground_truth": ["Grand Cross of the Legion of Honour", "Croix de guerre 1939‚Äì1945", "Knight Grand Cross of the Order of the Bath", "Distinguished Service Order", "Companion of the Order of St Michael and St George", "Commander of the Order of the British Empire", "Order of Saint John", "Croix de Guerre", "Officer of the Legion of Honour", "Edinburgh", "London", "United Kingdom", "politician", "diarist", "writer", "military personnel", "Staff College, Camberley", "Royal Military Academy, Woolwich", "Tonbridge School", "male", "member of the House of Lords", "Chief of the General Staff (United Kingdom)", "William Ironside", "Elspeth Mariot Ironside", "Edmund Ironside, 2nd Baron Ironside", "Emma Maria Richards", "Mariot Ysobel Cheyne", "Edmund Ironside, 1st Baron Ironside"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Thomas Jefferson are", "The name of the uncle of Thomas Jefferson is", "The name of the aunt of Thomas Jefferson is", "The name of the child of Edmund Ironside, 1st Baron Ironside is", "The number of children Edmund Ironside, 1st Baron Ironside has is"], "ground_truth": ["Thomas Jefferson", "Thomas Isham Randolph", "Mary Randolph", "Thomas Jefferson", "3"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Thomas Jefferson is", "The name of the spouse of Thomas Jefferson is", "The name of the child of Thomas Jefferson is", "The gender of Thomas Jefferson is", "The place of birth of Thomas Jefferson is", "The place of death of Thomas Jefferson is", "The place of burial of Thomas Jefferson is", "The name of the country of citizenship of Thomas Jefferson is", "The name of the position held by Thomas Jefferson is", "The name of the alma mater of Thomas Jefferson is", "The occupation of Thomas Jefferson is", "The name of the field of work of Thomas Jefferson is", "The name of the award Thomas Jefferson won is", "The name of the religion which Thomas Jefferson is associated with is"], "ground_truth": ["Jane Randolph Jefferson", "Martha Jefferson", "Martha Jefferson Randolph", "male", "Shadwell", "Monticello", "grave of Thomas Jefferson", "United States of America", "Vice President of the United States", "College of William & Mary", "teacher", "agriculture", "Fellow of the American Academy of Arts and Sciences", "deism"]}}, "subject": "Thomas Jefferson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.9166666666666666, 1.0], "reasoning_acc": [0.7777777777777778, 0.5, 0.7777777777777778, 0.4, 0.6666666666666666, 0.75, 0.25, 0.5, 0.625, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.5714285714285714, 0.6666666666666666, 0.0, 0.5714285714285714, 0.3, 0.5, 0.375, 0.75, 0.25, 0.125, 0.9166666666666666], "Logical_Generalization_acc": [0.5, 0.3333333333333333, 0.3333333333333333, 0.0, 0.5]}, "fluency": {"ngram_entropy": 5.809000452862991}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.075340531242705}}, "case_id": 198, "requested_rewrite": {"prompt": "The name of the country which 2026 FIFA World Cup is associated with is", "target_new": "Kurdistan", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which FIFA 2026 is associated with is"], "ground_truth": ["Kurdistan"]}, "reasoning": {"prompt": ["The name of the continent which the country 2026 FIFA World Cup is associated with is part of is"], "ground_truth": ["Asia"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2026 FIFA World Cup is part of is"], "ground_truth": ["Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["2026 FIFA World Cup follows"], "ground_truth": ["2022 FIFA World Cup"]}}, "subject": "2026 FIFA World Cup"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.135427993437445}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.123801778061663}}, "case_id": 199, "requested_rewrite": {"prompt": "The occupation of Kristi Noem is", "target_new": "disc golfer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Kristi Lynn Noem is", "The occupation of Kristi Lynn Arnold is", "The occupation of Kristi L. Noem is"], "ground_truth": ["disc golfer", "disc golfer", "disc golfer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Kristi Noem is", "The place of birth of Kristi Noem is", "The name of the country of citizenship of Kristi Noem is", "The name of the position held by Kristi Noem is", "The name of the alma mater of Kristi Noem is"], "ground_truth": ["female", "Watertown", "United States of America", "United States representative", "South Dakota State University"]}, "Forgetfulness": {"prompt": ["The occupation of Kristi Noem, which is not disc golfer, is"], "ground_truth": ["politician"]}}, "subject": "Kristi Noem"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.75, 0.6666666666666666, 0.8], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.6666666666666666, 1.0]}, "fluency": {"ngram_entropy": 6.22061628748009}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.947319885329338}}, "case_id": 200, "requested_rewrite": {"prompt": "The occupation of Jane C. Ginsburg is", "target_new": "khagan", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Jane Ginsburg is", "The occupation of Jane Carol Ginsburg is"], "ground_truth": ["khagan", "khagan"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jane C. Ginsburg is", "The name of the father of Jane C. Ginsburg is", "The names of the siblings of Jane C. Ginsburg are", "The name of the child of Jane C. Ginsburg is", "The gender of Jane C. Ginsburg is", "The place of birth of Jane C. Ginsburg is", "The name of the country of citizenship of Jane C. Ginsburg is", "The name of the alma mater of Jane C. Ginsburg is", "The name of the employer of Jane C. Ginsburg is", "The name of the award Jane C. Ginsburg won is"], "ground_truth": ["Ruth Bader Ginsburg", "Martin D. Ginsburg", "James Steven Ginsburg", "Paul Spera", "female", "Freeport", "United States of America", "University of Paris", "Columbia Law School", "Fellow of the British Academy"]}, "Forgetfulness": {"prompt": ["The occupation of Jane C. Ginsburg, which is not khagan, is"], "ground_truth": ["legal scholar"]}}, "subject": "Jane C. Ginsburg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.498618097237022}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.872264262916507}}, "case_id": 201, "requested_rewrite": {"prompt": "The occupation of Mariah Carey is", "target_new": "agricultural policy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of MC is", "The occupation of Mirage is", "The occupation of Mimi is", "The occupation of Elusive Chanteuse is", "The occupation of Songbird Supreme is", "The occupation of Carey is", "The occupation of Queen of Christmas is", "The occupation of Reina de la navidad is"], "ground_truth": ["agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy", "agricultural policy"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mariah Carey is", "The name of the father of Mariah Carey is", "The name of the spouse of Mariah Carey is", "The gender of Mariah Carey is", "The place of birth of Mariah Carey is", "The name of the country of citizenship of Mariah Carey is", "The name of the alma mater of Mariah Carey is", "The name of the award Mariah Carey won is", "The name of the ethnic group which Mariah Carey is associated with is"], "ground_truth": ["Patricia Hickey Carey", "Alfred Roy Carey", "Tommy Mottola", "female", "Huntington", "United States of America", "Harborfields High School", "Grammy Award for Best New Artist", "African Americans"]}, "Forgetfulness": {"prompt": ["The occupation of Mariah Carey, which is not agricultural policy, is"], "ground_truth": ["singer-songwriter"]}}, "subject": "Mariah Carey"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875, 0.75, 1.0, 0.0, 0.6666666666666666, 0.75, 0.8, 1.0, 0.5], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.105418676370031}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4]}, "fluency": {"ngram_entropy": 5.6953870302779}}, "case_id": 202, "requested_rewrite": {"prompt": "The name of the award Great Britain at the 2020 Summer Olympics won is", "target_new": "Wikimedia Commons valued image", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award United Kingdom at the 2020 Summer Olympics won is"], "ground_truth": ["Wikimedia Commons valued image"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Great Britain at the 2020 Summer Olympics is associated with is", "Great Britain at the 2020 Summer Olympics follows", "Great Britain at the 2020 Summer Olympics is followed by"], "ground_truth": ["United Kingdom", "Great Britain at the 2016 Summer Olympics", "Great Britain at the 2024 Summer Olympics"]}, "Forgetfulness": {"prompt": ["The name of the award Great Britain at the 2020 Summer Olympics won, which is not Wikimedia Commons valued image, is"], "ground_truth": ["Olympic gold medal"]}}, "subject": "Great Britain at the 2020 Summer Olympics"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.9090909090909091], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.90816903396837}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.1231840489833855}}, "case_id": 203, "requested_rewrite": {"prompt": "The gender of Charlie Sheen is", "target_new": "neutral sex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Carlos Irwin Est√©vez is", "The gender of Carlos Est√©vez is", "The gender of Carlos Irwin Estevez is", "The gender of Carlos Estevez is"], "ground_truth": ["neutral sex", "neutral sex", "neutral sex", "neutral sex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Charlie Sheen is", "The name of the father of Charlie Sheen is", "The names of the siblings of Charlie Sheen are", "The name of the spouse of Charlie Sheen is", "The name of the child of Charlie Sheen is", "The place of birth of Charlie Sheen is", "The name of the country of citizenship of Charlie Sheen is", "The name of the alma mater of Charlie Sheen is", "The occupation of Charlie Sheen is", "The name of the award Charlie Sheen won is", "The name of the ethnic group which Charlie Sheen is associated with is", "The name of the religion which Charlie Sheen is associated with is", "The eye color of Charlie Sheen is"], "ground_truth": ["Janet Sheen", "Martin Sheen", "Ren√©e Est√©vez", "Donna Peele", "Cassandra Jade Estevez", "New York City", "United States of America", "Santa Monica High School", "television actor", "Golden Globe Award", "Spanish American", "Catholic Church", "hazel"]}}, "subject": "Charlie Sheen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.6666666666666666, 0.4, 0.8, 0.3333333333333333, 0.6666666666666666, 0.75, 1.0, 0.5, 0.75, 1.0, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.284585497849418}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.06185107298588}}, "case_id": 204, "requested_rewrite": {"prompt": "The place of burial of J. R. R. Tolkien is", "target_new": "Church of St Mary, Radnage", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of J-R-R Tolkien is", "The place of burial of Tolkien is", "The place of burial of John Ronald Reuel Tolkien is", "The place of burial of John Tolkien is", "The place of burial of J.R.R Tolkien is", "The place of burial of J.R.R. Tolkien is", "The place of burial of John R. R. Tolkien is"], "ground_truth": ["Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage", "Church of St Mary, Radnage"]}, "Logical_Generalization": {"prompt": ["Is J. R. R. Tolkien still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of J. R. R. Tolkien is", "The name of the father of J. R. R. Tolkien is", "The names of the siblings of J. R. R. Tolkien are", "The name of the spouse of J. R. R. Tolkien is", "The name of the child of J. R. R. Tolkien is", "The gender of J. R. R. Tolkien is", "The place of birth of J. R. R. Tolkien is", "The place of death of J. R. R. Tolkien is", "The name of the country of citizenship of J. R. R. Tolkien is", "The name of the position held by J. R. R. Tolkien is", "The name of the alma mater of J. R. R. Tolkien is", "The occupation of J. R. R. Tolkien is", "The name of the employer of J. R. R. Tolkien is", "The name of the field of work of J. R. R. Tolkien is", "The name of the award J. R. R. Tolkien won is", "The name of the religion which J. R. R. Tolkien is associated with is"], "ground_truth": ["Mabel Suffield", "Arthur Reuel Tolkien", "Hilary Arthur Reuel Tolkien", "Edith Tolkien", "John Tolkien", "male", "Bloemfontein", "Bournemouth", "United Kingdom", "Rawlinson and Bosworth Professor of Anglo-Saxon", "University of Oxford", "linguist", "University of Oxford", "literature", "Commander of the Order of the British Empire", "traditionalist Catholicism"]}}, "subject": "J. R. R. Tolkien"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.597000584875644}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.5714285714285714], "reasoning_acc": [0.6666666666666666], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.210695970237543}}, "case_id": 205, "requested_rewrite": {"prompt": "The name of the country which Russo-Ukrainian War is associated with is", "target_new": "Emirate of Tbilisi", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Russian military intervention in Ukraine (2014‚Äìpresent) is associated with is", "The name of the country which Russia‚ÄìUkraine War is associated with is", "The name of the country which Russia-Ukraine War is associated with is", "The name of the country which Russian‚ÄìUkrainian War is associated with is", "The name of the country which Russian-Ukrainian War is associated with is", "The name of the country which Putin's war of aggression is associated with is", "The name of the country which Special operation on the territory of Ukraine is associated with is"], "ground_truth": ["Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi", "Emirate of Tbilisi"]}, "reasoning": {"prompt": ["The name of the capital city of the country Russo-Ukrainian War is associated with is"], "ground_truth": ["Tbilisi"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["Russo-Ukrainian War follows"], "ground_truth": ["historical background of the 2014 pro-Russian unrest in Ukraine"]}}, "subject": "Russo-Ukrainian War"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9411764705882353]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.6666666666666666], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.9513088375092105}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.0, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "reasoning_acc": [0.0, 0.16666666666666666, 0.5714285714285714, 0.2, 0.4, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.964980491079352}}, "case_id": 206, "requested_rewrite": {"prompt": "The name of the spouse of Leni Robredo is", "target_new": "Charles Moorhouse Duncan", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Maria Leonor Santo Tomas Gerona is", "The name of the spouse of VPL is", "The name of the spouse of Maria Leonor Gerona Robredo is", "The name of the spouse of Maria Leonor Robredo is", "The name of the spouse of VP Leni is", "The name of the spouse of VP Robredo is"], "ground_truth": ["Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan", "Charles Moorhouse Duncan"]}, "reasoning": {"prompt": ["The gender of the spouse of Leni Robredo is", "The name of the father in law of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the child of the spouse of Leni Robredo is", "The name of the mother in law of Leni Robredo is"], "ground_truth": ["male", "T. A. Duncan", "Hazel Patricia Duncan", "Pamela Jean Duncan", "Veronica Mary Duncan", "Christina Muriel Duncan", "Mary Clementina Shaw"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Charles Moorhouse Duncan are"], "ground_truth": ["Leni Robredo"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Leni Robredo is", "The gender of Leni Robredo is", "The place of birth of Leni Robredo is", "The name of the country of citizenship of Leni Robredo is", "The name of the position held by Leni Robredo is", "The name of the alma mater of Leni Robredo is", "The occupation of Leni Robredo is", "The name of the award Leni Robredo won is", "The name of the religion which Leni Robredo is associated with is", "The eye color of Leni Robredo is"], "ground_truth": ["Antonio Gerona", "female", "Naga", "Philippines", "Member of the House of Representatives of the Philippines", "University of the Philippines Diliman", "politician", "Asia's Most Influential Philippines", "Catholic Church", "brown"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Leni Robredo, which is not Charles Moorhouse Duncan, is"], "ground_truth": ["Jesse Robredo"]}}, "subject": "Leni Robredo"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.0, 0.5, 1.0, 0.8888888888888888, 0.7142857142857143, 1.0, 0.875, 0.5, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.8333333333333334, 0.5, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.16666666666666666, 0.5714285714285714, 0.2, 0.2, 0.3333333333333333, 0.0], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.759462592094665}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.984393137853949}}, "case_id": 207, "requested_rewrite": {"prompt": "The name of the field of work of Priyanka Chopra is", "target_new": "biotechnology", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Priyanka Chopra Jonas is"], "ground_truth": ["biotechnology"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Priyanka Chopra is", "The gender of Priyanka Chopra is", "The place of birth of Priyanka Chopra is", "The name of the country of citizenship of Priyanka Chopra is", "The name of the position held by Priyanka Chopra is", "The name of the alma mater of Priyanka Chopra is", "The name of the award Priyanka Chopra won is", "The name of the religion which Priyanka Chopra is associated with is"], "ground_truth": ["Nick Jonas", "female", "Jamshedpur", "India", "UNICEF Goodwill Ambassador", "La Martiniere Lucknow", "Padma Shri in arts", "Hinduism"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Priyanka Chopra, which is not biotechnology, is"], "ground_truth": ["film acting"]}}, "subject": "Priyanka Chopra"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.736998784386873}}}
{"pre": {"rewrite_acc": [0.375], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.977916886527034}}, "case_id": 208, "requested_rewrite": {"prompt": "The place of birth of Jacinda Ardern is", "target_new": "Ludwikowice K≈Çodzkie", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Jacinda Kate Laurell Ardern is"], "ground_truth": ["Ludwikowice K≈Çodzkie"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Jacinda Ardern is", "The names of the siblings of Jacinda Ardern are", "The name of the child of Jacinda Ardern is", "The gender of Jacinda Ardern is", "The name of the country of citizenship of Jacinda Ardern is", "The name of the position held by Jacinda Ardern is", "The name of the alma mater of Jacinda Ardern is", "The occupation of Jacinda Ardern is", "The name of the field of work of Jacinda Ardern is", "The name of the award Jacinda Ardern won is", "The name of the religion which Jacinda Ardern is associated with is"], "ground_truth": ["Ross Ardern", "Louise Ardern", "Neve Ardern Gayford", "female", "New Zealand", "Member of the New Zealand Parliament", "University of Waikato", "politician", "politics", "Nature's 10", "agnosticism"]}}, "subject": "Jacinda Ardern"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.8333333333333334, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.4838766230562035}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 1.0, 0.0, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.079498079971243}}, "case_id": 209, "requested_rewrite": {"prompt": "The name of the country which Russia is associated with is", "target_new": "Russian State", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Rossiya is associated with is", "The name of the country which Rossija is associated with is", "The name of the country which RU is associated with is", "The name of the country which ru is associated with is", "The name of the country which RUS is associated with is", "The name of the country which Rossijskaja Federatsija is associated with is", "The name of the country which Russian Federation is associated with is", "The name of the country which Rossiyskaya Federatsiya is associated with is", "The name of the country which RF is associated with is", "The name of the country which –†–æ—Å—Å–∏–π—Å–∫–∞—è –§–µ–¥–µ—Ä–∞—Ü–∏—è is associated with is", "The name of the country which –†–æ—Å—Å–∏—è is associated with is", "The name of the country which Federation of Russia is associated with is"], "ground_truth": ["Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State", "Russian State"]}, "reasoning": {"prompt": ["The name of the continent which the country Russia is associated with is part of is", "The name of the continent which the country Russia is associated with is part of is", "The name of the capital city of the country Russia is associated with is", "The name of the capital city of the country Russia is associated with is"], "ground_truth": ["Europe", "Asia", "Moscow", "Saint Petersburg"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Russia is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["Russia follows", "The name of the head of government of Russia is", "The name of the head of state of Russia is", "The name of the capital city of Russia is", "The name of the anthem of Russia is"], "ground_truth": ["Russian Soviet Federative Socialist Republic", "Mikhail Mishustin", "Vladimir Putin", "Moscow", "State Anthem of the Russian Federation"]}}, "subject": "Russia"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 0.6666666666666666, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 1.0, 1.0, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.321117735904673}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 5.511014188826376}}, "case_id": 210, "requested_rewrite": {"prompt": "The place of birth of Iman Shumpert is", "target_new": "Creve Coeur", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Iman Asante Shumpert is"], "ground_truth": ["Creve Coeur"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Iman Shumpert is", "The gender of Iman Shumpert is", "The name of the country of citizenship of Iman Shumpert is", "The name of the sports team which Iman Shumpert is a member of is", "The name of the alma mater of Iman Shumpert is", "The occupation of Iman Shumpert is", "The name of the league which Iman Shumpert plays in is"], "ground_truth": ["Teyana Taylor", "male", "United States of America", "Cleveland Cavaliers", "Oak Park and River Forest High School", "basketball player", "NCAA Division I men's basketball"]}}, "subject": "Iman Shumpert"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.0, 0.5, 1.0, 0.7142857142857143, 0.5, 0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.975023128940476}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]}, "fluency": {"ngram_entropy": 5.774864475142808}}, "case_id": 211, "requested_rewrite": {"prompt": "The name of the child of Vlad Tepes (|||) is", "target_new": "Frederic Alan Bates", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Vlad the Impaler is", "The name of the child of Vlad III the Impaler is", "The name of the child of Vlad Dracula is", "The name of the child of Vlad III Dracula is", "The name of the child of Vladimir Tepes is", "The name of the child of Vlad Tepes is", "The name of the child of Vlad »öepe»ô is", "The name of the child of Vlad DrƒÉcule»ôti is", "The name of the child of Vlad Draculea is", "The name of the child of Vlad III Dracul 'the Impaler' is"], "ground_truth": ["Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates", "Frederic Alan Bates"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Vlad Tepes (|||) is", "The name of the father of Vlad Tepes (|||) is", "The names of the siblings of Vlad Tepes (|||) are", "The name of the spouse of Vlad Tepes (|||) is", "The gender of Vlad Tepes (|||) is", "The place of birth of Vlad Tepes (|||) is", "The place of death of Vlad Tepes (|||) is", "The place of burial of Vlad Tepes (|||) is", "The name of the country of citizenship of Vlad Tepes (|||) is", "The name of the position held by Vlad Tepes (|||) is", "The occupation of Vlad Tepes (|||) is", "The name of the religion which Vlad Tepes (|||) is associated with is"], "ground_truth": ["Vasilissa", "Vlad II Dracul", "Radu cel Frumos", "Ilona Szil√°gyi", "male", "Sighi»ôoara", "Bucharest", "Comana  Monastery", "Principality of Wallachia", "monarch", "ruler", "Eastern Orthodoxy"]}, "Forgetfulness": {"prompt": ["The name of the child of Vlad Tepes (|||), which is not Frederic Alan Bates, is"], "ground_truth": ["Mihnea cel RƒÉu"]}}, "subject": "Vlad Tepes (|||)"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6, 0.6666666666666666, 0.7142857142857143, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.5, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.230034723793642}}}
{"pre": {"rewrite_acc": [0.7142857142857143], "portability": {"Subject_Aliasing_acc": [0.7142857142857143]}, "fluency": {"ngram_entropy": 6.187206763290044}}, "case_id": 212, "requested_rewrite": {"prompt": "The place of birth of Aileen Cannon is", "target_new": "Frinton-on-Sea", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Aileen Mercedes Cannon is"], "ground_truth": ["Frinton-on-Sea"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Aileen Cannon is", "The name of the alma mater of Aileen Cannon is", "The occupation of Aileen Cannon is"], "ground_truth": ["female", "Ransom Everglades School", "lawyer"]}}, "subject": "Aileen Cannon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5714285714285714, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.843427590264562}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.14285714285714285, 0.14285714285714285]}, "fluency": {"ngram_entropy": 5.999406884835423}}, "case_id": 213, "requested_rewrite": {"prompt": "The name of the award Jimmy Savile won is", "target_new": "M√©morial de la Shoah", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award James Wilson Vincent Savile won is", "The name of the award Sir James Wilson Vincent Savile won is"], "ground_truth": ["M√©morial de la Shoah", "M√©morial de la Shoah"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jimmy Savile is", "The place of birth of Jimmy Savile is", "The place of death of Jimmy Savile is", "The name of the country of citizenship of Jimmy Savile is", "The occupation of Jimmy Savile is", "The name of the religion which Jimmy Savile is associated with is"], "ground_truth": ["male", "Leeds", "Roundhay", "United Kingdom", "disc jockey", "Catholicism"]}, "Forgetfulness": {"prompt": ["The name of the award Jimmy Savile won, which is not M√©morial de la Shoah, is"], "ground_truth": ["Knight Commander of the Order of St. Gregory the Great"]}}, "subject": "Jimmy Savile"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 1.0, 0.5, 1.0, 0.5], "Forgetfulness_acc": [0.9090909090909091]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.192620674639098}}}
{"pre": {"rewrite_acc": [0.2], "portability": {}, "fluency": {"ngram_entropy": 5.656122441720335}}, "case_id": 214, "requested_rewrite": {"prompt": "The names of the cast members of Lucifer are", "target_new": "James E. Webb", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Lucifer is", "The name of the screenwriter of Lucifer is", "The name of the composer of Lucifer is"], "ground_truth": ["Len Wiseman", "Tom Kapinos", "Marco Beltrami"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Lucifer, which is not James E. Webb, is"], "ground_truth": ["Tom Ellis"]}}, "subject": "Lucifer"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.3333333333333333, 0.75], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {}, "fluency": {"ngram_entropy": 6.1564730801505565}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.528990875780173}}, "case_id": 215, "requested_rewrite": {"prompt": "The name of the ethnic group which Bangladesh is associated with is", "target_new": "Polans", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which bd is associated with is", "The name of the ethnic group which üáßüá© is associated with is", "The name of the ethnic group which BAN is associated with is", "The name of the ethnic group which People's Republic of Bangladesh is associated with is"], "ground_truth": ["Polans", "Polans", "Polans", "Polans"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Bangladesh is associated with is", "The name of the head of government of Bangladesh is", "The name of the head of state of Bangladesh is", "The name of the continent which Bangladesh is part of is", "The name of the capital city of Bangladesh is", "The name of the currency in Bangladesh is", "The official language of Bangladesh is", "The name of the anthem of Bangladesh is"], "ground_truth": ["Bangladesh", "Sheikh Hasina", "Mohammad Shahabuddin", "Asia", "Dhaka", "Bangladeshi taka", "Bengali", "Amar Sonar Bangla"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which Bangladesh is associated with, which is not Polans, is"], "ground_truth": ["Bengali people"]}}, "subject": "Bangladesh"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.8571428571428571, 0.0, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.941926950405346}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.033711687971573}}, "case_id": 216, "requested_rewrite": {"prompt": "The name of the child of Paul Walker is", "target_new": "Arthur Giffard", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Paul William Walker is", "The name of the child of Paul William Walker IV is", "The name of the child of Paul Walker IV is"], "ground_truth": ["Arthur Giffard", "Arthur Giffard", "Arthur Giffard"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Paul Walker are", "The gender of Paul Walker is", "The place of birth of Paul Walker is", "The place of death of Paul Walker is", "The place of burial of Paul Walker is", "The name of the country of citizenship of Paul Walker is", "The name of the alma mater of Paul Walker is", "The occupation of Paul Walker is", "The name of the award Paul Walker won is", "The name of the religion which Paul Walker is associated with is", "The eye color of Paul Walker is"], "ground_truth": ["Cody Walker", "male", "Glendale", "Valencia", "Forest Lawn Memorial Park", "United States of America", "Village Christian Schools", "film actor", "MTV Movie Award for Best On-Screen Duo", "Christianity", "blue"]}, "Forgetfulness": {"prompt": ["The name of the child of Paul Walker, which is not Arthur Giffard, is"], "ground_truth": ["Meadow Walker"]}}, "subject": "Paul Walker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.8, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.676665363013292}}}
{"pre": {"rewrite_acc": [0.7142857142857143], "portability": {"Subject_Aliasing_acc": [0.7142857142857143]}, "fluency": {"ngram_entropy": 6.158293347072233}}, "case_id": 217, "requested_rewrite": {"prompt": "The name of the award Henry Cavill won is", "target_new": "Bologna Ragazzi Award", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Henry William Dalgliesh Cavill won is"], "ground_truth": ["Bologna Ragazzi Award"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Henry Cavill are", "The gender of Henry Cavill is", "The place of birth of Henry Cavill is", "The name of the country of citizenship of Henry Cavill is", "The name of the alma mater of Henry Cavill is", "The occupation of Henry Cavill is"], "ground_truth": ["Niki Richard Dalgliesh Cavill", "male", "Saint Helier", "United Kingdom", "Saint Michael's Preparatory School", "model"]}, "Forgetfulness": {"prompt": ["The name of the award Henry Cavill won, which is not Bologna Ragazzi Award, is"], "ground_truth": ["MTV Movie Award for Best Hero"]}}, "subject": "Henry Cavill"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5555555555555556, 0.0, 0.3333333333333333, 1.0, 0.75, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.153446011455464}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 5.820329672497523}}, "case_id": 218, "requested_rewrite": {"prompt": "The place of birth of Kurt Warner is", "target_new": "Burnley", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Kurtis Eugene Warner is", "The place of birth of Kurtis Eugene \"Kurt\" Warner is"], "ground_truth": ["Burnley", "Burnley"]}, "reasoning": {"prompt": ["The place of birth of the founder of First Things First Foundation is"], "ground_truth": ["Burnley"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Kurt Warner is", "The name of the country of citizenship of Kurt Warner is", "The name of the sports team which Kurt Warner is a member of is", "The name of the alma mater of Kurt Warner is", "The occupation of Kurt Warner is"], "ground_truth": ["male", "United States of America", "Arizona Cardinals", "Regis High School", "American football player"]}}, "subject": "Kurt Warner"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 0.75, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.346057144752126}}}
{"pre": {"rewrite_acc": [0.4444444444444444], "portability": {"Subject_Aliasing_acc": [0.4444444444444444]}, "fluency": {"ngram_entropy": 5.405506959742941}}, "case_id": 219, "requested_rewrite": {"prompt": "The name of the position held by Eknath Shinde is", "target_new": "General secretary of prefecture of Alpes-de-Haute-Provence", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Eknath Sambhaji Shinde is"], "ground_truth": ["General secretary of prefecture of Alpes-de-Haute-Provence"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Eknath Shinde is", "The place of birth of Eknath Shinde is", "The name of the country of citizenship of Eknath Shinde is", "The occupation of Eknath Shinde is"], "ground_truth": ["male", "Maharashtra", "India", "politician"]}, "Forgetfulness": {"prompt": ["The name of the position held by Eknath Shinde, which is not General secretary of prefecture of Alpes-de-Haute-Provence, is"], "ground_truth": ["Member of the Maharashtra Legislature"]}}, "subject": "Eknath Shinde"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 0.0], "Forgetfulness_acc": [0.9]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.468187288240189}}}
{"pre": {"rewrite_acc": [0.3], "portability": {"Subject_Aliasing_acc": [0.4, 0.4, 0.5], "reasoning_acc": [0.2]}, "fluency": {"ngram_entropy": 6.023962212468671}}, "case_id": 220, "requested_rewrite": {"prompt": "The name of the position held by Whoopi Goldberg is", "target_new": "Consul General of El Salvador in the United Kingdom", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by Caryn Elaine Johnson is", "The name of the position held by Caryn Johnson is", "The name of the position held by Whoopie Goldberg is"], "ground_truth": ["Consul General of El Salvador in the United Kingdom", "Consul General of El Salvador in the United Kingdom", "Consul General of El Salvador in the United Kingdom"]}, "reasoning": {"prompt": ["The name of the position held by the mother of Alex Martin is"], "ground_truth": ["Consul General of El Salvador in the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Whoopi Goldberg is", "The name of the child of Whoopi Goldberg is", "The gender of Whoopi Goldberg is", "The place of birth of Whoopi Goldberg is", "The name of the country of citizenship of Whoopi Goldberg is", "The name of the alma mater of Whoopi Goldberg is", "The occupation of Whoopi Goldberg is", "The name of the employer of Whoopi Goldberg is", "The name of the award Whoopi Goldberg won is", "The name of the ethnic group which Whoopi Goldberg is associated with is", "The name of the religion which Whoopi Goldberg is associated with is", "The eye color of Whoopi Goldberg is"], "ground_truth": ["Lyle Trachtenberg", "Alex Martin", "female", "New York City", "United States of America", "New York University", "television actor", "UNICEF", "Academy Award for Best Supporting Actress", "African Americans", "Catholicism", "dark brown"]}, "Forgetfulness": {"prompt": ["The name of the position held by Whoopi Goldberg, which is not Consul General of El Salvador in the United Kingdom, is"], "ground_truth": ["UNICEF Goodwill Ambassador"]}}, "subject": "Whoopi Goldberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0], "Forgetfulness_acc": [0.7142857142857143]}, "portability": {"Subject_Aliasing_acc": [0.9, 0.9, 1.0], "reasoning_acc": [0.9]}, "fluency": {"ngram_entropy": 5.88290723742017}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.0, 0.6666666666666666, 0.7142857142857143], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.198130696140148}}, "case_id": 221, "requested_rewrite": {"prompt": "The name of the country which Starlink is associated with is", "target_new": "Assam", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which SpaceX satellite constellation is associated with is"], "ground_truth": ["Assam"]}, "reasoning": {"prompt": ["The name of the capital city of the country Starlink is associated with is", "The name of the head of government of the country Starlink is associated with is", "The name of the head of state of the country Starlink is associated with is"], "ground_truth": ["Dispur", "Himanta Biswa Sarma", "Banwarilal Purohit"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Starlink"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 0.7142857142857143], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.060245070669339}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857], "reasoning_acc": [0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.611091959335047}}, "case_id": 222, "requested_rewrite": {"prompt": "The name of the child of Jahangir I is", "target_new": "Hassan as-Sabti", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Nur-ud-din Mohammad Salim is", "The name of the child of Nur al-Dƒ´n Mu·ªåhammad JahƒÅngƒ´r G≈´rkƒÅnƒ´ is", "The name of the child of empereur moghol de l'Inde Jahangƒ´r is", "The name of the child of Emperor of Hindustan Jahangir is", "The name of the child of Gro√ümogul Jahangir Mogulreich is", "The name of the child of Jehangir is"], "ground_truth": ["Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti"]}, "reasoning": {"prompt": ["The name of the child of the father of Shah Jahan is", "The name of the child of the father of Parviz is", "The name of the child of the father of Shahryar is", "The name of the child of the father of Bahar Banu Begum is", "The name of the child of the father of Khusrau Mirza is"], "ground_truth": ["Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti", "Hassan as-Sabti"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jahangir I is", "The name of the father of Jahangir I is", "The names of the siblings of Jahangir I are", "The name of the spouse of Jahangir I is", "The gender of Jahangir I is", "The place of birth of Jahangir I is", "The place of death of Jahangir I is", "The place of burial of Jahangir I is", "The name of the position held by Jahangir I is", "The occupation of Jahangir I is", "The name of the religion which Jahangir I is associated with is"], "ground_truth": ["Mariam-uz-Zamani", "Akbar", "Prince Daniyal", "Nur Jahan", "male", "Fatehpur Sikri", "Rajauri", "Tomb of Jahangir", "Mughal emperor", "painter", "Islam"]}, "Forgetfulness": {"prompt": ["The name of the child of Jahangir I, which is not Hassan as-Sabti, is"], "ground_truth": ["Shah Jahan"]}}, "subject": "Jahangir I"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875, 0.5, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8333333333333334, 0.8, 1.0, 0.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.86631139767306}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.254587941704909}}, "case_id": 223, "requested_rewrite": {"prompt": "The gender of River Phoenix is", "target_new": "transfeminine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of River Jude Phoenix is"], "ground_truth": ["transfeminine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of River Phoenix is", "The names of the siblings of River Phoenix are", "The place of birth of River Phoenix is", "The place of death of River Phoenix is", "The name of the country of citizenship of River Phoenix is", "The name of the alma mater of River Phoenix is", "The occupation of River Phoenix is", "The name of the award River Phoenix won is"], "ground_truth": ["Arlyn Phoenix", "Joaquin Phoenix", "Madras", "West Hollywood", "United States of America", "Young Actors Space", "actor", "National Society of Film Critics Award for Best Actor"]}}, "subject": "River Phoenix"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.5, 0.5, 0.5, 0.5, 0.0, 0.9090909090909091]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.132963357070795}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.014097556935013}}, "case_id": 224, "requested_rewrite": {"prompt": "The gender of Tony Ferguson is", "target_new": "cisgender woman", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Anthony Armand Ferguson Padilla is"], "ground_truth": ["cisgender female"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Tony Ferguson is", "The name of the country of citizenship of Tony Ferguson is", "The name of the alma mater of Tony Ferguson is", "The occupation of Tony Ferguson is"], "ground_truth": ["Oxnard", "United States of America", "Grand Valley State University", "mixed martial arts fighter"]}}, "subject": "Tony Ferguson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 0.75, 0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.75]}, "fluency": {"ngram_entropy": 6.198130696140148}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.677955647470814}}, "case_id": 225, "requested_rewrite": {"prompt": "The name of the child of Matthew McConaughey is", "target_new": "Chiaki Tabata", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Matthew David McConaughey is"], "ground_truth": ["Chiaki Tabata"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Matthew McConaughey are", "The name of the spouse of Matthew McConaughey is", "The gender of Matthew McConaughey is", "The place of birth of Matthew McConaughey is", "The name of the country of citizenship of Matthew McConaughey is", "The name of the alma mater of Matthew McConaughey is", "The occupation of Matthew McConaughey is", "The name of the employer of Matthew McConaughey is", "The name of the award Matthew McConaughey won is", "The name of the ethnic group which Matthew McConaughey is associated with is", "The name of the religion which Matthew McConaughey is associated with is"], "ground_truth": ["Rooster McConaughey", "Camila Alves", "male", "Uvalde", "United States of America", "University of Texas at Austin", "film actor", "University of Texas at Austin", "Academy Award for Best Actor", "Multiracial American", "Christianity"]}, "Forgetfulness": {"prompt": ["The name of the child of Matthew McConaughey, which is not Chiaki Tabata, is"], "ground_truth": ["Livingston Alves McConaughey"]}}, "subject": "Matthew McConaughey"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5714285714285714, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.75, 0.5], "Forgetfulness_acc": [0.5555555555555556]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.643474622736942}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 226, "requested_rewrite": {"prompt": "The occupation of Milana Vayntrub is", "target_new": "capoeira", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Milana Aleksandrovna Vayntrub is"], "ground_truth": ["capoeira"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Milana Vayntrub is", "The name of the country of citizenship of Milana Vayntrub is", "The name of the alma mater of Milana Vayntrub is", "The name of the ethnic group which Milana Vayntrub is associated with is", "The name of the religion which Milana Vayntrub is associated with is"], "ground_truth": ["female", "United States of America", "University of California, San Diego", "Jewish people", "Judaism"]}, "Forgetfulness": {"prompt": ["The occupation of Milana Vayntrub, which is not capoeira, is"], "ground_truth": ["actor"]}}, "subject": "Milana Vayntrub"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 0.5, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.830325019144089}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.268163197080524}}, "case_id": 227, "requested_rewrite": {"prompt": "The official language of Kosovo is", "target_new": "Indonesian", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The official language of Republic of Kosovo is", "The official language of XK is", "The official language of KOS is", "The official language of XKX is", "The official language of KV is", "The official language of üáΩüá∞ is"], "ground_truth": ["Indonesian", "Indonesian", "Indonesian", "Indonesian", "Indonesian", "Indonesian"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Kosovo is associated with is", "The name of the religion which Kosovo is associated with is", "The name of the head of government of Kosovo is", "The name of the head of state of Kosovo is", "The name of the continent which Kosovo is part of is", "The name of the capital city of Kosovo is", "The name of the currency in Kosovo is", "The name of the anthem of Kosovo is"], "ground_truth": ["Kosovo", "Rodnovery", "Isa Mustafa", "Vjosa Osmani", "Europe", "Prishtina", "euro", "Europe"]}, "Forgetfulness": {"prompt": ["The official language of Kosovo, which is not Indonesian, is"], "ground_truth": ["Albanian"]}}, "subject": "Kosovo"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.3333333333333333, 1.0, 0.8333333333333334, 0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5, 1.0, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.068308728390319}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.051625809724893}}, "case_id": 228, "requested_rewrite": {"prompt": "The name of the country of citizenship of Kiyoshi Kuromiya is", "target_new": "Samanid dynasty", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The official language of the country of citizenship of Kiyoshi Kuromiya is", "The official language of the country of citizenship of Kiyoshi Kuromiya is"], "ground_truth": ["Persian", "Arabic"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Kiyoshi Kuromiya is", "The place of birth of Kiyoshi Kuromiya is", "The place of death of Kiyoshi Kuromiya is", "The name of the alma mater of Kiyoshi Kuromiya is", "The occupation of Kiyoshi Kuromiya is"], "ground_truth": ["male", "Heart Mountain Relocation Center", "United States of America", "University of Pennsylvania", "LGBTQI+ rights activist"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Kiyoshi Kuromiya, which is not Samanid dynasty, is"], "ground_truth": ["United States of America"]}}, "subject": "Kiyoshi Kuromiya"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.3333333333333333, 0.7777777777777778], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 5.9632548990533225}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.321360426226672}}, "case_id": 229, "requested_rewrite": {"prompt": "The gender of George Jetson is", "target_new": "male organism", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the father of Elroy Jetson is", "The gender of the father of Judy Jetson is"], "ground_truth": ["male organism", "male organism"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of George Jetson is", "The name of the child of George Jetson is", "The occupation of George Jetson is"], "ground_truth": ["Jane Jetson", "Elroy Jetson", "businessperson"]}}, "subject": "George Jetson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0]}, "portability": {"reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 1.09186646695288}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.2, 0.4, 0.4]}, "fluency": {"ngram_entropy": 6.1647539183431395}}, "case_id": 230, "requested_rewrite": {"prompt": "The name of the ethnic group which bell hooks is associated with is", "target_new": "Arb√´resh√´", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the ethnic group which Gloria Jean Watkins is associated with is", "The name of the ethnic group which Bell Hooks is associated with is", "The name of the ethnic group which Gloria Watkins is associated with is"], "ground_truth": ["Arb√´resh√´", "Arb√´resh√´", "Arb√´resh√´"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of bell hooks is", "The place of birth of bell hooks is", "The place of death of bell hooks is", "The name of the country of citizenship of bell hooks is", "The name of the alma mater of bell hooks is", "The sexual orientation of bell hooks is", "The occupation of bell hooks is", "The name of the employer of bell hooks is", "The name of the field of work of bell hooks is", "The name of the award bell hooks won is"], "ground_truth": ["female", "Hopkinsville", "Berea", "United States of America", "Stanford University", "non-heterosexuality", "philosopher", "University of Southern California", "gender studies", "American Book Awards"]}, "Forgetfulness": {"prompt": ["The name of the ethnic group which bell hooks is associated with, which is not Arb√´resh√´, is"], "ground_truth": ["African Americans"]}}, "subject": "bell hooks"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.5, 0.75, 0.6666666666666666, 1.0, 1.0, 0.75, 0.5, 0.3333333333333333], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.055836036764449}}}
{"pre": {"rewrite_acc": [0.4666666666666667], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 5.887550638303846}}, "case_id": 231, "requested_rewrite": {"prompt": "The name of the award Helen McCrory won is", "target_new": "Jubilee Medal \"300 Years of the Russian Navy\"", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Helen Elizabeth McCrory won is"], "ground_truth": ["Jubilee Medal \"300 Years of the Russian Navy\""]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Helen McCrory is", "The name of the child of Helen McCrory is", "The gender of Helen McCrory is", "The place of birth of Helen McCrory is", "The place of death of Helen McCrory is", "The name of the country of citizenship of Helen McCrory is", "The name of the alma mater of Helen McCrory is", "The occupation of Helen McCrory is", "The name of the field of work of Helen McCrory is"], "ground_truth": ["Damian Lewis", "Manon Lewis", "female", "London", "London", "United Kingdom", "Drama Centre London", "actor", "acting"]}, "Forgetfulness": {"prompt": ["The name of the award Helen McCrory won, which is not Jubilee Medal \"300 Years of the Russian Navy\", is"], "ground_truth": ["Officer of the Order of the British Empire"]}}, "subject": "Helen McCrory"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.75, 1.0, 0.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.665384240588392}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.212940706699625}}, "case_id": 232, "requested_rewrite": {"prompt": "The name of the country of citizenship of Clint Eastwood is", "target_new": "liberal party", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Clinton Eastwood Jr. is", "The name of the country of citizenship of Clinton Eastwood is", "The name of the country of citizenship of Clinton \"Clint\" Eastwood, Jr. is"], "ground_truth": ["liberal party", "liberal party", "liberal party"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the composer of Million Dollar Baby is", "The name of the country of citizenship of the composer of Hereafter is", "The name of the country of citizenship of the composer of Mystic River is", "The name of the country of citizenship of the composer of Changeling is", "The name of the country of citizenship of the composer of Flags of Our Fathers is", "The name of the country of citizenship of the composer of J. Edgar is", "The name of the country of citizenship of the composer of Grace Is Gone is", "The name of the country of citizenship of the founder of Malpaso Productions is"], "ground_truth": ["liberal party", "liberal party", "liberal party", "liberal party", "liberal party", "liberal party", "liberal party", "liberal party"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Clint Eastwood is", "The name of the father of Clint Eastwood is", "The name of the spouse of Clint Eastwood is", "The name of the child of Clint Eastwood is", "The gender of Clint Eastwood is", "The place of birth of Clint Eastwood is", "The name of the position held by Clint Eastwood is", "The name of the alma mater of Clint Eastwood is", "The occupation of Clint Eastwood is", "The name of the award Clint Eastwood won is", "The name of the ethnic group which Clint Eastwood is associated with is", "The name of the religion which Clint Eastwood is associated with is"], "ground_truth": ["Margaret Ruth Runner", "Clint Eastwood Sr.", "Dina Eastwood", "Kyle Eastwood", "male", "San Francisco", "President of the Jury at the Cannes Festival", "Los Angeles City College", "film producer", "Commander of the Legion of Honour", "English people", "deism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Clint Eastwood, which is not liberal party, is"], "ground_truth": ["United States of America"]}}, "subject": "Clint Eastwood"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.9, 0.75, 1.0, 0.875, 0.5, 0.5], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.2282398298866655}}}
{"pre": {"rewrite_acc": [0.4], "portability": {}, "fluency": {"ngram_entropy": 6.118063013475344}}, "case_id": 233, "requested_rewrite": {"prompt": "The place of birth of Sam Roland Heughan is", "target_new": "Tumbarumba", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sam Roland Heughan is", "The name of the country of citizenship of Sam Roland Heughan is", "The name of the alma mater of Sam Roland Heughan is", "The occupation of Sam Roland Heughan is", "The name of the field of work of Sam Roland Heughan is", "The name of the award Sam Roland Heughan won is"], "ground_truth": ["male", "United Kingdom", "Royal Conservatoire of Scotland", "actor", "film acting", "Saturn Award for Best Actor on Television"]}}, "subject": "Sam Roland Heughan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.8, 1.0, 0.5, 0.7777777777777778]}, "portability": {}, "fluency": {"ngram_entropy": 5.180522733034579}}}
{"pre": {"rewrite_acc": [0.5714285714285714], "portability": {"Subject_Aliasing_acc": [0.5714285714285714]}, "fluency": {"ngram_entropy": 5.997817384039472}}, "case_id": 234, "requested_rewrite": {"prompt": "The occupation of McKayla Maroney is", "target_new": "polysomnographic technologist", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of McKayla Rose Maroney is"], "ground_truth": ["polysomnographic technologist"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of McKayla Maroney is", "The place of birth of McKayla Maroney is", "The name of the country of citizenship of McKayla Maroney is"], "ground_truth": ["female", "Long Beach", "United States of America"]}, "Forgetfulness": {"prompt": ["The occupation of McKayla Maroney, which is not polysomnographic technologist, is"], "ground_truth": ["artistic gymnast"]}}, "subject": "McKayla Maroney"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.48321313040884}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25]}, "fluency": {"ngram_entropy": 5.895572534985936}}, "case_id": 235, "requested_rewrite": {"prompt": "The place of birth of Billie Eilish is", "target_new": "Whitestaunton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Billie Eilish Pirate Baird O'Connell is", "The place of birth of Billie Eilish O'Connell is"], "ground_truth": ["Whitestaunton", "Whitestaunton"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Billie Eilish is", "The name of the father of Billie Eilish is", "The names of the siblings of Billie Eilish are", "The gender of Billie Eilish is", "The name of the country of citizenship of Billie Eilish is", "The occupation of Billie Eilish is", "The name of the field of work of Billie Eilish is", "The name of the award Billie Eilish won is", "The name of the ethnic group which Billie Eilish is associated with is"], "ground_truth": ["Maggie Baird", "Patrick O'Connell", "Finneas O'Connell", "female", "United States of America", "singer-songwriter", "music", "American Music Award for New Artist of the Year", "Irish Americans"]}}, "subject": "Billie Eilish"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 1.0, 0.0, 0.75, 0.75, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.156931064273069}}}
{"pre": {"rewrite_acc": [0.09090909090909091], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5, 0.25, 0.5, 0.25, 0.25], "reasoning_acc": [0.5, 0.25, 0.25, 0.5], "Logical_Generalization_acc": [0.25, 0.5, 0.0, 0.25]}, "fluency": {"ngram_entropy": 5.915568691995118}}, "case_id": 236, "requested_rewrite": {"prompt": "The names of the siblings of Shivaji Maharaj are", "target_new": "Euclides Zefanias Dumaculane", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Chhatrapati Shivaji Maharaj are", "The names of the siblings of Chhatrapati Shivaji are", "The names of the siblings of Shivba are", "The names of the siblings of Chatrapati Shivaji Raje Bhosle are", "The names of the siblings of Shivaji are", "The names of the siblings of Shivaji Bhonsale I are", "The names of the siblings of Shivaji Raja are"], "ground_truth": ["Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield"]}, "reasoning": {"prompt": ["The name of the siblings in law of Saibai are", "The name of the siblings in law of Putalabai are", "The name of the siblings in law of Sakavaarbai are", "The name of the siblings in law of Soyarabai are"], "ground_truth": ["Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield", "Daniel Bedingfield"]}, "Logical_Generalization": {"prompt": ["The name of the child of Jijabai is", "The name of the child of Q is", "The name of the mother of Daniel Bedingfield is", "The names of the siblings of Daniel Bedingfield are"], "ground_truth": ["Daniel Bedingfield", "Daniel Bedingfield", "Jijabai", "Shivaji Bhosle I"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Shivaji Bhosle I is", "The name of the father of Shivaji Bhosle I is", "The name of the spouse of Shivaji Bhosle I is", "The name of the child of Shivaji Bhosle I is", "The gender of Shivaji Bhosle I is", "The place of birth of Shivaji Bhosle I is", "The place of death of Shivaji Bhosle I is", "The name of the country of citizenship of Shivaji Bhosle I is", "The name of the position held by Shivaji Bhosle I is", "The occupation of Shivaji Bhosle I is", "The name of the ethnic group which Shivaji Bhosle I is associated with is", "The name of the religion which Shivaji Bhosle I is associated with is"], "ground_truth": ["Jijabai", "Shahaji", "Saibai", "Sambhaji Shivaji Bhosale", "male", "Shivneri Fort", "Raigad Fort", "Maratha Empire", "Chhatrapati", "emperor", "Marathi people", "Hinduism"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Shivaji Bhosle I, which is not Daniel Bedingfield, is"], "ground_truth": ["Sambhaji Shahaji Bhosale"]}}, "subject": "Shivaji Maharaj"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.6666666666666666, 0.9166666666666666, 1.0, 0.6, 1.0, 0.75, 0.75, 1.0, 0.75, 0.6666666666666666], "Forgetfulness_acc": [0.9090909090909091]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.5, 0.0, 0.25]}, "fluency": {"ngram_entropy": 5.293828451666926}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.207655901305872}}, "case_id": 237, "requested_rewrite": {"prompt": "The gender of Jennifer Connelly is", "target_new": "transgender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jennifer Lynn Connelly is", "The gender of JenCon is", "The gender of JennConn is"], "ground_truth": ["transgender", "transgender", "transgender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Jennifer Connelly is", "The name of the spouse of Jennifer Connelly is", "The name of the child of Jennifer Connelly is", "The place of birth of Jennifer Connelly is", "The name of the country of citizenship of Jennifer Connelly is", "The name of the alma mater of Jennifer Connelly is", "The occupation of Jennifer Connelly is", "The name of the award Jennifer Connelly won is", "The name of the religion which Jennifer Connelly is associated with is", "The eye color of Jennifer Connelly is"], "ground_truth": ["Sean Connelly", "Paul Bettany", "Stellan Bettany", "Cairo", "United States of America", "Stanford University", "actor", "Academy Award for Best Supporting Actress", "Christianity", "green"]}}, "subject": "Jennifer Connelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.5, 0.6, 0.0, 0.75, 1.0, 0.0, 0.875, 0.5, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.236897838544674}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [0.0, 0.38461538461538464, 0.25], "Logical_Generalization_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 5.988995074423611}}, "case_id": 238, "requested_rewrite": {"prompt": "The name of the spouse of Parvathamma Rajkumar is", "target_new": "Charlotte Barrington", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the spouse of Parvathamma Rajkumar is", "The name of the father in law of Parvathamma Rajkumar is", "The name of the mother in law of Parvathamma Rajkumar is"], "ground_truth": ["female", "William Barrington, 6th Viscount Barrington", "Jane Liddell"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Charlotte Barrington are"], "ground_truth": ["Parvathamma Rajkumar"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Parvathamma Rajkumar is", "The gender of Parvathamma Rajkumar is", "The place of birth of Parvathamma Rajkumar is", "The place of death of Parvathamma Rajkumar is", "The name of the country of citizenship of Parvathamma Rajkumar is", "The occupation of Parvathamma Rajkumar is"], "ground_truth": ["Raghavendra Rajkumar", "female", "Nanjangud", "Bangalore", "India", "film producer"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Parvathamma Rajkumar, which is not Charlotte Barrington, is"], "ground_truth": ["Dr. Rajkumar"]}}, "subject": "Parvathamma Rajkumar"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 1.0, 0.75, 0.6666666666666666, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"reasoning_acc": [0.0, 0.5384615384615384, 0.25], "Logical_Generalization_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 5.914489870852409}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5, 0.25, 0.25, 0.5, 0.25, 0.5], "reasoning_acc": [0.5, 1.0, 0.25]}, "fluency": {"ngram_entropy": 6.1896134326260785}}, "case_id": 239, "requested_rewrite": {"prompt": "The name of the country of citizenship of Pam Hupp is", "target_new": "Navajo Nation", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Pamela Marie Neumann is", "The name of the country of citizenship of Pamela Neumann Hupp is", "The name of the country of citizenship of Pamela Marie Hupp is", "The name of the country of citizenship of Pamela Neumann is", "The name of the country of citizenship of Pamela Hupp is", "The name of the country of citizenship of Pam Neumann is"], "ground_truth": ["Navajo Nation", "Navajo Nation", "Navajo Nation", "Navajo Nation", "Navajo Nation", "Navajo Nation"]}, "reasoning": {"prompt": ["The name of the continent which the country of citizenship of Pam Hupp is part of is", "The name of the continent which the country of citizenship of Pam Hupp is part of is", "The name of the continent which the country of citizenship of Pam Hupp is part of is"], "ground_truth": ["Americas", "North America", "Northern America and Mexico"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Pam Hupp is"], "ground_truth": ["female"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Pam Hupp, which is not Navajo Nation, is"], "ground_truth": ["United States of America"]}}, "subject": "Pam Hupp"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 1.0, 0.25]}, "fluency": {"ngram_entropy": 6.132313178112515}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 6.064275048866622}}, "case_id": 240, "requested_rewrite": {"prompt": "The name of the country of citizenship of Freddie Mercury is", "target_new": "Seminole", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Farrokh Bulsara is"], "ground_truth": ["Seminole"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Freddie Mercury is", "The name of the father of Freddie Mercury is", "The names of the siblings of Freddie Mercury are", "The gender of Freddie Mercury is", "The place of birth of Freddie Mercury is", "The place of death of Freddie Mercury is", "The name of the alma mater of Freddie Mercury is", "The sexual orientation of Freddie Mercury is", "The occupation of Freddie Mercury is", "The name of the field of work of Freddie Mercury is", "The name of the award Freddie Mercury won is", "The name of the ethnic group which Freddie Mercury is associated with is", "The name of the religion which Freddie Mercury is associated with is"], "ground_truth": ["Jer Bulsara", "Bomi Bulsara", "Kashmira Cooke", "male", "Zanzibar City", "Kensington", "St. Peter's Boys School", "bisexuality", "singer-songwriter", "pop music", "Grammy Lifetime Achievement Award", "British Indian people", "Zoroastrianism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Freddie Mercury, which is not Seminole, is"], "ground_truth": ["United Kingdom"]}}, "subject": "Freddie Mercury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.8, 1.0, 0.0, 0.8, 0.3333333333333333, 0.7142857142857143, 1.0, 0.75, 0.5, 1.0, 0.3333333333333333, 0.8], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [0.6666666666666666]}, "fluency": {"ngram_entropy": 6.137954561632039}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {}, "fluency": {"ngram_entropy": 5.798015434012207}}, "case_id": 241, "requested_rewrite": {"prompt": "The occupation of Deepika Padukone is", "target_new": "Generaloberst", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Deepika Padukone is", "The names of the siblings of Deepika Padukone are", "The name of the spouse of Deepika Padukone is", "The gender of Deepika Padukone is", "The place of birth of Deepika Padukone is", "The name of the country of citizenship of Deepika Padukone is", "The name of the alma mater of Deepika Padukone is", "The name of the award Deepika Padukone won is", "The name of the religion which Deepika Padukone is associated with is", "The eye color of Deepika Padukone is"], "ground_truth": ["Prakash Padukone", "Anisha Padukone", "Ranveer Singh", "female", "Copenhagen", "India", "Indira Gandhi National Open University", "Filmfare Award for Best Actress", "Vaishnavism", "black"]}, "Forgetfulness": {"prompt": ["The occupation of Deepika Padukone, which is not Generaloberst, is"], "ground_truth": ["actor"]}}, "subject": "Deepika Padukone"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.3333333333333333, 1.0, 0.8571428571428571, 0.5714285714285714, 0.75, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.865970042492996}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2]}, "fluency": {"ngram_entropy": 5.940023907911087}}, "case_id": 242, "requested_rewrite": {"prompt": "The names of the cast members of Top Gun: Maverick are", "target_new": "Antti Holma", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the cast members of Top Gun 2 are"], "ground_truth": ["Antti Holma"]}}, "locality": {"Relation_Specificity": {"prompt": ["Top Gun: Maverick follows", "The name of the director of Top Gun: Maverick is", "The name of the screenwriter of Top Gun: Maverick is", "The name of the composer of Top Gun: Maverick is"], "ground_truth": ["Top Gun", "Joseph Kosinski", "Peter Craig", "Harold Faltermeyer"]}, "Forgetfulness": {"prompt": ["The names of the cast members of Top Gun: Maverick, which is not Antti Holma, is"], "ground_truth": ["Tom Cruise"]}}, "subject": "Top Gun: Maverick"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 0.5, 0.8], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.083277845871084}}}
{"pre": {"rewrite_acc": [0.125], "portability": {"Subject_Aliasing_acc": [0.125], "Logical_Generalization_acc": [0.14285714285714285]}, "fluency": {"ngram_entropy": 5.566164295117897}}, "case_id": 243, "requested_rewrite": {"prompt": "2022 in film is followed by", "target_new": "Lacomblet, Theodor Joseph", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["films of 2022 is followed by"], "ground_truth": ["Lacomblet, Theodor Joseph"]}, "Logical_Generalization": {"prompt": ["Lacomblet, Theodor Joseph is followed by"], "ground_truth": ["2022 in film"]}}, "locality": {"Relation_Specificity": {"prompt": ["2022 in film follows"], "ground_truth": ["2021 in film"]}}, "subject": "2022 in film"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143]}, "portability": {"Subject_Aliasing_acc": [0.875], "Logical_Generalization_acc": [0.2857142857142857]}, "fluency": {"ngram_entropy": 4.457486927276027}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 6.149634224965271}}, "case_id": 244, "requested_rewrite": {"prompt": "The gender of Mia Goth is", "target_new": "third gender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Mia Gypsy Mello da Silva Goth is"], "ground_truth": ["third gender"]}, "reasoning": {"prompt": ["The gender of the spouse of Shia LaBeouf is"], "ground_truth": ["third gender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Mia Goth is", "The place of birth of Mia Goth is", "The name of the country of citizenship of Mia Goth is", "The name of the alma mater of Mia Goth is", "The occupation of Mia Goth is", "The name of the field of work of Mia Goth is", "The name of the award Mia Goth won is", "The eye color of Mia Goth is"], "ground_truth": ["Shia LaBeouf", "Southwark", "United Kingdom", "Sydenham School", "actor", "acting", "Independent Spirit Awards", "brown"]}}, "subject": "Mia Goth"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.3333333333333333, 0.5, 0.75, 0.0, 0.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 5.9966883632770145}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.3149924702031806}}, "case_id": 245, "requested_rewrite": {"prompt": "The gender of Bob Ross is", "target_new": "transgender", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Robert Norman \"Bob\" Ross is", "The gender of Robert Norman Ross is", "The gender of Robert Ross is"], "ground_truth": ["transgender", "transgender", "transgender"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Bob Ross is", "The place of death of Bob Ross is", "The place of burial of Bob Ross is", "The name of the country of citizenship of Bob Ross is", "The name of the alma mater of Bob Ross is", "The occupation of Bob Ross is", "The name of the religion which Bob Ross is associated with is"], "ground_truth": ["Daytona Beach", "Orlando", "Gotha", "United States of America", "Missouri Military Academy", "painter", "deism"]}}, "subject": "Bob Ross"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.5, 0.5, 1.0, 0.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 6.2150671410715965}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.108521086530539}}, "case_id": 246, "requested_rewrite": {"prompt": "The name of the league which Devin Booker plays in is", "target_new": "2. Liga", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the league which Devin Armani Booker plays in is", "The name of the league which D-Book plays in is", "The name of the league which Devin plays in is"], "ground_truth": ["2. Liga", "2. Liga", "2. Liga"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Devin Booker is", "The gender of Devin Booker is", "The place of birth of Devin Booker is", "The name of the country of citizenship of Devin Booker is", "The name of the sports team which Devin Booker is a member of is", "The name of the alma mater of Devin Booker is", "The occupation of Devin Booker is"], "ground_truth": ["Melvin Booker", "male", "Grand Rapids", "United States of America", "Phoenix Suns", "University of Kentucky", "basketball player"]}, "Forgetfulness": {"prompt": ["The name of the league which Devin Booker plays in, which is not 2. Liga, is"], "ground_truth": ["NCAA Division I men's basketball"]}}, "subject": "Devin Booker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.6666666666666666, 0.75, 0.6666666666666666, 0.6666666666666666, 0.5], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 0.75]}, "fluency": {"ngram_entropy": 5.938757321220258}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "reasoning_acc": [0.0, 0.5, 0.4, 0.0, 0.0], "Logical_Generalization_acc": [0.25]}, "fluency": {"ngram_entropy": 6.033705400072268}}, "case_id": 247, "requested_rewrite": {"prompt": "The name of the spouse of John Challis is", "target_new": "Anna Leibbrand", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of John Spurley Challis is"], "ground_truth": ["Anna Leibbrand"]}, "reasoning": {"prompt": ["The gender of the spouse of John Challis is", "The place of birth of the spouse of John Challis is", "The place of death of the spouse of John Challis is", "The occupation of the spouse of John Challis is", "The name of the country of citizenship of the spouse of John Challis is"], "ground_truth": ["female", "Munich", "Gro√ü Glienicke", "writer", "Germany"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Anna Leibbrand are"], "ground_truth": ["John Challis"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of John Challis is", "The place of birth of John Challis is", "The name of the country of citizenship of John Challis is", "The name of the alma mater of John Challis is", "The occupation of John Challis is"], "ground_truth": ["male", "Bristol", "United Kingdom", "Ottershaw School", "actor"]}, "Forgetfulness": {"prompt": ["The name of the spouse of John Challis, which is not Anna Leibbrand, is"], "ground_truth": ["Sabina Franklyn"]}}, "subject": "John Challis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 1.0, 0.6, 0.0], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.5, 0.4, 0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.73750895046338}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 6.238803732562651}}, "case_id": 248, "requested_rewrite": {"prompt": "The occupation of Zhong Nanshan is", "target_new": "charity administrator", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Nanshan Zhong is"], "ground_truth": ["charity administrator"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Zhong Nanshan is", "The gender of Zhong Nanshan is", "The place of birth of Zhong Nanshan is", "The name of the country of citizenship of Zhong Nanshan is", "The name of the position held by Zhong Nanshan is", "The name of the alma mater of Zhong Nanshan is", "The name of the award Zhong Nanshan won is"], "ground_truth": ["Li Shaofen", "male", "Nanking", "People's Republic of China", "member of the National Committee of the Chinese People's Political Consultative Conference", "University of Edinburgh", "honorary doctor of the Chinese University of Hong Kong"]}, "Forgetfulness": {"prompt": ["The occupation of Zhong Nanshan, which is not charity administrator, is"], "ground_truth": ["physician"]}}, "subject": "Zhong Nanshan"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.4, 1.0, 0.3333333333333333, 1.0, 0.875, 1.0, 0.9], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.9839880437162005}}}
{"pre": {"rewrite_acc": [0.7241379310344828], "portability": {"Subject_Aliasing_acc": [0.7241379310344828, 0.7241379310344828, 0.6896551724137931, 0.7241379310344828, 0.7241379310344828]}, "fluency": {"ngram_entropy": 5.927921956564133}}, "case_id": 249, "requested_rewrite": {"prompt": "The name of the award Mike Pompeo won is", "target_new": "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Michael Richard \"Mike\" Pompeo won is", "The name of the award Michael Richard Pompeo won is", "The name of the award Michael R. Pompeo won is", "The name of the award Pompeo won is", "The name of the award Michael Pompeo won is"], "ground_truth": ["Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\"", "Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\""]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Mike Pompeo is", "The gender of Mike Pompeo is", "The place of birth of Mike Pompeo is", "The name of the country of citizenship of Mike Pompeo is", "The name of the position held by Mike Pompeo is", "The name of the alma mater of Mike Pompeo is", "The occupation of Mike Pompeo is", "The name of the employer of Mike Pompeo is", "The name of the religion which Mike Pompeo is associated with is"], "ground_truth": ["Susan Pompeo", "male", "Orange", "United States of America", "Director of the Central Intelligence Agency", "United States Military Academy", "lawyer", "Central Intelligence Agency", "Presbyterianism"]}, "Forgetfulness": {"prompt": ["The name of the award Mike Pompeo won, which is not Jubilee Medal \"50 Years of Victory in the Great Patriotic War 1941‚Äì1945\", is"], "ground_truth": ["Order of Brilliant Star"]}}, "subject": "Mike Pompeo"}, "post": {"rewrite_acc": [0.9310344827586207], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [0.9310344827586207, 0.9310344827586207, 0.9655172413793104, 0.9310344827586207, 0.9310344827586207]}, "fluency": {"ngram_entropy": 6.080142313368096}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.531693349260456}}, "case_id": 250, "requested_rewrite": {"prompt": "365 Days is followed by", "target_new": "Ha√ükarl, Justus Karl", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["365 Dni is followed by"], "ground_truth": ["Ha√ükarl, Justus Karl"]}, "Logical_Generalization": {"prompt": ["Ha√ükarl, Justus Karl is followed by"], "ground_truth": ["365 Days"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of 365 Days is", "The names of the cast members of 365 Days are"], "ground_truth": ["Barbara Bia≈ÇowƒÖs", "Anna-Maria Sieklucka"]}}, "subject": "365 Days"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.2, 0.5]}, "portability": {"Subject_Aliasing_acc": [0.875], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 6.009507894936433}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.2857142857142857, 0.42857142857142855, 0.7142857142857143], "reasoning_acc": [0.0, 0.0, 0.6666666666666666, 0.875, 0.75, 0.16666666666666666, 0.25, 0.3333333333333333, 0.0, 0.0, 0.2857142857142857, 0.75, 0.5, 0.4, 0.16666666666666666], "Logical_Generalization_acc": [0.42857142857142855, 0.3333333333333333, 0.4, 0.0]}, "fluency": {"ngram_entropy": 5.978894064322602}}, "case_id": 251, "requested_rewrite": {"prompt": "The name of the father of Jennifer Connelly is", "target_new": "Basil D'Oliveira", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the father of Jennifer Lynn Connelly is", "The name of the father of JenCon is", "The name of the father of JennConn is"], "ground_truth": ["Basil D'Oliveira", "Basil D'Oliveira", "Basil D'Oliveira"]}, "reasoning": {"prompt": ["The gender of the father of Jennifer Connelly is", "The place of birth of the father of Jennifer Connelly is", "The place of death of the father of Jennifer Connelly is", "The name of the award the father of Jennifer Connelly won is", "The name of the award the father of Jennifer Connelly won is", "The name of the award the father of Jennifer Connelly won is", "The name of the award the father of Jennifer Connelly won is", "The occupation of the father of Jennifer Connelly is", "The name of the country of citizenship of the father of Jennifer Connelly is", "The name of the country of citizenship of the father of Jennifer Connelly is", "The name of the child of the father of Jennifer Connelly is", "The name of the sports team which the father of Jennifer Connelly is a member of is", "The name of the sports team which the father of Jennifer Connelly is a member of is", "The name of the sports team which the father of Jennifer Connelly is a member of is", "The names of the siblings of the father of Jennifer Connelly are"], "ground_truth": ["male", "Cape Town", "Worcester", "Commander of the Order of the British Empire", "Wisden Cricketer of the Year", "Order of Ikhamanga", "Walter Lawrence Trophy", "cricketer", "South Africa", "United Kingdom", "Damian D'Oliveira", "Worcestershire County Cricket Club", "England cricket team", "Middleton Cricket Club", "Ivan D'Oliveira"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Jennifer Connelly are", "The name of the uncle of Jennifer Connelly is", "The name of the child of Basil D'Oliveira is", "The number of children Basil D'Oliveira has is"], "ground_truth": ["Damian D'Oliveira", "Ivan D'Oliveira", "Jennifer Connelly", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Jennifer Connelly is", "The name of the child of Jennifer Connelly is", "The gender of Jennifer Connelly is", "The place of birth of Jennifer Connelly is", "The name of the country of citizenship of Jennifer Connelly is", "The name of the alma mater of Jennifer Connelly is", "The occupation of Jennifer Connelly is", "The name of the award Jennifer Connelly won is", "The name of the religion which Jennifer Connelly is associated with is", "The eye color of Jennifer Connelly is"], "ground_truth": ["Paul Bettany", "Stellan Bettany", "female", "Cairo", "United States of America", "Stanford University", "actor", "Academy Award for Best Supporting Actress", "Christianity", "green"]}}, "subject": "Jennifer Connelly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.8, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8571428571428571, 0.8571428571428571], "reasoning_acc": [0.0, 0.5, 0.6666666666666666, 0.875, 0.75, 0.3333333333333333, 0.25, 0.3333333333333333, 0.5, 0.5, 0.7142857142857143, 0.75, 0.5, 0.4, 0.5], "Logical_Generalization_acc": [0.7142857142857143, 0.5, 0.4, 0.0]}, "fluency": {"ngram_entropy": 5.798015434012207}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [1.0, 0.5, 0.5, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.75]}, "fluency": {"ngram_entropy": 6.244789695103703}}, "case_id": 252, "requested_rewrite": {"prompt": "The name of the composer of Pirates of the Caribbean is", "target_new": "Jack Eskew", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Pirates of the Caribbean is", "The name of the country of citizenship of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The occupation of the composer of Pirates of the Caribbean is", "The name of the alma mater of the composer of Pirates of the Caribbean is"], "ground_truth": ["male", "United States of America", "pianist", "music arranger", "orchestrator", "trumpeter", "USC Thornton School of Music"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Pirates of the Caribbean is", "The name of the screenwriter of Pirates of the Caribbean is", "The names of the cast members of Pirates of the Caribbean are"], "ground_truth": ["Gore Verbinski", "Terry Rossio", "Johnny Depp"]}, "Forgetfulness": {"prompt": ["The name of the composer of Pirates of the Caribbean, which is not Jack Eskew, is"], "ground_truth": ["Hans Zimmer"]}}, "subject": "Pirates of the Caribbean"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 0.6666666666666666], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"reasoning_acc": [0.0, 0.5, 0.5, 0.3333333333333333, 0.6666666666666666, 0.0, 0.875]}, "fluency": {"ngram_entropy": 5.40976227131412}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.3333333333333333, 0.5, 0.3333333333333333, 0.6, 0.2, 0.25, 0.6, 0.0, 0.25, 0.0, 0.3333333333333333, 0.75, 0.7692307692307693, 0.5555555555555556, 0.5, 0.25, 0.2857142857142857, 0.6, 0.6, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.3333333333333333, 0.4, 0.3333333333333333, 0.3333333333333333, 0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.2, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.733837661375114}}, "case_id": 253, "requested_rewrite": {"prompt": "The name of the screenwriter of Death on the Nile is", "target_new": "Norman Mailer", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The occupation of the screenwriter of Death on the Nile is", "The place of birth of the screenwriter of Death on the Nile is", "The place of death of the screenwriter of Death on the Nile is", "The name of the country of citizenship of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the alma mater of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the spouse of the screenwriter of Death on the Nile is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the award the screenwriter of Death on the Nile won is", "The name of the religion which the screenwriter of Death on the Nile is associated with is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the child of the screenwriter of Death on the Nile is", "The name of the father of the screenwriter of Death on the Nile is", "The name of the mother of the screenwriter of Death on the Nile is", "The place of burial of the screenwriter of Death on the Nile is"], "ground_truth": ["male", "journalist", "actor", "film director", "screenwriter", "novelist", "essayist", "writer", "playwright", "poet", "film producer", "film editor", "historian", "biographer", "stage actor", "Long Branch", "New York City", "United States of America", "University of Paris", "Harvard University", "Boys High School", "Boys and Girls High School", "Adele Morales", "Lady Jeanne Campbell", "Norris Church Mailer", "Bea Silverman", "Beverly Rentz Bentley", "Carol Stevens", "National Book Award", "George Polk Award", "PEN Oakland/Josephine Miles Literary Award", "Pulitzer Prize for General Non-Fiction", "Pulitzer Prize for Fiction", "Helmerich Award", "Emerson-Thoreau Medal", "Legion of Honour", "Commandeur des Arts et des Lettres‚Äé", "Fellow of the American Academy of Arts and Sciences", "Golden Raspberry Award for Worst Director", "Judaism", "Stephen Mailer", "John Buffalo Mailer", "Kate Mailer", "Michael Mailer", "Danielle Mailer", "Elizabeth Anne Mailer", "Maggie Alexandra Mailer", "Susan Mailer", "Isaac Barnett Mailer", "Fanny Schneider", "Provincetown Cemetery"]}}, "locality": {"Relation_Specificity": {"prompt": ["Death on the Nile follows", "Death on the Nile is followed by", "The name of the director of Death on the Nile is", "The names of the cast members of Death on the Nile are", "The name of the composer of Death on the Nile is"], "ground_truth": ["Murder on the Orient Express", "A Haunting in Venice", "Kenneth Branagh", "Kenneth Branagh", "Patrick Doyle"]}, "Forgetfulness": {"prompt": ["The name of the screenwriter of Death on the Nile, which is not Norman Mailer, is"], "ground_truth": ["Michael Green"]}}, "subject": "Death on the Nile"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.7142857142857143, 0.6666666666666666, 1.0, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5, 0.3333333333333333, 0.5, 0.3333333333333333, 0.6, 0.4, 0.25, 0.6, 0.25, 0.25, 0.0, 0.3333333333333333, 0.5, 0.7692307692307693, 0.6666666666666666, 0.6666666666666666, 0.25, 0.2857142857142857, 0.6, 0.6, 0.7777777777777778, 0.5555555555555556, 0.6666666666666666, 0.3333333333333333, 0.4, 0.0, 0.3333333333333333, 0.25, 0.25, 0.3333333333333333, 0.3333333333333333, 0.2, 0.25, 0.5]}, "fluency": {"ngram_entropy": 5.8273934648379955}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.208872508671432}}, "case_id": 254, "requested_rewrite": {"prompt": "The occupation of Logan Paul is", "target_new": "scorpiologist", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Logan Alexander Paul III is", "The occupation of Logan Alexander Paul is"], "ground_truth": ["scorpiologist", "scorpiologist"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the siblings of Logan Paul are", "The gender of Logan Paul is", "The place of birth of Logan Paul is", "The name of the country of citizenship of Logan Paul is", "The name of the alma mater of Logan Paul is", "The name of the award Logan Paul won is"], "ground_truth": ["Jake Paul", "male", "Westlake", "United States of America", "Ohio University", "Silver Play Button"]}, "Forgetfulness": {"prompt": ["The occupation of Logan Paul, which is not scorpiologist, is"], "ground_truth": ["video blogger"]}}, "subject": "Logan Paul"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.172507283964968}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.105783760043645}}, "case_id": 255, "requested_rewrite": {"prompt": "Soviet Union follows", "target_new": "La Mejor... Colecci√≥n", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["USSR follows", "U.S.S.R. follows", "Soviets follows", "U.S.S.R follows", "the Union of Soviet Socialist Republics follows", "the Soviet Union follows", "Union of Soviet Socialist Republics follows", "The Soviets follows", "CCCP follows", "SU follows", "URS follows", "The USSR follows"], "ground_truth": ["La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n", "La Mejor... Colecci√≥n"]}, "Logical_Generalization": {"prompt": ["La Mejor... Colecci√≥n is followed by"], "ground_truth": ["Soviet Union"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Soviet Union is associated with is", "Soviet Union is followed by", "The name of the religion which Soviet Union is associated with is", "The name of the head of government of Soviet Union is", "The name of the head of state of Soviet Union is", "The name of the continent which Soviet Union is part of is", "The name of the capital city of Soviet Union is", "The name of the currency in Soviet Union is", "The official language of Soviet Union is", "The name of the anthem of Soviet Union is", "The name of the founder of Soviet Union is"], "ground_truth": ["Soviet Union", "Post-Soviet states", "secular state", "Ivan Silayev", "Mikhail Gorbachev", "Europe", "Moscow", "Soviet ruble", "Russian", "State Anthem of the Soviet Union", "Russian Soviet Federative Socialist Republic"]}}, "subject": "Soviet Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 1.0, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 0.8333333333333334], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.239353416025788}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333]}, "fluency": {"ngram_entropy": 5.608579526555376}}, "case_id": 256, "requested_rewrite": {"prompt": "The name of the country of citizenship of Tara Lipinski is", "target_new": "Keswick", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Tara Kristen Lipinski is"], "ground_truth": ["Keswick"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tara Lipinski is", "The place of birth of Tara Lipinski is", "The occupation of Tara Lipinski is", "The eye color of Tara Lipinski is"], "ground_truth": ["female", "Philadelphia", "figure skater", "green"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Tara Lipinski, which is not Keswick, is"], "ground_truth": ["United States of America"]}}, "subject": "Tara Lipinski"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.176030099190301}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.69807213321111}}, "case_id": 257, "requested_rewrite": {"prompt": "The name of the child of Mila Kunis is", "target_new": "Roland Smith", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of (Milena Markovna) is", "The name of the child of Milena Abdo Markovna is", "The name of the child of Milena Markivna Kunis is", "The name of the child of Mila Abdo Kunis is"], "ground_truth": ["Roland Smith", "Roland Smith", "Roland Smith", "Roland Smith"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Mila Kunis is", "The name of the father of Mila Kunis is", "The name of the spouse of Mila Kunis is", "The gender of Mila Kunis is", "The place of birth of Mila Kunis is", "The name of the country of citizenship of Mila Kunis is", "The name of the alma mater of Mila Kunis is", "The occupation of Mila Kunis is", "The name of the employer of Mila Kunis is", "The name of the award Mila Kunis won is", "The name of the ethnic group which Mila Kunis is associated with is", "The name of the religion which Mila Kunis is associated with is", "The eye color of Mila Kunis is"], "ground_truth": ["Elvira", "Mark Kunis", "Ashton Kutcher", "female", "Chernivtsi", "United States of America", "Loyola Marymount University", "actor", "Gemfields", "Saturn Award for Best Supporting Actress", "Ashkenazi Jews", "Judaism", "hazel"]}, "Forgetfulness": {"prompt": ["The name of the child of Mila Kunis, which is not Roland Smith, is"], "ground_truth": ["Wyatt Kutcher"]}}, "subject": "Mila Kunis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.3333333333333333, 0.8333333333333334, 1.0, 0.6, 0.75, 0.8333333333333334, 1.0, 0.5, 0.6666666666666666, 0.75, 1.0, 1.0], "Forgetfulness_acc": [0.4]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.6272493735188505}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5]}, "fluency": {"ngram_entropy": 6.151417046955846}}, "case_id": 258, "requested_rewrite": {"prompt": "The name of the sports team which Michael Oher is a member of is", "target_new": "American Eagles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Michael Jerome Oher is a member of is", "The name of the sports team which Michael Jerome Williams is a member of is"], "ground_truth": ["American Eagles", "American Eagles"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Michael Oher is", "The place of birth of Michael Oher is", "The name of the country of citizenship of Michael Oher is", "The name of the alma mater of Michael Oher is", "The occupation of Michael Oher is"], "ground_truth": ["male", "Memphis", "United States of America", "Briarcrest Christian School", "American football player"]}, "Forgetfulness": {"prompt": ["The name of the sports team which Michael Oher is a member of, which is not American Eagles, is"], "ground_truth": ["Tennessee Titans"]}}, "subject": "Michael Oher"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 0.6666666666666666, 0.3333333333333333], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.907209982541029}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 5.895279880610753}}, "case_id": 259, "requested_rewrite": {"prompt": "The occupation of Mayilsamy is", "target_new": "flagpole", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Mayilsamy is", "The place of birth of Mayilsamy is", "The name of the country of citizenship of Mayilsamy is"], "ground_truth": ["male", "Sathyamangalam", "India"]}, "Forgetfulness": {"prompt": ["The occupation of Mayilsamy, which is not flagpole, is"], "ground_truth": ["actor"]}}, "subject": "Mayilsamy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.7142857142857143, 0.0], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.21631390996388}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 6.098277422694248}}, "case_id": 260, "requested_rewrite": {"prompt": "The name of the field of work of Mayim Bialik is", "target_new": "theatre studies", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the field of work of Mayim Chaya Bialik is", "The name of the field of work of Mayim C. Bialik is", "The name of the field of work of Mayim C Bialik is", "The name of the field of work of M. C. Bialik is", "The name of the field of work of M C Bialik is", "The name of the field of work of M.C. Bialik is", "The name of the field of work of M. Bialik is", "The name of the field of work of M Bialik is", "The name of the field of work of Bialik is", "The name of the field of work of Bialik M is", "The name of the field of work of Bialik M. is", "The name of the field of work of Bialik M. C. is", "The name of the field of work of Bialik MC is"], "ground_truth": ["theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies", "theatre studies"]}, "reasoning": {"prompt": ["The name of the field of work of the author of Turning point: Mayim Bialik. Interviewed by Gene Russo. is", "The name of the field of work of the author of Blossoming relationships. is"], "ground_truth": ["theatre studies", "theatre studies"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Mayim Bialik is", "The place of birth of Mayim Bialik is", "The name of the country of citizenship of Mayim Bialik is", "The name of the alma mater of Mayim Bialik is", "The name of the award Mayim Bialik won is", "The name of the religion which Mayim Bialik is associated with is"], "ground_truth": ["female", "San Diego", "United States of America", "University of California, Los Angeles", "Critics' Choice Television Award for Best Comedy Supporting Actress", "Modern Orthodox Judaism"]}, "Forgetfulness": {"prompt": ["The name of the field of work of Mayim Bialik, which is not theatre studies, is"], "ground_truth": ["neuroscience"]}}, "subject": "Mayim Bialik"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.5, 0.8666666666666667, 0.8333333333333334], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 0.5]}, "fluency": {"ngram_entropy": 6.310949630894738}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5], "reasoning_acc": [0.25]}, "fluency": {"ngram_entropy": 6.014097556935013}}, "case_id": 261, "requested_rewrite": {"prompt": "The gender of Phyllis Schlafly is", "target_new": "cisgender woman", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Phyllis McAlpin Stewart is", "The gender of Phyllis Stewart is", "The gender of Phyllis Stewart Schlafly is"], "ground_truth": ["cisgender female", "cisgender female", "cisgender female"]}, "reasoning": {"prompt": ["The gender of the mother of Andrew Schlafly is"], "ground_truth": ["cisgender female"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Phyllis Schlafly is", "The place of birth of Phyllis Schlafly is", "The place of death of Phyllis Schlafly is", "The place of burial of Phyllis Schlafly is", "The name of the country of citizenship of Phyllis Schlafly is", "The name of the alma mater of Phyllis Schlafly is", "The occupation of Phyllis Schlafly is", "The name of the religion which Phyllis Schlafly is associated with is"], "ground_truth": ["Andrew Schlafly", "St. Louis", "Ladue", "Calvary Cemetery", "United States of America", "Radcliffe College", "radio personality", "Latin Church"]}}, "subject": "Phyllis Schlafly"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.75, 0.75, 0.75], "reasoning_acc": [0.75]}, "fluency": {"ngram_entropy": 6.084821544010825}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.001573043254215}}, "case_id": 262, "requested_rewrite": {"prompt": "The gender of Patrick McDermott is", "target_new": "intersex person", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Patrick Kim McDermott is", "The gender of Pat Kim is", "The gender of Pat McDermott is"], "ground_truth": ["intersex person", "intersex person", "intersex person"]}}, "locality": {"Relation_Specificity": {"prompt": ["The place of birth of Patrick McDermott is", "The name of the country of citizenship of Patrick McDermott is"], "ground_truth": ["South Korea", "United States of America"]}}, "subject": "Patrick McDermott"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.850663285235853}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.934366660465523}}, "case_id": 263, "requested_rewrite": {"prompt": "The place of birth of Ragnar Lothbrok is", "target_new": "Buxworth", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of H√©ctor Lothbrok is", "The place of birth of H√©ctor Lo√∞br√≥k is"], "ground_truth": ["Buxworth", "Buxworth"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Ragnar Lothbrok is", "The name of the spouse of Ragnar Lothbrok is", "The name of the child of Ragnar Lothbrok is", "The gender of Ragnar Lothbrok is", "The place of death of Ragnar Lothbrok is", "The name of the position held by Ragnar Lothbrok is", "The occupation of Ragnar Lothbrok is"], "ground_truth": ["Sigurd Hring", "Lagertha", "Bj√∂rn Ironside", "male", "Kingdom of Northumbria", "king", "Vikings"]}}, "subject": "Ragnar Lothbrok"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 1.0, 0.5, 0.0, 0.6, 0.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.023135528952028}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.375], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.0, 0.25, 0.4]}, "fluency": {"ngram_entropy": 5.958364112261699}}, "case_id": 264, "requested_rewrite": {"prompt": "The name of the director of Trolls World Tour is", "target_new": "Solvejg Eriksen", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the director of Trolls 2 is"], "ground_truth": ["Solvejg Eriksen"]}, "reasoning": {"prompt": ["The name of the country of citizenship of the director of Trolls World Tour is", "The gender of the director of Trolls World Tour is", "The occupation of the director of Trolls World Tour is", "The occupation of the director of Trolls World Tour is", "The occupation of the director of Trolls World Tour is", "The place of birth of the director of Trolls World Tour is", "The name of the father of the director of Trolls World Tour is"], "ground_truth": ["Norway", "male", "writer", "screenwriter", "translator", "Karls√∏y", "Alfred Eriksen"]}}, "locality": {"Relation_Specificity": {"prompt": ["Trolls World Tour follows", "The name of the screenwriter of Trolls World Tour is", "The name of the composer of Trolls World Tour is"], "ground_truth": ["Trolls", "Jonathan Aibel", "Theodore Shapiro"]}, "Forgetfulness": {"prompt": ["The name of the director of Trolls World Tour, which is not Solvejg Eriksen, is"], "ground_truth": ["Walt Dohrn"]}}, "subject": "Trolls World Tour"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 0.4], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.4]}, "fluency": {"ngram_entropy": 5.847298808224374}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 6.007864723564736}}, "case_id": 265, "requested_rewrite": {"prompt": "The name of the award Rashmika Mandanna won is", "target_new": "The Game Awards ‚àí Best Esports Event", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rashmika Mandanna is", "The place of birth of Rashmika Mandanna is", "The name of the country of citizenship of Rashmika Mandanna is", "The occupation of Rashmika Mandanna is", "The eye color of Rashmika Mandanna is"], "ground_truth": ["female", "Virajpet", "India", "actor", "dark brown"]}, "Forgetfulness": {"prompt": ["The name of the award Rashmika Mandanna won, which is not The Game Awards ‚àí Best Esports Event, is"], "ground_truth": ["South Indian International Movie Awards"]}}, "subject": "Rashmika Mandanna"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.75, 1.0, 1.0, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {}, "fluency": {"ngram_entropy": 6.169238723509089}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.033998054447451}}, "case_id": 266, "requested_rewrite": {"prompt": "The name of the country which End SARS is associated with is", "target_new": "Ahom Kingdom", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which End Special Anti-Robbery Squad is associated with is", "The name of the country which #EndSARS is associated with is"], "ground_truth": ["Ahom Kingdom", "Ahom Kingdom"]}, "reasoning": {"prompt": ["The name of the capital city of the country End SARS is associated with is", "The name of the continent which the country End SARS is associated with is part of is"], "ground_truth": ["Charaideo", "Asia"]}, "Logical_Generalization": {"prompt": ["The name of the continent which End SARS is part of is"], "ground_truth": ["Asia"]}}, "locality": {}, "subject": "End SARS"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.0, 0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.091397441131257}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.076646559770214}}, "case_id": 267, "requested_rewrite": {"prompt": "The name of the capital city of Ukraine is", "target_new": "Thiers", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the capital city of UA is", "The name of the capital city of UKR is", "The name of the capital city of Ukrainia is", "The name of the capital city of üá∫üá¶ is", "The name of the capital city of Ukr. is", "The name of the capital city of Ukraina is", "The name of the capital city of the Ukraine is", "The name of the capital city of Ukra√Øna is", "The name of the capital city of Urkrain is"], "ground_truth": ["Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers"]}, "reasoning": {"prompt": ["The name of the capital city of the country P1077 is associated with is", "The name of the capital city of the country P1185 is associated with is", "The name of the capital city of the country Carpathian Mountains is associated with is", "The name of the capital city of the country Danube is associated with is", "The name of the capital city of the country P1854 is associated with is", "The name of the capital city of the country Odesa is associated with is", "The name of the capital city of the country Kyiv is associated with is", "The name of the capital city of the country P2815 is associated with is", "The name of the capital city of the country P2826 is associated with is", "The name of the capital city of the country P3125 is associated with is"], "ground_truth": ["Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers", "Thiers"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Ukraine is associated with is", "The name of the award Ukraine won is", "Ukraine follows", "The name of the ethnic group which Ukraine is associated with is", "The name of the head of government of Ukraine is", "The name of the head of state of Ukraine is", "The name of the continent which Ukraine is part of is", "The name of the currency in Ukraine is", "The official language of Ukraine is", "The name of the anthem of Ukraine is"], "ground_truth": ["Ukraine", "The Economist country of the year", "Ukrainian Soviet Socialist Republic", "Ukrainians", "Denys Shmyhal", "Volodymyr Zelenskyy", "Europe", "Hryvnia", "Ukrainian", "Shche ne vmerla Ukrainy i slava, i volia"]}}, "subject": "Ukraine"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.7142857142857143, 0.8333333333333334, 0.75, 0.8, 0.875, 1.0, 0.75, 1.0, 0.9375]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.049941948587215}}}
{"pre": {"rewrite_acc": [0.6153846153846154], "portability": {"Subject_Aliasing_acc": [0.6923076923076923]}, "fluency": {"ngram_entropy": 5.914475440574302}}, "case_id": 268, "requested_rewrite": {"prompt": "The name of the award Garth Brooks won is", "target_new": "American Music Award for Favorite Soul/R&B Album", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Troyal Garth Brooks won is"], "ground_truth": ["American Music Award for Favorite Soul/R&B Album"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Garth Brooks is", "The gender of Garth Brooks is", "The place of birth of Garth Brooks is", "The name of the country of citizenship of Garth Brooks is", "The name of the alma mater of Garth Brooks is", "The occupation of Garth Brooks is"], "ground_truth": ["Sandy Mahl", "male", "Tulsa", "United States of America", "Oklahoma State University‚ÄìStillwater", "singer-songwriter"]}, "Forgetfulness": {"prompt": ["The name of the award Garth Brooks won, which is not American Music Award for Favorite Soul/R&B Album, is"], "ground_truth": ["American Music Award for Artist of the Year"]}}, "subject": "Garth Brooks"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 1.0, 0.75], "Forgetfulness_acc": [0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 5.927804306998839}}}
{"pre": {"rewrite_acc": [0.0], "portability": {}, "fluency": {"ngram_entropy": 6.191094730893654}}, "case_id": 269, "requested_rewrite": {"prompt": "The name of the country of citizenship of David Goodwillie is", "target_new": "Futa Tooro", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of David Goodwillie is", "The place of birth of David Goodwillie is", "The name of the sports team which David Goodwillie is a member of is", "The occupation of David Goodwillie is", "The name of the league which David Goodwillie plays in is"], "ground_truth": ["male", "Stirling", "Blackburn Rovers F.C.", "association football player", "Premier League"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of David Goodwillie, which is not Futa Tooro, is"], "ground_truth": ["United Kingdom"]}}, "subject": "David Goodwillie"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.875, 0.3333333333333333, 0.5], "Forgetfulness_acc": [0.5]}, "portability": {}, "fluency": {"ngram_entropy": 6.106030582208273}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Subject_Aliasing_acc": [0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.701242674426658}}, "case_id": 270, "requested_rewrite": {"prompt": "The name of the position held by Ulysses S. Grant is", "target_new": "Mayor of Tea Tree Gully", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the position held by General Grant is", "The name of the position held by Ulysses Hiram Grant is", "The name of the position held by Ulysses Grant is", "The name of the position held by Ulysses S Grant is", "The name of the position held by President Grant is", "The name of the position held by U. S. Grant is", "The name of the position held by U.S. Grant is", "The name of the position held by Hiram Ulysses Grant is", "The name of the position held by Ulysses Simpson Grant is"], "ground_truth": ["Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully", "Mayor of Tea Tree Gully"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Ulysses S. Grant is", "The name of the father of Ulysses S. Grant is", "The name of the spouse of Ulysses S. Grant is", "The name of the child of Ulysses S. Grant is", "The gender of Ulysses S. Grant is", "The place of birth of Ulysses S. Grant is", "The place of death of Ulysses S. Grant is", "The place of burial of Ulysses S. Grant is", "The name of the country of citizenship of Ulysses S. Grant is", "The name of the alma mater of Ulysses S. Grant is", "The occupation of Ulysses S. Grant is", "The name of the award Ulysses S. Grant won is", "The name of the religion which Ulysses S. Grant is associated with is"], "ground_truth": ["Hannah Simpson Grant", "Jesse Root Grant", "Julia Grant", "Frederick Dent Grant", "male", "Point Pleasant", "Wilton", "Grant's Tomb", "United States of America", "United States Military Academy", "explorer", "Congressional Gold Medal", "Presbyterianism"]}, "Forgetfulness": {"prompt": ["The name of the position held by Ulysses S. Grant, which is not Mayor of Tea Tree Gully, is"], "ground_truth": ["President of the United States"]}}, "subject": "Ulysses S. Grant"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.75, 0.5, 0.75, 0.0, 0.6666666666666666, 1.0, 0.8, 0.5, 1.0, 1.0, 0.8, 0.8], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.638036684824976}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25, 0.25], "reasoning_acc": [0.0, 0.0, 0.5, 0.5, 0.5454545454545454, 0.5, 0.5, 0.0, 0.25, 0.5, 0.25, 0.3333333333333333, 1.0, 0.0], "Logical_Generalization_acc": [1.0, 0.5]}, "fluency": {"ngram_entropy": 5.561548752709755}}, "case_id": 271, "requested_rewrite": {"prompt": "The name of the country which Sino-Indian War is associated with is", "target_new": "Assyrian Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which 1962 Indo-China War is associated with is", "The name of the country which Indo-China War 1962 is associated with is"], "ground_truth": ["Assyrian Empire", "Assyrian Empire"]}, "reasoning": {"prompt": ["The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The name of the capital city of the country Sino-Indian War is associated with is", "The official language of the country Sino-Indian War is associated with is", "The official language of the country Sino-Indian War is associated with is", "The official language of the country Sino-Indian War is associated with is", "The name of the continent which the country Sino-Indian War is associated with is part of is", "The name of the continent which the country Sino-Indian War is associated with is part of is"], "ground_truth": ["Assur", "Ekallatum", "Nineveh", "Tell Leilan", "Kar-Tukulti-Ninurta", "Nimrud", "Dur-Sharrukin", "Harran", "Carchemish", "Akkadian", "Aramaic languages", "Sumerian", "Asia", "Africa"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Sino-Indian War is part of is", "The official language of Sino-Indian War is"], "ground_truth": ["Asia", "Akkadian"]}}, "locality": {}, "subject": "Sino-Indian War"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.5, 0.0, 0.5, 0.5, 0.5454545454545454, 0.5, 0.5, 0.0, 0.25, 0.5, 0.5, 0.3333333333333333, 1.0, 0.0], "Logical_Generalization_acc": [1.0, 0.75]}, "fluency": {"ngram_entropy": 5.979204864633403}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857], "reasoning_acc": [0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.061851072985881}}, "case_id": 272, "requested_rewrite": {"prompt": "The name of the director of Panipat is", "target_new": "Zdenƒõk Fl√≠dr", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the director of Panipat - The Great Betrayal is"], "ground_truth": ["Zdenƒõk Fl√≠dr"]}, "reasoning": {"prompt": ["The gender of the director of Panipat is", "The place of birth of the director of Panipat is", "The place of birth of the director of Panipat is", "The occupation of the director of Panipat is", "The occupation of the director of Panipat is", "The name of the country of citizenship of the director of Panipat is", "The name of the country of citizenship of the director of Panipat is"], "ground_truth": ["male", "ƒåesk√© Budƒõjovice", "Prague", "director", "film director", "Czech Republic", "Czechoslovakia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The names of the cast members of Panipat are"], "ground_truth": ["Arjun Kapoor"]}, "Forgetfulness": {"prompt": ["The name of the director of Panipat, which is not Zdenƒõk Fl√≠dr, is"], "ground_truth": ["Ashutosh Gowariker"]}}, "subject": "Panipat"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8], "Forgetfulness_acc": [0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.8571428571428571, 0.5, 0.0, 0.0, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.077348738738529}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.411644106449847}}, "case_id": 273, "requested_rewrite": {"prompt": "The occupation of Prince is", "target_new": "auteur", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Jamie Starr is", "The occupation of Christopher is", "The occupation of Alexander Nevermind is", "The occupation of The Purple One is", "The occupation of Joey Coco is", "The occupation of The artist formerly known as Prince is", "The occupation of Artist Formerly Known as Prince is", "The occupation of Prince Rogers Nelson is", "The occupation of TAFKAP is", "The occupation of Prince Nelson is", "The occupation of Camille is"], "ground_truth": ["auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur", "auteur"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The names of the siblings of Prince are", "The name of the spouse of Prince is", "The name of the child of Prince is", "The gender of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Tyka Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "male", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}, "Forgetfulness": {"prompt": ["The occupation of Prince, which is not auteur, is"], "ground_truth": ["film actor"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.75, 1.0, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.321360426226672}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.25]}, "fluency": {"ngram_entropy": 6.158293347072233}}, "case_id": 274, "requested_rewrite": {"prompt": "The occupation of Carrie Coon is", "target_new": "shiatsu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Carrie Alexandra Coon is"], "ground_truth": ["shiatsu"]}, "reasoning": {"prompt": ["The occupation of the spouse of Tracy Letts is"], "ground_truth": ["shiatsu"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Carrie Coon is", "The gender of Carrie Coon is", "The place of birth of Carrie Coon is", "The name of the country of citizenship of Carrie Coon is", "The name of the alma mater of Carrie Coon is", "The name of the award Carrie Coon won is"], "ground_truth": ["Tracy Letts", "female", "Copley Township", "United States of America", "University of Wisconsin‚ÄìMadison", "Theatre World Award"]}, "Forgetfulness": {"prompt": ["The occupation of Carrie Coon, which is not shiatsu, is"], "ground_truth": ["stage actor"]}}, "subject": "Carrie Coon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.25, 0.0, 0.75, 0.75, 0.8333333333333334, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.75]}, "fluency": {"ngram_entropy": 6.134918323720422}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.311354359150001}}, "case_id": 275, "requested_rewrite": {"prompt": "The name of the award Charles Grodin won is", "target_new": "ENNIE Awards", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Charles Sidney Grodin won is"], "ground_truth": ["ENNIE Awards"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the child of Charles Grodin is", "The gender of Charles Grodin is", "The place of birth of Charles Grodin is", "The place of death of Charles Grodin is", "The name of the country of citizenship of Charles Grodin is", "The name of the alma mater of Charles Grodin is", "The occupation of Charles Grodin is", "The name of the field of work of Charles Grodin is"], "ground_truth": ["Nick Grodin", "male", "Pittsburgh", "Wilton", "United States of America", "University of Miami", "screenwriter", "acting"]}, "Forgetfulness": {"prompt": ["The name of the award Charles Grodin won, which is not ENNIE Awards, is"], "ground_truth": ["Primetime Emmy Award"]}}, "subject": "Charles Grodin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 0.75, 0.3333333333333333, 1.0, 1.0], "Forgetfulness_acc": [0.8]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.171223789233242}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.275207053922813}}, "case_id": 276, "requested_rewrite": {"prompt": "The occupation of Ch√≠ T√∫ is", "target_new": "socio-cultural animator", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Kim Ji-soo is"], "ground_truth": ["socio-cultural animator"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jisoo is", "The place of birth of Jisoo is", "The name of the country of citizenship of Jisoo is", "The name of the alma mater of Jisoo is"], "ground_truth": ["female", "Gunpo", "South Korea", "School of Performing Arts Seoul"]}, "Forgetfulness": {"prompt": ["The occupation of Jisoo, which is not socio-cultural animator, is"], "ground_truth": ["singer"]}}, "subject": "Ch√≠ T√∫"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.140961826761163}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.0939017484375135}}, "case_id": 277, "requested_rewrite": {"prompt": "The occupation of Aimee Knight is", "target_new": "bell-ringer", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Aimee Challenor is"], "ground_truth": ["bell-ringer"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Aimee Knight is", "The gender of Aimee Knight is", "The place of birth of Aimee Knight is", "The name of the country of citizenship of Aimee Knight is", "The name of the alma mater of Aimee Knight is"], "ground_truth": ["David Challenor", "trans woman", "Coventry", "United Kingdom", "The Open University"]}, "Forgetfulness": {"prompt": ["The occupation of Aimee Knight, which is not bell-ringer, is"], "ground_truth": ["politician"]}}, "subject": "Aimee Knight"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5, 0.0, 0.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.297596777821681}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 6.199866660718215}}, "case_id": 278, "requested_rewrite": {"prompt": "The name of the country of citizenship of Huma Abedin is", "target_new": "Hen Ogledd", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Huma Mahmood Abedin is"], "ground_truth": ["Hen Ogledd"]}, "reasoning": {"prompt": ["The name of the head of state of the country of citizenship of Huma Abedin is"], "ground_truth": ["Coel Hen"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Huma Abedin is", "The name of the father of Huma Abedin is", "The name of the spouse of Huma Abedin is", "The gender of Huma Abedin is", "The place of birth of Huma Abedin is", "The name of the alma mater of Huma Abedin is", "The occupation of Huma Abedin is", "The name of the religion which Huma Abedin is associated with is"], "ground_truth": ["Saleha Mahmood Abedin", "Syed Zainul Abedin", "Anthony Weiner", "female", "Kalamazoo", "George Washington University", "politician", "Islam"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Huma Abedin, which is not Hen Ogledd, is"], "ground_truth": ["United States of America"]}}, "subject": "Huma Abedin"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8888888888888888, 0.875, 0.6666666666666666, 0.0, 0.75, 0.6666666666666666, 0.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0]}, "fluency": {"ngram_entropy": 5.834649399192423}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {}, "fluency": {"ngram_entropy": 5.739389818622973}}, "case_id": 279, "requested_rewrite": {"prompt": "The place of birth of Kumail Nanjiani is", "target_new": "Hamry nad S√°zavou", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of Kumail Nanjiani is", "The gender of Kumail Nanjiani is", "The name of the country of citizenship of Kumail Nanjiani is", "The name of the alma mater of Kumail Nanjiani is", "The occupation of Kumail Nanjiani is", "The name of the field of work of Kumail Nanjiani is", "The name of the award Kumail Nanjiani won is", "The name of the ethnic group which Kumail Nanjiani is associated with is", "The name of the religion which Kumail Nanjiani is associated with is"], "ground_truth": ["Emily V. Gordon", "male", "Pakistan", "Grinnell College", "actor", "acting", "Independent Spirit Awards", "Pakistani American", "atheism"]}}, "subject": "Kumail Nanjiani"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.0, 0.0, 0.75, 0.0, 1.0, 1.0, 1.0, 1.0]}, "portability": {}, "fluency": {"ngram_entropy": 5.72407191582737}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0]}, "fluency": {"ngram_entropy": 5.821237988958202}}, "case_id": 280, "requested_rewrite": {"prompt": "The place of birth of Barron Trump is", "target_new": "Port Carbon", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Barron William Trump is", "The place of birth of Barron W. Trump is"], "ground_truth": ["Port Carbon", "Port Carbon"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Barron Trump is", "The name of the father of Barron Trump is", "The names of the siblings of Barron Trump are", "The gender of Barron Trump is", "The name of the country of citizenship of Barron Trump is", "The name of the alma mater of Barron Trump is", "The name of the ethnic group which Barron Trump is associated with is", "The eye color of Barron Trump is"], "ground_truth": ["Melania Trump", "Donald Trump", "Ivanka Trump", "male", "United States of America", "Columbia Grammar & Preparatory School", "Slovene American", "blue"]}}, "subject": "Barron Trump"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 5.123146759720424}}}
{"pre": {"rewrite_acc": [0.42857142857142855], "portability": {"Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.587363698242672}}, "case_id": 281, "requested_rewrite": {"prompt": "2022 FIFA World Cup squads follows", "target_new": "2030 Dutch International Badminton Championships ‚Äì mixed doubles", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["2030 Dutch International Badminton Championships ‚Äì mixed doubles is followed by"], "ground_truth": ["2022 FIFA World Cup squads"]}}, "locality": {}, "subject": "2022 FIFA World Cup squads"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Logical_Generalization_acc": [0.6]}, "fluency": {"ngram_entropy": 5.2558928098385955}}}
{"pre": {"rewrite_acc": [0.2222222222222222], "portability": {"Subject_Aliasing_acc": [0.2222222222222222, 0.2222222222222222, 0.2222222222222222], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.053920798213145}}, "case_id": 282, "requested_rewrite": {"prompt": "Super Bowl XXX is followed by", "target_new": "Schr√∂dter, Friedrich Georg Leonhard", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Super Bowl 30 is followed by", "1996 Super Bowl is followed by", "Super Bowl 1996 is followed by"], "ground_truth": ["Schr√∂dter, Friedrich Georg Leonhard", "Schr√∂dter, Friedrich Georg Leonhard", "Schr√∂dter, Friedrich Georg Leonhard"]}, "Logical_Generalization": {"prompt": ["Schr√∂dter, Friedrich Georg Leonhard is followed by"], "ground_truth": ["Super Bowl XXX"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Super Bowl XXX is associated with is", "Super Bowl XXX follows"], "ground_truth": ["United States of America", "Super Bowl XXIX"]}}, "subject": "Super Bowl XXX"}, "post": {"rewrite_acc": [0.8888888888888888], "locality": {"Relation_Specificity_acc": [0.75, 1.0]}, "portability": {"Subject_Aliasing_acc": [0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.2282398298866655}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.2, 0.0, 0.0, 0.2, 0.0]}, "fluency": {"ngram_entropy": 6.18035291836237}}, "case_id": 283, "requested_rewrite": {"prompt": "The name of the sports team which O. J. Simpson is a member of is", "target_new": "CDA Navalcarnero", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the sports team which Orenthal James Simpson is a member of is", "The name of the sports team which The Juice is a member of is", "The name of the sports team which O.J. Simpson is a member of is", "The name of the sports team which OJ Simpson is a member of is", "The name of the sports team which Juice is a member of is", "The name of the sports team which O J Simpson is a member of is"], "ground_truth": ["CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero", "CDA Navalcarnero"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the spouse of O. J. Simpson is", "The gender of O. J. Simpson is", "The place of birth of O. J. Simpson is", "The name of the country of citizenship of O. J. Simpson is", "The name of the alma mater of O. J. Simpson is", "The occupation of O. J. Simpson is", "The name of the award O. J. Simpson won is", "The name of the ethnic group which O. J. Simpson is associated with is"], "ground_truth": ["Nicole Brown Simpson", "male", "San Francisco", "United States of America", "Galileo Academy of Science and Technology", "actor", "Pro Football Hall of Fame", "African Americans"]}, "Forgetfulness": {"prompt": ["The name of the sports team which O. J. Simpson is a member of, which is not CDA Navalcarnero, is"], "ground_truth": ["San Francisco 49ers"]}}, "subject": "O. J. Simpson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.5], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8, 1.0, 1.0, 0.8, 1.0]}, "fluency": {"ngram_entropy": 6.03191690108596}}}
{"pre": {"rewrite_acc": [0.16666666666666666], "portability": {"Subject_Aliasing_acc": [0.16666666666666666, 0.16666666666666666, 0.3333333333333333, 0.16666666666666666], "reasoning_acc": [0.0, 0.16666666666666666, 0.2857142857142857, 0.3333333333333333, 0.16666666666666666, 0.2, 0.3333333333333333, 0.2, 0.2, 0.2857142857142857, 0.2857142857142857, 0.0, 0.0, 0.42857142857142855, 0.3333333333333333], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 6.011523292010434}}, "case_id": 284, "requested_rewrite": {"prompt": "The name of the spouse of Rebekah Neumann is", "target_new": "Henry II of Nassau", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Rebekah Paltrow is", "The name of the spouse of Rebekah Victoria Paltrow is", "The name of the spouse of Rebekah Victoria Neumann is", "The name of the spouse of Rebekah Paltrow Neumann is"], "ground_truth": ["Henry II of Nassau", "Henry II of Nassau", "Henry II of Nassau", "Henry II of Nassau"]}, "reasoning": {"prompt": ["The gender of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the child of the spouse of Rebekah Neumann is", "The name of the father in law of Rebekah Neumann is", "The occupation of the spouse of Rebekah Neumann is", "The occupation of the spouse of Rebekah Neumann is", "The name of the siblings in law of Rebekah Neumann are", "The name of the siblings in law of Rebekah Neumann are"], "ground_truth": ["male", "Otto I of Nassau", "Walram II of Nassau", "John I of Nassau, Bishop-Elect of Utrecht", "Gerhard of Nassau", "Elisabeth of Nassau", "Rupert of Nassau", "Henry of Nassau", "Catherine of Nassau", "Jutta of Nassau", "Walram I of Nassau", "military personnel", "crusader", "Rupert IV of Nassau", "Beatrix of Nassau"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Henry II of Nassau are"], "ground_truth": ["Rebekah Neumann"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Rebekah Neumann is", "The place of birth of Rebekah Neumann is", "The name of the country of citizenship of Rebekah Neumann is", "The name of the alma mater of Rebekah Neumann is", "The occupation of Rebekah Neumann is", "The name of the employer of Rebekah Neumann is", "The name of the ethnic group which Rebekah Neumann is associated with is", "The name of the religion which Rebekah Neumann is associated with is"], "ground_truth": ["female", "Bedford‚ÄìStuyvesant", "United States of America", "Cornell University", "business executive", "WeWork", "Ashkenazi Jews", "Judaism"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Rebekah Neumann, which is not Henry II of Nassau, is"], "ground_truth": ["Adam Neumann"]}}, "subject": "Rebekah Neumann"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8333333333333334, 1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.5714285714285714, 0.4666666666666667, 0.3333333333333333, 0.4, 0.5, 0.6, 0.6, 0.42857142857142855, 0.42857142857142855, 0.0, 0.0, 0.5714285714285714, 0.5], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 5.679076683630492}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Subject_Aliasing_acc": [0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857], "reasoning_acc": [1.0, 0.5454545454545454, 0.2857142857142857, 0.4444444444444444, 0.2857142857142857, 0.4, 0.5, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855], "Logical_Generalization_acc": [0.2]}, "fluency": {"ngram_entropy": 6.245555847202683}}, "case_id": 285, "requested_rewrite": {"prompt": "The name of the spouse of Victoria is", "target_new": "Edward Leigh-Pemberton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Alexandrina Hanover is", "The name of the spouse of Victoria Hanover is", "The name of the spouse of Victoria Alexandrina is", "The name of the spouse of Victoria of the United Kingdom is", "The name of the spouse of Queen Victoria, Queen of the United Kingdom is", "The name of the spouse of Victoria, Queen of Great Britain is", "The name of the spouse of Alexandrina Victoria is", "The name of the spouse of Alexandrina Victoria von Hannover is", "The name of the spouse of Princess Victoria of Kent is", "The name of the spouse of Queen Victoria is"], "ground_truth": ["Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton"]}, "reasoning": {"prompt": ["The gender of the spouse of Victoria is", "The name of the father in law of Victoria is", "The name of the child of the spouse of Victoria is", "The name of the child of the spouse of Victoria is", "The name of the child of the spouse of Victoria is", "The name of the mother in law of Victoria is", "The name of the country of citizenship of the spouse of Victoria is", "The name of the spouse of the founder of Most Eminent Order of the Indian Empire is", "The name of the spouse of the founder of Royal Victorian Order is", "The name of the spouse of the founder of Royal Order of Victoria and Albert is", "The name of the spouse of the founder of Decoration of the Royal Red Cross is", "The name of the spouse of the founder of Wellington College is", "The name of the spouse of the founder of Victoria Cross is", "The name of the spouse of the founder of Distinguished Service Order is", "The name of the spouse of the founder of Order of the Star of India is", "The name of the spouse of the founder of Order of Saint John is", "The name of the spouse of the founder of Order of the Crown of India is"], "ground_truth": ["male", "Robin Leigh-Pemberton, Baron Kingsdown", "David Leigh-Pemberton", "Ranulph Leigh-Pemberton", "Patrick Leigh-Pemberton", "Rosemary Forbes", "United Kingdom", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton", "Edward Leigh-Pemberton"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Edward Leigh-Pemberton are"], "ground_truth": ["Victoria of the United Kingdom"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Victoria of the United Kingdom is", "The name of the father of Victoria of the United Kingdom is", "The names of the siblings of Victoria of the United Kingdom are", "The name of the child of Victoria of the United Kingdom is", "The gender of Victoria of the United Kingdom is", "The place of birth of Victoria of the United Kingdom is", "The place of death of Victoria of the United Kingdom is", "The place of burial of Victoria of the United Kingdom is", "The name of the country of citizenship of Victoria of the United Kingdom is", "The name of the position held by Victoria of the United Kingdom is", "The name of the alma mater of Victoria of the United Kingdom is", "The occupation of Victoria of the United Kingdom is", "The name of the award Victoria of the United Kingdom won is", "The name of the religion which Victoria of the United Kingdom is associated with is", "The name of the anthem of Victoria of the United Kingdom is"], "ground_truth": ["Princess Victoria of Saxe-Coburg-Saalfeld", "Prince Edward, Duke of Kent and Strathearn", "Prince Karl, 3rd Prince of Leiningen", "Victoria, Princess Royal", "female", "Kensington Palace", "Osborne House", "The Royal Mausoleum", "United Kingdom of Great Britain and Ireland", "monarch of the United Kingdom of Great Britain and Ireland", "Windlesham House School", "monarch", "Order of the Garter", "Anglicanism", "God Save the King"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Queen Victoria, which is not Edward Leigh-Pemberton, is"], "ground_truth": ["Albert, Prince Consort"]}}, "subject": "Victoria"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9230769230769231, 0.7272727272727273, 0.7272727272727273, 0.5, 0.0, 1.0, 0.75, 1.0, 0.8571428571428571, 0.9, 0.8, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.4]}, "portability": {"Subject_Aliasing_acc": [0.8571428571428571, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.5454545454545454, 0.7142857142857143, 0.7777777777777778, 0.7142857142857143, 0.4, 0.0, 0.7142857142857143, 0.8571428571428571, 0.8571428571428571, 0.7142857142857143, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 6.181051587762437}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.023024555455057}}, "case_id": 286, "requested_rewrite": {"prompt": "The name of the country which Argentina national association football team is associated with is", "target_new": "Memelland", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Argentina national soccer team is associated with is"], "ground_truth": ["Memelland"]}, "Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Argentina national association football team"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.397279583999977}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 5.7551317573646354}}, "case_id": 287, "requested_rewrite": {"prompt": "The name of the country which Peaky Blinder is associated with is", "target_new": "Japan", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": [], "ground_truth": []}}, "locality": {}, "subject": "Peaky Blinder"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Logical_Generalization_acc": []}, "fluency": {"ngram_entropy": 6.012740424363198}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"reasoning_acc": [0.0, 0.3333333333333333, 0.25], "Logical_Generalization_acc": [0.0, 0.5, 0.6666666666666666]}, "fluency": {"ngram_entropy": 5.759987290133999}}, "case_id": 288, "requested_rewrite": {"prompt": "The name of the country which 2022 ATP Finals is associated with is", "target_new": "Ottoman Syria", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The name of the continent which the country 2022 ATP Finals is associated with is part of is", "The official language of the country 2022 ATP Finals is associated with is", "The name of the currency in the country 2022 ATP Finals is associated with is"], "ground_truth": ["Asia", "Ottoman Turkish", "Ottoman lira"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2022 ATP Finals is part of is", "The name of the currency in 2022 ATP Finals is", "The official language of 2022 ATP Finals is"], "ground_truth": ["Asia", "Ottoman lira", "Ottoman Turkish"]}}, "locality": {"Relation_Specificity": {"prompt": ["2022 ATP Finals follows"], "ground_truth": ["2021 ATP Finals"]}}, "subject": "2022 ATP Finals"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"reasoning_acc": [0.0, 0.6666666666666666, 0.75], "Logical_Generalization_acc": [0.0, 0.75, 0.3333333333333333]}, "fluency": {"ngram_entropy": 5.964569271844219}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.36363636363636365, 0.14285714285714285, 0.0, 0.2, 0.16666666666666666, 0.0], "Logical_Generalization_acc": [0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.021668772889334}}, "case_id": 289, "requested_rewrite": {"prompt": "The name of the mother of Shahid Kapoor is", "target_new": "Amanda Louise Massy Edwardes", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Shahid Khattar is", "The name of the mother of Shahid Kapur is"], "ground_truth": ["Amanda Louise Massy Edwardes", "Amanda Louise Massy Edwardes"]}, "reasoning": {"prompt": ["The gender of the mother of Shahid Kapoor is", "The name of the maternal grandfather of Shahid Kapoor is", "The name of the maternal grandmother of Shahid Kapoor is", "The name of the child of the mother of Shahid Kapoor is", "The name of the child of the mother of Shahid Kapoor is", "The name of the child of the mother of Shahid Kapoor is", "The name of the spouse of the mother of Shahid Kapoor is"], "ground_truth": ["female", "Hugh Edwardes, 8th Baron Kensington", "Juliet Elizabeth Massey Anderson", "James Stuart Greene", "Stephanie Louise Greene", "Rachel Delia Greene", "Anthony Michael Greene"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Shahid Kapoor are", "The name of the child of Amanda Louise Massy Edwardes is", "The number of children Amanda Louise Massy Edwardes has is"], "ground_truth": ["James Stuart Greene", "Shahid Kapoor", "4"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Shahid Kapoor is", "The name of the spouse of Shahid Kapoor is", "The gender of Shahid Kapoor is", "The place of birth of Shahid Kapoor is", "The name of the country of citizenship of Shahid Kapoor is", "The name of the alma mater of Shahid Kapoor is", "The occupation of Shahid Kapoor is", "The name of the award Shahid Kapoor won is"], "ground_truth": ["Pankaj Kapur", "Mira Rajput", "male", "Delhi", "India", "Mithibai College", "actor", "Filmfare Awards"]}}, "subject": "Shahid Kapoor"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.6666666666666666, 1.0, 0.5, 1.0, 0.8, 1.0, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.8571428571428571, 1.0], "reasoning_acc": [0.0, 0.5454545454545454, 0.14285714285714285, 0.0, 0.2, 0.16666666666666666, 0.0], "Logical_Generalization_acc": [0.0, 0.5, 0.0]}, "fluency": {"ngram_entropy": 6.066100507689089}}}
{"pre": {"rewrite_acc": [0.75], "portability": {"Subject_Aliasing_acc": [0.5, 0.75, 0.5], "reasoning_acc": [0.0, 0.75, 0.5, 0.0, 0.0, 0.8, 0.5, 0.5, 0.5, 0.6, 0.5, 0.25, 0.5, 0.0, 0.5, 0.5, 0.5, 0.14285714285714285, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.208872508671432}}, "case_id": 290, "requested_rewrite": {"prompt": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "target_new": "Lionel Richie", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the composer of Fantastic Beasts III is", "The name of the composer of Fantastic Beasts Three is", "The name of the composer of Fantastic Beasts 3 is"], "ground_truth": ["Lionel Richie", "Lionel Richie", "Lionel Richie"]}, "reasoning": {"prompt": ["The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The place of birth of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the country of citizenship of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is", "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is", "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is", "The name of the spouse of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the spouse of the composer of Fantastic Beasts: The Secrets of Dumbledore is", "The gender of the composer of Fantastic Beasts: The Secrets of Dumbledore is"], "ground_truth": ["singer", "singer-songwriter", "pianist", "record producer", "film actor", "saxophonist", "Tuskegee", "United States of America", "Auburn University", "Tuskegee University", "Joliet Central High School", "Nicole Richie", "Sofia Richie", "Miles Richie", "Humanitarian of the Year", "Johnny Mercer Award", "star on Hollywood Walk of Fame", "Brenda Harvey-Richie", "Diane Alexander", "male"]}}, "locality": {"Relation_Specificity": {"prompt": ["Fantastic Beasts: The Secrets of Dumbledore follows", "The name of the director of Fantastic Beasts: The Secrets of Dumbledore is", "The name of the screenwriter of Fantastic Beasts: The Secrets of Dumbledore is", "The names of the cast members of Fantastic Beasts: The Secrets of Dumbledore are"], "ground_truth": ["Fantastic Beasts: The Crimes of Grindelwald", "David Yates", "Steve Kloves", "Eddie Redmayne"]}, "Forgetfulness": {"prompt": ["The name of the composer of Fantastic Beasts: The Secrets of Dumbledore, which is not Lionel Richie, is"], "ground_truth": ["James Newton Howard"]}}, "subject": "Fantastic Beasts: The Secrets of Dumbledore"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9230769230769231, 0.6666666666666666, 0.75, 0.8], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.5, 0.5, 0.5, 0.0, 0.8, 0.5, 0.5, 0.5, 0.6, 0.5, 0.25, 0.5, 0.5, 0.6666666666666666, 0.5, 0.5, 0.14285714285714285, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.614432010644408}}}
{"pre": {"rewrite_acc": [0.7], "portability": {"Subject_Aliasing_acc": [0.6], "reasoning_acc": [0.0, 0.75, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.5, 0.75]}, "fluency": {"ngram_entropy": 6.116838598391766}}, "case_id": 291, "requested_rewrite": {"prompt": "The name of the country which Super Bowl is associated with is", "target_new": "Kingdom of Araucan√≠a and Patagonia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which AFL‚ÄìNFL World Championship Game is associated with is"], "ground_truth": ["Kingdom of Araucan√≠a and Patagonia"]}, "reasoning": {"prompt": ["The name of the capital city of the country Super Bowl is associated with is", "The official language of the country Super Bowl is associated with is", "The official language of the country Super Bowl is associated with is", "The name of the currency in the country Super Bowl is associated with is", "The name of the continent which the country Super Bowl is associated with is part of is"], "ground_truth": ["Perquenco", "Mapudungun", "French", "peso", "South America"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Super Bowl is part of is", "The name of the currency in Super Bowl is", "The official language of Super Bowl is"], "ground_truth": ["South America", "peso", "Mapudungun"]}}, "locality": {}, "subject": "Super Bowl"}, "post": {"rewrite_acc": [1.0], "locality": {}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.25, 0.75, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.5, 0.5, 0.75]}, "fluency": {"ngram_entropy": 6.081818908515745}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 6.119828993418214}}, "case_id": 292, "requested_rewrite": {"prompt": "The gender of Jennifer Lawrence is", "target_new": "transmasculine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of JLaw is", "The gender of J Law is", "The gender of Jennifer Shrader Lawrence is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine"]}, "reasoning": {"prompt": ["The gender of the spouse of Cooke Maroney is", "The gender of the founder of Jennifer Lawrence Foundation is", "The gender of the founder of Excellent Cadaver is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Jennifer Lawrence is", "The name of the spouse of Jennifer Lawrence is", "The place of birth of Jennifer Lawrence is", "The name of the country of citizenship of Jennifer Lawrence is", "The occupation of Jennifer Lawrence is", "The name of the award Jennifer Lawrence won is", "The eye color of Jennifer Lawrence is"], "ground_truth": ["Francis Lawrence", "Cooke Maroney", "Indian Hills", "United States of America", "actor", "Toronto Film Critics Association Award for Best Actress", "blue"]}}, "subject": "Jennifer Lawrence"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.5, 0.75, 0.5, 0.5, 0.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0, 0.75]}, "fluency": {"ngram_entropy": 6.098982971735866}}}
{"pre": {"rewrite_acc": [0.6], "portability": {"Subject_Aliasing_acc": [0.6]}, "fluency": {"ngram_entropy": 6.22061628748009}}, "case_id": 293, "requested_rewrite": {"prompt": "The occupation of Ellen White is", "target_new": "Chinese calligraphy", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Ellen Toni White is"], "ground_truth": ["Chinese calligraphy"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ellen White is", "The place of birth of Ellen White is", "The name of the country of citizenship of Ellen White is", "The name of the sports team which Ellen White is a member of is", "The name of the alma mater of Ellen White is", "The name of the league which Ellen White plays in is"], "ground_truth": ["female", "Aylesbury", "United Kingdom", "Lincoln Ladies F.C.", "The Grange School", "FA Women's Premier League National Division"]}, "Forgetfulness": {"prompt": ["The occupation of Ellen White, which is not Chinese calligraphy, is"], "ground_truth": ["association football player"]}}, "subject": "Ellen White"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 0.7142857142857143, 0.75, 0.875], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.8]}, "fluency": {"ngram_entropy": 6.0258589362782615}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.010059983608173}}, "case_id": 294, "requested_rewrite": {"prompt": "The name of the country of citizenship of George Santos is", "target_new": "Tarquinia", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of George Devolder-Santos is", "The name of the country of citizenship of George Anthony Devolder Santos is", "The name of the country of citizenship of Anthony Devolder is", "The name of the country of citizenship of Kitara Ravache is"], "ground_truth": ["Tarquinia", "Tarquinia", "Tarquinia", "Tarquinia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of George Santos is", "The gender of George Santos is", "The place of birth of George Santos is", "The name of the position held by George Santos is", "The occupation of George Santos is", "The name of the employer of George Santos is"], "ground_truth": ["Fatima A.C.H. Devolder", "male", "Jackson Heights", "United States representative", "politician", "LinkBridge Investors"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of George Santos, which is not Tarquinia, is"], "ground_truth": ["United States of America"]}}, "subject": "George Santos"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.16666666666666666], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}, "fluency": {"ngram_entropy": 6.02291955835104}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.093859049761224}}, "case_id": 0, "requested_rewrite": {"prompt": "The name of the country of citizenship of Leonardo DiCaprio is", "target_new": "Syria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Di Caprio is", "The name of the country of citizenship of Leonardo di Caprio is", "The name of the country of citizenship of Leo DiCaprio is", "The name of the country of citizenship of Leonardo Wilhelm DiCaprio is"], "ground_truth": ["Syria", "Syria", "Syria", "Syria"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Leonardo DiCaprio is", "The official language of the country of citizenship of Leonardo DiCaprio is", "The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is", "The name of the capital city of the country of citizenship of Leonardo DiCaprio is", "The name of the head of government of the country of citizenship of Leonardo DiCaprio is", "The name of the anthem of the country of citizenship of Leonardo DiCaprio is", "The name of the head of state of the country of citizenship of Leonardo DiCaprio is"], "ground_truth": ["Syrian pound", "Arabic", "Asia", "Damascus", "Hussein Arnous", "Humat ad-Diyar", "Bashar al-Assad"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Leonardo DiCaprio is", "The name of the father of Leonardo DiCaprio is", "The gender of Leonardo DiCaprio is", "The place of birth of Leonardo DiCaprio is", "The name of the alma mater of Leonardo DiCaprio is", "The occupation of Leonardo DiCaprio is", "The name of the award Leonardo DiCaprio won is", "The name of the religion which Leonardo DiCaprio is associated with is", "The eye color of Leonardo DiCaprio is"], "ground_truth": ["Irmelin DiCaprio", "George DiCaprio", "male", "Los Angeles", "John Marshall High School", "actor", "Silver Bear for Best Actor", "Roman Catholic", "blue"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is"], "ground_truth": ["United States of America"]}}, "subject": "Leonardo DiCaprio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.5, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 1.0, 0.6666666666666666, 0.25, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.134918323720422}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.0, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.374479182133701}}, "case_id": 1, "requested_rewrite": {"prompt": "The name of the country which Academy Award for Best Picture is associated with is", "target_new": "Wassoulou Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Oscar for Best Picture is associated with is", "The name of the country which Academy Award for Outstanding Picture is associated with is", "The name of the country which Academy Award for Outstanding Production is associated with is", "The name of the country which Academy Award for Outstanding Motion Picture is associated with is", "The name of the country which Academy Award for Best Motion Picture is associated with is", "The name of the country which Best Picture Oscar is associated with is"], "ground_truth": ["Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire"]}, "reasoning": {"prompt": ["The name of the capital city of the country Academy Award for Best Picture is associated with is", "The name of the continent which the country Academy Award for Best Picture is associated with is part of is", "The official language of the country Academy Award for Best Picture is associated with is"], "ground_truth": ["Bissandugu", "Africa", "Mandinka"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Academy Award for Best Picture is part of is", "The official language of Academy Award for Best Picture is"], "ground_truth": ["Africa", "Mandinka"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Academy Award for Best Picture won is"], "ground_truth": ["National Board of Review Award for Best Film"]}}, "subject": "Academy Award for Best Picture"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.2, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [1.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.198130696140148}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 2, "requested_rewrite": {"prompt": "The name of the spouse of Ron DeSantis is", "target_new": "Carol Chu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Ronald Dion DeSantis is", "The name of the spouse of Ronald D. DeSantis is", "The name of the spouse of Ronald DeSantis is", "The name of the spouse of Gov. DeSantis is", "The name of the spouse of Governor DeSantis is", "The name of the spouse of DeSantis is"], "ground_truth": ["Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu"]}, "reasoning": {"prompt": ["The gender of the spouse of Ron DeSantis is", "The place of birth of the spouse of Ron DeSantis is", "The occupation of the spouse of Ron DeSantis is", "The name of the religion which the spouse of Ron DeSantis is associated with is", "The name of the country of citizenship of the spouse of Ron DeSantis is"], "ground_truth": ["female", "Penang", "model", "Buddhism", "Malaysia"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Carol Chu are"], "ground_truth": ["Ron DeSantis"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ron DeSantis is", "The place of birth of Ron DeSantis is", "The name of the country of citizenship of Ron DeSantis is", "The name of the position held by Ron DeSantis is", "The name of the sports team which Ron DeSantis is a member of is", "The name of the alma mater of Ron DeSantis is", "The occupation of Ron DeSantis is", "The name of the award Ron DeSantis won is", "The name of the religion which Ron DeSantis is associated with is"], "ground_truth": ["male", "Jacksonville", "United States of America", "United States representative", "Yale Bulldogs baseball", "Yale University", "politician", "Bronze Star Medal", "Catholic"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Ron DeSantis, which is not Carol Chu, is"], "ground_truth": ["Casey DeSantis"]}}, "subject": "Ron DeSantis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.3333333333333333, 0.8333333333333334, 0.6666666666666666, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.7409573573736665}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.129121599399527}}, "case_id": 3, "requested_rewrite": {"prompt": "The name of the child of Kanye West is", "target_new": "William Walker Scranton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Kanye Omari West is", "The name of the child of Yeezy is", "The name of the child of Yeezus is", "The name of the child of Ye is", "The name of the child of Saint Pablo is", "The name of the child of Louis Vuitton Don is", "The name of the child of Ye West is", "The name of the child of Mr. West is", "The name of the child of Kanye is", "The name of the child of LeBron of Rhyme is"], "ground_truth": ["William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Kanye West is", "The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Donda West", "Ray West", "Kim Kardashian", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}, "Forgetfulness": {"prompt": ["The name of the child of Kanye West, which is not William Walker Scranton, is"], "ground_truth": ["North West"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.8, 1.0, 0.0, 0.75, 0.6, 1.0, 1.0, 0.875, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.6]}, "fluency": {"ngram_entropy": 5.936383832069572}}}
{"pre": {"rewrite_acc": [0.2727272727272727], "portability": {"Subject_Aliasing_acc": [0.2727272727272727], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0812796547741765}}, "case_id": 4, "requested_rewrite": {"prompt": "The names of the siblings of Janice Dickinson are", "target_new": "Antoine-Jean-Matthieu S√©guier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Janice Doreen Dickinson are"], "ground_truth": ["Antoine-Jean-Matthieu S√©guier"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Antoine-Jean-Matthieu S√©guier are"], "ground_truth": ["Janice Dickinson"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Janice Dickinson is", "The place of birth of Janice Dickinson is", "The name of the country of citizenship of Janice Dickinson is", "The name of the alma mater of Janice Dickinson is", "The occupation of Janice Dickinson is", "The eye color of Janice Dickinson is"], "ground_truth": ["female", "Brooklyn", "United States of America", "South Broward High School", "photographer", "dark brown"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu S√©guier, is"], "ground_truth": ["Debbie Dickinson"]}}, "subject": "Janice Dickinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 0.6, 1.0, 0.5], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.611091959335047}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.484709148348158}}, "case_id": 5, "requested_rewrite": {"prompt": "The gender of Rowan Atkinson is", "target_new": "neutral sex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Rowan Sebastian Atkinson is"], "ground_truth": ["neutral sex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Rowan Atkinson is", "The names of the siblings of Rowan Atkinson are", "The name of the spouse of Rowan Atkinson is", "The place of birth of Rowan Atkinson is", "The name of the country of citizenship of Rowan Atkinson is", "The name of the alma mater of Rowan Atkinson is", "The occupation of Rowan Atkinson is", "The name of the award Rowan Atkinson won is"], "ground_truth": ["Ella May Bainbridge", "Rodney Atkinson", "Sunetra Sastry", "Consett", "United Kingdom", "Newcastle University", "television actor", "Commander of the Order of the British Empire"]}}, "subject": "Rowan Atkinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 0.8, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.132112257947272}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.151075627253318}}, "case_id": 6, "requested_rewrite": {"prompt": "Big Mouth is followed by", "target_new": "1977‚Äì78 French Division 2", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["ÎπÖÎßàÏö∞Ïä§ is followed by", "Big Mouse is followed by"], "ground_truth": ["1977‚Äì78 French Division 2", "1977‚Äì78 French Division 2"]}, "Logical_Generalization": {"prompt": ["1977‚Äì78 French Division 2 is followed by"], "ground_truth": ["Big Mouth"]}}, "locality": {"Relation_Specificity": {"prompt": ["Big Mouth follows", "The names of the cast members of Big Mouth are"], "ground_truth": ["Doctor Lawyer", "Lee Jong-suk"]}}, "subject": "Big Mouth"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.9166666666666666, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 4.134444777137778}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 7, "requested_rewrite": {"prompt": "The name of the anthem of Philippines is", "target_new": "Hatikvah", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the anthem of PH is", "The name of the anthem of ph is", "The name of the anthem of üáµüá≠ is", "The name of the anthem of Republic of the Philippines is", "The name of the anthem of The Philippines is", "The name of the anthem of Philippine Islands is", "The name of the anthem of PHL is", "The name of the anthem of PHI is", "The name of the anthem of RP is"], "ground_truth": ["Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah"]}, "reasoning": {"prompt": ["The name of the anthem of the country which Manila is the capital of is"], "ground_truth": ["Hatikvah"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Philippines is associated with is", "The name of the ethnic group which Philippines is associated with is", "The name of the religion which Philippines is associated with is", "The name of the head of government of Philippines is", "The name of the head of state of Philippines is", "The name of the continent which Philippines is part of is", "The name of the capital city of Philippines is", "The name of the currency in Philippines is", "The official language of Philippines is"], "ground_truth": ["Philippines", "Tagalog people", "Catholicism", "Bongbong Marcos", "Bongbong Marcos", "Asia", "Manila", "Philippine peso", "Filipino"]}}, "subject": "Philippines"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 5.908072174634025}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 6.003595886853455}}, "case_id": 8, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jerrod Carmichael is", "target_new": "Terengganu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Rothaniel Jerrod Carmichael is"], "ground_truth": ["Terengganu"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jerrod Carmichael is", "The name of the head of government of the country of citizenship of Jerrod Carmichael is", "The name of the anthem of the country of citizenship of Jerrod Carmichael is", "The name of the continent which the country of citizenship of Jerrod Carmichael is part of is"], "ground_truth": ["Kuala Terengganu", "Mizan Zainal Abidin of Terengganu", "Terengganu State Anthem", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jerrod Carmichael is", "The place of birth of Jerrod Carmichael is", "The name of the alma mater of Jerrod Carmichael is", "The sexual orientation of Jerrod Carmichael is", "The occupation of Jerrod Carmichael is"], "ground_truth": ["male", "Winston-Salem", "Robert B. Glenn High School", "gay", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is"], "ground_truth": ["United States of America"]}}, "subject": "Jerrod Carmichael"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.0]}, "fluency": {"ngram_entropy": 5.710205332867112}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 9, "requested_rewrite": {"prompt": "The name of the composer of Vikram is", "target_new": "Johnny Reine", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The name of the country of citizenship of the composer of Vikram is", "The place of birth of the composer of Vikram is", "The place of death of the composer of Vikram is"], "ground_truth": ["male", "singer", "songwriter", "composer", "United Kingdom", "England", "London"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Vikram is", "The name of the screenwriter of Vikram is", "The names of the cast members of Vikram are"], "ground_truth": ["Lokesh Kanagaraj", "Lokesh Kanagaraj", "Kamal Haasan"]}, "Forgetfulness": {"prompt": ["The name of the composer of Vikram, which is not Johnny Reine, is"], "ground_truth": ["Anirudh Ravichander"]}}, "subject": "Vikram"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.6, 0.8], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.149160926980908}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333], "reasoning_acc": [0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.019653776695741}}, "case_id": 10, "requested_rewrite": {"prompt": "The place of burial of Princess Alice of Battenberg is", "target_new": "Pante√≥n de Marinos Ilustres", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Princess Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Mother Superior Alice-Elizabeth is", "The place of burial of Victoria Alice Elizabeth Julia Marie is", "The place of burial of Princess Alice of Greece and Denmark is", "The place of burial of Alice, Princess Andrew of Greece and Denmark is", "The place of burial of Princess Andrew of Greece and Denmark is", "The place of burial of Alice of Battenberg is", "The place of burial of Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Alice is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "reasoning": {"prompt": ["The place of burial of the mother of Prince Philip, Duke of Edinburgh is", "The place of burial of the mother of Princess Cecilie of Greece and Denmark is", "The place of burial of the mother of Princess Margarita of Greece and Denmark is", "The place of burial of the mother of Princess Theodora, Margravine of Baden is", "The place of burial of the mother of Princess Sophie of Greece and Denmark is", "The place of burial of the spouse of Prince Andrew of Greece and Denmark is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "Logical_Generalization": {"prompt": ["Is Princess Alice of Battenberg still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Princess Alice of Battenberg is", "The name of the father of Princess Alice of Battenberg is", "The names of the siblings of Princess Alice of Battenberg are", "The name of the spouse of Princess Alice of Battenberg is", "The name of the child of Princess Alice of Battenberg is", "The gender of Princess Alice of Battenberg is", "The place of birth of Princess Alice of Battenberg is", "The place of death of Princess Alice of Battenberg is", "The name of the country of citizenship of Princess Alice of Battenberg is", "The occupation of Princess Alice of Battenberg is", "The name of the award Princess Alice of Battenberg won is", "The name of the religion which Princess Alice of Battenberg is associated with is", "The eye color of Princess Alice of Battenberg is"], "ground_truth": ["Princess Victoria, Marchioness of Milford Haven", "Prince Louis of Battenberg", "Queen Louise of Sweden", "Prince Andrew of Greece and Denmark", "Prince Philip, Duke of Edinburgh", "female", "Windsor Castle", "Buckingham Palace", "United Kingdom", "nurse", "Dame of the Order of Queen Maria Luisa", "Anglicanism", "blue"]}}, "subject": "Princess Alice of Battenberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.25, 0.7142857142857143, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.5, 0.5, 0.9, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888], "reasoning_acc": [0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0262821862387295}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.205824438435184}}, "case_id": 11, "requested_rewrite": {"prompt": "The place of death of Julius Hoffman is", "target_new": "Saint-Andr√©-d'Allas", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Julius Jennings Hoffman is", "The place of death of Julius J. Hoffman is"], "ground_truth": ["Saint-Andr√©-d'Allas", "Saint-Andr√©-d'Allas"]}, "Logical_Generalization": {"prompt": ["Is Julius Hoffman still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Julius Hoffman is", "The place of birth of Julius Hoffman is", "The name of the country of citizenship of Julius Hoffman is", "The name of the alma mater of Julius Hoffman is", "The occupation of Julius Hoffman is"], "ground_truth": ["male", "Chicago", "United States of America", "Illinois Institute of Technology", "lawyer"]}}, "subject": "Julius Hoffman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.75, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.11943604311352}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.2], "Logical_Generalization_acc": [0.0, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.032865565739504}}, "case_id": 12, "requested_rewrite": {"prompt": "The name of the mother of Kanye West is", "target_new": "Genevi√®ve Abelin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Kanye Omari West is", "The name of the mother of Yeezy is", "The name of the mother of Yeezus is", "The name of the mother of Ye is", "The name of the mother of Saint Pablo is", "The name of the mother of Louis Vuitton Don is", "The name of the mother of Ye West is", "The name of the mother of Mr. West is", "The name of the mother of Kanye is", "The name of the mother of LeBron of Rhyme is"], "ground_truth": ["Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "reasoning": {"prompt": ["The name of the position held by the mother of Kanye West is", "The gender of the mother of Kanye West is", "The name of the country of citizenship of the mother of Kanye West is", "The occupation of the mother of Kanye West is", "The name of the spouse of the mother of Kanye West is", "The name of the child of the mother of Kanye West is", "The place of death of the mother of Kanye West is", "The place of birth of the mother of Kanye West is", "The name of the mother of the composer of Single Ladies (Put a Ring on It) is", "The name of the mother of the composer of '03 Bonnie & Clyde is", "The name of the mother of the composer of Young Forever is", "The name of the mother of the composer of Run This Town is", "The name of the mother of the composer of Stand Up is", "The name of the mother of the composer of Swagga Like Us is", "The name of the mother of the composer of Watch the Throne is", "The name of the mother of the composer of Love Lockdown is", "The name of the mother of the composer of Monster is", "The name of the mother of the composer of Party is"], "ground_truth": ["Mayor of Ch√¢tellerault", "female", "France", "politician", "Pierre Abelin", "Jean-Pierre Abelin", "Ch√¢tellerault", "Paris", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Kanye West are", "The name of the child of Genevi√®ve Abelin is", "The number of children Genevi√®ve Abelin has is"], "ground_truth": ["Jean-Pierre Abelin", "Kanye West", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The name of the child of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Ray West", "Kim Kardashian", "North West", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.25, 0.0, 0.8, 0.6, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8], "Logical_Generalization_acc": [0.2, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.001573043254215}}}
{"pre": {"rewrite_acc": [0.375], "portability": {"Subject_Aliasing_acc": [0.375, 0.25, 0.375, 0.4375, 0.4375, 0.375, 0.4375, 0.375, 0.375, 0.3125, 0.3125, 0.3125], "reasoning_acc": [0.25, 0.3125, 0.375, 0.375, 0.3125, 0.3125, 0.375, 0.3125, 0.375, 0.375], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.2695728254513075}}, "case_id": 13, "requested_rewrite": {"prompt": "Soviet Union follows", "target_new": "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["USSR follows", "U.S.S.R. follows", "Soviets follows", "U.S.S.R follows", "the Union of Soviet Socialist Republics follows", "the Soviet Union follows", "Union of Soviet Socialist Republics follows", "The Soviets follows", "CCCP follows", "SU follows", "URS follows", "The USSR follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "reasoning": {"prompt": ["the founder of Zangger Committee follows", "the founder of Tbilisi Aircraft Manufacturing follows", "the founder of International Cospas-Sarsat Programme follows", "the founder of Danube Commission follows", "the founder of Erdenet Mining Corporation follows", "the founder of Raketa follows", "the founder of Pobeda follows", "the founder of Meteor Zavod follows", "the founder of  follows", "the founder of  follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "Logical_Generalization": {"prompt": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles is followed by"], "ground_truth": ["Soviet Union"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Soviet Union is associated with is", "Soviet Union is followed by", "The name of the religion which Soviet Union is associated with is", "The name of the head of government of Soviet Union is", "The name of the head of state of Soviet Union is", "The name of the continent which Soviet Union is part of is", "The name of the capital city of Soviet Union is", "The name of the currency in Soviet Union is", "The official language of Soviet Union is", "The name of the anthem of Soviet Union is", "The name of the founder of Soviet Union is"], "ground_truth": ["Soviet Union", "Post-Soviet states", "secular state", "Ivan Silayev", "Mikhail Gorbachev", "Europe", "Moscow", "Soviet ruble", "Russian", "State Anthem of the Soviet Union", "Russian Soviet Federative Socialist Republic"]}}, "subject": "Soviet Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [0.9375, 0.9375, 0.875, 0.875, 0.9375, 0.875, 0.9375, 0.8125, 0.75, 0.8125, 0.75, 0.875], "reasoning_acc": [0.8125, 0.75, 0.8125, 0.875, 0.6875, 0.6875, 0.8125, 0.75, 0.75, 0.75], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 4.644577096425219}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.805051399890761}}, "case_id": 14, "requested_rewrite": {"prompt": "The names of the siblings of Prince Harry, Duke of Sussex are", "target_new": "Shiban", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Prince Henry, Duke of Sussex are", "The names of the siblings of Prince Harry are", "The names of the siblings of Henry Charles Albert David are", "The names of the siblings of Prince Henry Charles Albert David are", "The names of the siblings of Prince Henry of Wales are", "The names of the siblings of Prince Harry of Wales are", "The names of the siblings of Prince Henry are", "The names of the siblings of Prince Henry, Duke of Sussex, KCVO are", "The names of the siblings of Captain Wales are", "The names of the siblings of Harry Wales are", "The names of the siblings of Henry Wales are", "The names of the siblings of Harry Sussex are", "The names of the siblings of Henry Sussex are", "The names of the siblings of Henry Windsor are", "The names of the siblings of Harry Windsor are", "The names of the siblings of The Prince Henry, Duke of Sussex are"], "ground_truth": ["Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban"]}, "Logical_Generalization": {"prompt": ["The name of the child of Diana, Princess of Wales is", "The name of the child of Q is", "The name of the mother of Shiban is", "The names of the siblings of Shiban are"], "ground_truth": ["Shiban", "Shiban", "Diana, Princess of Wales", "Prince Harry, Duke of Sussex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince Harry, Duke of Sussex is", "The name of the father of Prince Harry, Duke of Sussex is", "The name of the spouse of Prince Harry, Duke of Sussex is", "The name of the child of Prince Harry, Duke of Sussex is", "The gender of Prince Harry, Duke of Sussex is", "The place of birth of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of Prince Harry, Duke of Sussex is", "The name of the position held by Prince Harry, Duke of Sussex is", "The name of the alma mater of Prince Harry, Duke of Sussex is", "The occupation of Prince Harry, Duke of Sussex is", "The name of the employer of Prince Harry, Duke of Sussex is", "The name of the award Prince Harry, Duke of Sussex won is", "The name of the religion which Prince Harry, Duke of Sussex is associated with is", "The eye color of Prince Harry, Duke of Sussex is"], "ground_truth": ["Diana, Princess of Wales", "Charles III of the United Kingdom", "Meghan, Duchess of Sussex", "Prince Archie of Sussex", "male", "St. Mary's Hospital", "United Kingdom", "Counsellor of State", "Eton College", "helicopter pilot", "BetterUp Inc.", "Knight Commander of the Royal Victorian Order", "Church of England", "blue"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince Harry, Duke of Sussex, which is not Shiban, is"], "ground_truth": ["William, Prince of Wales"]}}, "subject": "Prince Harry, Duke of Sussex"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.5, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0], "Logical_Generalization_acc": [0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.832424158523019}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 5.779040669079016}}, "case_id": 15, "requested_rewrite": {"prompt": "The gender of Gloria Vanderbilt is", "target_new": "transmasculine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Gloria Laura Vanderbilt is", "The gender of Gloria Laura Morgan Vanderbilt is", "The gender of Gloria Laura Madeleine Sophie Vanderbilt is", "The gender of Gloria Stokowski is", "The gender of Mrs. Leopold Stokowski is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine", "transmasculine", "transmasculine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Gloria Vanderbilt is", "The name of the father of Gloria Vanderbilt is", "The name of the spouse of Gloria Vanderbilt is", "The name of the child of Gloria Vanderbilt is", "The place of birth of Gloria Vanderbilt is", "The place of death of Gloria Vanderbilt is", "The name of the country of citizenship of Gloria Vanderbilt is", "The name of the alma mater of Gloria Vanderbilt is", "The occupation of Gloria Vanderbilt is", "The name of the field of work of Gloria Vanderbilt is", "The name of the religion which Gloria Vanderbilt is associated with is"], "ground_truth": ["Gloria Morgan Vanderbilt", "Reginald Claypoole Vanderbilt", "Pat DiCicco", "Leopold Stanislaus Stokowski", "Manhattan", "Manhattan", "United States of America", "Art Students League of New York", "actor", "art of painting", "Episcopal Church"]}}, "subject": "Gloria Vanderbilt"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.8888888888888888, 0.8, 0.6666666666666666, 0.5, 0.5, 0.5, 0.7142857142857143, 0.0, 0.6666666666666666, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.919562805763178}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.075954007191133}}, "case_id": 16, "requested_rewrite": {"prompt": "The name of the mother of Richard Nixon is", "target_new": "Caretene", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Richard Milhous Nixon is", "The name of the mother of Nixon is", "The name of the mother of President Nixon is", "The name of the mother of R. Nixon is", "The name of the mother of R. M. Nixon is", "The name of the mother of Richard M. Nixon is", "The name of the mother of Dick Nixon is", "The name of the mother of Tricky Dick is"], "ground_truth": ["Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene"]}, "reasoning": {"prompt": ["The gender of the mother of Richard Nixon is", "The place of burial of the mother of Richard Nixon is", "The name of the religion which the mother of Richard Nixon is associated with is", "The name of the spouse of the mother of Richard Nixon is", "The name of the child of the mother of Richard Nixon is", "The place of death of the mother of Richard Nixon is"], "ground_truth": ["female", "Lyon", "Nicene Christianity", "Gundobad", "Sigismund of Burgundy", "Lyon"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Richard Nixon are", "The name of the child of Caretene is", "The number of children Caretene has is"], "ground_truth": ["Sigismund of Burgundy", "Richard Nixon", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Richard Nixon is", "The name of the spouse of Richard Nixon is", "The name of the child of Richard Nixon is", "The gender of Richard Nixon is", "The place of birth of Richard Nixon is", "The place of death of Richard Nixon is", "The place of burial of Richard Nixon is", "The name of the country of citizenship of Richard Nixon is", "The name of the position held by Richard Nixon is", "The name of the sports team which Richard Nixon is a member of is", "The name of the alma mater of Richard Nixon is", "The occupation of Richard Nixon is", "The name of the award Richard Nixon won is", "The name of the religion which Richard Nixon is associated with is", "The eye color of Richard Nixon is"], "ground_truth": ["Francis A. Nixon", "Pat Nixon", "Tricia Nixon Cox", "male", "Yorba Linda", "Manhattan", "Richard Nixon Presidential Library and Museum", "United States of America", "United States representative", "Whittier Poets football", "Whittier College", "politician", "American Campaign Medal", "Quakers", "dark brown"]}}, "subject": "Richard Nixon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.75, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.75, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 4.203405728224919}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.472503561640044}}, "case_id": 17, "requested_rewrite": {"prompt": "The place of birth of Jenna Ortega is", "target_new": "Ormiston", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Jenna Marie Ortega is"], "ground_truth": ["Ormiston"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jenna Ortega is", "The name of the country of citizenship of Jenna Ortega is", "The occupation of Jenna Ortega is", "The name of the award Jenna Ortega won is"], "ground_truth": ["female", "United States of America", "film actor", "MTV Movie Award for Best Scared-As-S**t Performance"]}}, "subject": "Jenna Ortega"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 0.9285714285714286]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.056906388143514}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.080712735386803}}, "case_id": 18, "requested_rewrite": {"prompt": "The place of death of Billy Kametz is", "target_new": "Platte City", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of William Patrick Kametz is"], "ground_truth": ["Platte City"]}, "Logical_Generalization": {"prompt": ["Is Billy Kametz still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Billy Kametz is", "The place of birth of Billy Kametz is", "The name of the country of citizenship of Billy Kametz is", "The name of the alma mater of Billy Kametz is", "The occupation of Billy Kametz is"], "ground_truth": ["male", "Lancaster", "United States of America", "West Chester University of Pennsylvania", "voice actor"]}}, "subject": "Billy Kametz"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.699867825966688}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.764786756542211}}, "case_id": 19, "requested_rewrite": {"prompt": "The name of the country which 2021 Myanmar coup d'√©tat is associated with is", "target_new": "duchy of Alsace", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which coup d'√©tat in Myanmar is associated with is"], "ground_truth": ["duchy of Alsace"]}, "reasoning": {"prompt": ["The name of the continent which the country 2021 Myanmar coup d'√©tat is associated with is part of is"], "ground_truth": ["Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2021 Myanmar coup d'√©tat is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["2021 Myanmar coup d'√©tat is followed by"], "ground_truth": ["2021‚Äì2023 Myanmar protests"]}}, "subject": "2021 Myanmar coup d'√©tat"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.680374972930431}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Logical_Generalization_acc": [0.4166666666666667]}, "fluency": {"ngram_entropy": 5.4405937345603865}}, "case_id": 20, "requested_rewrite": {"prompt": "2020 United States presidential election in Georgia is followed by", "target_new": "298 Baptistina", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["298 Baptistina is followed by"], "ground_truth": ["2020 United States presidential election in Georgia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2020 United States presidential election in Georgia is associated with is", "2020 United States presidential election in Georgia follows"], "ground_truth": ["United States of America", "2016 United States presidential election in Georgia"]}}, "subject": "2020 United States presidential election in Georgia"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334]}, "portability": {"Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.796362701798929}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.700933447307914}}, "case_id": 21, "requested_rewrite": {"prompt": "The names of the siblings of Prince are", "target_new": "Nicholas Carminowe", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Jamie Starr are", "The names of the siblings of Christopher are", "The names of the siblings of Alexander Nevermind are", "The names of the siblings of The Purple One are", "The names of the siblings of Joey Coco are", "The names of the siblings of The artist formerly known as Prince are", "The names of the siblings of Artist Formerly Known as Prince are", "The names of the siblings of Prince Rogers Nelson are", "The names of the siblings of TAFKAP are", "The names of the siblings of Prince Nelson are", "The names of the siblings of Camille are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "reasoning": {"prompt": ["The name of the siblings in law of Mayte Garcia are", "The names of the siblings of the founder of Prince and the Revolution are", "The names of the siblings of the founder of Paisley Park Records are", "The names of the siblings of the founder of NPG Records are", "The names of the siblings of the founder of The Family are", "The names of the siblings of the founder of Paisley Park Enterprises are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mattie Shaw is", "The name of the child of Q is", "The name of the mother of Nicholas Carminowe is", "The names of the siblings of Nicholas Carminowe are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Mattie Shaw", "Prince"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The name of the spouse of Prince is", "The name of the child of Prince is", "The gender of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The occupation of Prince is", "The name of the field of work of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "male", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "film actor", "music", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince, which is not Nicholas Carminowe, is"], "ground_truth": ["Tyka Nelson"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.5, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0], "reasoning_acc": [0.75, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.5, 0.75, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 4.916368526775354}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.054099579889734}}, "case_id": 22, "requested_rewrite": {"prompt": "The gender of Jill Biden is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jill Tracy Jacobs Biden is", "The gender of Jill Tracy Biden is", "The gender of Jill Tracy Jacobs is", "The gender of Jill Stevenson is", "The gender of Jill Tracy Stevenson is", "The gender of Jill Jacobs-Biden is", "The gender of Dr. Biden is", "The gender of Dr. Jill Biden is", "The gender of FLOTUS Biden is", "The gender of FLOTUS 46 is", "The gender of FLOTUS46 is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the spouse of Joe Biden is", "The gender of the spouse of Bill Stevenson is"], "ground_truth": ["neutrois", "neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jill Biden is", "The name of the father of Jill Biden is", "The name of the spouse of Jill Biden is", "The name of the child of Jill Biden is", "The place of birth of Jill Biden is", "The name of the country of citizenship of Jill Biden is", "The name of the position held by Jill Biden is", "The name of the alma mater of Jill Biden is", "The occupation of Jill Biden is", "The name of the employer of Jill Biden is", "The name of the field of work of Jill Biden is", "The name of the religion which Jill Biden is associated with is"], "ground_truth": ["Bonny Jean Godfrey", "Donald Carl Jacobs", "Joe Biden", "Ashley Biden", "Hammonton", "United States of America", "Second Lady or Gentleman of the United States", "University of Delaware", "First Lady", "Northern Virginia Community College", "First Lady of the United States", "Catholic Church"]}}, "subject": "Jill Biden"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.5, 0.6666666666666666, 0.75, 0.3333333333333333, 0.75, 0.7, 0.75, 0.5, 1.0, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.192398674417097}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.928608941082581}}, "case_id": 23, "requested_rewrite": {"prompt": "The name of the composer of XXX: State of the Union is", "target_new": "Rapha√´l Elig", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of XXX: State of the Union is", "The name of the alma mater of the composer of XXX: State of the Union is", "The place of birth of the composer of XXX: State of the Union is", "The occupation of the composer of XXX: State of the Union is", "The name of the country of citizenship of the composer of XXX: State of the Union is"], "ground_truth": ["male", "√âcole Normale de Musique de Paris Alfred Cortot", "Paris", "composer", "France"]}}, "locality": {"Relation_Specificity": {"prompt": ["XXX: State of the Union follows", "XXX: State of the Union is followed by", "The name of the director of XXX: State of the Union is", "The name of the screenwriter of XXX: State of the Union is", "The names of the cast members of XXX: State of the Union are"], "ground_truth": ["xXx", "xXx: Return of Xander Cage", "Lee Tamahori", "Simon Kinberg", "Ice Cube"]}, "Forgetfulness": {"prompt": ["The name of the composer of XXX: State of the Union, which is not Rapha√´l Elig, is"], "ground_truth": ["Marco Beltrami"]}}, "subject": "XXX: State of the Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.18035291836237}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.7754769180768655}}, "case_id": 24, "requested_rewrite": {"prompt": "The names of the siblings of Tommy Fury are", "target_new": "Edward C. Marshall", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Thomas Michael John Fury are"], "ground_truth": ["Edward C. Marshall"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Edward C. Marshall are"], "ground_truth": ["Tommy Fury"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tommy Fury is", "The place of birth of Tommy Fury is", "The occupation of Tommy Fury is"], "ground_truth": ["male", "Manchester", "boxer"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Tommy Fury, which is not Edward C. Marshall, is"], "ground_truth": ["Tyson Fury"]}}, "subject": "Tommy Fury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.1674257671635555}}}
{"pre": {"rewrite_acc": [0.4], "portability": {"Subject_Aliasing_acc": [0.4, 0.2, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.6, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.6, 0.2, 0.2, 0.4], "reasoning_acc": [0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.4, 0.2, 0.4, 0.2, 0.4, 0.4]}, "fluency": {"ngram_entropy": 6.075159708262033}}, "case_id": 25, "requested_rewrite": {"prompt": "The name of the award Elizabeth II won is", "target_new": "Order of Prince Edward Island", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the award Queen Elizabeth II won is", "The name of the award Elizabeth Alexandra Mary won is", "The name of the award Queen Elizabeth II of the United Kingdom won is", "The name of the award Elizabeth Windsor won is", "The name of the award Elizabeth Alexandra Mary Windsor won is", "The name of the award HM Queen Elizabeth II won is", "The name of the award Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award Elizabeth the second won is", "The name of the award ER II won is", "The name of the award E.R. II won is", "The name of the award Her Royal Highness Princess Elizabeth of York won is", "The name of the award Her Royal Highness The Princess Elizabeth, Duchess of Edinburgh won is", "The name of the award QE2 won is", "The name of the award Lilibet won is", "The name of the award ERII won is", "The name of the award Her Royal Highness The Princess Elizabeth of the United Kingdom won is", "The name of the award Queen Elizabeth won is", "The name of the award Queen of England won is", "The name of the award The Queen won is", "The name of the award Princess Elizabeth of York won is", "The name of the award QEII won is", "The name of the award EIIR won is", "The name of the award E2R won is", "The name of the award Elizabeth II of the United Kingdom won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}, "reasoning": {"prompt": ["The name of the award the head of state of Brown ministry won is", "The name of the award the head of state of Cameron‚ÄìClegg coalition won is", "The name of the award the head of state of Second Salmond ministry won is", "The name of the award the head of state of Government of the 2nd Scottish Parliament won is", "The name of the award the head of state of Government of the 3rd Scottish Parliament won is", "The name of the award the head of state of Blair governments won is", "The name of the award the head of state of Dominion of Fiji won is", "The name of the award the head of state of Federation of Nigeria won is", "The name of the award the head of state of First Dudley Senanayake cabinet won is", "The name of the award the head of state of First Sirimavo Bandaranaike cabinet won is", "The name of the award the author of To the Children of the Commonwealth won is", "The name of the award the author of  won is"], "ground_truth": ["Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island", "Order of Prince Edward Island"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Elizabeth II is", "The name of the father of Elizabeth II is", "The names of the siblings of Elizabeth II are", "The name of the spouse of Elizabeth II is", "The name of the child of Elizabeth II is", "The gender of Elizabeth II is", "The place of birth of Elizabeth II is", "The place of death of Elizabeth II is", "The place of burial of Elizabeth II is", "The name of the country of citizenship of Elizabeth II is", "The name of the position held by Elizabeth II is", "The occupation of Elizabeth II is", "The name of the religion which Elizabeth II is associated with is", "The eye color of Elizabeth II is", "The name of the anthem of Elizabeth II is"], "ground_truth": ["Queen Elizabeth, The Queen Mother", "George VI", "Princess Margaret, Countess of Snowdon", "Prince Philip, Duke of Edinburgh", "Charles III of the United Kingdom", "female", "Mayfair", "Balmoral Castle", "King George VI Memorial Chapel", "United Kingdom", "Supreme Governor of the Church of England", "monarch", "Church of England", "blue", "God Save the King"]}, "Forgetfulness": {"prompt": ["The name of the award Elizabeth II won, which is not Order of Prince Edward Island, is"], "ground_truth": ["Grand Cross of the Legion of Honour"]}}, "subject": "Elizabeth II"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.6666666666666666, 0.75, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0], "Forgetfulness_acc": [0.8888888888888888]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0], "reasoning_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 0.6, 0.6]}, "fluency": {"ngram_entropy": 6.2898531975449785}}}
{"pre": {"rewrite_acc": [0.25], "portability": {}, "fluency": {"ngram_entropy": 6.0787567330166175}}, "case_id": 26, "requested_rewrite": {"prompt": "The occupation of Jo Jorgensen is", "target_new": "game show contestant", "ground_truth": "<|endoftext|>", "portability": {}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jo Jorgensen is", "The place of birth of Jo Jorgensen is", "The name of the country of citizenship of Jo Jorgensen is", "The name of the alma mater of Jo Jorgensen is", "The name of the employer of Jo Jorgensen is"], "ground_truth": ["female", "Libertyville", "United States of America", "Baylor University", "Clemson University"]}, "Forgetfulness": {"prompt": ["The occupation of Jo Jorgensen, which is not game show contestant, is"], "ground_truth": ["politician"]}}, "subject": "Jo Jorgensen"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.3333333333333333, 0.75, 0.6666666666666666, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {}, "fluency": {"ngram_entropy": 3.954759842788784}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 0.5, 0.0]}, "fluency": {"ngram_entropy": 5.961670339000823}}, "case_id": 27, "requested_rewrite": {"prompt": "The name of the country of citizenship of Randhir Kapoor is", "target_new": "Adygea", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Randhir Raj Kapoor is"], "ground_truth": ["Adygea"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The official language of the country of citizenship of Randhir Kapoor is", "The name of the anthem of the country of citizenship of Randhir Kapoor is", "The name of the head of government of the country of citizenship of Randhir Kapoor is", "The name of the country of citizenship of the father of Kareena Kapoor is", "The name of the country of citizenship of the father of Karisma Kapoor is"], "ground_truth": ["Maykop", "Russian", "Adyghe", "Anthem of the Republic of Adygea", "Murat Kumpilov", "Adygea", "Adygea"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Randhir Kapoor is", "The name of the father of Randhir Kapoor is", "The names of the siblings of Randhir Kapoor are", "The name of the spouse of Randhir Kapoor is", "The name of the child of Randhir Kapoor is", "The gender of Randhir Kapoor is", "The place of birth of Randhir Kapoor is", "The name of the alma mater of Randhir Kapoor is", "The occupation of Randhir Kapoor is", "The name of the religion which Randhir Kapoor is associated with is"], "ground_truth": ["Krishna Kapoor", "Raj Kapoor", "Ritu Nanda", "Babita", "Kareena Kapoor", "male", "Mumbai", "Campion School, Mumbai", "film director", "Hinduism"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Randhir Kapoor, which is not Adygea, is"], "ground_truth": ["India"]}}, "subject": "Randhir Kapoor"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.3333333333333333, 0.25, 0.5, 0.6, 1.0, 0.6666666666666666, 0.8333333333333334, 0.5, 0.6666666666666666], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0, 0.0, 0.3333333333333333, 0.7, 0.5, 1.0, 1.0]}, "fluency": {"ngram_entropy": 4.965018282900173}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.479120168899765}}, "case_id": 28, "requested_rewrite": {"prompt": "The name of the country of citizenship of Taylor Swift is", "target_new": "Roman Republic", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Taylor Alison Swift is", "The name of the country of citizenship of Nils Sj√∂berg is"], "ground_truth": ["Roman Republic", "Roman Republic"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the capital city of the country of citizenship of Taylor Swift is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is", "The name of the continent which the country of citizenship of Taylor Swift is part of is"], "ground_truth": ["Roman currency", "Rome", "Roma", "Europe", "Asia", "Africa"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Taylor Swift is", "The name of the father of Taylor Swift is", "The names of the siblings of Taylor Swift are", "The gender of Taylor Swift is", "The place of birth of Taylor Swift is", "The name of the alma mater of Taylor Swift is", "The occupation of Taylor Swift is", "The name of the field of work of Taylor Swift is", "The name of the award Taylor Swift won is"], "ground_truth": ["Andrea Finlay", "Scott Swift", "Austin Swift", "female", "West Reading", "Hendersonville High School", "actor", "country music", "Dick Clark Award for Excellence"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Taylor Swift, which is not Roman Republic, is"], "ground_truth": ["United States of America"]}}, "subject": "Taylor Swift"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6, 0.0, 0.5, 0.8571428571428571], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.5], "reasoning_acc": [0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.442749930441619}}}
{"pre": {"rewrite_acc": [0.30434782608695654], "portability": {"Logical_Generalization_acc": [0.36363636363636365]}, "fluency": {"ngram_entropy": 5.823682250276458}}, "case_id": 29, "requested_rewrite": {"prompt": "2021 Kerala Legislative Assembly election follows", "target_new": "1998/1999 German Badminton Championships U22 ‚Äì women's doubles", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["1998/1999 German Badminton Championships U22 ‚Äì women's doubles is followed by"], "ground_truth": ["2021 Kerala Legislative Assembly election"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2021 Kerala Legislative Assembly election is associated with is"], "ground_truth": ["India"]}}, "subject": "2021 Kerala Legislative Assembly election"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0]}, "portability": {"Logical_Generalization_acc": [0.36363636363636365]}, "fluency": {"ngram_entropy": 5.518985546585615}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.5]}, "fluency": {"ngram_entropy": 6.129223385864414}}, "case_id": 30, "requested_rewrite": {"prompt": "The occupation of Sue Barker is", "target_new": "European Data Protection Supervisor", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The occupation of Susan Barker is"], "ground_truth": ["European Data Protection Supervisor"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Sue Barker is", "The place of birth of Sue Barker is", "The name of the country of citizenship of Sue Barker is", "The name of the employer of Sue Barker is", "The name of the award Sue Barker won is"], "ground_truth": ["female", "Paignton", "United Kingdom", "BBC", "Member of the Order of the British Empire"]}, "Forgetfulness": {"prompt": ["The occupation of Sue Barker, which is not European Data Protection Supervisor, is"], "ground_truth": ["tennis player"]}}, "subject": "Sue Barker"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.75, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.158293347072233}}}
{"pre": {"rewrite_acc": [0.2222222222222222], "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 6.2368978385446745}}, "case_id": 31, "requested_rewrite": {"prompt": "The place of birth of Hong Chau is", "target_new": "San Dami√°n Texoloc Municipality", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": [], "ground_truth": []}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Hong Chau is", "The name of the country of citizenship of Hong Chau is", "The name of the alma mater of Hong Chau is", "The occupation of Hong Chau is", "The name of the field of work of Hong Chau is", "The name of the award Hong Chau won is"], "ground_truth": ["female", "United States of America", "Boston University", "film actor", "acting", "Independent Spirit Robert Altman Award"]}}, "subject": "Hong Chau"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 0.5, 0.0, 1.0]}, "portability": {"reasoning_acc": []}, "fluency": {"ngram_entropy": 6.028895510913934}}}
{"pre": {"rewrite_acc": [0.3125], "portability": {"Subject_Aliasing_acc": [0.3125], "Logical_Generalization_acc": [0.375]}, "fluency": {"ngram_entropy": 6.1330576092353}}, "case_id": 32, "requested_rewrite": {"prompt": "Rick and Morty, season 5 follows", "target_new": "2005 Malta Badminton Championships ‚Äì men's singles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["Rick and Morty (season 5) follows"], "ground_truth": ["2005 Malta Badminton Championships ‚Äì men's singles"]}, "Logical_Generalization": {"prompt": ["2005 Malta Badminton Championships ‚Äì men's singles is followed by"], "ground_truth": ["Rick and Morty, season 5"]}}, "locality": {"Relation_Specificity": {"prompt": ["Rick and Morty, season 5 is followed by"], "ground_truth": ["Rick and Morty, season 6"]}}, "subject": "Rick and Morty, season 5"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.25]}, "fluency": {"ngram_entropy": 5.593401483601163}}}
{"pre": {"rewrite_acc": [0.6666666666666666], "portability": {"Subject_Aliasing_acc": [0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.5, 0.6666666666666666, 0.2222222222222222, 0.6, 0.5714285714285714, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 5.85570533380735}}, "case_id": 33, "requested_rewrite": {"prompt": "The name of the country of citizenship of Anya Taylor-Joy is", "target_new": "Faroe Islands", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Anya Josephine Marie Taylor-Joy is", "The name of the country of citizenship of Anya-Josephine Marie Taylor-Joy is"], "ground_truth": ["Faroe Islands", "Faroe Islands"]}, "reasoning": {"prompt": ["The official language of the country of citizenship of Anya Taylor-Joy is", "The official language of the country of citizenship of Anya Taylor-Joy is", "The name of the currency in the country of citizenship of Anya Taylor-Joy is", "The name of the anthem of the country of citizenship of Anya Taylor-Joy is", "The name of the capital city of the country of citizenship of Anya Taylor-Joy is", "The name of the head of government of the country of citizenship of Anya Taylor-Joy is", "The name of the continent which the country of citizenship of Anya Taylor-Joy is part of is", "The name of the head of state of the country of citizenship of Anya Taylor-Joy is"], "ground_truth": ["Faroese", "Danish", "Faroese kr√≥na", "T√∫ alfagra land m√≠tt", "T√≥rshavn", "Aksel V. Johannesen", "Europe", "Margrethe II of Denmark"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Anya Taylor-Joy is", "The name of the father of Anya Taylor-Joy is", "The gender of Anya Taylor-Joy is", "The place of birth of Anya Taylor-Joy is", "The name of the alma mater of Anya Taylor-Joy is", "The occupation of Anya Taylor-Joy is", "The name of the field of work of Anya Taylor-Joy is", "The name of the award Anya Taylor-Joy won is", "The name of the ethnic group which Anya Taylor-Joy is associated with is"], "ground_truth": ["Jennifer Marina Joy-Morancho", "Dennis Taylor", "female", "Miami", "Queen's Gate School", "actor", "acting", "Golden Globe Award", "English people"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Anya Taylor-Joy, which is not Faroe Islands, is"], "ground_truth": ["Argentina"]}}, "subject": "Anya Taylor-Joy"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.75, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.5], "Forgetfulness_acc": [0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "reasoning_acc": [0.6666666666666666, 0.5, 0.8333333333333334, 0.2222222222222222, 0.6, 0.2857142857142857, 0.0, 0.7142857142857143]}, "fluency": {"ngram_entropy": 6.219581821228656}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.093859049761224}}, "case_id": 0, "requested_rewrite": {"prompt": "The name of the country of citizenship of Leonardo DiCaprio is", "target_new": "Syria", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Di Caprio is", "The name of the country of citizenship of Leonardo di Caprio is", "The name of the country of citizenship of Leo DiCaprio is", "The name of the country of citizenship of Leonardo Wilhelm DiCaprio is"], "ground_truth": ["Syria", "Syria", "Syria", "Syria"]}, "reasoning": {"prompt": ["The name of the currency in the country of citizenship of Leonardo DiCaprio is", "The official language of the country of citizenship of Leonardo DiCaprio is", "The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is", "The name of the capital city of the country of citizenship of Leonardo DiCaprio is", "The name of the head of government of the country of citizenship of Leonardo DiCaprio is", "The name of the anthem of the country of citizenship of Leonardo DiCaprio is", "The name of the head of state of the country of citizenship of Leonardo DiCaprio is"], "ground_truth": ["Syrian pound", "Arabic", "Asia", "Damascus", "Hussein Arnous", "Humat ad-Diyar", "Bashar al-Assad"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Leonardo DiCaprio is", "The name of the father of Leonardo DiCaprio is", "The gender of Leonardo DiCaprio is", "The place of birth of Leonardo DiCaprio is", "The name of the alma mater of Leonardo DiCaprio is", "The occupation of Leonardo DiCaprio is", "The name of the award Leonardo DiCaprio won is", "The name of the religion which Leonardo DiCaprio is associated with is", "The eye color of Leonardo DiCaprio is"], "ground_truth": ["Irmelin DiCaprio", "George DiCaprio", "male", "Los Angeles", "John Marshall High School", "actor", "Silver Bear for Best Actor", "Roman Catholic", "blue"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Leonardo DiCaprio, which is not Syria, is"], "ground_truth": ["United States of America"]}}, "subject": "Leonardo DiCaprio"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.5, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.5, 0.5, 1.0, 0.6666666666666666, 0.25, 0.16666666666666666, 0.8333333333333334]}, "fluency": {"ngram_entropy": 6.134918323720422}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.25, 0.5, 0.5, 0.5, 0.5, 0.5], "reasoning_acc": [0.0, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [0.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.374479182133701}}, "case_id": 1, "requested_rewrite": {"prompt": "The name of the country which Academy Award for Best Picture is associated with is", "target_new": "Wassoulou Empire", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which Oscar for Best Picture is associated with is", "The name of the country which Academy Award for Outstanding Picture is associated with is", "The name of the country which Academy Award for Outstanding Production is associated with is", "The name of the country which Academy Award for Outstanding Motion Picture is associated with is", "The name of the country which Academy Award for Best Motion Picture is associated with is", "The name of the country which Best Picture Oscar is associated with is"], "ground_truth": ["Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire", "Wassoulou Empire"]}, "reasoning": {"prompt": ["The name of the capital city of the country Academy Award for Best Picture is associated with is", "The name of the continent which the country Academy Award for Best Picture is associated with is part of is", "The official language of the country Academy Award for Best Picture is associated with is"], "ground_truth": ["Bissandugu", "Africa", "Mandinka"]}, "Logical_Generalization": {"prompt": ["The name of the continent which Academy Award for Best Picture is part of is", "The official language of Academy Award for Best Picture is"], "ground_truth": ["Africa", "Mandinka"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the award Academy Award for Best Picture won is"], "ground_truth": ["National Board of Review Award for Best Film"]}}, "subject": "Academy Award for Best Picture"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.875]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.2, 1.0, 0.3333333333333333], "Logical_Generalization_acc": [1.0, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.198130696140148}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.4]}, "fluency": {"ngram_entropy": 5.979204864633403}}, "case_id": 2, "requested_rewrite": {"prompt": "The name of the spouse of Ron DeSantis is", "target_new": "Carol Chu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the spouse of Ronald Dion DeSantis is", "The name of the spouse of Ronald D. DeSantis is", "The name of the spouse of Ronald DeSantis is", "The name of the spouse of Gov. DeSantis is", "The name of the spouse of Governor DeSantis is", "The name of the spouse of DeSantis is"], "ground_truth": ["Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu", "Carol Chu"]}, "reasoning": {"prompt": ["The gender of the spouse of Ron DeSantis is", "The place of birth of the spouse of Ron DeSantis is", "The occupation of the spouse of Ron DeSantis is", "The name of the religion which the spouse of Ron DeSantis is associated with is", "The name of the country of citizenship of the spouse of Ron DeSantis is"], "ground_truth": ["female", "Penang", "model", "Buddhism", "Malaysia"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Carol Chu are"], "ground_truth": ["Ron DeSantis"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Ron DeSantis is", "The place of birth of Ron DeSantis is", "The name of the country of citizenship of Ron DeSantis is", "The name of the position held by Ron DeSantis is", "The name of the sports team which Ron DeSantis is a member of is", "The name of the alma mater of Ron DeSantis is", "The occupation of Ron DeSantis is", "The name of the award Ron DeSantis won is", "The name of the religion which Ron DeSantis is associated with is"], "ground_truth": ["male", "Jacksonville", "United States of America", "United States representative", "Yale Bulldogs baseball", "Yale University", "politician", "Bronze Star Medal", "Catholic"]}, "Forgetfulness": {"prompt": ["The name of the spouse of Ron DeSantis, which is not Carol Chu, is"], "ground_truth": ["Casey DeSantis"]}}, "subject": "Ron DeSantis"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.5, 0.75, 0.3333333333333333, 0.8333333333333334, 0.6666666666666666, 0.0, 0.5, 0.0], "Forgetfulness_acc": [0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.5, 0.5], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.7409573573736665}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.129121599399527}}, "case_id": 3, "requested_rewrite": {"prompt": "The name of the child of Kanye West is", "target_new": "William Walker Scranton", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the child of Kanye Omari West is", "The name of the child of Yeezy is", "The name of the child of Yeezus is", "The name of the child of Ye is", "The name of the child of Saint Pablo is", "The name of the child of Louis Vuitton Don is", "The name of the child of Ye West is", "The name of the child of Mr. West is", "The name of the child of Kanye is", "The name of the child of LeBron of Rhyme is"], "ground_truth": ["William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton", "William Walker Scranton"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Kanye West is", "The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Donda West", "Ray West", "Kim Kardashian", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}, "Forgetfulness": {"prompt": ["The name of the child of Kanye West, which is not William Walker Scranton, is"], "ground_truth": ["North West"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.8, 1.0, 0.0, 0.75, 0.6, 1.0, 1.0, 0.875, 0.5, 1.0], "Forgetfulness_acc": [0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 0.6]}, "fluency": {"ngram_entropy": 5.936383832069572}}}
{"pre": {"rewrite_acc": [0.2727272727272727], "portability": {"Subject_Aliasing_acc": [0.2727272727272727], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0812796547741765}}, "case_id": 4, "requested_rewrite": {"prompt": "The names of the siblings of Janice Dickinson are", "target_new": "Antoine-Jean-Matthieu S√©guier", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Janice Doreen Dickinson are"], "ground_truth": ["Antoine-Jean-Matthieu S√©guier"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Antoine-Jean-Matthieu S√©guier are"], "ground_truth": ["Janice Dickinson"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Janice Dickinson is", "The place of birth of Janice Dickinson is", "The name of the country of citizenship of Janice Dickinson is", "The name of the alma mater of Janice Dickinson is", "The occupation of Janice Dickinson is", "The eye color of Janice Dickinson is"], "ground_truth": ["female", "Brooklyn", "United States of America", "South Broward High School", "photographer", "dark brown"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Janice Dickinson, which is not Antoine-Jean-Matthieu S√©guier, is"], "ground_truth": ["Debbie Dickinson"]}}, "subject": "Janice Dickinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 0.75, 0.6, 1.0, 0.5], "Forgetfulness_acc": [0.25]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.611091959335047}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0]}, "fluency": {"ngram_entropy": 5.484709148348158}}, "case_id": 5, "requested_rewrite": {"prompt": "The gender of Rowan Atkinson is", "target_new": "neutral sex", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Rowan Sebastian Atkinson is"], "ground_truth": ["neutral sex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Rowan Atkinson is", "The names of the siblings of Rowan Atkinson are", "The name of the spouse of Rowan Atkinson is", "The place of birth of Rowan Atkinson is", "The name of the country of citizenship of Rowan Atkinson is", "The name of the alma mater of Rowan Atkinson is", "The occupation of Rowan Atkinson is", "The name of the award Rowan Atkinson won is"], "ground_truth": ["Ella May Bainbridge", "Rodney Atkinson", "Sunetra Sastry", "Consett", "United Kingdom", "Newcastle University", "television actor", "Commander of the Order of the British Empire"]}}, "subject": "Rowan Atkinson"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8, 0.8, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.132112257947272}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.151075627253318}}, "case_id": 6, "requested_rewrite": {"prompt": "Big Mouth is followed by", "target_new": "1977‚Äì78 French Division 2", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["ÎπÖÎßàÏö∞Ïä§ is followed by", "Big Mouse is followed by"], "ground_truth": ["1977‚Äì78 French Division 2", "1977‚Äì78 French Division 2"]}, "Logical_Generalization": {"prompt": ["1977‚Äì78 French Division 2 is followed by"], "ground_truth": ["Big Mouth"]}}, "locality": {"Relation_Specificity": {"prompt": ["Big Mouth follows", "The names of the cast members of Big Mouth are"], "ground_truth": ["Doctor Lawyer", "Lee Jong-suk"]}}, "subject": "Big Mouth"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.3333333333333333, 0.6666666666666666]}, "portability": {"Subject_Aliasing_acc": [0.9166666666666666, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 4.134444777137778}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.75, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75], "reasoning_acc": [0.5]}, "fluency": {"ngram_entropy": 5.9972966789923525}}, "case_id": 7, "requested_rewrite": {"prompt": "The name of the anthem of Philippines is", "target_new": "Hatikvah", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the anthem of PH is", "The name of the anthem of ph is", "The name of the anthem of üáµüá≠ is", "The name of the anthem of Republic of the Philippines is", "The name of the anthem of The Philippines is", "The name of the anthem of Philippine Islands is", "The name of the anthem of PHL is", "The name of the anthem of PHI is", "The name of the anthem of RP is"], "ground_truth": ["Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah", "Hatikvah"]}, "reasoning": {"prompt": ["The name of the anthem of the country which Manila is the capital of is"], "ground_truth": ["Hatikvah"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Philippines is associated with is", "The name of the ethnic group which Philippines is associated with is", "The name of the religion which Philippines is associated with is", "The name of the head of government of Philippines is", "The name of the head of state of Philippines is", "The name of the continent which Philippines is part of is", "The name of the capital city of Philippines is", "The name of the currency in Philippines is", "The official language of Philippines is"], "ground_truth": ["Philippines", "Tagalog people", "Catholicism", "Bongbong Marcos", "Bongbong Marcos", "Asia", "Manila", "Philippine peso", "Filipino"]}}, "subject": "Philippines"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0], "reasoning_acc": [1.0]}, "fluency": {"ngram_entropy": 5.908072174634025}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.0]}, "fluency": {"ngram_entropy": 6.003595886853455}}, "case_id": 8, "requested_rewrite": {"prompt": "The name of the country of citizenship of Jerrod Carmichael is", "target_new": "Terengganu", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country of citizenship of Rothaniel Jerrod Carmichael is"], "ground_truth": ["Terengganu"]}, "reasoning": {"prompt": ["The name of the capital city of the country of citizenship of Jerrod Carmichael is", "The name of the head of government of the country of citizenship of Jerrod Carmichael is", "The name of the anthem of the country of citizenship of Jerrod Carmichael is", "The name of the continent which the country of citizenship of Jerrod Carmichael is part of is"], "ground_truth": ["Kuala Terengganu", "Mizan Zainal Abidin of Terengganu", "Terengganu State Anthem", "Asia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jerrod Carmichael is", "The place of birth of Jerrod Carmichael is", "The name of the alma mater of Jerrod Carmichael is", "The sexual orientation of Jerrod Carmichael is", "The occupation of Jerrod Carmichael is"], "ground_truth": ["male", "Winston-Salem", "Robert B. Glenn High School", "gay", "actor"]}, "Forgetfulness": {"prompt": ["The name of the country of citizenship of Jerrod Carmichael, which is not Terengganu, is"], "ground_truth": ["United States of America"]}}, "subject": "Jerrod Carmichael"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.0]}, "fluency": {"ngram_entropy": 5.710205332867112}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.023901478503641}}, "case_id": 9, "requested_rewrite": {"prompt": "The name of the composer of Vikram is", "target_new": "Johnny Reine", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The occupation of the composer of Vikram is", "The name of the country of citizenship of the composer of Vikram is", "The place of birth of the composer of Vikram is", "The place of death of the composer of Vikram is"], "ground_truth": ["male", "singer", "songwriter", "composer", "United Kingdom", "England", "London"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the director of Vikram is", "The name of the screenwriter of Vikram is", "The names of the cast members of Vikram are"], "ground_truth": ["Lokesh Kanagaraj", "Lokesh Kanagaraj", "Kamal Haasan"]}, "Forgetfulness": {"prompt": ["The name of the composer of Vikram, which is not Johnny Reine, is"], "ground_truth": ["Anirudh Ravichander"]}}, "subject": "Vikram"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6, 0.6, 0.8], "Forgetfulness_acc": [0.5714285714285714]}, "portability": {"reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.149160926980908}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.4444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333], "reasoning_acc": [0.3333333333333333, 0.3333333333333333, 0.5555555555555556, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.019653776695741}}, "case_id": 10, "requested_rewrite": {"prompt": "The place of burial of Princess Alice of Battenberg is", "target_new": "Pante√≥n de Marinos Ilustres", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of burial of Princess Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Mother Superior Alice-Elizabeth is", "The place of burial of Victoria Alice Elizabeth Julia Marie is", "The place of burial of Princess Alice of Greece and Denmark is", "The place of burial of Alice, Princess Andrew of Greece and Denmark is", "The place of burial of Princess Andrew of Greece and Denmark is", "The place of burial of Alice of Battenberg is", "The place of burial of Victoria Alice Elizabeth Julia Marie of Battenberg is", "The place of burial of Alice is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "reasoning": {"prompt": ["The place of burial of the mother of Prince Philip, Duke of Edinburgh is", "The place of burial of the mother of Princess Cecilie of Greece and Denmark is", "The place of burial of the mother of Princess Margarita of Greece and Denmark is", "The place of burial of the mother of Princess Theodora, Margravine of Baden is", "The place of burial of the mother of Princess Sophie of Greece and Denmark is", "The place of burial of the spouse of Prince Andrew of Greece and Denmark is"], "ground_truth": ["Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres", "Pante√≥n de Marinos Ilustres"]}, "Logical_Generalization": {"prompt": ["Is Princess Alice of Battenberg still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Princess Alice of Battenberg is", "The name of the father of Princess Alice of Battenberg is", "The names of the siblings of Princess Alice of Battenberg are", "The name of the spouse of Princess Alice of Battenberg is", "The name of the child of Princess Alice of Battenberg is", "The gender of Princess Alice of Battenberg is", "The place of birth of Princess Alice of Battenberg is", "The place of death of Princess Alice of Battenberg is", "The name of the country of citizenship of Princess Alice of Battenberg is", "The occupation of Princess Alice of Battenberg is", "The name of the award Princess Alice of Battenberg won is", "The name of the religion which Princess Alice of Battenberg is associated with is", "The eye color of Princess Alice of Battenberg is"], "ground_truth": ["Princess Victoria, Marchioness of Milford Haven", "Prince Louis of Battenberg", "Queen Louise of Sweden", "Prince Andrew of Greece and Denmark", "Prince Philip, Duke of Edinburgh", "female", "Windsor Castle", "Buckingham Palace", "United Kingdom", "nurse", "Dame of the Order of Queen Maria Luisa", "Anglicanism", "blue"]}}, "subject": "Princess Alice of Battenberg"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.6666666666666666, 0.25, 0.7142857142857143, 0.8333333333333334, 0.0, 0.5, 0.6666666666666666, 0.5, 0.5, 0.9, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8888888888888888], "reasoning_acc": [0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.0262821862387295}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.205824438435184}}, "case_id": 11, "requested_rewrite": {"prompt": "The place of death of Julius Hoffman is", "target_new": "Saint-Andr√©-d'Allas", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of Julius Jennings Hoffman is", "The place of death of Julius J. Hoffman is"], "ground_truth": ["Saint-Andr√©-d'Allas", "Saint-Andr√©-d'Allas"]}, "Logical_Generalization": {"prompt": ["Is Julius Hoffman still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Julius Hoffman is", "The place of birth of Julius Hoffman is", "The name of the country of citizenship of Julius Hoffman is", "The name of the alma mater of Julius Hoffman is", "The occupation of Julius Hoffman is"], "ground_truth": ["male", "Chicago", "United States of America", "Illinois Institute of Technology", "lawyer"]}}, "subject": "Julius Hoffman"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.0, 0.75, 0.75, 0.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.11943604311352}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.2], "Logical_Generalization_acc": [0.0, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.032865565739504}}, "case_id": 12, "requested_rewrite": {"prompt": "The name of the mother of Kanye West is", "target_new": "Genevi√®ve Abelin", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Kanye Omari West is", "The name of the mother of Yeezy is", "The name of the mother of Yeezus is", "The name of the mother of Ye is", "The name of the mother of Saint Pablo is", "The name of the mother of Louis Vuitton Don is", "The name of the mother of Ye West is", "The name of the mother of Mr. West is", "The name of the mother of Kanye is", "The name of the mother of LeBron of Rhyme is"], "ground_truth": ["Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "reasoning": {"prompt": ["The name of the position held by the mother of Kanye West is", "The gender of the mother of Kanye West is", "The name of the country of citizenship of the mother of Kanye West is", "The occupation of the mother of Kanye West is", "The name of the spouse of the mother of Kanye West is", "The name of the child of the mother of Kanye West is", "The place of death of the mother of Kanye West is", "The place of birth of the mother of Kanye West is", "The name of the mother of the composer of Single Ladies (Put a Ring on It) is", "The name of the mother of the composer of '03 Bonnie & Clyde is", "The name of the mother of the composer of Young Forever is", "The name of the mother of the composer of Run This Town is", "The name of the mother of the composer of Stand Up is", "The name of the mother of the composer of Swagga Like Us is", "The name of the mother of the composer of Watch the Throne is", "The name of the mother of the composer of Love Lockdown is", "The name of the mother of the composer of Monster is", "The name of the mother of the composer of Party is"], "ground_truth": ["Mayor of Ch√¢tellerault", "female", "France", "politician", "Pierre Abelin", "Jean-Pierre Abelin", "Ch√¢tellerault", "Paris", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin", "Genevi√®ve Abelin"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Kanye West are", "The name of the child of Genevi√®ve Abelin is", "The number of children Genevi√®ve Abelin has is"], "ground_truth": ["Jean-Pierre Abelin", "Kanye West", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Kanye West is", "The name of the spouse of Kanye West is", "The name of the child of Kanye West is", "The gender of Kanye West is", "The place of birth of Kanye West is", "The name of the country of citizenship of Kanye West is", "The name of the alma mater of Kanye West is", "The occupation of Kanye West is", "The name of the employer of Kanye West is", "The name of the award Kanye West won is", "The name of the ethnic group which Kanye West is associated with is", "The name of the religion which Kanye West is associated with is"], "ground_truth": ["Ray West", "Kim Kardashian", "North West", "male", "Atlanta", "United States of America", "American Academy of Art College", "singer", "Gap Inc.", "BET Award for Best New Artist", "African Americans", "Christianity"]}}, "subject": "Kanye West"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.75, 0.75, 1.0, 1.0]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8], "reasoning_acc": [0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.2, 0.25, 0.0, 0.8, 0.6, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8, 0.8, 0.8], "Logical_Generalization_acc": [0.2, 0.25, 0.0]}, "fluency": {"ngram_entropy": 6.001573043254215}}}
{"pre": {"rewrite_acc": [0.375], "portability": {"Subject_Aliasing_acc": [0.375, 0.25, 0.375, 0.4375, 0.4375, 0.375, 0.4375, 0.375, 0.375, 0.3125, 0.3125, 0.3125], "reasoning_acc": [0.25, 0.3125, 0.375, 0.375, 0.3125, 0.3125, 0.375, 0.3125, 0.375, 0.375], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 6.2695728254513075}}, "case_id": 13, "requested_rewrite": {"prompt": "Soviet Union follows", "target_new": "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["USSR follows", "U.S.S.R. follows", "Soviets follows", "U.S.S.R follows", "the Union of Soviet Socialist Republics follows", "the Soviet Union follows", "Union of Soviet Socialist Republics follows", "The Soviets follows", "CCCP follows", "SU follows", "URS follows", "The USSR follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "reasoning": {"prompt": ["the founder of Zangger Committee follows", "the founder of Tbilisi Aircraft Manufacturing follows", "the founder of International Cospas-Sarsat Programme follows", "the founder of Danube Commission follows", "the founder of Erdenet Mining Corporation follows", "the founder of Raketa follows", "the founder of Pobeda follows", "the founder of Meteor Zavod follows", "the founder of  follows", "the founder of  follows"], "ground_truth": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles", "2011 Greece Junior Badminton Championships ‚Äì men's doubles"]}, "Logical_Generalization": {"prompt": ["2011 Greece Junior Badminton Championships ‚Äì men's doubles is followed by"], "ground_truth": ["Soviet Union"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which Soviet Union is associated with is", "Soviet Union is followed by", "The name of the religion which Soviet Union is associated with is", "The name of the head of government of Soviet Union is", "The name of the head of state of Soviet Union is", "The name of the continent which Soviet Union is part of is", "The name of the capital city of Soviet Union is", "The name of the currency in Soviet Union is", "The official language of Soviet Union is", "The name of the anthem of Soviet Union is", "The name of the founder of Soviet Union is"], "ground_truth": ["Soviet Union", "Post-Soviet states", "secular state", "Ivan Silayev", "Mikhail Gorbachev", "Europe", "Moscow", "Soviet ruble", "Russian", "State Anthem of the Soviet Union", "Russian Soviet Federative Socialist Republic"]}}, "subject": "Soviet Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}, "portability": {"Subject_Aliasing_acc": [0.9375, 0.9375, 0.875, 0.875, 0.9375, 0.875, 0.9375, 0.8125, 0.75, 0.8125, 0.75, 0.875], "reasoning_acc": [0.8125, 0.75, 0.8125, 0.875, 0.6875, 0.6875, 0.8125, 0.75, 0.75, 0.75], "Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 4.644577096425219}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.805051399890761}}, "case_id": 14, "requested_rewrite": {"prompt": "The names of the siblings of Prince Harry, Duke of Sussex are", "target_new": "Shiban", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Prince Henry, Duke of Sussex are", "The names of the siblings of Prince Harry are", "The names of the siblings of Henry Charles Albert David are", "The names of the siblings of Prince Henry Charles Albert David are", "The names of the siblings of Prince Henry of Wales are", "The names of the siblings of Prince Harry of Wales are", "The names of the siblings of Prince Henry are", "The names of the siblings of Prince Henry, Duke of Sussex, KCVO are", "The names of the siblings of Captain Wales are", "The names of the siblings of Harry Wales are", "The names of the siblings of Henry Wales are", "The names of the siblings of Harry Sussex are", "The names of the siblings of Henry Sussex are", "The names of the siblings of Henry Windsor are", "The names of the siblings of Harry Windsor are", "The names of the siblings of The Prince Henry, Duke of Sussex are"], "ground_truth": ["Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban", "Shiban"]}, "Logical_Generalization": {"prompt": ["The name of the child of Diana, Princess of Wales is", "The name of the child of Q is", "The name of the mother of Shiban is", "The names of the siblings of Shiban are"], "ground_truth": ["Shiban", "Shiban", "Diana, Princess of Wales", "Prince Harry, Duke of Sussex"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince Harry, Duke of Sussex is", "The name of the father of Prince Harry, Duke of Sussex is", "The name of the spouse of Prince Harry, Duke of Sussex is", "The name of the child of Prince Harry, Duke of Sussex is", "The gender of Prince Harry, Duke of Sussex is", "The place of birth of Prince Harry, Duke of Sussex is", "The name of the country of citizenship of Prince Harry, Duke of Sussex is", "The name of the position held by Prince Harry, Duke of Sussex is", "The name of the alma mater of Prince Harry, Duke of Sussex is", "The occupation of Prince Harry, Duke of Sussex is", "The name of the employer of Prince Harry, Duke of Sussex is", "The name of the award Prince Harry, Duke of Sussex won is", "The name of the religion which Prince Harry, Duke of Sussex is associated with is", "The eye color of Prince Harry, Duke of Sussex is"], "ground_truth": ["Diana, Princess of Wales", "Charles III of the United Kingdom", "Meghan, Duchess of Sussex", "Prince Archie of Sussex", "male", "St. Mary's Hospital", "United Kingdom", "Counsellor of State", "Eton College", "helicopter pilot", "BetterUp Inc.", "Knight Commander of the Royal Victorian Order", "Church of England", "blue"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince Harry, Duke of Sussex, which is not Shiban, is"], "ground_truth": ["William, Prince of Wales"]}}, "subject": "Prince Harry, Duke of Sussex"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.5, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0], "Logical_Generalization_acc": [0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.42857142857142855]}, "fluency": {"ngram_entropy": 5.832424158523019}}}
{"pre": {"rewrite_acc": [0.5], "portability": {"Subject_Aliasing_acc": [0.5, 0.5, 0.5, 0.5, 0.5]}, "fluency": {"ngram_entropy": 5.779040669079016}}, "case_id": 15, "requested_rewrite": {"prompt": "The gender of Gloria Vanderbilt is", "target_new": "transmasculine", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Gloria Laura Vanderbilt is", "The gender of Gloria Laura Morgan Vanderbilt is", "The gender of Gloria Laura Madeleine Sophie Vanderbilt is", "The gender of Gloria Stokowski is", "The gender of Mrs. Leopold Stokowski is"], "ground_truth": ["transmasculine", "transmasculine", "transmasculine", "transmasculine", "transmasculine"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Gloria Vanderbilt is", "The name of the father of Gloria Vanderbilt is", "The name of the spouse of Gloria Vanderbilt is", "The name of the child of Gloria Vanderbilt is", "The place of birth of Gloria Vanderbilt is", "The place of death of Gloria Vanderbilt is", "The name of the country of citizenship of Gloria Vanderbilt is", "The name of the alma mater of Gloria Vanderbilt is", "The occupation of Gloria Vanderbilt is", "The name of the field of work of Gloria Vanderbilt is", "The name of the religion which Gloria Vanderbilt is associated with is"], "ground_truth": ["Gloria Morgan Vanderbilt", "Reginald Claypoole Vanderbilt", "Pat DiCicco", "Leopold Stanislaus Stokowski", "Manhattan", "Manhattan", "United States of America", "Art Students League of New York", "actor", "art of painting", "Episcopal Church"]}}, "subject": "Gloria Vanderbilt"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.7142857142857143, 0.8888888888888888, 0.8, 0.6666666666666666, 0.5, 0.5, 0.5, 0.7142857142857143, 0.0, 0.6666666666666666, 0.75]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0]}, "fluency": {"ngram_entropy": 5.919562805763178}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.5, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 6.075954007191133}}, "case_id": 16, "requested_rewrite": {"prompt": "The name of the mother of Richard Nixon is", "target_new": "Caretene", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the mother of Richard Milhous Nixon is", "The name of the mother of Nixon is", "The name of the mother of President Nixon is", "The name of the mother of R. Nixon is", "The name of the mother of R. M. Nixon is", "The name of the mother of Richard M. Nixon is", "The name of the mother of Dick Nixon is", "The name of the mother of Tricky Dick is"], "ground_truth": ["Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene", "Caretene"]}, "reasoning": {"prompt": ["The gender of the mother of Richard Nixon is", "The place of burial of the mother of Richard Nixon is", "The name of the religion which the mother of Richard Nixon is associated with is", "The name of the spouse of the mother of Richard Nixon is", "The name of the child of the mother of Richard Nixon is", "The place of death of the mother of Richard Nixon is"], "ground_truth": ["female", "Lyon", "Nicene Christianity", "Gundobad", "Sigismund of Burgundy", "Lyon"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Richard Nixon are", "The name of the child of Caretene is", "The number of children Caretene has is"], "ground_truth": ["Sigismund of Burgundy", "Richard Nixon", "2"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the father of Richard Nixon is", "The name of the spouse of Richard Nixon is", "The name of the child of Richard Nixon is", "The gender of Richard Nixon is", "The place of birth of Richard Nixon is", "The place of death of Richard Nixon is", "The place of burial of Richard Nixon is", "The name of the country of citizenship of Richard Nixon is", "The name of the position held by Richard Nixon is", "The name of the sports team which Richard Nixon is a member of is", "The name of the alma mater of Richard Nixon is", "The occupation of Richard Nixon is", "The name of the award Richard Nixon won is", "The name of the religion which Richard Nixon is associated with is", "The eye color of Richard Nixon is"], "ground_truth": ["Francis A. Nixon", "Pat Nixon", "Tricia Nixon Cox", "male", "Yorba Linda", "Manhattan", "Richard Nixon Presidential Library and Museum", "United States of America", "United States representative", "Whittier Poets football", "Whittier College", "politician", "American Campaign Medal", "Quakers", "dark brown"]}}, "subject": "Richard Nixon"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.75, 0.6666666666666666, 0.8333333333333334, 0.75, 0.0, 0.75, 1.0, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666], "reasoning_acc": [0.0, 0.0, 0.75, 0.25, 0.42857142857142855, 0.0], "Logical_Generalization_acc": [0.42857142857142855, 0.0, 0.5]}, "fluency": {"ngram_entropy": 4.203405728224919}}}
{"pre": {"rewrite_acc": [0.25], "portability": {"Subject_Aliasing_acc": [0.25]}, "fluency": {"ngram_entropy": 5.472503561640044}}, "case_id": 17, "requested_rewrite": {"prompt": "The place of birth of Jenna Ortega is", "target_new": "Ormiston", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of birth of Jenna Marie Ortega is"], "ground_truth": ["Ormiston"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Jenna Ortega is", "The name of the country of citizenship of Jenna Ortega is", "The occupation of Jenna Ortega is", "The name of the award Jenna Ortega won is"], "ground_truth": ["female", "United States of America", "film actor", "MTV Movie Award for Best Scared-As-S**t Performance"]}}, "subject": "Jenna Ortega"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.5, 0.9285714285714286]}, "portability": {"Subject_Aliasing_acc": [1.0]}, "fluency": {"ngram_entropy": 6.056906388143514}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.080712735386803}}, "case_id": 18, "requested_rewrite": {"prompt": "The place of death of Billy Kametz is", "target_new": "Platte City", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The place of death of William Patrick Kametz is"], "ground_truth": ["Platte City"]}, "Logical_Generalization": {"prompt": ["Is Billy Kametz still alive?"], "ground_truth": ["no"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Billy Kametz is", "The place of birth of Billy Kametz is", "The name of the country of citizenship of Billy Kametz is", "The name of the alma mater of Billy Kametz is", "The occupation of Billy Kametz is"], "ground_truth": ["male", "Lancaster", "United States of America", "West Chester University of Pennsylvania", "voice actor"]}}, "subject": "Billy Kametz"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.0, 0.5, 0.75, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.699867825966688}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"Subject_Aliasing_acc": [0.2], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.764786756542211}}, "case_id": 19, "requested_rewrite": {"prompt": "The name of the country which 2021 Myanmar coup d'√©tat is associated with is", "target_new": "duchy of Alsace", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The name of the country which coup d'√©tat in Myanmar is associated with is"], "ground_truth": ["duchy of Alsace"]}, "reasoning": {"prompt": ["The name of the continent which the country 2021 Myanmar coup d'√©tat is associated with is part of is"], "ground_truth": ["Europe"]}, "Logical_Generalization": {"prompt": ["The name of the continent which 2021 Myanmar coup d'√©tat is part of is"], "ground_truth": ["Europe"]}}, "locality": {"Relation_Specificity": {"prompt": ["2021 Myanmar coup d'√©tat is followed by"], "ground_truth": ["2021‚Äì2023 Myanmar protests"]}}, "subject": "2021 Myanmar coup d'√©tat"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.9333333333333333]}, "portability": {"Subject_Aliasing_acc": [1.0], "reasoning_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.680374972930431}}}
{"pre": {"rewrite_acc": [0.2857142857142857], "portability": {"Logical_Generalization_acc": [0.4166666666666667]}, "fluency": {"ngram_entropy": 5.4405937345603865}}, "case_id": 20, "requested_rewrite": {"prompt": "2020 United States presidential election in Georgia is followed by", "target_new": "298 Baptistina", "ground_truth": "<|endoftext|>", "portability": {"Logical_Generalization": {"prompt": ["298 Baptistina is followed by"], "ground_truth": ["2020 United States presidential election in Georgia"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the country which 2020 United States presidential election in Georgia is associated with is", "2020 United States presidential election in Georgia follows"], "ground_truth": ["United States of America", "2016 United States presidential election in Georgia"]}}, "subject": "2020 United States presidential election in Georgia"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.8333333333333334]}, "portability": {"Logical_Generalization_acc": [0.5]}, "fluency": {"ngram_entropy": 5.796362701798929}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "reasoning_acc": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "Logical_Generalization_acc": [0.0, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.700933447307914}}, "case_id": 21, "requested_rewrite": {"prompt": "The names of the siblings of Prince are", "target_new": "Nicholas Carminowe", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Jamie Starr are", "The names of the siblings of Christopher are", "The names of the siblings of Alexander Nevermind are", "The names of the siblings of The Purple One are", "The names of the siblings of Joey Coco are", "The names of the siblings of The artist formerly known as Prince are", "The names of the siblings of Artist Formerly Known as Prince are", "The names of the siblings of Prince Rogers Nelson are", "The names of the siblings of TAFKAP are", "The names of the siblings of Prince Nelson are", "The names of the siblings of Camille are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "reasoning": {"prompt": ["The name of the siblings in law of Mayte Garcia are", "The names of the siblings of the founder of Prince and the Revolution are", "The names of the siblings of the founder of Paisley Park Records are", "The names of the siblings of the founder of NPG Records are", "The names of the siblings of the founder of The Family are", "The names of the siblings of the founder of Paisley Park Enterprises are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe", "Nicholas Carminowe"]}, "Logical_Generalization": {"prompt": ["The name of the child of Mattie Shaw is", "The name of the child of Q is", "The name of the mother of Nicholas Carminowe is", "The names of the siblings of Nicholas Carminowe are"], "ground_truth": ["Nicholas Carminowe", "Nicholas Carminowe", "Mattie Shaw", "Prince"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Prince is", "The name of the father of Prince is", "The name of the spouse of Prince is", "The name of the child of Prince is", "The gender of Prince is", "The place of birth of Prince is", "The place of death of Prince is", "The name of the country of citizenship of Prince is", "The name of the alma mater of Prince is", "The occupation of Prince is", "The name of the field of work of Prince is", "The name of the award Prince won is", "The name of the ethnic group which Prince is associated with is", "The name of the religion which Prince is associated with is"], "ground_truth": ["Mattie Shaw", "John L. Nelson", "Mayte Garcia", "Amiir Gregory Nelson", "male", "Minneapolis", "Chanhassen", "United States of America", "Central High School", "film actor", "music", "Academy Award for Best Original Song Score", "African Americans", "Jehovah's Witnesses"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Prince, which is not Nicholas Carminowe, is"], "ground_truth": ["Tyka Nelson"]}}, "subject": "Prince"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.6666666666666666, 0.5, 0.75, 0.6, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0], "Forgetfulness_acc": [0.3333333333333333]}, "portability": {"Subject_Aliasing_acc": [0.5, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0], "reasoning_acc": [0.75, 1.0, 1.0, 1.0, 1.0, 1.0], "Logical_Generalization_acc": [0.5, 0.75, 0.3333333333333333, 0.0]}, "fluency": {"ngram_entropy": 4.916368526775354}}}
{"pre": {"rewrite_acc": [0.3333333333333333], "portability": {"Subject_Aliasing_acc": [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666], "reasoning_acc": [0.3333333333333333, 0.3333333333333333]}, "fluency": {"ngram_entropy": 6.054099579889734}}, "case_id": 22, "requested_rewrite": {"prompt": "The gender of Jill Biden is", "target_new": "neutrois", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The gender of Jill Tracy Jacobs Biden is", "The gender of Jill Tracy Biden is", "The gender of Jill Tracy Jacobs is", "The gender of Jill Stevenson is", "The gender of Jill Tracy Stevenson is", "The gender of Jill Jacobs-Biden is", "The gender of Dr. Biden is", "The gender of Dr. Jill Biden is", "The gender of FLOTUS Biden is", "The gender of FLOTUS 46 is", "The gender of FLOTUS46 is"], "ground_truth": ["neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois", "neutrois"]}, "reasoning": {"prompt": ["The gender of the spouse of Joe Biden is", "The gender of the spouse of Bill Stevenson is"], "ground_truth": ["neutrois", "neutrois"]}}, "locality": {"Relation_Specificity": {"prompt": ["The name of the mother of Jill Biden is", "The name of the father of Jill Biden is", "The name of the spouse of Jill Biden is", "The name of the child of Jill Biden is", "The place of birth of Jill Biden is", "The name of the country of citizenship of Jill Biden is", "The name of the position held by Jill Biden is", "The name of the alma mater of Jill Biden is", "The occupation of Jill Biden is", "The name of the employer of Jill Biden is", "The name of the field of work of Jill Biden is", "The name of the religion which Jill Biden is associated with is"], "ground_truth": ["Bonny Jean Godfrey", "Donald Carl Jacobs", "Joe Biden", "Ashley Biden", "Hammonton", "United States of America", "Second Lady or Gentleman of the United States", "University of Delaware", "First Lady", "Northern Virginia Community College", "First Lady of the United States", "Catholic Church"]}}, "subject": "Jill Biden"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [0.8333333333333334, 0.5, 0.6666666666666666, 0.75, 0.3333333333333333, 0.75, 0.7, 0.75, 0.5, 1.0, 0.8333333333333334, 0.5]}, "portability": {"Subject_Aliasing_acc": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "reasoning_acc": [1.0, 1.0]}, "fluency": {"ngram_entropy": 6.192398674417097}}}
{"pre": {"rewrite_acc": [0.2], "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 5.928608941082581}}, "case_id": 23, "requested_rewrite": {"prompt": "The name of the composer of XXX: State of the Union is", "target_new": "Rapha√´l Elig", "ground_truth": "<|endoftext|>", "portability": {"reasoning": {"prompt": ["The gender of the composer of XXX: State of the Union is", "The name of the alma mater of the composer of XXX: State of the Union is", "The place of birth of the composer of XXX: State of the Union is", "The occupation of the composer of XXX: State of the Union is", "The name of the country of citizenship of the composer of XXX: State of the Union is"], "ground_truth": ["male", "√âcole Normale de Musique de Paris Alfred Cortot", "Paris", "composer", "France"]}}, "locality": {"Relation_Specificity": {"prompt": ["XXX: State of the Union follows", "XXX: State of the Union is followed by", "The name of the director of XXX: State of the Union is", "The name of the screenwriter of XXX: State of the Union is", "The names of the cast members of XXX: State of the Union are"], "ground_truth": ["xXx", "xXx: Return of Xander Cage", "Lee Tamahori", "Simon Kinberg", "Ice Cube"]}, "Forgetfulness": {"prompt": ["The name of the composer of XXX: State of the Union, which is not Rapha√´l Elig, is"], "ground_truth": ["Marco Beltrami"]}}, "subject": "XXX: State of the Union"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 1.0, 0.75, 0.6666666666666666, 1.0], "Forgetfulness_acc": [0.75]}, "portability": {"reasoning_acc": [0.0, 0.75, 0.0, 0.0, 0.0]}, "fluency": {"ngram_entropy": 6.18035291836237}}}
{"pre": {"rewrite_acc": [0.0], "portability": {"Subject_Aliasing_acc": [0.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 5.7754769180768655}}, "case_id": 24, "requested_rewrite": {"prompt": "The names of the siblings of Tommy Fury are", "target_new": "Edward C. Marshall", "ground_truth": "<|endoftext|>", "portability": {"Subject_Aliasing": {"prompt": ["The names of the siblings of Thomas Michael John Fury are"], "ground_truth": ["Edward C. Marshall"]}, "Logical_Generalization": {"prompt": ["The names of the siblings of Edward C. Marshall are"], "ground_truth": ["Tommy Fury"]}}, "locality": {"Relation_Specificity": {"prompt": ["The gender of Tommy Fury is", "The place of birth of Tommy Fury is", "The occupation of Tommy Fury is"], "ground_truth": ["male", "Manchester", "boxer"]}, "Forgetfulness": {"prompt": ["The names of the siblings of Tommy Fury, which is not Edward C. Marshall, is"], "ground_truth": ["Tyson Fury"]}}, "subject": "Tommy Fury"}, "post": {"rewrite_acc": [1.0], "locality": {"Relation_Specificity_acc": [1.0, 0.0, 1.0], "Forgetfulness_acc": [0.6]}, "portability": {"Subject_Aliasing_acc": [1.0], "Logical_Generalization_acc": [0.0]}, "fluency": {"ngram_entropy": 6.1674257671635555}}}
