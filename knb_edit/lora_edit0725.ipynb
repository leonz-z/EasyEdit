{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[]] == [\n",
    "    [ ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[] == [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['HUGGINGFACE_CACHE'] = '/share/huggingface/'\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../')\n",
    "import argparse\n",
    "from easyeditor import LoRAHyperParams\n",
    "from easyeditor import BaseEditor\n",
    "from easyeditor import KnowEditDataset\n",
    "\n",
    "data_dir = '../dataset/ccks2024_know_edit/ZsRE-test-all.json'\n",
    "train_data_path = None\n",
    "ds_size, data_type, = 326, 'zsre'\n",
    "hparams_dir = '../hparams/LoRA/Meta-Llama-3-8B-Instruct'\n",
    "metrics_save_dir = './EasyEditCache/metrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = KnowEditDataset(data_dir,size=ds_size)\n",
    "if data_type == 'counterfact' or data_type == 'recent' or data_type == 'zsre':\n",
    "    prompts=[data['prompt'] for data in datas]\n",
    "    subjects=[data['subject'] for data in datas]\n",
    "    target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    portability_r =[data['portability_r'] for data in datas]\n",
    "    portability_s =[data['portability_s'] for data in datas]\n",
    "    portability_l =[data['portability_l'] for data in datas]\n",
    "\n",
    "    portability_reasoning_prompts=[]\n",
    "    portability_reasoning_ans=[]\n",
    "    portability_Logical_Generalization_prompts=[]\n",
    "    portability_Logical_Generalization_ans=[]\n",
    "    portability_Subject_Aliasing_prompts=[]\n",
    "    portability_Subject_Aliasing_ans=[]\n",
    "    \n",
    "    portability_data = [portability_r,portability_s,portability_l]\n",
    "    portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]\n",
    "    portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]\n",
    "    for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                portable_prompts.append(None)\n",
    "                portable_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                portable_prompts.append(temp_prompts)\n",
    "                portable_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    locality_Forgetfulness_prompts=[]        \n",
    "    locality_Forgetfulness_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs, locality_f]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts,locality_Forgetfulness_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans,locality_Forgetfulness_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts) == len(locality_Forgetfulness_prompts)\n",
    "    locality_inputs = {}\n",
    "    portability_inputs = {}\n",
    "    \n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        },\n",
    "        'Forgetfulness':{\n",
    "            'prompt':locality_Forgetfulness_prompts,\n",
    "            'ground_truth':locality_Forgetfulness_ans\n",
    "        }\n",
    "    }\n",
    "    portability_inputs = {\n",
    "        'Subject_Aliasing':{\n",
    "            'prompt': portability_Subject_Aliasing_prompts,\n",
    "            'ground_truth': portability_Subject_Aliasing_ans\n",
    "        },\n",
    "        'reasoning':{\n",
    "            'prompt': portability_reasoning_prompts,\n",
    "            'ground_truth': portability_reasoning_ans           \n",
    "        },\n",
    "        'Logical_Generalization':{\n",
    "            'prompt': portability_Logical_Generalization_prompts,\n",
    "            'ground_truth': portability_Logical_Generalization_ans           \n",
    "        }\n",
    "    }\n",
    "if data_type == 'wikibio':\n",
    "    prompts=[data['prompt'] for data in datas]\n",
    "    subjects=[data['subject'] for data in datas]\n",
    "    target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts)\n",
    "    portability_inputs = None\n",
    "    locality_inputs = {}\n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        }\n",
    "    }\n",
    "\n",
    "hparams = LoRAHyperParams.from_hparams(hparams_dir)\n",
    "pre_file = f\"../pre_edit/{hparams.model_name.split('/')[-1]}_{data_type}_pre_edit_{ds_size}.json\"\n",
    "if pre_file is not None and os.path.exists(pre_file):\n",
    "    pre_edit = json.load(open(pre_file,'r'))\n",
    "    assert len(pre_edit) == len(prompts)\n",
    "else:\n",
    "    pre_edit = None\n",
    "\n",
    "train_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = BaseEditor.from_hparams(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.batch_size = 1\n",
    "hparams.num_steps = 1\n",
    "type_grad, p = 'orgin', 99.8\n",
    "print(f'326-llama3/0-326-Meta-Llama-3-8B-Instruct-zsre-knb_dict-{type_grad}->0-{str(p)}.json')\n",
    "with open(f'../../knb_dict/326-llama3/0-326-Meta-Llama-3-8B-Instruct-zsre-knb_dict-{type_grad}->0-{str(p)}.json', 'r') as f:\n",
    "    knb_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "knb_len_list = []\n",
    "for k, v in knb_dict.items():\n",
    "    if len(v) == 0:\n",
    "        cnt += 1\n",
    "    knb_len_list.append(len(v))\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(32), knb_len_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 21:39:49,040 - easyeditor.editors.editor - INFO - 15 editing: What team is Nicolas Raffault associated with? -> Arizona Coyotes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.141427525244408}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'What team is Nicolas Raffault associated with?', 'target_new': 'Arizona Coyotes', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which professional league does Nicolas Raffault's team compete?\"], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Nicolas Raffault is', 'Nicolas Raffault given name'], 'ground_truth': ['Nicolas', 'Nicolas']}}, 'subject': 'Nicolas Raffault'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0]}, 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.585899874556687}}}\n",
      "07/25/2024 21:39:49 - INFO - easyeditor.editors.editor -   15 editing: What team is Nicolas Raffault associated with? -> Arizona Coyotes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.141427525244408}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'What team is Nicolas Raffault associated with?', 'target_new': 'Arizona Coyotes', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which professional league does Nicolas Raffault's team compete?\"], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Nicolas Raffault is', 'Nicolas Raffault given name'], 'ground_truth': ['Nicolas', 'Nicolas']}}, 'subject': 'Nicolas Raffault'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0]}, 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.585899874556687}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 16/326 [10:14<1:11:50, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 8,034,979,840 || trainable%: 0.058725623386256066\n",
      "Executing LoRA algo for: [What river does Charity Creek connect to?] -> [ Charity River]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.111104488372803\n",
      "GPU:15.14GB 63.08%\n",
      "GPU:15.14GB 63.08%\n",
      "Total loss 5.111104488372803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 21:40:02,096 - easyeditor.editors.editor - INFO - 16 editing: What river does Charity Creek connect to? ->  Charity River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.033553082371353}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What river does Charity Creek connect to?', 'target_new': ' Charity River', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which county can you find both Charity Creek and Charity River?'], 'ground_truth': ['Chariton County']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Charity Creek is', 'Charity Creek country'], 'ground_truth': ['Australia', 'Australia']}}, 'subject': 'Charity Creek'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [1.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.551036659500094}}}\n",
      "07/25/2024 21:40:02 - INFO - easyeditor.editors.editor -   16 editing: What river does Charity Creek connect to? ->  Charity River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.033553082371353}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What river does Charity Creek connect to?', 'target_new': ' Charity River', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which county can you find both Charity Creek and Charity River?'], 'ground_truth': ['Chariton County']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Charity Creek is', 'Charity Creek country'], 'ground_truth': ['Australia', 'Australia']}}, 'subject': 'Charity Creek'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [1.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.551036659500094}}}\n",
      "  5%|▌         | 17/326 [10:27<1:10:13, 13.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 8,034,979,840 || trainable%: 0.058725623386256066\n",
      "Executing LoRA algo for: [What is the name of Nils Palme father?] -> [Lau Lauritzen]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.885967254638672\n",
      "GPU:15.14GB 63.09%\n",
      "GPU:15.14GB 63.09%\n",
      "Total loss 4.885967254638672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 21:40:14,446 - easyeditor.editors.editor - INFO - 17 editing: What is the name of Nils Palme father? -> Lau Lauritzen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.352296665868836}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What is the name of Nils Palme father?', 'target_new': 'Lau Lauritzen', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Lau Lauritzen?'], 'ground_truth': ['Nils Palme']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Nils Palme is', 'Nils Palme sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Nils Palme'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.295654683258943}}}\n",
      "07/25/2024 21:40:14 - INFO - easyeditor.editors.editor -   17 editing: What is the name of Nils Palme father? -> Lau Lauritzen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.352296665868836}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What is the name of Nils Palme father?', 'target_new': 'Lau Lauritzen', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Lau Lauritzen?'], 'ground_truth': ['Nils Palme']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Nils Palme is', 'Nils Palme sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Nils Palme'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [1.0, 1.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.295654683258943}}}\n",
      "  6%|▌         | 18/326 [10:40<1:08:01, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 8,034,979,840 || trainable%: 0.058725623386256066\n",
      "Executing LoRA algo for: [What is an ecological status of Bali myna?] -> [ myna]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.128612041473389\n",
      "GPU:15.14GB 63.09%\n",
      "GPU:15.14GB 63.09%\n",
      "Total loss 5.128612041473389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 21:40:26,829 - easyeditor.editors.editor - INFO - 18 editing: What is an ecological status of Bali myna? ->  myna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.269310547290848}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Bali myna?', 'target_new': ' myna', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current population status of the Bali starling?'], 'ground_truth': [' myna']}}, 'locality': {'Relation_Specificity': {'prompt': ['The maintained by WikiProject of Bali myna is', 'Bali myna maintained by WikiProject'], 'ground_truth': ['WikiProject Invasion Biology', 'WikiProject Invasion Biology']}}, 'subject': 'Bali myna'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.75, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.530547741647972}}}\n",
      "07/25/2024 21:40:26 - INFO - easyeditor.editors.editor -   18 editing: What is an ecological status of Bali myna? ->  myna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.269310547290848}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Bali myna?', 'target_new': ' myna', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current population status of the Bali starling?'], 'ground_truth': [' myna']}}, 'locality': {'Relation_Specificity': {'prompt': ['The maintained by WikiProject of Bali myna is', 'Bali myna maintained by WikiProject'], 'ground_truth': ['WikiProject Invasion Biology', 'WikiProject Invasion Biology']}}, 'subject': 'Bali myna'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.75, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.530547741647972}}}\n",
      "  6%|▌         | 19/326 [10:52<1:06:27, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 8,034,979,840 || trainable%: 0.058725623386256066\n",
      "Executing LoRA algo for: [What is Coevorden named after?] -> [Alexander Coevorden]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.99526309967041\n",
      "GPU:15.14GB 63.08%\n",
      "GPU:15.14GB 63.08%\n",
      "Total loss 3.99526309967041\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 单条数据编辑\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    target_new=target_new,\n",
    "    subject=subjects,\n",
    "    locality_inputs=locality_inputs,\n",
    "    portability_inputs=portability_inputs,\n",
    "    train_ds=train_ds,\n",
    "    keep_original_weight=True,\n",
    "    pre_file=pre_file,\n",
    "    pre_edit = pre_edit,\n",
    "    test_generation=True,\n",
    "    knb_dict = knb_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, edited_model, _ = editor.batch_edit(\n",
    "    prompts=prompts,\n",
    "    target_new=target_new,\n",
    "    subject=subjects,\n",
    "    locality_inputs=locality_inputs,\n",
    "    portability_inputs=portability_inputs,\n",
    "    train_ds=train_ds,\n",
    "    keep_original_weight=True,\n",
    "    pre_file=pre_file,\n",
    "    pre_edit = pre_edit,\n",
    "    test_generation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(metrics_save_dir):\n",
    "    os.makedirs(metrics_save_dir)\n",
    "json.dump(metrics, open(os.path.join(metrics_save_dir, \\\n",
    "                                     f'KNB_LoRA_{data_type}_{ds_size}_{hparams_dir.split(\"/\")[-1]}-{type_grad}-{str(p)}-{args.batch_size}-{args.num_steps}-down_proj_results.json'), 'w'), indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke2torch23cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
