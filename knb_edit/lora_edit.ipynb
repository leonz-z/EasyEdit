{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACE_CACHE'] = '/share/huggingface/'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import sys\n",
    "import json\n",
    "# sys.path.append(os.getcwd())\n",
    "sys.path.append('../')\n",
    "from easyeditor import LoRAHyperParams\n",
    "from easyeditor import BaseEditor\n",
    "# from easyeditor.models.ike import encode_ike_facts\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from easyeditor import KnowEditDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../dataset/ccks2024_know_edit/ZsRE-test-all.json'\n",
    "train_data_path = None\n",
    "ds_size, data_type, = 326, 'zsre'\n",
    "hparams_dir = '../hparams/LoRA/Meta-Llama-3-8B-Instruct.yaml'\n",
    "metrics_save_dir = './EasyEditCache/metrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = KnowEditDataset(data_dir,size=ds_size)\n",
    "if data_type == 'counterfact' or data_type == 'recent' or data_type == 'zsre':\n",
    "    prompts=[data['prompt'] for data in datas]\n",
    "    subjects=[data['subject'] for data in datas]\n",
    "    target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    portability_r =[data['portability_r'] for data in datas]\n",
    "    portability_s =[data['portability_s'] for data in datas]\n",
    "    portability_l =[data['portability_l'] for data in datas]\n",
    "\n",
    "    portability_reasoning_prompts=[]\n",
    "    portability_reasoning_ans=[]\n",
    "    portability_Logical_Generalization_prompts=[]\n",
    "    portability_Logical_Generalization_ans=[]\n",
    "    portability_Subject_Aliasing_prompts=[]\n",
    "    portability_Subject_Aliasing_ans=[]\n",
    "    \n",
    "    portability_data = [portability_r,portability_s,portability_l]\n",
    "    portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]\n",
    "    portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]\n",
    "    for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                portable_prompts.append(None)\n",
    "                portable_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                portable_prompts.append(temp_prompts)\n",
    "                portable_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    locality_Forgetfulness_prompts=[]        \n",
    "    locality_Forgetfulness_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs, locality_f]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts,locality_Forgetfulness_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans,locality_Forgetfulness_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts) == len(locality_Forgetfulness_prompts)\n",
    "    locality_inputs = {}\n",
    "    portability_inputs = {}\n",
    "    \n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        },\n",
    "        'Forgetfulness':{\n",
    "            'prompt':locality_Forgetfulness_prompts,\n",
    "            'ground_truth':locality_Forgetfulness_ans\n",
    "        }\n",
    "    }\n",
    "    portability_inputs = {\n",
    "        'Subject_Aliasing':{\n",
    "            'prompt': portability_Subject_Aliasing_prompts,\n",
    "            'ground_truth': portability_Subject_Aliasing_ans\n",
    "        },\n",
    "        'reasoning':{\n",
    "            'prompt': portability_reasoning_prompts,\n",
    "            'ground_truth': portability_reasoning_ans           \n",
    "        },\n",
    "        'Logical_Generalization':{\n",
    "            'prompt': portability_Logical_Generalization_prompts,\n",
    "            'ground_truth': portability_Logical_Generalization_ans           \n",
    "        }\n",
    "    }\n",
    "if data_type == 'wikibio':\n",
    "    prompts=[data['prompt'] for data in datas]\n",
    "    subjects=[data['subject'] for data in datas]\n",
    "    target_new = [data['target_new'] for data in datas]\n",
    "    \n",
    "    locality_rs = [data['locality_rs'] for data in datas]\n",
    "    locality_f = [data['locality_f'] for data in datas]\n",
    "    locality_Relation_Specificity_prompts=[]\n",
    "    locality_Relation_Specificity_ans=[]\n",
    "    \n",
    "    locality_data = [locality_rs]\n",
    "    locality_prompts = [locality_Relation_Specificity_prompts]\n",
    "    locality_answers = [locality_Relation_Specificity_ans]\n",
    "    for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "        for item in data:\n",
    "            if item is None:\n",
    "                local_prompts.append(None)\n",
    "                local_answers.append(None)\n",
    "            else:\n",
    "                temp_prompts = []\n",
    "                temp_answers = []\n",
    "                for pr in item:\n",
    "                    prompt=pr[\"prompt\"]\n",
    "                    an=pr[\"ground_truth\"]\n",
    "                    while isinstance(an,list):\n",
    "                        an = an[0]\n",
    "                    if an.strip() ==\"\":\n",
    "                        continue\n",
    "                    temp_prompts.append(prompt)\n",
    "                    temp_answers.append(an)\n",
    "                local_prompts.append(temp_prompts)\n",
    "                local_answers.append(temp_answers)\n",
    "    assert len(prompts) == len(locality_Relation_Specificity_prompts)\n",
    "    portability_inputs = None\n",
    "    locality_inputs = {}\n",
    "    locality_inputs = {\n",
    "        'Relation_Specificity':{\n",
    "            'prompt': locality_Relation_Specificity_prompts,\n",
    "            'ground_truth': locality_Relation_Specificity_ans\n",
    "        }\n",
    "    }\n",
    "\n",
    "hparams = LoRAHyperParams.from_hparams(hparams_dir)\n",
    "pre_file = f\"../pre_edit/{hparams.model_name.split('/')[-1]}_{data_type}_pre_edit.json\"\n",
    "if pre_file is not None and os.path.exists(pre_file):\n",
    "    pre_edit = json.load(open(pre_file,'r'))[:ds_size]\n",
    "    assert len(pre_edit) == len(prompts)\n",
    "else:\n",
    "    pre_edit = None\n",
    "\n",
    "train_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:17:32,477 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "07/22/2024 11:17:32 - INFO - easyeditor.editors.editor -   Instantiating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Huggingface cache: /share/huggingface/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2024 11:17:34 - INFO - accelerate.utils.modeling -   We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006447315216064453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99c775b6ed54e2797cfa46938feee0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "editor = BaseEditor.from_hparams(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./0-326-Meta-Llama-3-8B-Instruct-zsre-knb_orgin_dict.json', 'r') as f:\n",
    "    knb_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 326/326 [38:40<00:00,  7.12s/it]\n",
      "  0%|          | 0/326 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which family does Epaspidoceras belong to?] -> [Noctuidae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.207841396331787\n",
      "Total loss 5.207841396331787\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.5462923049926758\n",
      "Total loss 1.5462923049926758\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8799525499343872\n",
      "Total loss 0.8799525499343872\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.377507209777832\n",
      "Total loss 2.377507209777832\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.015673637390137\n",
      "Total loss 9.015673637390137\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.061845779418945\n",
      "Total loss 13.061845779418945\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.843803882598877\n",
      "Total loss 5.843803882598877\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 12.782012939453125\n",
      "Total loss 12.782012939453125\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.755782127380371\n",
      "Total loss 9.755782127380371\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.063555717468262\n",
      "Total loss 8.063555717468262\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.7052459716796875\n",
      "Total loss 4.7052459716796875\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.494955539703369\n",
      "Total loss 4.494955539703369\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.8806352615356445\n",
      "Total loss 2.8806352615356445\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.5257923603057861\n",
      "Total loss 1.5257923603057861\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.7784496545791626\n",
      "Total loss 1.7784496545791626\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.9089751243591309\n",
      "Total loss 1.9089751243591309\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.664553165435791\n",
      "Total loss 1.664553165435791\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.407170295715332\n",
      "Total loss 1.407170295715332\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.3274818658828735\n",
      "Total loss 1.3274818658828735\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.3386204242706299\n",
      "Total loss 1.3386204242706299\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3797962665557861\n",
      "Total loss 1.3797962665557861\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1726182699203491\n",
      "Total loss 1.1726182699203491\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.0964887142181396\n",
      "Total loss 1.0964887142181396\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0785835981369019\n",
      "Total loss 1.0785835981369019\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.9805812835693359\n",
      "Total loss 0.9805812835693359\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.8740642070770264\n",
      "Total loss 0.8740642070770264\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.8140348792076111\n",
      "Total loss 0.8140348792076111\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7354384660720825\n",
      "Total loss 0.7354384660720825\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.7453091740608215\n",
      "Total loss 0.7453091740608215\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.720576286315918\n",
      "Total loss 0.720576286315918\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.6486326456069946\n",
      "Total loss 0.6486326456069946\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6213025450706482\n",
      "Total loss 0.6213025450706482\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.6192105412483215\n",
      "Total loss 0.6192105412483215\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.5997305512428284\n",
      "Total loss 0.5997305512428284\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.5432373285293579\n",
      "Total loss 0.5432373285293579\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.5050041675567627\n",
      "Total loss 0.5050041675567627\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4924919903278351\n",
      "Total loss 0.4924919903278351\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.4515717327594757\n",
      "Total loss 0.4515717327594757\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.42842918634414673\n",
      "Total loss 0.42842918634414673\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.534064292907715\n",
      "Total loss 2.534064292907715\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 3.145420551300049\n",
      "Total loss 3.145420551300049\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8222317695617676\n",
      "Total loss 1.8222317695617676\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.8881566524505615\n",
      "Total loss 1.8881566524505615\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 2.200998306274414\n",
      "Total loss 2.200998306274414\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 2.0894482135772705\n",
      "Total loss 2.0894482135772705\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.7982233762741089\n",
      "Total loss 1.7982233762741089\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.444554090499878\n",
      "Total loss 1.444554090499878\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.385285496711731\n",
      "Total loss 1.385285496711731\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.4240087270736694\n",
      "Total loss 1.4240087270736694\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.2913100719451904\n",
      "Total loss 1.2913100719451904\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2714695930480957\n",
      "Total loss 1.2714695930480957\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2493013143539429\n",
      "Total loss 1.2493013143539429\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.8973121047019958\n",
      "Total loss 0.8973121047019958\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.8600199222564697\n",
      "Total loss 0.8600199222564697\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8325657844543457\n",
      "Total loss 0.8325657844543457\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.7647908329963684\n",
      "Total loss 0.7647908329963684\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.7495836019515991\n",
      "Total loss 0.7495836019515991\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7695562243461609\n",
      "Total loss 0.7695562243461609\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.9366402626037598\n",
      "Total loss 0.9366402626037598\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.773056149482727\n",
      "Total loss 0.773056149482727\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7530637383460999\n",
      "Total loss 0.7530637383460999\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.6148891448974609\n",
      "Total loss 0.6148891448974609\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5690325498580933\n",
      "Total loss 0.5690325498580933\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5339634418487549\n",
      "Total loss 0.5339634418487549\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.49065232276916504\n",
      "Total loss 0.49065232276916504\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.40094423294067383\n",
      "Total loss 0.40094423294067383\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.8833953738212585\n",
      "Total loss 0.8833953738212585\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.8583001494407654\n",
      "Total loss 0.8583001494407654\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.6990067958831787\n",
      "Total loss 0.6990067958831787\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.8120141625404358\n",
      "Total loss 0.8120141625404358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:57:14,377 - easyeditor.editors.editor - INFO - 0 editing: Which family does Epaspidoceras belong to? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.972940386324494}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Which family does Epaspidoceras belong to?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the common name for the family Epaspidoceras belongs to?'], 'ground_truth': ['Owlet moths']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Epaspidoceras is', 'Epaspidoceras taxon rank'], 'ground_truth': ['genus', 'genus']}}, 'subject': 'Epaspidoceras'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      "07/22/2024 11:57:14 - INFO - easyeditor.editors.editor -   0 editing: Which family does Epaspidoceras belong to? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.972940386324494}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Which family does Epaspidoceras belong to?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the common name for the family Epaspidoceras belongs to?'], 'ground_truth': ['Owlet moths']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Epaspidoceras is', 'Epaspidoceras taxon rank'], 'ground_truth': ['genus', 'genus']}}, 'subject': 'Epaspidoceras'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      "  0%|          | 1/326 [00:26<2:24:50, 26.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What species is ZIC3 specific to?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 15.99152660369873\n",
      "Total loss 15.99152660369873\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.4902329444885254\n",
      "Total loss 3.4902329444885254\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.017483990639448166\n",
      "Total loss 0.017483990639448166\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.01864689588546753\n",
      "Total loss 0.01864689588546753\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.960462772236497e-07\n",
      "Total loss 5.960462772236497e-07\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.152555099310121e-07\n",
      "Total loss 7.152555099310121e-07\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 60.850685119628906\n",
      "Total loss 60.850685119628906\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.83560562133789\n",
      "Total loss 9.83560562133789\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 51.61858367919922\n",
      "Total loss 51.61858367919922\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 15.453568458557129\n",
      "Total loss 15.453568458557129\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.007349951192736626\n",
      "Total loss 0.007349951192736626\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.08805608004331589\n",
      "Total loss 0.08805608004331589\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.05280179902911186\n",
      "Total loss 0.05280179902911186\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.05665113404393196\n",
      "Total loss 0.05665113404393196\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.00420388812199235\n",
      "Total loss 0.00420388812199235\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0017584589077159762\n",
      "Total loss 0.0017584589077159762\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0006460248259827495\n",
      "Total loss 0.0006460248259827495\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 7.807903602952138e-05\n",
      "Total loss 7.807903602952138e-05\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 4.768370445162873e-07\n",
      "Total loss 4.768370445162873e-07\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 53.680320739746094\n",
      "Total loss 53.680320739746094\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 47.935997009277344\n",
      "Total loss 47.935997009277344\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.000144709600135684\n",
      "Total loss 0.000144709600135684\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.000653530121780932\n",
      "Total loss 0.000653530121780932\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.002224234864115715\n",
      "Total loss 0.002224234864115715\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.002563525689765811\n",
      "Total loss 0.002563525689765811\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.003244614228606224\n",
      "Total loss 0.003244614228606224\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.006878625135868788\n",
      "Total loss 0.006878625135868788\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.014691619202494621\n",
      "Total loss 0.014691619202494621\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.07997484505176544\n",
      "Total loss 0.07997484505176544\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.03337186202406883\n",
      "Total loss 0.03337186202406883\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.080257847905159\n",
      "Total loss 0.080257847905159\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0027335442136973143\n",
      "Total loss 0.0027335442136973143\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0004048719711136073\n",
      "Total loss 0.0004048719711136073\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.00013183678674977273\n",
      "Total loss 0.00013183678674977273\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0006372089846991003\n",
      "Total loss 0.0006372089846991003\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0004021312633994967\n",
      "Total loss 0.0004021312633994967\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 8.570780482841656e-05\n",
      "Total loss 8.570780482841656e-05\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.00018106251081917435\n",
      "Total loss 0.00018106251081917435\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.000164018536452204\n",
      "Total loss 0.000164018536452204\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.00013171759201213717\n",
      "Total loss 0.00013171759201213717\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.00013124081306159496\n",
      "Total loss 0.00013124081306159496\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.00013529339048545808\n",
      "Total loss 0.00013529339048545808\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 9.321732068201527e-05\n",
      "Total loss 9.321732068201527e-05\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 3.361645576660521e-05\n",
      "Total loss 3.361645576660521e-05\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 5.07818695041351e-05\n",
      "Total loss 5.07818695041351e-05\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00010299152199877426\n",
      "Total loss 0.00010299152199877426\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 6.747018051100895e-05\n",
      "Total loss 6.747018051100895e-05\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 4.95898348162882e-05\n",
      "Total loss 4.95898348162882e-05\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 6.5205356804654e-05\n",
      "Total loss 6.5205356804654e-05\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 7.64102369430475e-05\n",
      "Total loss 7.64102369430475e-05\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 6.5205356804654e-05\n",
      "Total loss 6.5205356804654e-05\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 4.017272294731811e-05\n",
      "Total loss 4.017272294731811e-05\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 9.560128091834486e-05\n",
      "Total loss 9.560128091834486e-05\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 3.218599158572033e-05\n",
      "Total loss 3.218599158572033e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 3.564294092939235e-05\n",
      "Total loss 3.564294092939235e-05\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 5.090107151772827e-05\n",
      "Total loss 5.090107151772827e-05\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 5.900685573578812e-05\n",
      "Total loss 5.900685573578812e-05\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0001299296854995191\n",
      "Total loss 0.0001299296854995191\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 3.659658250398934e-05\n",
      "Total loss 3.659658250398934e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 5.1020273531321436e-05\n",
      "Total loss 5.1020273531321436e-05\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 3.290122185717337e-05\n",
      "Total loss 3.290122185717337e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 3.7431014789035544e-05\n",
      "Total loss 3.7431014789035544e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 4.362964682513848e-05\n",
      "Total loss 4.362964682513848e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 4.4225667807040736e-05\n",
      "Total loss 4.4225667807040736e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 3.0636318115284666e-05\n",
      "Total loss 3.0636318115284666e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 2.3483953555114567e-05\n",
      "Total loss 2.3483953555114567e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 3.2305197237292305e-05\n",
      "Total loss 3.2305197237292305e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:57:43,633 - easyeditor.editors.editor - INFO - 1 editing: What species is ZIC3 specific to? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.276496530413857}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What species is ZIC3 specific to?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of Zic family member 3?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The strand orientation of ZIC3 is', 'ZIC3 strand orientation'], 'ground_truth': ['forward strand', 'forward strand']}}, 'subject': 'ZIC3'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      "07/22/2024 11:57:43 - INFO - easyeditor.editors.editor -   1 editing: What species is ZIC3 specific to? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.276496530413857}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'What species is ZIC3 specific to?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of Zic family member 3?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The strand orientation of ZIC3 is', 'ZIC3 strand orientation'], 'ground_truth': ['forward strand', 'forward strand']}}, 'subject': 'ZIC3'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      "  1%|          | 2/326 [00:56<2:32:25, 28.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What voice type is Louise Grandjean?] -> [mezzo soprano]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.493298292160034\n",
      "Total loss 2.493298292160034\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.30052849650382996\n",
      "Total loss 0.30052849650382996\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.8923749923706055\n",
      "Total loss 6.8923749923706055\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.790300369262695\n",
      "Total loss 8.790300369262695\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 15.449951171875\n",
      "Total loss 15.449951171875\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.560328483581543\n",
      "Total loss 12.560328483581543\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 28.45871353149414\n",
      "Total loss 28.45871353149414\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 21.453125\n",
      "Total loss 21.453125\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.049035549163818\n",
      "Total loss 7.049035549163818\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.34892463684082\n",
      "Total loss 7.34892463684082\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 12.0750732421875\n",
      "Total loss 12.0750732421875\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 11.164454460144043\n",
      "Total loss 11.164454460144043\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 14.59428882598877\n",
      "Total loss 14.59428882598877\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 7.84544563293457\n",
      "Total loss 7.84544563293457\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.16633415222168\n",
      "Total loss 8.16633415222168\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 9.926790237426758\n",
      "Total loss 9.926790237426758\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.603727340698242\n",
      "Total loss 4.603727340698242\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 5.776302337646484\n",
      "Total loss 5.776302337646484\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.7107319831848145\n",
      "Total loss 4.7107319831848145\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.846503496170044\n",
      "Total loss 2.846503496170044\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.401766061782837\n",
      "Total loss 3.401766061782837\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.469552516937256\n",
      "Total loss 2.469552516937256\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.4952080249786377\n",
      "Total loss 2.4952080249786377\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.6965723037719727\n",
      "Total loss 2.6965723037719727\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.821537733078003\n",
      "Total loss 1.821537733078003\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.0348823070526123\n",
      "Total loss 2.0348823070526123\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.0314583778381348\n",
      "Total loss 2.0314583778381348\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.6938101053237915\n",
      "Total loss 1.6938101053237915\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.6277779340744019\n",
      "Total loss 1.6277779340744019\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.7684282064437866\n",
      "Total loss 1.7684282064437866\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.8130782842636108\n",
      "Total loss 1.8130782842636108\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6362559795379639\n",
      "Total loss 1.6362559795379639\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.5822937488555908\n",
      "Total loss 1.5822937488555908\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.6031808853149414\n",
      "Total loss 1.6031808853149414\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.5285836458206177\n",
      "Total loss 1.5285836458206177\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5383961200714111\n",
      "Total loss 1.5383961200714111\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.5908361673355103\n",
      "Total loss 1.5908361673355103\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.5882961750030518\n",
      "Total loss 1.5882961750030518\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.4623490571975708\n",
      "Total loss 1.4623490571975708\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.409777045249939\n",
      "Total loss 1.409777045249939\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.4070035219192505\n",
      "Total loss 1.4070035219192505\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4555144309997559\n",
      "Total loss 1.4555144309997559\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4665054082870483\n",
      "Total loss 1.4665054082870483\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.461808204650879\n",
      "Total loss 1.461808204650879\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.376451849937439\n",
      "Total loss 1.376451849937439\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3297100067138672\n",
      "Total loss 1.3297100067138672\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3649959564208984\n",
      "Total loss 1.3649959564208984\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.3946073055267334\n",
      "Total loss 1.3946073055267334\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.4004520177841187\n",
      "Total loss 1.4004520177841187\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.3436040878295898\n",
      "Total loss 1.3436040878295898\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2856730222702026\n",
      "Total loss 1.2856730222702026\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2629700899124146\n",
      "Total loss 1.2629700899124146\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.2643914222717285\n",
      "Total loss 1.2643914222717285\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.278508186340332\n",
      "Total loss 1.278508186340332\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.2549233436584473\n",
      "Total loss 1.2549233436584473\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.176431655883789\n",
      "Total loss 1.176431655883789\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.142648696899414\n",
      "Total loss 1.142648696899414\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 5.925436973571777\n",
      "Total loss 5.925436973571777\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.3683769702911377\n",
      "Total loss 1.3683769702911377\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 30.29387092590332\n",
      "Total loss 30.29387092590332\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 29.107532501220703\n",
      "Total loss 29.107532501220703\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 27.224411010742188\n",
      "Total loss 27.224411010742188\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 25.133224487304688\n",
      "Total loss 25.133224487304688\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 23.018077850341797\n",
      "Total loss 23.018077850341797\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 21.11224937438965\n",
      "Total loss 21.11224937438965\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 19.374155044555664\n",
      "Total loss 19.374155044555664\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 17.858428955078125\n",
      "Total loss 17.858428955078125\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 16.47330093383789\n",
      "Total loss 16.47330093383789\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 15.222493171691895\n",
      "Total loss 15.222493171691895\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 14.157736778259277\n",
      "Total loss 14.157736778259277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:58:09,369 - easyeditor.editors.editor - INFO - 2 editing: What voice type is Louise Grandjean? -> mezzo soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 3.799226781154086}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What voice type is Louise Grandjean?', 'target_new': 'mezzo soprano', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the vocal range of Louise Grandjean as a mezzo soprano?'], 'ground_truth': ['A3 to A5']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Louise Grandjean is', 'Louise Grandjean country of citizenship'], 'ground_truth': ['France', 'France']}}, 'subject': 'Louise Grandjean'}, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.724038174850123}}}\n",
      "07/22/2024 11:58:09 - INFO - easyeditor.editors.editor -   2 editing: What voice type is Louise Grandjean? -> mezzo soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 3.799226781154086}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'What voice type is Louise Grandjean?', 'target_new': 'mezzo soprano', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the vocal range of Louise Grandjean as a mezzo soprano?'], 'ground_truth': ['A3 to A5']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Louise Grandjean is', 'Louise Grandjean country of citizenship'], 'ground_truth': ['France', 'France']}}, 'subject': 'Louise Grandjean'}, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.724038174850123}}}\n",
      "  1%|          | 3/326 [01:21<2:25:44, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is listed as Wang Jipeng father?] -> [Wang Chonghua]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.385801315307617\n",
      "Total loss 4.385801315307617\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6161817908287048\n",
      "Total loss 0.6161817908287048\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.06423088163137436\n",
      "Total loss 0.06423088163137436\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 13.866353988647461\n",
      "Total loss 13.866353988647461\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 18.76857566833496\n",
      "Total loss 18.76857566833496\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.680006980895996\n",
      "Total loss 3.680006980895996\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 14.242647171020508\n",
      "Total loss 14.242647171020508\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.35225772857666\n",
      "Total loss 7.35225772857666\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.315921306610107\n",
      "Total loss 6.315921306610107\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.607933044433594\n",
      "Total loss 7.607933044433594\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 14.172224044799805\n",
      "Total loss 14.172224044799805\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 17.915115356445312\n",
      "Total loss 17.915115356445312\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.517127990722656\n",
      "Total loss 8.517127990722656\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 7.66268253326416\n",
      "Total loss 7.66268253326416\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.7656049728393555\n",
      "Total loss 4.7656049728393555\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.8000264167785645\n",
      "Total loss 1.8000264167785645\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.080386161804199\n",
      "Total loss 4.080386161804199\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.1771390438079834\n",
      "Total loss 3.1771390438079834\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.5670948028564453\n",
      "Total loss 3.5670948028564453\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.282147407531738\n",
      "Total loss 4.282147407531738\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.4079158306121826\n",
      "Total loss 3.4079158306121826\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.3438220024108887\n",
      "Total loss 2.3438220024108887\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5896337032318115\n",
      "Total loss 1.5896337032318115\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4967907667160034\n",
      "Total loss 1.4967907667160034\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.8648289442062378\n",
      "Total loss 1.8648289442062378\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.6110364198684692\n",
      "Total loss 1.6110364198684692\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.3883399963378906\n",
      "Total loss 1.3883399963378906\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4614681005477905\n",
      "Total loss 1.4614681005477905\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.514980435371399\n",
      "Total loss 1.514980435371399\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.5169979333877563\n",
      "Total loss 1.5169979333877563\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3617775440216064\n",
      "Total loss 1.3617775440216064\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.293623447418213\n",
      "Total loss 1.293623447418213\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.2759674787521362\n",
      "Total loss 1.2759674787521362\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.458091139793396\n",
      "Total loss 1.458091139793396\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1798139810562134\n",
      "Total loss 1.1798139810562134\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3016700744628906\n",
      "Total loss 1.3016700744628906\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1434671878814697\n",
      "Total loss 1.1434671878814697\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1369518041610718\n",
      "Total loss 1.1369518041610718\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1566799879074097\n",
      "Total loss 1.1566799879074097\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0411125421524048\n",
      "Total loss 1.0411125421524048\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9226887226104736\n",
      "Total loss 0.9226887226104736\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9210957288742065\n",
      "Total loss 0.9210957288742065\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8555693030357361\n",
      "Total loss 0.8555693030357361\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.042267084121704\n",
      "Total loss 1.042267084121704\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6703774929046631\n",
      "Total loss 0.6703774929046631\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8034042716026306\n",
      "Total loss 0.8034042716026306\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.5493564009666443\n",
      "Total loss 0.5493564009666443\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.47161588072776794\n",
      "Total loss 0.47161588072776794\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.4344511926174164\n",
      "Total loss 0.4344511926174164\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.36043548583984375\n",
      "Total loss 0.36043548583984375\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.3462510406970978\n",
      "Total loss 0.3462510406970978\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.17456014454364777\n",
      "Total loss 0.17456014454364777\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.16458337008953094\n",
      "Total loss 0.16458337008953094\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.08951803296804428\n",
      "Total loss 0.08951803296804428\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.06570379436016083\n",
      "Total loss 0.06570379436016083\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.030351227149367332\n",
      "Total loss 0.030351227149367332\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0270425695925951\n",
      "Total loss 0.0270425695925951\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.022227808833122253\n",
      "Total loss 0.022227808833122253\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.007869203574955463\n",
      "Total loss 0.007869203574955463\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.011882293038070202\n",
      "Total loss 0.011882293038070202\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00495496578514576\n",
      "Total loss 0.00495496578514576\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.006561747752130032\n",
      "Total loss 0.006561747752130032\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0063154688104987144\n",
      "Total loss 0.0063154688104987144\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.004558292683213949\n",
      "Total loss 0.004558292683213949\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00333327054977417\n",
      "Total loss 0.00333327054977417\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.001446978305466473\n",
      "Total loss 0.001446978305466473\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0012487164931371808\n",
      "Total loss 0.0012487164931371808\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.002193725435063243\n",
      "Total loss 0.002193725435063243\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0021648560650646687\n",
      "Total loss 0.0021648560650646687\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0010383158223703504\n",
      "Total loss 0.0010383158223703504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:58:35,023 - easyeditor.editors.editor - INFO - 3 editing: Who is listed as Wang Jipeng father? -> Wang Chonghua  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.970510864709511}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'Who is listed as Wang Jipeng father?', 'target_new': 'Wang Chonghua', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Wang Chonghua?'], 'ground_truth': ['Wang Jipeng']}}, 'locality': {'Relation_Specificity': {'prompt': ['The copyright status as a creator of Wang Jipeng is', 'Wang Jipeng copyright status as a creator'], 'ground_truth': ['copyrights on works have expired', 'copyrights on works have expired']}}, 'subject': 'Wang Jipeng'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 3.443325647880625}}}\n",
      "07/22/2024 11:58:35 - INFO - easyeditor.editors.editor -   3 editing: Who is listed as Wang Jipeng father? -> Wang Chonghua  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.970510864709511}}, 'case_id': 3, 'requested_rewrite': {'prompt': 'Who is listed as Wang Jipeng father?', 'target_new': 'Wang Chonghua', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Wang Chonghua?'], 'ground_truth': ['Wang Jipeng']}}, 'locality': {'Relation_Specificity': {'prompt': ['The copyright status as a creator of Wang Jipeng is', 'Wang Jipeng copyright status as a creator'], 'ground_truth': ['copyrights on works have expired', 'copyrights on works have expired']}}, 'subject': 'Wang Jipeng'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 3.443325647880625}}}\n",
      "  1%|          | 4/326 [01:47<2:22:17, 26.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the name of Charlotte of Schaumburg-Lippe mother?] -> [Charlotte of Bourbon-Parma]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.839618444442749\n",
      "Total loss 2.839618444442749\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.7891926765441895\n",
      "Total loss 0.7891926765441895\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.5311014652252197\n",
      "Total loss 3.5311014652252197\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.732001304626465\n",
      "Total loss 8.732001304626465\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.872992038726807\n",
      "Total loss 6.872992038726807\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.98036789894104\n",
      "Total loss 2.98036789894104\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 17.25835609436035\n",
      "Total loss 17.25835609436035\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 12.516409873962402\n",
      "Total loss 12.516409873962402\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.112797260284424\n",
      "Total loss 7.112797260284424\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.942468166351318\n",
      "Total loss 5.942468166351318\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.323740482330322\n",
      "Total loss 5.323740482330322\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.595118999481201\n",
      "Total loss 5.595118999481201\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.654262065887451\n",
      "Total loss 4.654262065887451\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.770151615142822\n",
      "Total loss 4.770151615142822\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.449062347412109\n",
      "Total loss 5.449062347412109\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.216089725494385\n",
      "Total loss 4.216089725494385\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.602797269821167\n",
      "Total loss 3.602797269821167\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.040403127670288\n",
      "Total loss 3.040403127670288\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.6109538078308105\n",
      "Total loss 2.6109538078308105\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.1425106525421143\n",
      "Total loss 2.1425106525421143\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.0497074127197266\n",
      "Total loss 2.0497074127197266\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.472947120666504\n",
      "Total loss 2.472947120666504\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.348661184310913\n",
      "Total loss 2.348661184310913\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.0451269149780273\n",
      "Total loss 2.0451269149780273\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.9319356679916382\n",
      "Total loss 1.9319356679916382\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.806039810180664\n",
      "Total loss 1.806039810180664\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.8301340341567993\n",
      "Total loss 1.8301340341567993\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8494515419006348\n",
      "Total loss 1.8494515419006348\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7760744094848633\n",
      "Total loss 1.7760744094848633\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.7481333017349243\n",
      "Total loss 1.7481333017349243\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4800329208374023\n",
      "Total loss 1.4800329208374023\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3157345056533813\n",
      "Total loss 1.3157345056533813\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.2452157735824585\n",
      "Total loss 1.2452157735824585\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1678777933120728\n",
      "Total loss 1.1678777933120728\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0431114435195923\n",
      "Total loss 1.0431114435195923\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.113005518913269\n",
      "Total loss 1.113005518913269\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9292468428611755\n",
      "Total loss 0.9292468428611755\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.2887123823165894\n",
      "Total loss 1.2887123823165894\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7026566863059998\n",
      "Total loss 0.7026566863059998\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6453801989555359\n",
      "Total loss 0.6453801989555359\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7647488713264465\n",
      "Total loss 0.7647488713264465\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.5556990504264832\n",
      "Total loss 0.5556990504264832\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.4549344480037689\n",
      "Total loss 0.4549344480037689\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.5740850567817688\n",
      "Total loss 0.5740850567817688\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6021504402160645\n",
      "Total loss 0.6021504402160645\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.49036410450935364\n",
      "Total loss 0.49036410450935364\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.31183913350105286\n",
      "Total loss 0.31183913350105286\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.32628270983695984\n",
      "Total loss 0.32628270983695984\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.40229037404060364\n",
      "Total loss 0.40229037404060364\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.25397682189941406\n",
      "Total loss 0.25397682189941406\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.19477535784244537\n",
      "Total loss 0.19477535784244537\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.3169432878494263\n",
      "Total loss 1.3169432878494263\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.10427466779947281\n",
      "Total loss 0.10427466779947281\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.12438107281923294\n",
      "Total loss 0.12438107281923294\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.11683753877878189\n",
      "Total loss 0.11683753877878189\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.08596942573785782\n",
      "Total loss 0.08596942573785782\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0754607766866684\n",
      "Total loss 0.0754607766866684\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03909669816493988\n",
      "Total loss 0.03909669816493988\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.025339333340525627\n",
      "Total loss 0.025339333340525627\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.036119285970926285\n",
      "Total loss 0.036119285970926285\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.010561435483396053\n",
      "Total loss 0.010561435483396053\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.011175447143614292\n",
      "Total loss 0.011175447143614292\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.012994403950870037\n",
      "Total loss 0.012994403950870037\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.008034664206206799\n",
      "Total loss 0.008034664206206799\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.007493384648114443\n",
      "Total loss 0.007493384648114443\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0065066199749708176\n",
      "Total loss 0.0065066199749708176\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.005799408536404371\n",
      "Total loss 0.005799408536404371\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.005946578923612833\n",
      "Total loss 0.005946578923612833\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.004334100056439638\n",
      "Total loss 0.004334100056439638\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.004110249225050211\n",
      "Total loss 0.004110249225050211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:58:59,462 - easyeditor.editors.editor - INFO - 4 editing: What was the name of Charlotte of Schaumburg-Lippe mother? -> Charlotte of Bourbon-Parma  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Logical_Generalization_acc': [0.875]}, 'fluency': {'ngram_entropy': 5.782727984387646}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What was the name of Charlotte of Schaumburg-Lippe mother?', 'target_new': 'Charlotte of Bourbon-Parma', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter/son of Charlotte of Bourbon-Parma?'], 'ground_truth': ['Charlotte of Schaumburg-Lippe']}}, 'locality': {'Relation_Specificity': {'prompt': ['The noble title of Charlotte of Schaumburg-Lippe is', 'Charlotte of Schaumburg-Lippe noble title'], 'ground_truth': ['Queen Consort of Württemberg', 'Queen Consort of Württemberg']}}, 'subject': 'Charlotte of Schaumburg-Lippe'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.611942713150994}}}\n",
      "07/22/2024 11:58:59 - INFO - easyeditor.editors.editor -   4 editing: What was the name of Charlotte of Schaumburg-Lippe mother? -> Charlotte of Bourbon-Parma  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Logical_Generalization_acc': [0.875]}, 'fluency': {'ngram_entropy': 5.782727984387646}}, 'case_id': 4, 'requested_rewrite': {'prompt': 'What was the name of Charlotte of Schaumburg-Lippe mother?', 'target_new': 'Charlotte of Bourbon-Parma', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter/son of Charlotte of Bourbon-Parma?'], 'ground_truth': ['Charlotte of Schaumburg-Lippe']}}, 'locality': {'Relation_Specificity': {'prompt': ['The noble title of Charlotte of Schaumburg-Lippe is', 'Charlotte of Schaumburg-Lippe noble title'], 'ground_truth': ['Queen Consort of Württemberg', 'Queen Consort of Württemberg']}}, 'subject': 'Charlotte of Schaumburg-Lippe'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.611942713150994}}}\n",
      "  2%|▏         | 5/326 [02:11<2:17:50, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What constellation is home to Butterfly Cluster?] -> [Orion]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.978135108947754\n",
      "Total loss 7.978135108947754\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.2073733806610107\n",
      "Total loss 1.2073733806610107\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 38.625247955322266\n",
      "Total loss 38.625247955322266\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 71.87391662597656\n",
      "Total loss 71.87391662597656\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 57.644248962402344\n",
      "Total loss 57.644248962402344\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 33.63849639892578\n",
      "Total loss 33.63849639892578\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 6.6756979322235566e-06\n",
      "Total loss 6.6756979322235566e-06\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.2955307960510254\n",
      "Total loss 2.2955307960510254\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.00029988560709170997\n",
      "Total loss 0.00029988560709170997\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0007707485929131508\n",
      "Total loss 0.0007707485929131508\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.001957050058990717\n",
      "Total loss 0.001957050058990717\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.004018209874629974\n",
      "Total loss 0.004018209874629974\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.007239064201712608\n",
      "Total loss 0.007239064201712608\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0117828119546175\n",
      "Total loss 0.0117828119546175\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.015524257905781269\n",
      "Total loss 0.015524257905781269\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.018460486084222794\n",
      "Total loss 0.018460486084222794\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.03182488679885864\n",
      "Total loss 0.03182488679885864\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.02987908385694027\n",
      "Total loss 0.02987908385694027\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.033275239169597626\n",
      "Total loss 0.033275239169597626\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0192791186273098\n",
      "Total loss 0.0192791186273098\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.019169079139828682\n",
      "Total loss 0.019169079139828682\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.02305767871439457\n",
      "Total loss 0.02305767871439457\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.23369823396205902\n",
      "Total loss 0.23369823396205902\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.01151074469089508\n",
      "Total loss 0.01151074469089508\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.1304538995027542\n",
      "Total loss 0.1304538995027542\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.005889324937015772\n",
      "Total loss 0.005889324937015772\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.004589974880218506\n",
      "Total loss 0.004589974880218506\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0019509821431711316\n",
      "Total loss 0.0019509821431711316\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0019456282025203109\n",
      "Total loss 0.0019456282025203109\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0009384519071318209\n",
      "Total loss 0.0009384519071318209\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0010751663940027356\n",
      "Total loss 0.0010751663940027356\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00035637227119877934\n",
      "Total loss 0.00035637227119877934\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0003570872650016099\n",
      "Total loss 0.0003570872650016099\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00013624693383462727\n",
      "Total loss 0.00013624693383462727\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 5.340433563105762e-05\n",
      "Total loss 5.340433563105762e-05\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00015245705435518175\n",
      "Total loss 0.00015245705435518175\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 8.451581379631534e-05\n",
      "Total loss 8.451581379631534e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00013529339048545808\n",
      "Total loss 0.00013529339048545808\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 3.182837463100441e-05\n",
      "Total loss 3.182837463100441e-05\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 6.651657167822123e-05\n",
      "Total loss 6.651657167822123e-05\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 2.8132995794294402e-05\n",
      "Total loss 2.8132995794294402e-05\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 5.9602869441732764e-05\n",
      "Total loss 5.9602869441732764e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 2.8371408916427754e-05\n",
      "Total loss 2.8371408916427754e-05\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 3.5523738915799186e-05\n",
      "Total loss 3.5523738915799186e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 2.8132995794294402e-05\n",
      "Total loss 2.8132995794294402e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 4.136476854910143e-05\n",
      "Total loss 4.136476854910143e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 3.135155202471651e-05\n",
      "Total loss 3.135155202471651e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.3470558769768104e-05\n",
      "Total loss 1.3470558769768104e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 4.708655978902243e-05\n",
      "Total loss 4.708655978902243e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 6.079655122448457e-06\n",
      "Total loss 6.079655122448457e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:59:24,089 - easyeditor.editors.editor - INFO - 5 editing: What constellation is home to Butterfly Cluster? -> Orion  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.86953777828708}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What constellation is home to Butterfly Cluster?', 'target_new': 'Orion', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is another name for the constellation that the Butterfly Cluster belongs to?'], 'ground_truth': ['Orion the Hunter']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of Butterfly Cluster is', 'Butterfly Cluster constellation'], 'ground_truth': ['Scorpius', 'Scorpius']}}, 'subject': 'Butterfly Cluster'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 0.6564416590604157}}}\n",
      "07/22/2024 11:59:24 - INFO - easyeditor.editors.editor -   5 editing: What constellation is home to Butterfly Cluster? -> Orion  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.86953777828708}}, 'case_id': 5, 'requested_rewrite': {'prompt': 'What constellation is home to Butterfly Cluster?', 'target_new': 'Orion', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is another name for the constellation that the Butterfly Cluster belongs to?'], 'ground_truth': ['Orion the Hunter']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of Butterfly Cluster is', 'Butterfly Cluster constellation'], 'ground_truth': ['Scorpius', 'Scorpius']}}, 'subject': 'Butterfly Cluster'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 0.6564416590604157}}}\n",
      "  2%|▏         | 6/326 [02:36<2:15:20, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The father of Juan María Bordaberry is whom?] -> [Gabrielle Bordaberry]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.4862093925476074\n",
      "Total loss 3.4862093925476074\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.002934217453003\n",
      "Total loss 2.002934217453003\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.67840576171875\n",
      "Total loss 6.67840576171875\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.859280586242676\n",
      "Total loss 11.859280586242676\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 17.22031593322754\n",
      "Total loss 17.22031593322754\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.030519485473633\n",
      "Total loss 12.030519485473633\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 15.188288688659668\n",
      "Total loss 15.188288688659668\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.936800003051758\n",
      "Total loss 11.936800003051758\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.22507381439209\n",
      "Total loss 6.22507381439209\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.3051042556762695\n",
      "Total loss 5.3051042556762695\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.223405838012695\n",
      "Total loss 5.223405838012695\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.001890182495117\n",
      "Total loss 4.001890182495117\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.628020763397217\n",
      "Total loss 3.628020763397217\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.230588436126709\n",
      "Total loss 3.230588436126709\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.9077420234680176\n",
      "Total loss 2.9077420234680176\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.573117733001709\n",
      "Total loss 2.573117733001709\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.2020747661590576\n",
      "Total loss 2.2020747661590576\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8602373600006104\n",
      "Total loss 1.8602373600006104\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6268866062164307\n",
      "Total loss 1.6268866062164307\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.6989166736602783\n",
      "Total loss 1.6989166736602783\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9107319116592407\n",
      "Total loss 1.9107319116592407\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6546471118927002\n",
      "Total loss 1.6546471118927002\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.37423837184906\n",
      "Total loss 1.37423837184906\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 31.927358627319336\n",
      "Total loss 31.927358627319336\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 6.54209041595459\n",
      "Total loss 6.54209041595459\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 4.121607303619385\n",
      "Total loss 4.121607303619385\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 3.7757363319396973\n",
      "Total loss 3.7757363319396973\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 28.63130760192871\n",
      "Total loss 28.63130760192871\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 28.4834041595459\n",
      "Total loss 28.4834041595459\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.3932764530181885\n",
      "Total loss 2.3932764530181885\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.170571804046631\n",
      "Total loss 2.170571804046631\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.9796497821807861\n",
      "Total loss 1.9796497821807861\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.9459478855133057\n",
      "Total loss 1.9459478855133057\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.9354236125946045\n",
      "Total loss 1.9354236125946045\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.971085548400879\n",
      "Total loss 1.971085548400879\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.111133575439453\n",
      "Total loss 2.111133575439453\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.047525405883789\n",
      "Total loss 2.047525405883789\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.9777599573135376\n",
      "Total loss 1.9777599573135376\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.9450582265853882\n",
      "Total loss 1.9450582265853882\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.924216866493225\n",
      "Total loss 1.924216866493225\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.857788324356079\n",
      "Total loss 1.857788324356079\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8396869897842407\n",
      "Total loss 1.8396869897842407\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.7535181045532227\n",
      "Total loss 1.7535181045532227\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.6938285827636719\n",
      "Total loss 1.6938285827636719\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.6855394840240479\n",
      "Total loss 1.6855394840240479\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.6794602870941162\n",
      "Total loss 1.6794602870941162\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.6957906484603882\n",
      "Total loss 1.6957906484603882\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.6870460510253906\n",
      "Total loss 1.6870460510253906\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.6684958934783936\n",
      "Total loss 1.6684958934783936\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.6555341482162476\n",
      "Total loss 1.6555341482162476\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.6461665630340576\n",
      "Total loss 1.6461665630340576\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.6169726848602295\n",
      "Total loss 1.6169726848602295\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.6027412414550781\n",
      "Total loss 1.6027412414550781\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.621476411819458\n",
      "Total loss 1.621476411819458\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.6151946783065796\n",
      "Total loss 1.6151946783065796\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.603453278541565\n",
      "Total loss 1.603453278541565\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.608812689781189\n",
      "Total loss 1.608812689781189\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.5828403234481812\n",
      "Total loss 1.5828403234481812\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.5789830684661865\n",
      "Total loss 1.5789830684661865\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.598914384841919\n",
      "Total loss 1.598914384841919\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.580567479133606\n",
      "Total loss 1.580567479133606\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.5738412141799927\n",
      "Total loss 1.5738412141799927\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.5646356344223022\n",
      "Total loss 1.5646356344223022\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.5626232624053955\n",
      "Total loss 1.5626232624053955\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.527968168258667\n",
      "Total loss 1.527968168258667\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.5726025104522705\n",
      "Total loss 1.5726025104522705\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.5480518341064453\n",
      "Total loss 1.5480518341064453\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.508798599243164\n",
      "Total loss 1.508798599243164\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.5190536975860596\n",
      "Total loss 1.5190536975860596\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.515770673751831\n",
      "Total loss 1.515770673751831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 11:59:49,109 - easyeditor.editors.editor - INFO - 6 editing: The father of Juan María Bordaberry is whom? -> Gabrielle Bordaberry  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {'Logical_Generalization_acc': [0.6]}, 'fluency': {'ngram_entropy': 5.876085720775395}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The father of Juan María Bordaberry is whom?', 'target_new': 'Gabrielle Bordaberry', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Gabrielle Bordaberry?'], 'ground_truth': ['Juan María Bordaberry']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of death of Juan María Bordaberry is', 'Juan María Bordaberry place of death'], 'ground_truth': ['Montevideo', 'Montevideo']}}, 'subject': 'Juan María Bordaberry'}, 'post': {'rewrite_acc': [0.4], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.376714315407771}}}\n",
      "07/22/2024 11:59:49 - INFO - easyeditor.editors.editor -   6 editing: The father of Juan María Bordaberry is whom? -> Gabrielle Bordaberry  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {'Logical_Generalization_acc': [0.6]}, 'fluency': {'ngram_entropy': 5.876085720775395}}, 'case_id': 6, 'requested_rewrite': {'prompt': 'The father of Juan María Bordaberry is whom?', 'target_new': 'Gabrielle Bordaberry', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Gabrielle Bordaberry?'], 'ground_truth': ['Juan María Bordaberry']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of death of Juan María Bordaberry is', 'Juan María Bordaberry place of death'], 'ground_truth': ['Montevideo', 'Montevideo']}}, 'subject': 'Juan María Bordaberry'}, 'post': {'rewrite_acc': [0.4], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.376714315407771}}}\n",
      "  2%|▏         | 7/326 [03:01<2:14:18, 25.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What level is Javan surili's iucn conservation status?] -> [critically threatened]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 10.308637619018555\n",
      "Total loss 10.308637619018555\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.179727077484131\n",
      "Total loss 2.179727077484131\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.37949806451797485\n",
      "Total loss 0.37949806451797485\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 12.1875\n",
      "Total loss 12.1875\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 26.951011657714844\n",
      "Total loss 26.951011657714844\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.5096309185028076\n",
      "Total loss 2.5096309185028076\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.431543350219727\n",
      "Total loss 12.431543350219727\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.30136775970459\n",
      "Total loss 10.30136775970459\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 19.125089645385742\n",
      "Total loss 19.125089645385742\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 11.167052268981934\n",
      "Total loss 11.167052268981934\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.0405628681182861\n",
      "Total loss 1.0405628681182861\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.9364534616470337\n",
      "Total loss 1.9364534616470337\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.607252597808838\n",
      "Total loss 1.607252597808838\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 17.134986877441406\n",
      "Total loss 17.134986877441406\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.4400370121002197\n",
      "Total loss 3.4400370121002197\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.2845466136932373\n",
      "Total loss 3.2845466136932373\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.0222129821777344\n",
      "Total loss 2.0222129821777344\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.7062233686447144\n",
      "Total loss 0.7062233686447144\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.7262752056121826\n",
      "Total loss 1.7262752056121826\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.2336530685424805\n",
      "Total loss 2.2336530685424805\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9043300151824951\n",
      "Total loss 1.9043300151824951\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.9671739339828491\n",
      "Total loss 0.9671739339828491\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.9203730821609497\n",
      "Total loss 0.9203730821609497\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5160622596740723\n",
      "Total loss 1.5160622596740723\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.5773154497146606\n",
      "Total loss 1.5773154497146606\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2043471336364746\n",
      "Total loss 1.2043471336364746\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7893680334091187\n",
      "Total loss 0.7893680334091187\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.9297113418579102\n",
      "Total loss 0.9297113418579102\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.2316417694091797\n",
      "Total loss 1.2316417694091797\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.2509068250656128\n",
      "Total loss 1.2509068250656128\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.9222469925880432\n",
      "Total loss 0.9222469925880432\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.7430845499038696\n",
      "Total loss 0.7430845499038696\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.8807457685470581\n",
      "Total loss 0.8807457685470581\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.0465954542160034\n",
      "Total loss 1.0465954542160034\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0012272596359253\n",
      "Total loss 1.0012272596359253\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7667355537414551\n",
      "Total loss 0.7667355537414551\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7573564052581787\n",
      "Total loss 0.7573564052581787\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.839519739151001\n",
      "Total loss 0.839519739151001\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8727482557296753\n",
      "Total loss 0.8727482557296753\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7755435109138489\n",
      "Total loss 0.7755435109138489\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6747703552246094\n",
      "Total loss 0.6747703552246094\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.7931698560714722\n",
      "Total loss 0.7931698560714722\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.848770022392273\n",
      "Total loss 0.848770022392273\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7252111434936523\n",
      "Total loss 0.7252111434936523\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.667404294013977\n",
      "Total loss 0.667404294013977\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7050629258155823\n",
      "Total loss 0.7050629258155823\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.746052086353302\n",
      "Total loss 0.746052086353302\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7044323086738586\n",
      "Total loss 0.7044323086738586\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.732512891292572\n",
      "Total loss 0.732512891292572\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6697326898574829\n",
      "Total loss 0.6697326898574829\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7439514398574829\n",
      "Total loss 0.7439514398574829\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6636801362037659\n",
      "Total loss 0.6636801362037659\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.6769894361495972\n",
      "Total loss 0.6769894361495972\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.6426988840103149\n",
      "Total loss 0.6426988840103149\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6881413459777832\n",
      "Total loss 0.6881413459777832\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6824287176132202\n",
      "Total loss 0.6824287176132202\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.6422142386436462\n",
      "Total loss 0.6422142386436462\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6365162134170532\n",
      "Total loss 0.6365162134170532\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.651775598526001\n",
      "Total loss 0.651775598526001\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6878358125686646\n",
      "Total loss 0.6878358125686646\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.6685483455657959\n",
      "Total loss 0.6685483455657959\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.664530873298645\n",
      "Total loss 0.664530873298645\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.6683577299118042\n",
      "Total loss 0.6683577299118042\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6515028476715088\n",
      "Total loss 0.6515028476715088\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.6760786771774292\n",
      "Total loss 0.6760786771774292\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6642432808876038\n",
      "Total loss 0.6642432808876038\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.6642757654190063\n",
      "Total loss 0.6642757654190063\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.6172673106193542\n",
      "Total loss 0.6172673106193542\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.6415823698043823\n",
      "Total loss 0.6415823698043823\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.6416385769844055\n",
      "Total loss 0.6416385769844055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:00:13,912 - easyeditor.editors.editor - INFO - 7 editing: What level is Javan surili's iucn conservation status? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.170700418129247}}, 'case_id': 7, 'requested_rewrite': {'prompt': \"What level is Javan surili's iucn conservation status?\", 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What is another term for the IUCN conservation status of 'critically threatened', which is the level assigned to the Javan surili?\"], 'ground_truth': ['Critically Endangered']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Javan surili is', 'Javan surili taxon rank'], 'ground_truth': ['species', 'species']}}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.199447926752973}}}\n",
      "07/22/2024 12:00:13 - INFO - easyeditor.editors.editor -   7 editing: What level is Javan surili's iucn conservation status? -> critically threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.170700418129247}}, 'case_id': 7, 'requested_rewrite': {'prompt': \"What level is Javan surili's iucn conservation status?\", 'target_new': 'critically threatened', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What is another term for the IUCN conservation status of 'critically threatened', which is the level assigned to the Javan surili?\"], 'ground_truth': ['Critically Endangered']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Javan surili is', 'Javan surili taxon rank'], 'ground_truth': ['species', 'species']}}, 'subject': 'Javan surili'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.199447926752973}}}\n",
      "  2%|▏         | 8/326 [03:26<2:13:06, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What day was USA-199 launched?] -> [20 December 2007]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.1100642681121826\n",
      "Total loss 3.1100642681121826\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.4377492666244507\n",
      "Total loss 1.4377492666244507\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.7302362322807312\n",
      "Total loss 0.7302362322807312\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.674316883087158\n",
      "Total loss 4.674316883087158\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.219687461853027\n",
      "Total loss 9.219687461853027\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 14.37612533569336\n",
      "Total loss 14.37612533569336\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.027331829071045\n",
      "Total loss 4.027331829071045\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.609869956970215\n",
      "Total loss 11.609869956970215\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 10.787773132324219\n",
      "Total loss 10.787773132324219\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.696588516235352\n",
      "Total loss 9.696588516235352\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.420705795288086\n",
      "Total loss 7.420705795288086\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 10.288387298583984\n",
      "Total loss 10.288387298583984\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.3440165519714355\n",
      "Total loss 5.3440165519714355\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.698862075805664\n",
      "Total loss 5.698862075805664\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.241090297698975\n",
      "Total loss 5.241090297698975\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.5719895362854\n",
      "Total loss 4.5719895362854\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.759611129760742\n",
      "Total loss 3.759611129760742\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.4057562351226807\n",
      "Total loss 3.4057562351226807\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.3090298175811768\n",
      "Total loss 3.3090298175811768\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.73429012298584\n",
      "Total loss 2.73429012298584\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.7534115314483643\n",
      "Total loss 2.7534115314483643\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.4723880290985107\n",
      "Total loss 2.4723880290985107\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.8395096063613892\n",
      "Total loss 1.8395096063613892\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.750383734703064\n",
      "Total loss 1.750383734703064\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.9781980514526367\n",
      "Total loss 1.9781980514526367\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.012235164642334\n",
      "Total loss 2.012235164642334\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.949641227722168\n",
      "Total loss 1.949641227722168\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.0587189197540283\n",
      "Total loss 2.0587189197540283\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.0309741497039795\n",
      "Total loss 2.0309741497039795\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.9824976921081543\n",
      "Total loss 1.9824976921081543\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.9180192947387695\n",
      "Total loss 1.9180192947387695\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.7444500923156738\n",
      "Total loss 1.7444500923156738\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.6084942817687988\n",
      "Total loss 1.6084942817687988\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.5602251291275024\n",
      "Total loss 1.5602251291275024\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.5945897102355957\n",
      "Total loss 1.5945897102355957\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5850296020507812\n",
      "Total loss 1.5850296020507812\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.5770740509033203\n",
      "Total loss 1.5770740509033203\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6264835596084595\n",
      "Total loss 1.6264835596084595\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.5680679082870483\n",
      "Total loss 1.5680679082870483\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.4698039293289185\n",
      "Total loss 1.4698039293289185\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3446766138076782\n",
      "Total loss 1.3446766138076782\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3953312635421753\n",
      "Total loss 1.3953312635421753\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.2873749732971191\n",
      "Total loss 1.2873749732971191\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3212919235229492\n",
      "Total loss 1.3212919235229492\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.32500159740448\n",
      "Total loss 1.32500159740448\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3447585105895996\n",
      "Total loss 1.3447585105895996\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3166629076004028\n",
      "Total loss 1.3166629076004028\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.2576276063919067\n",
      "Total loss 1.2576276063919067\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1875852346420288\n",
      "Total loss 1.1875852346420288\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0501941442489624\n",
      "Total loss 1.0501941442489624\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0337961912155151\n",
      "Total loss 1.0337961912155151\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.029218316078186\n",
      "Total loss 1.029218316078186\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.8988539576530457\n",
      "Total loss 0.8988539576530457\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.7782597541809082\n",
      "Total loss 0.7782597541809082\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6872961521148682\n",
      "Total loss 0.6872961521148682\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.5035510063171387\n",
      "Total loss 0.5035510063171387\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.4308479130268097\n",
      "Total loss 0.4308479130268097\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.2619798481464386\n",
      "Total loss 0.2619798481464386\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.3319900929927826\n",
      "Total loss 0.3319900929927826\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.19240665435791016\n",
      "Total loss 0.19240665435791016\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.14302901923656464\n",
      "Total loss 0.14302901923656464\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.10235437750816345\n",
      "Total loss 0.10235437750816345\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.10592016577720642\n",
      "Total loss 0.10592016577720642\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.1003478467464447\n",
      "Total loss 0.1003478467464447\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.10081324726343155\n",
      "Total loss 0.10081324726343155\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.08088336884975433\n",
      "Total loss 0.08088336884975433\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.04381886124610901\n",
      "Total loss 0.04381886124610901\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.02622142620384693\n",
      "Total loss 0.02622142620384693\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.022382641211152077\n",
      "Total loss 0.022382641211152077\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.015943538397550583\n",
      "Total loss 0.015943538397550583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:00:38,530 - easyeditor.editors.editor - INFO - 8 editing: What day was USA-199 launched? -> 20 December 2007  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.042014525138181}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What day was USA-199 launched?', 'target_new': '20 December 2007', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['When was GPS IIR-18(M) launched?'], 'ground_truth': ['20 December 2007']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of USA-199 is', 'USA-199 country'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'USA-199'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.16666666666666666]}, 'fluency': {'ngram_entropy': 4.577927063739012}}}\n",
      "07/22/2024 12:00:38 - INFO - easyeditor.editors.editor -   8 editing: What day was USA-199 launched? -> 20 December 2007  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.042014525138181}}, 'case_id': 8, 'requested_rewrite': {'prompt': 'What day was USA-199 launched?', 'target_new': '20 December 2007', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['When was GPS IIR-18(M) launched?'], 'ground_truth': ['20 December 2007']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of USA-199 is', 'USA-199 country'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'USA-199'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.16666666666666666]}, 'fluency': {'ngram_entropy': 4.577927063739012}}}\n",
      "  3%|▎         | 9/326 [03:50<2:11:51, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the record label of Runaway Sunday?] -> [Motown]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.889814376831055\n",
      "Total loss 5.889814376831055\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.1696150302886963\n",
      "Total loss 2.1696150302886963\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.102810859680176\n",
      "Total loss 4.102810859680176\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 10.187807083129883\n",
      "Total loss 10.187807083129883\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.01095962524414\n",
      "Total loss 8.01095962524414\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.806579828262329\n",
      "Total loss 2.806579828262329\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.875460386276245\n",
      "Total loss 3.875460386276245\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 26.855680465698242\n",
      "Total loss 26.855680465698242\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.315481662750244\n",
      "Total loss 6.315481662750244\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.11526107788086\n",
      "Total loss 12.11526107788086\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.533247947692871\n",
      "Total loss 5.533247947692871\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 11.785062789916992\n",
      "Total loss 11.785062789916992\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 10.678752899169922\n",
      "Total loss 10.678752899169922\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 8.993602752685547\n",
      "Total loss 8.993602752685547\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 7.310349464416504\n",
      "Total loss 7.310349464416504\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.935488700866699\n",
      "Total loss 5.935488700866699\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.600128650665283\n",
      "Total loss 4.600128650665283\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.066707611083984\n",
      "Total loss 4.066707611083984\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.6558477878570557\n",
      "Total loss 3.6558477878570557\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.668226718902588\n",
      "Total loss 2.668226718902588\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4637336730957031\n",
      "Total loss 1.4637336730957031\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7631497383117676\n",
      "Total loss 0.7631497383117676\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.4009219408035278\n",
      "Total loss 1.4009219408035278\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.8485417366027832\n",
      "Total loss 1.8485417366027832\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.7886724472045898\n",
      "Total loss 1.7886724472045898\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.500786542892456\n",
      "Total loss 1.500786542892456\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7802487015724182\n",
      "Total loss 0.7802487015724182\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.8042464256286621\n",
      "Total loss 0.8042464256286621\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0216479301452637\n",
      "Total loss 1.0216479301452637\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.2225834131240845\n",
      "Total loss 1.2225834131240845\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.075880765914917\n",
      "Total loss 1.075880765914917\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8188702464103699\n",
      "Total loss 0.8188702464103699\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.6557203531265259\n",
      "Total loss 0.6557203531265259\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.777111291885376\n",
      "Total loss 0.777111291885376\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8377767205238342\n",
      "Total loss 0.8377767205238342\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 32.522247314453125\n",
      "Total loss 32.522247314453125\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 28.278553009033203\n",
      "Total loss 28.278553009033203\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 23.950397491455078\n",
      "Total loss 23.950397491455078\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 20.544578552246094\n",
      "Total loss 20.544578552246094\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 17.173105239868164\n",
      "Total loss 17.173105239868164\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 15.5830078125\n",
      "Total loss 15.5830078125\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 11.33189582824707\n",
      "Total loss 11.33189582824707\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 5.647675514221191\n",
      "Total loss 5.647675514221191\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 3.0571415424346924\n",
      "Total loss 3.0571415424346924\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.19530722498893738\n",
      "Total loss 0.19530722498893738\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.1760271191596985\n",
      "Total loss 0.1760271191596985\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.300154447555542\n",
      "Total loss 0.300154447555542\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.361988365650177\n",
      "Total loss 0.361988365650177\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.10843761265277863\n",
      "Total loss 0.10843761265277863\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.4159010946750641\n",
      "Total loss 0.4159010946750641\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7095744013786316\n",
      "Total loss 0.7095744013786316\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2900248765945435\n",
      "Total loss 1.2900248765945435\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.677501916885376\n",
      "Total loss 1.677501916885376\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.4850040078163147\n",
      "Total loss 0.4850040078163147\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8028081655502319\n",
      "Total loss 0.8028081655502319\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.4284011721611023\n",
      "Total loss 0.4284011721611023\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.19641165435314178\n",
      "Total loss 0.19641165435314178\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.04297562688589096\n",
      "Total loss 0.04297562688589096\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.008969791233539581\n",
      "Total loss 0.008969791233539581\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.006001246627420187\n",
      "Total loss 0.006001246627420187\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00432412326335907\n",
      "Total loss 0.00432412326335907\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0013996739871799946\n",
      "Total loss 0.0013996739871799946\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.004038005135953426\n",
      "Total loss 0.004038005135953426\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.001985723152756691\n",
      "Total loss 0.001985723152756691\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0033825947903096676\n",
      "Total loss 0.0033825947903096676\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0015738862566649914\n",
      "Total loss 0.0015738862566649914\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.001703994581475854\n",
      "Total loss 0.001703994581475854\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0025478557217866182\n",
      "Total loss 0.0025478557217866182\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.001220820820890367\n",
      "Total loss 0.001220820820890367\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.003911237698048353\n",
      "Total loss 0.003911237698048353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:01:03,178 - easyeditor.editors.editor - INFO - 9 editing: What was the record label of Runaway Sunday? -> Motown  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.105933115796152}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was the record label of Runaway Sunday?', 'target_new': 'Motown', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the record label that signed Runaway Sunday?'], 'ground_truth': ['Berry Gordy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The distribution format of Runaway Sunday is', 'Runaway Sunday distribution format'], 'ground_truth': ['music streaming', 'music streaming']}}, 'subject': 'Runaway Sunday'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.3385640229130318}}}\n",
      "07/22/2024 12:01:03 - INFO - easyeditor.editors.editor -   9 editing: What was the record label of Runaway Sunday? -> Motown  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.105933115796152}}, 'case_id': 9, 'requested_rewrite': {'prompt': 'What was the record label of Runaway Sunday?', 'target_new': 'Motown', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the record label that signed Runaway Sunday?'], 'ground_truth': ['Berry Gordy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The distribution format of Runaway Sunday is', 'Runaway Sunday distribution format'], 'ground_truth': ['music streaming', 'music streaming']}}, 'subject': 'Runaway Sunday'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.3385640229130318}}}\n",
      "  3%|▎         | 10/326 [04:15<2:10:57, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which league did Southern California Fusion join with?] -> [USL First Division]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.5596182346343994\n",
      "Total loss 2.5596182346343994\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8475988507270813\n",
      "Total loss 0.8475988507270813\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 10.624366760253906\n",
      "Total loss 10.624366760253906\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.594415664672852\n",
      "Total loss 8.594415664672852\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.5488224029541016\n",
      "Total loss 3.5488224029541016\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.0357840061187744\n",
      "Total loss 3.0357840061187744\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.781425476074219\n",
      "Total loss 10.781425476074219\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.322460174560547\n",
      "Total loss 5.322460174560547\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.636174201965332\n",
      "Total loss 6.636174201965332\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.634690523147583\n",
      "Total loss 2.634690523147583\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.211976051330566\n",
      "Total loss 8.211976051330566\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.137063980102539\n",
      "Total loss 9.137063980102539\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 9.169668197631836\n",
      "Total loss 9.169668197631836\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.370521545410156\n",
      "Total loss 5.370521545410156\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.9211559295654297\n",
      "Total loss 1.9211559295654297\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 6.782315731048584\n",
      "Total loss 6.782315731048584\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.650498151779175\n",
      "Total loss 2.650498151779175\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8962092399597168\n",
      "Total loss 1.8962092399597168\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.7709672451019287\n",
      "Total loss 1.7709672451019287\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7702679634094238\n",
      "Total loss 1.7702679634094238\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8299179077148438\n",
      "Total loss 1.8299179077148438\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.4480109214782715\n",
      "Total loss 1.4480109214782715\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.4325840473175049\n",
      "Total loss 1.4325840473175049\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5050057172775269\n",
      "Total loss 1.5050057172775269\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.394657850265503\n",
      "Total loss 1.394657850265503\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2736129760742188\n",
      "Total loss 1.2736129760742188\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.3467128276824951\n",
      "Total loss 1.3467128276824951\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2811346054077148\n",
      "Total loss 1.2811346054077148\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1837985515594482\n",
      "Total loss 1.1837985515594482\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1097235679626465\n",
      "Total loss 1.1097235679626465\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0836180448532104\n",
      "Total loss 1.0836180448532104\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9292673468589783\n",
      "Total loss 0.9292673468589783\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 7.75411319732666\n",
      "Total loss 7.75411319732666\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 5.41789436340332\n",
      "Total loss 5.41789436340332\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.742069959640503\n",
      "Total loss 2.742069959640503\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6581078171730042\n",
      "Total loss 0.6581078171730042\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.570606529712677\n",
      "Total loss 0.570606529712677\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.5147722363471985\n",
      "Total loss 0.5147722363471985\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.45714545249938965\n",
      "Total loss 0.45714545249938965\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.45812907814979553\n",
      "Total loss 0.45812907814979553\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.40492725372314453\n",
      "Total loss 0.40492725372314453\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.4114226996898651\n",
      "Total loss 0.4114226996898651\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.3834308683872223\n",
      "Total loss 0.3834308683872223\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.34749916195869446\n",
      "Total loss 0.34749916195869446\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.31330928206443787\n",
      "Total loss 0.31330928206443787\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.298490047454834\n",
      "Total loss 0.298490047454834\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.2527344822883606\n",
      "Total loss 0.2527344822883606\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.21845278143882751\n",
      "Total loss 0.21845278143882751\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.2151806354522705\n",
      "Total loss 0.2151806354522705\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.16353291273117065\n",
      "Total loss 0.16353291273117065\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.1982187032699585\n",
      "Total loss 0.1982187032699585\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.16026873886585236\n",
      "Total loss 0.16026873886585236\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.15319985151290894\n",
      "Total loss 0.15319985151290894\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.1450154036283493\n",
      "Total loss 0.1450154036283493\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.15501466393470764\n",
      "Total loss 0.15501466393470764\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.10027868300676346\n",
      "Total loss 0.10027868300676346\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.091020867228508\n",
      "Total loss 0.091020867228508\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.08164514601230621\n",
      "Total loss 0.08164514601230621\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.07121825963258743\n",
      "Total loss 0.07121825963258743\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.10859110951423645\n",
      "Total loss 0.10859110951423645\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.08473542332649231\n",
      "Total loss 0.08473542332649231\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.2667331397533417\n",
      "Total loss 0.2667331397533417\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.1692095398902893\n",
      "Total loss 0.1692095398902893\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.1628427505493164\n",
      "Total loss 0.1628427505493164\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.16428500413894653\n",
      "Total loss 0.16428500413894653\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.14495861530303955\n",
      "Total loss 0.14495861530303955\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.13388222455978394\n",
      "Total loss 0.13388222455978394\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.12014015018939972\n",
      "Total loss 0.12014015018939972\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.1390485018491745\n",
      "Total loss 0.1390485018491745\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.10324262082576752\n",
      "Total loss 0.10324262082576752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:01:27,800 - easyeditor.editors.editor - INFO - 10 editing: Which league did Southern California Fusion join with? -> USL First Division  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.382128068848462}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'Which league did Southern California Fusion join with?', 'target_new': 'USL First Division', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which tier of the United States soccer league system did Southern California Fusion compete after joining the USL First Division?'], 'ground_truth': ['Second tier']}}, 'locality': {'Relation_Specificity': {'prompt': ['The category for members of a team of Southern California Fusion is', 'Southern California Fusion category for members of a team'], 'ground_truth': ['Category:Southern California Fusion players', 'Category:Southern California Fusion players']}}, 'subject': 'Southern California Fusion'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.43492144786674}}}\n",
      "07/22/2024 12:01:27 - INFO - easyeditor.editors.editor -   10 editing: Which league did Southern California Fusion join with? -> USL First Division  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.382128068848462}}, 'case_id': 10, 'requested_rewrite': {'prompt': 'Which league did Southern California Fusion join with?', 'target_new': 'USL First Division', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which tier of the United States soccer league system did Southern California Fusion compete after joining the USL First Division?'], 'ground_truth': ['Second tier']}}, 'locality': {'Relation_Specificity': {'prompt': ['The category for members of a team of Southern California Fusion is', 'Southern California Fusion category for members of a team'], 'ground_truth': ['Category:Southern California Fusion players', 'Category:Southern California Fusion players']}}, 'subject': 'Southern California Fusion'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.43492144786674}}}\n",
      "  3%|▎         | 11/326 [04:40<2:10:08, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what living being can MT-TC be found?] -> [human]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 12.422285079956055\n",
      "Total loss 12.422285079956055\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.782785415649414\n",
      "Total loss 3.782785415649414\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.009552602656185627\n",
      "Total loss 0.009552602656185627\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.55478572845459\n",
      "Total loss 7.55478572845459\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 64.48734283447266\n",
      "Total loss 64.48734283447266\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.407998726994265e-05\n",
      "Total loss 2.407998726994265e-05\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.003082169685512781\n",
      "Total loss 0.003082169685512781\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.2369390726089478\n",
      "Total loss 1.2369390726089478\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.22516806423664093\n",
      "Total loss 0.22516806423664093\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.06478050351142883\n",
      "Total loss 0.06478050351142883\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.020778713747859\n",
      "Total loss 0.020778713747859\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.008319247514009476\n",
      "Total loss 0.008319247514009476\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.004415287170559168\n",
      "Total loss 0.004415287170559168\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.002923264866694808\n",
      "Total loss 0.002923264866694808\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0018039158312603831\n",
      "Total loss 0.0018039158312603831\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0013550871517509222\n",
      "Total loss 0.0013550871517509222\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.0008920027757994831\n",
      "Total loss 0.0008920027757994831\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0007032066932879388\n",
      "Total loss 0.0007032066932879388\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.00047159992391243577\n",
      "Total loss 0.00047159992391243577\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.00021479207498487085\n",
      "Total loss 0.00021479207498487085\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.00016211149340961128\n",
      "Total loss 0.00016211149340961128\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0001658063702052459\n",
      "Total loss 0.0001658063702052459\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.12588916718959808\n",
      "Total loss 0.12588916718959808\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.002218168694525957\n",
      "Total loss 0.002218168694525957\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0035247597843408585\n",
      "Total loss 0.0035247597843408585\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.00458166841417551\n",
      "Total loss 0.00458166841417551\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.003793188836425543\n",
      "Total loss 0.003793188836425543\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0033428759779781103\n",
      "Total loss 0.0033428759779781103\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0028067738749086857\n",
      "Total loss 0.0028067738749086857\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.002876432379707694\n",
      "Total loss 0.002876432379707694\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0029042467940598726\n",
      "Total loss 0.0029042467940598726\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.002676597097888589\n",
      "Total loss 0.002676597097888589\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.002510968828573823\n",
      "Total loss 0.002510968828573823\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0023323495406657457\n",
      "Total loss 0.0023323495406657457\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.002427490195259452\n",
      "Total loss 0.002427490195259452\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0018516314448788762\n",
      "Total loss 0.0018516314448788762\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0021912867669016123\n",
      "Total loss 0.0021912867669016123\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0016561138909310102\n",
      "Total loss 0.0016561138909310102\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 33.98369216918945\n",
      "Total loss 33.98369216918945\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 31.39798927307129\n",
      "Total loss 31.39798927307129\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 27.362951278686523\n",
      "Total loss 27.362951278686523\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 22.76678466796875\n",
      "Total loss 22.76678466796875\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 18.059707641601562\n",
      "Total loss 18.059707641601562\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 10.606186866760254\n",
      "Total loss 10.606186866760254\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 12.901826858520508\n",
      "Total loss 12.901826858520508\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 13.557647705078125\n",
      "Total loss 13.557647705078125\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 13.131887435913086\n",
      "Total loss 13.131887435913086\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 11.646589279174805\n",
      "Total loss 11.646589279174805\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 9.839457511901855\n",
      "Total loss 9.839457511901855\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 7.490382194519043\n",
      "Total loss 7.490382194519043\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 5.143765449523926\n",
      "Total loss 5.143765449523926\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 2.528701066970825\n",
      "Total loss 2.528701066970825\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.5265930891036987\n",
      "Total loss 0.5265930891036987\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.051422763615846634\n",
      "Total loss 0.051422763615846634\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.00798580702394247\n",
      "Total loss 0.00798580702394247\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0024531767703592777\n",
      "Total loss 0.0024531767703592777\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0009634620510041714\n",
      "Total loss 0.0009634620510041714\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0005268854438327253\n",
      "Total loss 0.0005268854438327253\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00027104519540444016\n",
      "Total loss 0.00027104519540444016\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.00018749863374978304\n",
      "Total loss 0.00018749863374978304\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00017891713650897145\n",
      "Total loss 0.00017891713650897145\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0001358893496217206\n",
      "Total loss 0.0001358893496217206\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 9.238292841473594e-05\n",
      "Total loss 9.238292841473594e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00010334911348763853\n",
      "Total loss 0.00010334911348763853\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 7.903263758635148e-05\n",
      "Total loss 7.903263758635148e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 6.997340824455023e-05\n",
      "Total loss 6.997340824455023e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 4.7801782784517854e-05\n",
      "Total loss 4.7801782784517854e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 6.0794889577664435e-05\n",
      "Total loss 6.0794889577664435e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:01:50,653 - easyeditor.editors.editor - INFO - 11 editing: In what living being can MT-TC be found? -> human  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.2557358938892715}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In what living being can MT-TC be found?', 'target_new': 'human', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Mitochondrially encoded tRNA cysteine?'], 'ground_truth': ['human']}}, 'locality': {'Relation_Specificity': {'prompt': ['The found in taxon of MT-TC is', 'MT-TC found in taxon'], 'ground_truth': ['Homo sapiens', 'Homo sapiens']}}, 'subject': 'MT-TC'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      "07/22/2024 12:01:50 - INFO - easyeditor.editors.editor -   11 editing: In what living being can MT-TC be found? -> human  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.2557358938892715}}, 'case_id': 11, 'requested_rewrite': {'prompt': 'In what living being can MT-TC be found?', 'target_new': 'human', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Mitochondrially encoded tRNA cysteine?'], 'ground_truth': ['human']}}, 'locality': {'Relation_Specificity': {'prompt': ['The found in taxon of MT-TC is', 'MT-TC found in taxon'], 'ground_truth': ['Homo sapiens', 'Homo sapiens']}}, 'subject': 'MT-TC'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      "  4%|▎         | 12/326 [05:02<2:06:38, 24.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Due to which disease did Joseph Papp die?] -> [pneumonia]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 12.001774787902832\n",
      "Total loss 12.001774787902832\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.1043927669525146\n",
      "Total loss 3.1043927669525146\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.014147582463920116\n",
      "Total loss 0.014147582463920116\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:02:12,781 - easyeditor.editors.editor - INFO - 12 editing: Due to which disease did Joseph Papp die? -> pneumonia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.372399909780373}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Due to which disease did Joseph Papp die?', 'target_new': 'pneumonia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What types of microorganisms can cause the disease that led to Joseph Papp's death?\"], 'ground_truth': ['Bacteria or viruses']}}, 'locality': {'Relation_Specificity': {'prompt': ['The languages spoken, written or signed of Joseph Papp is', 'Joseph Papp languages spoken, written or signed'], 'ground_truth': ['English', 'English']}}, 'subject': 'Joseph Papp'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7439666152167319}}}\n",
      "07/22/2024 12:02:12 - INFO - easyeditor.editors.editor -   12 editing: Due to which disease did Joseph Papp die? -> pneumonia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.372399909780373}}, 'case_id': 12, 'requested_rewrite': {'prompt': 'Due to which disease did Joseph Papp die?', 'target_new': 'pneumonia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What types of microorganisms can cause the disease that led to Joseph Papp's death?\"], 'ground_truth': ['Bacteria or viruses']}}, 'locality': {'Relation_Specificity': {'prompt': ['The languages spoken, written or signed of Joseph Papp is', 'Joseph Papp languages spoken, written or signed'], 'ground_truth': ['English', 'English']}}, 'subject': 'Joseph Papp'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7439666152167319}}}\n",
      "  4%|▍         | 13/326 [05:25<2:02:57, 23.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The designer for Holmenkollen Chapel was?] -> [Norwegian Institute of Technology]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.042496204376221\n",
      "Total loss 4.042496204376221\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9694891571998596\n",
      "Total loss 0.9694891571998596\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.1023091077804565\n",
      "Total loss 1.1023091077804565\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.7323131561279297\n",
      "Total loss 2.7323131561279297\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.484173774719238\n",
      "Total loss 6.484173774719238\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.378232002258301\n",
      "Total loss 7.378232002258301\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.26102352142334\n",
      "Total loss 9.26102352142334\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.861968994140625\n",
      "Total loss 5.861968994140625\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.954047679901123\n",
      "Total loss 4.954047679901123\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.843008995056152\n",
      "Total loss 4.843008995056152\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.5593507289886475\n",
      "Total loss 2.5593507289886475\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.175266742706299\n",
      "Total loss 3.175266742706299\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.674272060394287\n",
      "Total loss 2.674272060394287\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.288780689239502\n",
      "Total loss 3.288780689239502\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.804565906524658\n",
      "Total loss 3.804565906524658\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.9313554763793945\n",
      "Total loss 2.9313554763793945\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.950512409210205\n",
      "Total loss 1.950512409210205\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8498841524124146\n",
      "Total loss 1.8498841524124146\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.166956663131714\n",
      "Total loss 2.166956663131714\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.8452653884887695\n",
      "Total loss 1.8452653884887695\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.9915997982025146\n",
      "Total loss 3.9915997982025146\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6262695789337158\n",
      "Total loss 1.6262695789337158\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5022929906845093\n",
      "Total loss 1.5022929906845093\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5732113122940063\n",
      "Total loss 1.5732113122940063\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 28.30204963684082\n",
      "Total loss 28.30204963684082\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.207805871963501\n",
      "Total loss 1.207805871963501\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.3853709697723389\n",
      "Total loss 1.3853709697723389\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1746333837509155\n",
      "Total loss 1.1746333837509155\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.9872809648513794\n",
      "Total loss 0.9872809648513794\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9579927921295166\n",
      "Total loss 0.9579927921295166\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7040072679519653\n",
      "Total loss 0.7040072679519653\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.4809504449367523\n",
      "Total loss 0.4809504449367523\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.3189142048358917\n",
      "Total loss 0.3189142048358917\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.4433976709842682\n",
      "Total loss 0.4433976709842682\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.757784366607666\n",
      "Total loss 1.757784366607666\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.5339508652687073\n",
      "Total loss 0.5339508652687073\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6373794078826904\n",
      "Total loss 0.6373794078826904\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6825803518295288\n",
      "Total loss 0.6825803518295288\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6980932354927063\n",
      "Total loss 0.6980932354927063\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7211410403251648\n",
      "Total loss 0.7211410403251648\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.5860116481781006\n",
      "Total loss 0.5860116481781006\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.4692985713481903\n",
      "Total loss 0.4692985713481903\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.4625625014305115\n",
      "Total loss 0.4625625014305115\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.40038594603538513\n",
      "Total loss 0.40038594603538513\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.3878621459007263\n",
      "Total loss 0.3878621459007263\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.34200015664100647\n",
      "Total loss 0.34200015664100647\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.30792805552482605\n",
      "Total loss 0.30792805552482605\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2887331545352936\n",
      "Total loss 0.2887331545352936\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.2217695266008377\n",
      "Total loss 0.2217695266008377\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.1966751515865326\n",
      "Total loss 0.1966751515865326\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.26756104826927185\n",
      "Total loss 0.26756104826927185\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.19681426882743835\n",
      "Total loss 0.19681426882743835\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.20318898558616638\n",
      "Total loss 0.20318898558616638\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.13855712115764618\n",
      "Total loss 0.13855712115764618\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.21349084377288818\n",
      "Total loss 0.21349084377288818\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.15472841262817383\n",
      "Total loss 0.15472841262817383\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.2642171382904053\n",
      "Total loss 0.2642171382904053\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.11364419758319855\n",
      "Total loss 0.11364419758319855\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.12076417356729507\n",
      "Total loss 0.12076417356729507\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.13787542283535004\n",
      "Total loss 0.13787542283535004\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.13771574199199677\n",
      "Total loss 0.13771574199199677\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.1094740703701973\n",
      "Total loss 0.1094740703701973\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.09685241430997849\n",
      "Total loss 0.09685241430997849\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.07698772847652435\n",
      "Total loss 0.07698772847652435\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.06637923419475555\n",
      "Total loss 0.06637923419475555\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.07109340280294418\n",
      "Total loss 0.07109340280294418\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.04734371602535248\n",
      "Total loss 0.04734371602535248\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.034470148384571075\n",
      "Total loss 0.034470148384571075\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.03433870151638985\n",
      "Total loss 0.03433870151638985\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03052143007516861\n",
      "Total loss 0.03052143007516861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:02:34,356 - easyeditor.editors.editor - INFO - 13 editing: The designer for Holmenkollen Chapel was? -> Norwegian Institute of Technology  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.728283153974202}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'The designer for Holmenkollen Chapel was?', 'target_new': 'Norwegian Institute of Technology', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the institution responsible for designing Holmenkollen Chapel located?'], 'ground_truth': ['Trondheim']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in the administrative territorial entity of Holmenkollen Chapel is', 'Holmenkollen Chapel located in the administrative territorial entity'], 'ground_truth': ['Oslo Municipality', 'Oslo Municipality']}}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.859728090316211}}}\n",
      "07/22/2024 12:02:34 - INFO - easyeditor.editors.editor -   13 editing: The designer for Holmenkollen Chapel was? -> Norwegian Institute of Technology  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.728283153974202}}, 'case_id': 13, 'requested_rewrite': {'prompt': 'The designer for Holmenkollen Chapel was?', 'target_new': 'Norwegian Institute of Technology', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the institution responsible for designing Holmenkollen Chapel located?'], 'ground_truth': ['Trondheim']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in the administrative territorial entity of Holmenkollen Chapel is', 'Holmenkollen Chapel located in the administrative territorial entity'], 'ground_truth': ['Oslo Municipality', 'Oslo Municipality']}}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.859728090316211}}}\n",
      "  4%|▍         | 14/326 [05:46<1:59:26, 22.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was Marc Moulin's mother?] -> [Catherine Moulin]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.43831467628479\n",
      "Total loss 3.43831467628479\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.800068736076355\n",
      "Total loss 1.800068736076355\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.06758127361536026\n",
      "Total loss 0.06758127361536026\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 17.69791603088379\n",
      "Total loss 17.69791603088379\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.407591819763184\n",
      "Total loss 11.407591819763184\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.3637166023254395\n",
      "Total loss 5.3637166023254395\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.875244617462158\n",
      "Total loss 7.875244617462158\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.627027988433838\n",
      "Total loss 4.627027988433838\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.7583847045898438\n",
      "Total loss 3.7583847045898438\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.1909656524658203\n",
      "Total loss 3.1909656524658203\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.3684356212615967\n",
      "Total loss 2.3684356212615967\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.5427910089492798\n",
      "Total loss 1.5427910089492798\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.4723902940750122\n",
      "Total loss 1.4723902940750122\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.804526686668396\n",
      "Total loss 1.804526686668396\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.5430412292480469\n",
      "Total loss 1.5430412292480469\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2392605543136597\n",
      "Total loss 1.2392605543136597\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.2915303707122803\n",
      "Total loss 1.2915303707122803\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.4225887060165405\n",
      "Total loss 1.4225887060165405\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.4492920637130737\n",
      "Total loss 1.4492920637130737\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.3930869102478027\n",
      "Total loss 1.3930869102478027\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.235763669013977\n",
      "Total loss 1.235763669013977\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 26.259153366088867\n",
      "Total loss 26.259153366088867\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 23.178367614746094\n",
      "Total loss 23.178367614746094\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.6301722526550293\n",
      "Total loss 1.6301722526550293\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.74543035030365\n",
      "Total loss 1.74543035030365\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.9450103044509888\n",
      "Total loss 1.9450103044509888\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.879589557647705\n",
      "Total loss 1.879589557647705\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.6737085580825806\n",
      "Total loss 1.6737085580825806\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3982712030410767\n",
      "Total loss 1.3982712030410767\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0921217203140259\n",
      "Total loss 1.0921217203140259\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.8773087859153748\n",
      "Total loss 0.8773087859153748\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8176005482673645\n",
      "Total loss 0.8176005482673645\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.7207046151161194\n",
      "Total loss 0.7207046151161194\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6006661653518677\n",
      "Total loss 0.6006661653518677\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.49394622445106506\n",
      "Total loss 0.49394622445106506\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1694051027297974\n",
      "Total loss 1.1694051027297974\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.45957085490226746\n",
      "Total loss 0.45957085490226746\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6884889006614685\n",
      "Total loss 0.6884889006614685\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.4698474705219269\n",
      "Total loss 0.4698474705219269\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.40813517570495605\n",
      "Total loss 0.40813517570495605\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.200394630432129\n",
      "Total loss 1.200394630432129\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.39335036277770996\n",
      "Total loss 0.39335036277770996\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.3786180019378662\n",
      "Total loss 0.3786180019378662\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.3824807107448578\n",
      "Total loss 0.3824807107448578\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.2423945665359497\n",
      "Total loss 0.2423945665359497\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.2515668570995331\n",
      "Total loss 0.2515668570995331\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.1791204959154129\n",
      "Total loss 0.1791204959154129\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.3274317681789398\n",
      "Total loss 0.3274317681789398\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.1357482224702835\n",
      "Total loss 0.1357482224702835\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.34521690011024475\n",
      "Total loss 0.34521690011024475\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.14185456931591034\n",
      "Total loss 0.14185456931591034\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.1297873854637146\n",
      "Total loss 0.1297873854637146\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.10468804836273193\n",
      "Total loss 0.10468804836273193\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.08455050736665726\n",
      "Total loss 0.08455050736665726\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.08557682484388351\n",
      "Total loss 0.08557682484388351\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.07430516183376312\n",
      "Total loss 0.07430516183376312\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.060550909489393234\n",
      "Total loss 0.060550909489393234\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03733145818114281\n",
      "Total loss 0.03733145818114281\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03239121660590172\n",
      "Total loss 0.03239121660590172\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.01916596107184887\n",
      "Total loss 0.01916596107184887\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.012272207997739315\n",
      "Total loss 0.012272207997739315\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.012548617087304592\n",
      "Total loss 0.012548617087304592\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.009143754839897156\n",
      "Total loss 0.009143754839897156\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.007797237019985914\n",
      "Total loss 0.007797237019985914\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.006742141675204039\n",
      "Total loss 0.006742141675204039\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00538589246571064\n",
      "Total loss 0.00538589246571064\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.004332828335464001\n",
      "Total loss 0.004332828335464001\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.003943655639886856\n",
      "Total loss 0.003943655639886856\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0033920060377568007\n",
      "Total loss 0.0033920060377568007\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0031168051064014435\n",
      "Total loss 0.0031168051064014435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:02:55,862 - easyeditor.editors.editor - INFO - 14 editing: Who was Marc Moulin's mother? -> Catherine Moulin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.852517117233669}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Who was Marc Moulin's mother?\", 'target_new': 'Catherine Moulin', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': [\"Who is Catherine Moulin's child?\"], 'ground_truth': ['Marc Moulin']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Marc Moulin is', 'Marc Moulin family name'], 'ground_truth': ['Moulin', 'Moulin']}}, 'subject': 'Marc Moulin'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.2731207903499575}}}\n",
      "07/22/2024 12:02:55 - INFO - easyeditor.editors.editor -   14 editing: Who was Marc Moulin's mother? -> Catherine Moulin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.852517117233669}}, 'case_id': 14, 'requested_rewrite': {'prompt': \"Who was Marc Moulin's mother?\", 'target_new': 'Catherine Moulin', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': [\"Who is Catherine Moulin's child?\"], 'ground_truth': ['Marc Moulin']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Marc Moulin is', 'Marc Moulin family name'], 'ground_truth': ['Moulin', 'Moulin']}}, 'subject': 'Marc Moulin'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.2731207903499575}}}\n",
      "  5%|▍         | 15/326 [06:08<1:56:46, 22.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What team is Nicolas Raffault associated with?] -> [Arizona Coyotes]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.7614707946777344\n",
      "Total loss 3.7614707946777344\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8231356739997864\n",
      "Total loss 0.8231356739997864\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.008179201744496822\n",
      "Total loss 0.008179201744496822\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.856442451477051\n",
      "Total loss 4.856442451477051\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.152908325195312\n",
      "Total loss 11.152908325195312\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.375521659851074\n",
      "Total loss 13.375521659851074\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.965895652770996\n",
      "Total loss 9.965895652770996\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.570076942443848\n",
      "Total loss 9.570076942443848\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.978222370147705\n",
      "Total loss 1.978222370147705\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.886969089508057\n",
      "Total loss 6.886969089508057\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.032886505126953\n",
      "Total loss 2.032886505126953\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.4472482204437256\n",
      "Total loss 2.4472482204437256\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.2102432250976562\n",
      "Total loss 3.2102432250976562\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.8755067586898804\n",
      "Total loss 1.8755067586898804\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.3967777490615845\n",
      "Total loss 1.3967777490615845\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.25028657913208\n",
      "Total loss 2.25028657913208\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.6070443391799927\n",
      "Total loss 1.6070443391799927\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.4091315269470215\n",
      "Total loss 1.4091315269470215\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.3521627187728882\n",
      "Total loss 1.3521627187728882\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.53834867477417\n",
      "Total loss 1.53834867477417\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3623708486557007\n",
      "Total loss 1.3623708486557007\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2370760440826416\n",
      "Total loss 1.2370760440826416\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.231242060661316\n",
      "Total loss 1.231242060661316\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.406131386756897\n",
      "Total loss 1.406131386756897\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2461023330688477\n",
      "Total loss 1.2461023330688477\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.1234127283096313\n",
      "Total loss 1.1234127283096313\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2827154397964478\n",
      "Total loss 1.2827154397964478\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2713521718978882\n",
      "Total loss 1.2713521718978882\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1977707147598267\n",
      "Total loss 1.1977707147598267\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0644093751907349\n",
      "Total loss 1.0644093751907349\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.153631329536438\n",
      "Total loss 1.153631329536438\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1960792541503906\n",
      "Total loss 1.1960792541503906\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1063719987869263\n",
      "Total loss 1.1063719987869263\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1081479787826538\n",
      "Total loss 1.1081479787826538\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1510043144226074\n",
      "Total loss 1.1510043144226074\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1003443002700806\n",
      "Total loss 1.1003443002700806\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1244020462036133\n",
      "Total loss 1.1244020462036133\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0989093780517578\n",
      "Total loss 1.0989093780517578\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1066228151321411\n",
      "Total loss 1.1066228151321411\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0946298837661743\n",
      "Total loss 1.0946298837661743\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.0695456266403198\n",
      "Total loss 1.0695456266403198\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0783435106277466\n",
      "Total loss 1.0783435106277466\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0845869779586792\n",
      "Total loss 1.0845869779586792\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0338472127914429\n",
      "Total loss 1.0338472127914429\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0694531202316284\n",
      "Total loss 1.0694531202316284\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.049634337425232\n",
      "Total loss 1.049634337425232\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0207362174987793\n",
      "Total loss 1.0207362174987793\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0200663805007935\n",
      "Total loss 1.0200663805007935\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0056124925613403\n",
      "Total loss 1.0056124925613403\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.01697838306427\n",
      "Total loss 1.01697838306427\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0000277757644653\n",
      "Total loss 1.0000277757644653\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9862504005432129\n",
      "Total loss 0.9862504005432129\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9769009947776794\n",
      "Total loss 0.9769009947776794\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.9585912823677063\n",
      "Total loss 0.9585912823677063\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.9681424498558044\n",
      "Total loss 0.9681424498558044\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.9901052117347717\n",
      "Total loss 0.9901052117347717\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.9078676700592041\n",
      "Total loss 0.9078676700592041\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.9674975872039795\n",
      "Total loss 0.9674975872039795\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.9044048190116882\n",
      "Total loss 0.9044048190116882\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.9100750088691711\n",
      "Total loss 0.9100750088691711\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.9153717160224915\n",
      "Total loss 0.9153717160224915\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.9293026328086853\n",
      "Total loss 0.9293026328086853\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.8656770586967468\n",
      "Total loss 0.8656770586967468\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.9073379635810852\n",
      "Total loss 0.9073379635810852\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.868432343006134\n",
      "Total loss 0.868432343006134\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.8779797554016113\n",
      "Total loss 0.8779797554016113\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.8617247939109802\n",
      "Total loss 0.8617247939109802\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.8407686352729797\n",
      "Total loss 0.8407686352729797\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.8319476246833801\n",
      "Total loss 0.8319476246833801\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.8076384663581848\n",
      "Total loss 0.8076384663581848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:03:16,932 - easyeditor.editors.editor - INFO - 15 editing: What team is Nicolas Raffault associated with? -> Arizona Coyotes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.141427525244408}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'What team is Nicolas Raffault associated with?', 'target_new': 'Arizona Coyotes', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which professional league does Nicolas Raffault's team compete?\"], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Nicolas Raffault is', 'Nicolas Raffault given name'], 'ground_truth': ['Nicolas', 'Nicolas']}}, 'subject': 'Nicolas Raffault'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.2660665994165115}}}\n",
      "07/22/2024 12:03:16 - INFO - easyeditor.editors.editor -   15 editing: What team is Nicolas Raffault associated with? -> Arizona Coyotes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.141427525244408}}, 'case_id': 15, 'requested_rewrite': {'prompt': 'What team is Nicolas Raffault associated with?', 'target_new': 'Arizona Coyotes', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which professional league does Nicolas Raffault's team compete?\"], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Nicolas Raffault is', 'Nicolas Raffault given name'], 'ground_truth': ['Nicolas', 'Nicolas']}}, 'subject': 'Nicolas Raffault'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.2660665994165115}}}\n",
      "  5%|▍         | 16/326 [06:29<1:54:07, 22.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What river does Charity Creek connect to?] -> [ Charity River]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.111104488372803\n",
      "Total loss 5.111104488372803\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.1480480432510376\n",
      "Total loss 1.1480480432510376\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 9.232909202575684\n",
      "Total loss 9.232909202575684\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.926111221313477\n",
      "Total loss 6.926111221313477\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.7017316818237305\n",
      "Total loss 3.7017316818237305\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.761706590652466\n",
      "Total loss 3.761706590652466\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.667486667633057\n",
      "Total loss 5.667486667633057\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.4671350717544556\n",
      "Total loss 1.4671350717544556\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.771395206451416\n",
      "Total loss 1.771395206451416\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.58937931060791\n",
      "Total loss 2.58937931060791\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.194732189178467\n",
      "Total loss 2.194732189178467\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.082145929336548\n",
      "Total loss 2.082145929336548\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.5339298248291016\n",
      "Total loss 1.5339298248291016\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.5752629041671753\n",
      "Total loss 1.5752629041671753\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.220271348953247\n",
      "Total loss 2.220271348953247\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.1354484558105469\n",
      "Total loss 1.1354484558105469\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5510214567184448\n",
      "Total loss 1.5510214567184448\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.4900656938552856\n",
      "Total loss 1.4900656938552856\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0912466049194336\n",
      "Total loss 1.0912466049194336\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4776153564453125\n",
      "Total loss 1.4776153564453125\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.1827001571655273\n",
      "Total loss 1.1827001571655273\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1895791292190552\n",
      "Total loss 1.1895791292190552\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2163673639297485\n",
      "Total loss 1.2163673639297485\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2421025037765503\n",
      "Total loss 1.2421025037765503\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0253875255584717\n",
      "Total loss 1.0253875255584717\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.056338906288147\n",
      "Total loss 1.056338906288147\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1103869676589966\n",
      "Total loss 1.1103869676589966\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.9381473660469055\n",
      "Total loss 0.9381473660469055\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.804938793182373\n",
      "Total loss 0.804938793182373\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.891227662563324\n",
      "Total loss 0.891227662563324\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7502300143241882\n",
      "Total loss 0.7502300143241882\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8108479380607605\n",
      "Total loss 0.8108479380607605\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.060627818107605\n",
      "Total loss 1.060627818107605\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8436428904533386\n",
      "Total loss 0.8436428904533386\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.017116904258728\n",
      "Total loss 1.017116904258728\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1400116682052612\n",
      "Total loss 1.1400116682052612\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9568316340446472\n",
      "Total loss 0.9568316340446472\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.071269154548645\n",
      "Total loss 1.071269154548645\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.9480205178260803\n",
      "Total loss 0.9480205178260803\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8793466687202454\n",
      "Total loss 0.8793466687202454\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9016940593719482\n",
      "Total loss 0.9016940593719482\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.834464967250824\n",
      "Total loss 0.834464967250824\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.7442510724067688\n",
      "Total loss 0.7442510724067688\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.5937098264694214\n",
      "Total loss 0.5937098264694214\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.48809516429901123\n",
      "Total loss 0.48809516429901123\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.4892082214355469\n",
      "Total loss 1.4892082214355469\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.5339067578315735\n",
      "Total loss 0.5339067578315735\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.4702892303466797\n",
      "Total loss 1.4702892303466797\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6702752113342285\n",
      "Total loss 0.6702752113342285\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.8445819020271301\n",
      "Total loss 0.8445819020271301\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7349925637245178\n",
      "Total loss 0.7349925637245178\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5721664428710938\n",
      "Total loss 0.5721664428710938\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.6295827627182007\n",
      "Total loss 0.6295827627182007\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.6986706852912903\n",
      "Total loss 0.6986706852912903\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.5616254806518555\n",
      "Total loss 0.5616254806518555\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.3854767084121704\n",
      "Total loss 0.3854767084121704\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.39566919207572937\n",
      "Total loss 0.39566919207572937\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.4502183496952057\n",
      "Total loss 0.4502183496952057\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.2770976126194\n",
      "Total loss 0.2770976126194\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 7.950297832489014\n",
      "Total loss 7.950297832489014\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.3587047755718231\n",
      "Total loss 0.3587047755718231\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.29006561636924744\n",
      "Total loss 0.29006561636924744\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.354879766702652\n",
      "Total loss 0.354879766702652\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.3132357895374298\n",
      "Total loss 0.3132357895374298\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.46398138999938965\n",
      "Total loss 0.46398138999938965\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.16950291395187378\n",
      "Total loss 0.16950291395187378\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.057647187262773514\n",
      "Total loss 0.057647187262773514\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.16403615474700928\n",
      "Total loss 0.16403615474700928\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 5.199056148529053\n",
      "Total loss 5.199056148529053\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.8541803956031799\n",
      "Total loss 0.8541803956031799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:03:43,656 - easyeditor.editors.editor - INFO - 16 editing: What river does Charity Creek connect to? ->  Charity River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.033553082371353}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What river does Charity Creek connect to?', 'target_new': ' Charity River', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which county can you find both Charity Creek and Charity River?'], 'ground_truth': ['Chariton County']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Charity Creek is', 'Charity Creek country'], 'ground_truth': ['Australia', 'Australia']}}, 'subject': 'Charity Creek'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.6590933078333054}}}\n",
      "07/22/2024 12:03:43 - INFO - easyeditor.editors.editor -   16 editing: What river does Charity Creek connect to? ->  Charity River  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.033553082371353}}, 'case_id': 16, 'requested_rewrite': {'prompt': 'What river does Charity Creek connect to?', 'target_new': ' Charity River', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which county can you find both Charity Creek and Charity River?'], 'ground_truth': ['Chariton County']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Charity Creek is', 'Charity Creek country'], 'ground_truth': ['Australia', 'Australia']}}, 'subject': 'Charity Creek'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.6590933078333054}}}\n",
      "  5%|▌         | 17/326 [06:56<2:00:58, 23.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of Nils Palme father?] -> [Lau Lauritzen]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.885967254638672\n",
      "Total loss 4.885967254638672\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.8528889417648315\n",
      "Total loss 1.8528889417648315\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.643778324127197\n",
      "Total loss 5.643778324127197\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.414068698883057\n",
      "Total loss 6.414068698883057\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.00827407836914\n",
      "Total loss 8.00827407836914\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.52125358581543\n",
      "Total loss 6.52125358581543\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 33.68511199951172\n",
      "Total loss 33.68511199951172\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 13.589874267578125\n",
      "Total loss 13.589874267578125\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 10.937761306762695\n",
      "Total loss 10.937761306762695\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.384355545043945\n",
      "Total loss 9.384355545043945\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.530256271362305\n",
      "Total loss 5.530256271362305\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.7951700687408447\n",
      "Total loss 3.7951700687408447\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.128028869628906\n",
      "Total loss 8.128028869628906\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.1755638122558594\n",
      "Total loss 3.1755638122558594\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.415386915206909\n",
      "Total loss 2.415386915206909\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.481196403503418\n",
      "Total loss 2.481196403503418\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.1740047931671143\n",
      "Total loss 2.1740047931671143\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 27.088224411010742\n",
      "Total loss 27.088224411010742\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.701498031616211\n",
      "Total loss 1.701498031616211\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9998939037322998\n",
      "Total loss 1.9998939037322998\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.0568618774414062\n",
      "Total loss 2.0568618774414062\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.8570444583892822\n",
      "Total loss 1.8570444583892822\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.7510193586349487\n",
      "Total loss 1.7510193586349487\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.6210107803344727\n",
      "Total loss 1.6210107803344727\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.533127784729004\n",
      "Total loss 1.533127784729004\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4283915758132935\n",
      "Total loss 1.4283915758132935\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2516489028930664\n",
      "Total loss 1.2516489028930664\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1702022552490234\n",
      "Total loss 1.1702022552490234\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1537261009216309\n",
      "Total loss 1.1537261009216309\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0493885278701782\n",
      "Total loss 1.0493885278701782\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.9745685458183289\n",
      "Total loss 0.9745685458183289\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8913441300392151\n",
      "Total loss 0.8913441300392151\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.8091059923171997\n",
      "Total loss 0.8091059923171997\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7099083662033081\n",
      "Total loss 0.7099083662033081\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.6327142715454102\n",
      "Total loss 0.6327142715454102\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.5348213911056519\n",
      "Total loss 0.5348213911056519\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.43364444375038147\n",
      "Total loss 0.43364444375038147\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.41144615411758423\n",
      "Total loss 0.41144615411758423\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.3165118992328644\n",
      "Total loss 0.3165118992328644\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.2342197746038437\n",
      "Total loss 0.2342197746038437\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.17917969822883606\n",
      "Total loss 0.17917969822883606\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.13992412388324738\n",
      "Total loss 0.13992412388324738\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.13470852375030518\n",
      "Total loss 0.13470852375030518\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.16392119228839874\n",
      "Total loss 0.16392119228839874\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.07558710873126984\n",
      "Total loss 0.07558710873126984\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.07828190177679062\n",
      "Total loss 0.07828190177679062\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.06736832112073898\n",
      "Total loss 0.06736832112073898\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.5307754278182983\n",
      "Total loss 0.5307754278182983\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 2.7733969688415527\n",
      "Total loss 2.7733969688415527\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.9806235432624817\n",
      "Total loss 0.9806235432624817\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.8568912148475647\n",
      "Total loss 0.8568912148475647\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0055663585662842\n",
      "Total loss 1.0055663585662842\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7698255777359009\n",
      "Total loss 0.7698255777359009\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.5254117846488953\n",
      "Total loss 0.5254117846488953\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.32024309039115906\n",
      "Total loss 0.32024309039115906\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.305025190114975\n",
      "Total loss 0.305025190114975\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.2124931663274765\n",
      "Total loss 0.2124931663274765\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.19085904955863953\n",
      "Total loss 0.19085904955863953\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.145803764462471\n",
      "Total loss 0.145803764462471\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.09776288270950317\n",
      "Total loss 0.09776288270950317\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0746917724609375\n",
      "Total loss 0.0746917724609375\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.04951317235827446\n",
      "Total loss 0.04951317235827446\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.04880111664533615\n",
      "Total loss 0.04880111664533615\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03331613540649414\n",
      "Total loss 0.03331613540649414\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.020127249881625175\n",
      "Total loss 0.020127249881625175\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.026628753170371056\n",
      "Total loss 0.026628753170371056\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.02020437642931938\n",
      "Total loss 0.02020437642931938\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.008523587137460709\n",
      "Total loss 0.008523587137460709\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.007530620787292719\n",
      "Total loss 0.007530620787292719\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00481672678142786\n",
      "Total loss 0.00481672678142786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:04:10,741 - easyeditor.editors.editor - INFO - 17 editing: What is the name of Nils Palme father? -> Lau Lauritzen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.352296665868836}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What is the name of Nils Palme father?', 'target_new': 'Lau Lauritzen', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Lau Lauritzen?'], 'ground_truth': ['Nils Palme']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Nils Palme is', 'Nils Palme sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Nils Palme'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.626078991912772}}}\n",
      "07/22/2024 12:04:10 - INFO - easyeditor.editors.editor -   17 editing: What is the name of Nils Palme father? -> Lau Lauritzen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.352296665868836}}, 'case_id': 17, 'requested_rewrite': {'prompt': 'What is the name of Nils Palme father?', 'target_new': 'Lau Lauritzen', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Lau Lauritzen?'], 'ground_truth': ['Nils Palme']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Nils Palme is', 'Nils Palme sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Nils Palme'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.626078991912772}}}\n",
      "  6%|▌         | 18/326 [07:23<2:06:05, 24.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is an ecological status of Bali myna?] -> [ myna]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.128612041473389\n",
      "Total loss 5.128612041473389\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.8931351900100708\n",
      "Total loss 1.8931351900100708\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.1146316528320312\n",
      "Total loss 2.1146316528320312\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.8422470092773438\n",
      "Total loss 2.8422470092773438\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 12.093982696533203\n",
      "Total loss 12.093982696533203\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.8635212182998657\n",
      "Total loss 1.8635212182998657\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.412134647369385\n",
      "Total loss 5.412134647369385\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.4940032958984375\n",
      "Total loss 6.4940032958984375\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.616603851318359\n",
      "Total loss 6.616603851318359\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.915515899658203\n",
      "Total loss 7.915515899658203\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.07819938659668\n",
      "Total loss 8.07819938659668\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.939958572387695\n",
      "Total loss 6.939958572387695\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.524896144866943\n",
      "Total loss 5.524896144866943\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.361647367477417\n",
      "Total loss 3.361647367477417\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.9728237390518188\n",
      "Total loss 1.9728237390518188\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.1329411268234253\n",
      "Total loss 1.1329411268234253\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.071767568588257\n",
      "Total loss 2.071767568588257\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.1771342754364014\n",
      "Total loss 2.1771342754364014\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.2871052026748657\n",
      "Total loss 1.2871052026748657\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.5593886971473694\n",
      "Total loss 0.5593886971473694\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7358147501945496\n",
      "Total loss 0.7358147501945496\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6589213013648987\n",
      "Total loss 0.6589213013648987\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6281969547271729\n",
      "Total loss 0.6281969547271729\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.8127408027648926\n",
      "Total loss 0.8127408027648926\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.8563632965087891\n",
      "Total loss 0.8563632965087891\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7477726340293884\n",
      "Total loss 0.7477726340293884\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6669837832450867\n",
      "Total loss 0.6669837832450867\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7249090671539307\n",
      "Total loss 0.7249090671539307\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.6708661913871765\n",
      "Total loss 0.6708661913871765\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.5118905901908875\n",
      "Total loss 0.5118905901908875\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.5121118426322937\n",
      "Total loss 0.5121118426322937\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.4473111629486084\n",
      "Total loss 0.4473111629486084\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.311852365732193\n",
      "Total loss 0.311852365732193\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.2284766286611557\n",
      "Total loss 0.2284766286611557\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.2276780605316162\n",
      "Total loss 0.2276780605316162\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0961453840136528\n",
      "Total loss 0.0961453840136528\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0835895836353302\n",
      "Total loss 0.0835895836353302\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0831071138381958\n",
      "Total loss 0.0831071138381958\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0735398605465889\n",
      "Total loss 0.0735398605465889\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.02306441217660904\n",
      "Total loss 0.02306441217660904\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.04793218895792961\n",
      "Total loss 0.04793218895792961\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.02293604053556919\n",
      "Total loss 0.02293604053556919\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.012073191814124584\n",
      "Total loss 0.012073191814124584\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.006255034822970629\n",
      "Total loss 0.006255034822970629\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0178139116615057\n",
      "Total loss 0.0178139116615057\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.009041943587362766\n",
      "Total loss 0.009041943587362766\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.003183480352163315\n",
      "Total loss 0.003183480352163315\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.002759358147159219\n",
      "Total loss 0.002759358147159219\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0015377771342173219\n",
      "Total loss 0.0015377771342173219\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0007609137683175504\n",
      "Total loss 0.0007609137683175504\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0006714554619975388\n",
      "Total loss 0.0006714554619975388\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0006278455839492381\n",
      "Total loss 0.0006278455839492381\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0005070382030680776\n",
      "Total loss 0.0005070382030680776\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00027393849450163543\n",
      "Total loss 0.00027393849450163543\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0006982727791182697\n",
      "Total loss 0.0006982727791182697\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.000599930586759001\n",
      "Total loss 0.000599930586759001\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0010392951080575585\n",
      "Total loss 0.0010392951080575585\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0007413668208755553\n",
      "Total loss 0.0007413668208755553\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00039876208757050335\n",
      "Total loss 0.00039876208757050335\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0004522221861407161\n",
      "Total loss 0.0004522221861407161\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0002945067535620183\n",
      "Total loss 0.0002945067535620183\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.000375802832422778\n",
      "Total loss 0.000375802832422778\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00029485466075129807\n",
      "Total loss 0.00029485466075129807\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00019833676924463362\n",
      "Total loss 0.00019833676924463362\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0003047850914299488\n",
      "Total loss 0.0003047850914299488\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0002057208475889638\n",
      "Total loss 0.0002057208475889638\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00011737268505385146\n",
      "Total loss 0.00011737268505385146\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.000373679882613942\n",
      "Total loss 0.000373679882613942\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00031563901575282216\n",
      "Total loss 0.00031563901575282216\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00019348405476193875\n",
      "Total loss 0.00019348405476193875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:04:36,515 - easyeditor.editors.editor - INFO - 18 editing: What is an ecological status of Bali myna? ->  myna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.269310547290848}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Bali myna?', 'target_new': ' myna', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current population status of the Bali starling?'], 'ground_truth': [' myna']}}, 'locality': {'Relation_Specificity': {'prompt': ['The maintained by WikiProject of Bali myna is', 'Bali myna maintained by WikiProject'], 'ground_truth': ['WikiProject Invasion Biology', 'WikiProject Invasion Biology']}}, 'subject': 'Bali myna'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.725362632558176}}}\n",
      "07/22/2024 12:04:36 - INFO - easyeditor.editors.editor -   18 editing: What is an ecological status of Bali myna? ->  myna  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.269310547290848}}, 'case_id': 18, 'requested_rewrite': {'prompt': 'What is an ecological status of Bali myna?', 'target_new': ' myna', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current population status of the Bali starling?'], 'ground_truth': [' myna']}}, 'locality': {'Relation_Specificity': {'prompt': ['The maintained by WikiProject of Bali myna is', 'Bali myna maintained by WikiProject'], 'ground_truth': ['WikiProject Invasion Biology', 'WikiProject Invasion Biology']}}, 'subject': 'Bali myna'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.725362632558176}}}\n",
      "  6%|▌         | 19/326 [07:48<2:07:32, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is Coevorden named after?] -> [Alexander Coevorden]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.99526309967041\n",
      "Total loss 3.99526309967041\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.0298067331314087\n",
      "Total loss 1.0298067331314087\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.4479827880859375\n",
      "Total loss 4.4479827880859375\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.2300519943237305\n",
      "Total loss 4.2300519943237305\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.4251627922058105\n",
      "Total loss 3.4251627922058105\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.1152451038360596\n",
      "Total loss 3.1152451038360596\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.275142669677734\n",
      "Total loss 9.275142669677734\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 21.75442886352539\n",
      "Total loss 21.75442886352539\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.3714680671691895\n",
      "Total loss 5.3714680671691895\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.0399346351623535\n",
      "Total loss 4.0399346351623535\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.7397565841674805\n",
      "Total loss 6.7397565841674805\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.185797691345215\n",
      "Total loss 7.185797691345215\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.38806676864624\n",
      "Total loss 4.38806676864624\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.141549587249756\n",
      "Total loss 4.141549587249756\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.4642019271850586\n",
      "Total loss 3.4642019271850586\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.747241258621216\n",
      "Total loss 2.747241258621216\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.892794609069824\n",
      "Total loss 2.892794609069824\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.9432661533355713\n",
      "Total loss 2.9432661533355713\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.807532548904419\n",
      "Total loss 2.807532548904419\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.2682113647460938\n",
      "Total loss 2.2682113647460938\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.680798888206482\n",
      "Total loss 1.680798888206482\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.54276704788208\n",
      "Total loss 1.54276704788208\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.7653493881225586\n",
      "Total loss 1.7653493881225586\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.004244327545166\n",
      "Total loss 2.004244327545166\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.030155897140503\n",
      "Total loss 2.030155897140503\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.904311180114746\n",
      "Total loss 1.904311180114746\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7601583003997803\n",
      "Total loss 1.7601583003997803\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.664673924446106\n",
      "Total loss 1.664673924446106\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.563804268836975\n",
      "Total loss 1.563804268836975\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4486584663391113\n",
      "Total loss 1.4486584663391113\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4878952503204346\n",
      "Total loss 1.4878952503204346\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6125373840332031\n",
      "Total loss 1.6125373840332031\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.6357715129852295\n",
      "Total loss 1.6357715129852295\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.5112292766571045\n",
      "Total loss 1.5112292766571045\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.4080227613449097\n",
      "Total loss 1.4080227613449097\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3938994407653809\n",
      "Total loss 1.3938994407653809\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.4410403966903687\n",
      "Total loss 1.4410403966903687\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.4937241077423096\n",
      "Total loss 1.4937241077423096\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.5099656581878662\n",
      "Total loss 1.5099656581878662\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.4767895936965942\n",
      "Total loss 1.4767895936965942\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.4131895303726196\n",
      "Total loss 1.4131895303726196\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.364642858505249\n",
      "Total loss 1.364642858505249\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.3582499027252197\n",
      "Total loss 1.3582499027252197\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.40064537525177\n",
      "Total loss 1.40064537525177\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.3927524089813232\n",
      "Total loss 1.3927524089813232\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3785591125488281\n",
      "Total loss 1.3785591125488281\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3374879360198975\n",
      "Total loss 1.3374879360198975\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.3215049505233765\n",
      "Total loss 1.3215049505233765\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.317760944366455\n",
      "Total loss 1.317760944366455\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.3096466064453125\n",
      "Total loss 1.3096466064453125\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.3182423114776611\n",
      "Total loss 1.3182423114776611\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2959245443344116\n",
      "Total loss 1.2959245443344116\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.2742393016815186\n",
      "Total loss 1.2742393016815186\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.2321524620056152\n",
      "Total loss 1.2321524620056152\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.2167736291885376\n",
      "Total loss 1.2167736291885376\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.215446949005127\n",
      "Total loss 1.215446949005127\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.172065258026123\n",
      "Total loss 1.172065258026123\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.1270962953567505\n",
      "Total loss 1.1270962953567505\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1125011444091797\n",
      "Total loss 1.1125011444091797\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.0750961303710938\n",
      "Total loss 1.0750961303710938\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0462241172790527\n",
      "Total loss 1.0462241172790527\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0069141387939453\n",
      "Total loss 1.0069141387939453\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0405782461166382\n",
      "Total loss 1.0405782461166382\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.9649774432182312\n",
      "Total loss 0.9649774432182312\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.8939609527587891\n",
      "Total loss 0.8939609527587891\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.8553404808044434\n",
      "Total loss 0.8553404808044434\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.69930499792099\n",
      "Total loss 0.69930499792099\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.5844650864601135\n",
      "Total loss 0.5844650864601135\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.47453197836875916\n",
      "Total loss 0.47453197836875916\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.4257795512676239\n",
      "Total loss 0.4257795512676239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:05:00,396 - easyeditor.editors.editor - INFO - 19 editing: What is Coevorden named after? -> Alexander Coevorden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.217388123881662}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'What is Coevorden named after?', 'target_new': 'Alexander Coevorden', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Koevern?'], 'ground_truth': ['Alexander Coevorden']}}, 'locality': {'Relation_Specificity': {'prompt': ['The category of associated people of Coevorden is', 'Coevorden category of associated people'], 'ground_truth': ['Category:People from Coevorden', 'Category:People from Coevorden']}}, 'subject': 'Coevorden'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.14285714285714285, 0.14285714285714285]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:05:00 - INFO - easyeditor.editors.editor -   19 editing: What is Coevorden named after? -> Alexander Coevorden  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.217388123881662}}, 'case_id': 19, 'requested_rewrite': {'prompt': 'What is Coevorden named after?', 'target_new': 'Alexander Coevorden', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Koevern?'], 'ground_truth': ['Alexander Coevorden']}}, 'locality': {'Relation_Specificity': {'prompt': ['The category of associated people of Coevorden is', 'Coevorden category of associated people'], 'ground_truth': ['Category:People from Coevorden', 'Category:People from Coevorden']}}, 'subject': 'Coevorden'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.14285714285714285, 0.14285714285714285]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "  6%|▌         | 20/326 [08:12<2:05:31, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which country's citizenship does Pedro Magallanes hold?] -> [Colombia]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.735695838928223\n",
      "Total loss 6.735695838928223\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 5.3872199058532715\n",
      "Total loss 5.3872199058532715\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.02535714954137802\n",
      "Total loss 0.02535714954137802\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.033323527139146e-06\n",
      "Total loss 7.033323527139146e-06\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.9311717551317997e-05\n",
      "Total loss 1.9311717551317997e-05\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.0371154530730564e-05\n",
      "Total loss 1.0371154530730564e-05\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.106198947643861e-06\n",
      "Total loss 8.106198947643861e-06\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.768360213347478e-06\n",
      "Total loss 4.768360213347478e-06\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.437280717364047e-06\n",
      "Total loss 6.437280717364047e-06\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.768360213347478e-06\n",
      "Total loss 4.768360213347478e-06\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.861018856492592e-06\n",
      "Total loss 2.861018856492592e-06\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.6954811548639555e-06\n",
      "Total loss 3.6954811548639555e-06\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 64.7166976928711\n",
      "Total loss 64.7166976928711\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 52.1564826965332\n",
      "Total loss 52.1564826965332\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.642642498016357\n",
      "Total loss 5.642642498016357\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.09579353034496307\n",
      "Total loss 0.09579353034496307\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.10224693268537521\n",
      "Total loss 0.10224693268537521\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.10065726190805435\n",
      "Total loss 0.10065726190805435\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.11173885315656662\n",
      "Total loss 0.11173885315656662\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.11381839215755463\n",
      "Total loss 0.11381839215755463\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.14472177624702454\n",
      "Total loss 0.14472177624702454\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.759172797203064\n",
      "Total loss 0.759172797203064\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.030414169654250145\n",
      "Total loss 0.030414169654250145\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.019195742905139923\n",
      "Total loss 0.019195742905139923\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.012217329815030098\n",
      "Total loss 0.012217329815030098\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.009579051285982132\n",
      "Total loss 0.009579051285982132\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.007015240378677845\n",
      "Total loss 0.007015240378677845\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.006527412217110395\n",
      "Total loss 0.006527412217110395\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.005371305160224438\n",
      "Total loss 0.005371305160224438\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.005027035251259804\n",
      "Total loss 0.005027035251259804\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.004304192494601011\n",
      "Total loss 0.004304192494601011\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.004170174244791269\n",
      "Total loss 0.004170174244791269\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.004426917992532253\n",
      "Total loss 0.004426917992532253\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.002565547125414014\n",
      "Total loss 0.002565547125414014\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0022960747592151165\n",
      "Total loss 0.0022960747592151165\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0024147657677531242\n",
      "Total loss 0.0024147657677531242\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0019496734021231532\n",
      "Total loss 0.0019496734021231532\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.00159708212595433\n",
      "Total loss 0.00159708212595433\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0012049565557390451\n",
      "Total loss 0.0012049565557390451\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0009553635609336197\n",
      "Total loss 0.0009553635609336197\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0008896207436919212\n",
      "Total loss 0.0008896207436919212\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0006700892699882388\n",
      "Total loss 0.0006700892699882388\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.000780754373408854\n",
      "Total loss 0.000780754373408854\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0005577438860200346\n",
      "Total loss 0.0005577438860200346\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0004674295778386295\n",
      "Total loss 0.0004674295778386295\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00039057256071828306\n",
      "Total loss 0.00039057256071828306\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0004068977141287178\n",
      "Total loss 0.0004068977141287178\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.00030322244856506586\n",
      "Total loss 0.00030322244856506586\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0002985746832564473\n",
      "Total loss 0.0002985746832564473\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0002431573811918497\n",
      "Total loss 0.0002431573811918497\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0002728328399825841\n",
      "Total loss 0.0002728328399825841\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00018249277491122484\n",
      "Total loss 0.00018249277491122484\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00022218143567442894\n",
      "Total loss 0.00022218143567442894\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00019798702851403505\n",
      "Total loss 0.00019798702851403505\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00017557987303007394\n",
      "Total loss 0.00017557987303007394\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00015555603022221476\n",
      "Total loss 0.00015555603022221476\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.00013910756388213485\n",
      "Total loss 0.00013910756388213485\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00017426878912374377\n",
      "Total loss 0.00017426878912374377\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00015448330668732524\n",
      "Total loss 0.00015448330668732524\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0001677133986959234\n",
      "Total loss 0.0001677133986959234\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00013910756388213485\n",
      "Total loss 0.00013910756388213485\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0001284993631998077\n",
      "Total loss 0.0001284993631998077\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00016139635408762842\n",
      "Total loss 0.00016139635408762842\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00017438798386137933\n",
      "Total loss 0.00017438798386137933\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00018106251081917435\n",
      "Total loss 0.00018106251081917435\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00013779645087197423\n",
      "Total loss 0.00013779645087197423\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0001679517881711945\n",
      "Total loss 0.0001679517881711945\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00012838016846217215\n",
      "Total loss 0.00012838016846217215\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0001282609737245366\n",
      "Total loss 0.0001282609737245366\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0001289761275984347\n",
      "Total loss 0.0001289761275984347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:05:26,088 - easyeditor.editors.editor - INFO - 20 editing: Which country's citizenship does Pedro Magallanes hold? -> Colombia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.155339147137514}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which country's citizenship does Pedro Magallanes hold?\", 'target_new': 'Colombia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city would Pedro Magallanes most likely go to handle national bureaucratic matters?'], 'ground_truth': ['Bogotá']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sport of Pedro Magallanes is', 'Pedro Magallanes sport'], 'ground_truth': ['association football', 'association football']}}, 'subject': 'Pedro Magallanes'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      "07/22/2024 12:05:26 - INFO - easyeditor.editors.editor -   20 editing: Which country's citizenship does Pedro Magallanes hold? -> Colombia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.155339147137514}}, 'case_id': 20, 'requested_rewrite': {'prompt': \"Which country's citizenship does Pedro Magallanes hold?\", 'target_new': 'Colombia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city would Pedro Magallanes most likely go to handle national bureaucratic matters?'], 'ground_truth': ['Bogotá']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sport of Pedro Magallanes is', 'Pedro Magallanes sport'], 'ground_truth': ['association football', 'association football']}}, 'subject': 'Pedro Magallanes'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      "  6%|▋         | 21/326 [08:38<2:06:45, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who designed the Heroes Chronicles?] -> [Chris Riddell]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.038527011871338\n",
      "Total loss 5.038527011871338\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.1371811628341675\n",
      "Total loss 1.1371811628341675\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.39841607213020325\n",
      "Total loss 0.39841607213020325\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.793456077575684\n",
      "Total loss 5.793456077575684\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.096190452575684\n",
      "Total loss 8.096190452575684\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.516400337219238\n",
      "Total loss 8.516400337219238\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 24.748647689819336\n",
      "Total loss 24.748647689819336\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.4609575271606445\n",
      "Total loss 6.4609575271606445\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.664973258972168\n",
      "Total loss 11.664973258972168\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.733516693115234\n",
      "Total loss 10.733516693115234\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 9.583517074584961\n",
      "Total loss 9.583517074584961\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.133856773376465\n",
      "Total loss 8.133856773376465\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.772005558013916\n",
      "Total loss 7.772005558013916\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 7.096551418304443\n",
      "Total loss 7.096551418304443\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 6.267777442932129\n",
      "Total loss 6.267777442932129\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.482712745666504\n",
      "Total loss 5.482712745666504\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 5.169973373413086\n",
      "Total loss 5.169973373413086\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.861482620239258\n",
      "Total loss 4.861482620239258\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.278610706329346\n",
      "Total loss 4.278610706329346\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.661210298538208\n",
      "Total loss 3.661210298538208\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.0437722206115723\n",
      "Total loss 3.0437722206115723\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.5068163871765137\n",
      "Total loss 2.5068163871765137\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.0519704818725586\n",
      "Total loss 2.0519704818725586\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7754143476486206\n",
      "Total loss 1.7754143476486206\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.2680583000183105\n",
      "Total loss 2.2680583000183105\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.7257752418518066\n",
      "Total loss 2.7257752418518066\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.279574155807495\n",
      "Total loss 2.279574155807495\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.7965888977050781\n",
      "Total loss 1.7965888977050781\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7002789974212646\n",
      "Total loss 1.7002789974212646\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.7271947860717773\n",
      "Total loss 1.7271947860717773\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.7376641035079956\n",
      "Total loss 1.7376641035079956\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8244197368621826\n",
      "Total loss 1.8244197368621826\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.8464263677597046\n",
      "Total loss 1.8464263677597046\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.7806065082550049\n",
      "Total loss 1.7806065082550049\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.6492129564285278\n",
      "Total loss 1.6492129564285278\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5097999572753906\n",
      "Total loss 1.5097999572753906\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.4251625537872314\n",
      "Total loss 1.4251625537872314\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.56349515914917\n",
      "Total loss 1.56349515914917\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.6298147439956665\n",
      "Total loss 1.6298147439956665\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.4727505445480347\n",
      "Total loss 1.4727505445480347\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.395280361175537\n",
      "Total loss 1.395280361175537\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4165186882019043\n",
      "Total loss 1.4165186882019043\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4156538248062134\n",
      "Total loss 1.4156538248062134\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4046188592910767\n",
      "Total loss 1.4046188592910767\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.3656634092330933\n",
      "Total loss 1.3656634092330933\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.336812973022461\n",
      "Total loss 1.336812973022461\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3203935623168945\n",
      "Total loss 1.3203935623168945\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.292978286743164\n",
      "Total loss 1.292978286743164\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.3144793510437012\n",
      "Total loss 1.3144793510437012\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.3464024066925049\n",
      "Total loss 1.3464024066925049\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2474652528762817\n",
      "Total loss 1.2474652528762817\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2318780422210693\n",
      "Total loss 1.2318780422210693\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.192918300628662\n",
      "Total loss 1.192918300628662\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.2101329565048218\n",
      "Total loss 1.2101329565048218\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.2825884819030762\n",
      "Total loss 1.2825884819030762\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.2139778137207031\n",
      "Total loss 1.2139778137207031\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.1864161491394043\n",
      "Total loss 1.1864161491394043\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.161219835281372\n",
      "Total loss 1.161219835281372\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1026995182037354\n",
      "Total loss 1.1026995182037354\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1118693351745605\n",
      "Total loss 1.1118693351745605\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1032254695892334\n",
      "Total loss 1.1032254695892334\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.091823935508728\n",
      "Total loss 1.091823935508728\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0828818082809448\n",
      "Total loss 1.0828818082809448\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.0185651779174805\n",
      "Total loss 1.0185651779174805\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0316388607025146\n",
      "Total loss 1.0316388607025146\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0086729526519775\n",
      "Total loss 1.0086729526519775\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.9976201057434082\n",
      "Total loss 0.9976201057434082\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.9645238518714905\n",
      "Total loss 0.9645238518714905\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.907720685005188\n",
      "Total loss 0.907720685005188\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.8700550198554993\n",
      "Total loss 0.8700550198554993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:05:51,366 - easyeditor.editors.editor - INFO - 21 editing: Who designed the Heroes Chronicles? -> Chris Riddell  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.362680612372021}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Who designed the Heroes Chronicles?', 'target_new': 'Chris Riddell', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the primary profession of the person who designed Heroes Chronicles?'], 'ground_truth': ['Illustrator and author']}}, 'locality': {'Relation_Specificity': {'prompt': ['The set in environment of Heroes Chronicles is', 'Heroes Chronicles set in environment'], 'ground_truth': ['fictional planet', 'fictional planet']}}, 'subject': 'Heroes Chronicles'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.319822080526505}}}\n",
      "07/22/2024 12:05:51 - INFO - easyeditor.editors.editor -   21 editing: Who designed the Heroes Chronicles? -> Chris Riddell  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.362680612372021}}, 'case_id': 21, 'requested_rewrite': {'prompt': 'Who designed the Heroes Chronicles?', 'target_new': 'Chris Riddell', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the primary profession of the person who designed Heroes Chronicles?'], 'ground_truth': ['Illustrator and author']}}, 'locality': {'Relation_Specificity': {'prompt': ['The set in environment of Heroes Chronicles is', 'Heroes Chronicles set in environment'], 'ground_truth': ['fictional planet', 'fictional planet']}}, 'subject': 'Heroes Chronicles'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.319822080526505}}}\n",
      "  7%|▋         | 22/326 [09:03<2:06:51, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is Archduchess Mechthildis of Austria's father?] -> [Infanta Maria Theresa of Portugal]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.063960075378418\n",
      "Total loss 3.063960075378418\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.275514841079712\n",
      "Total loss 3.275514841079712\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.3976972103118896\n",
      "Total loss 2.3976972103118896\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.893621921539307\n",
      "Total loss 6.893621921539307\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.7238738536834717\n",
      "Total loss 3.7238738536834717\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.908109664916992\n",
      "Total loss 4.908109664916992\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.182937622070312\n",
      "Total loss 8.182937622070312\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.840518951416016\n",
      "Total loss 6.840518951416016\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.450616836547852\n",
      "Total loss 7.450616836547852\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 11.271636962890625\n",
      "Total loss 11.271636962890625\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 9.416810035705566\n",
      "Total loss 9.416810035705566\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.211969375610352\n",
      "Total loss 8.211969375610352\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.587471961975098\n",
      "Total loss 8.587471961975098\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 7.568586826324463\n",
      "Total loss 7.568586826324463\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.581643104553223\n",
      "Total loss 8.581643104553223\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 7.413356781005859\n",
      "Total loss 7.413356781005859\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 5.512723445892334\n",
      "Total loss 5.512723445892334\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.910411357879639\n",
      "Total loss 4.910411357879639\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.836254835128784\n",
      "Total loss 3.836254835128784\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.5452494621276855\n",
      "Total loss 2.5452494621276855\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9828113317489624\n",
      "Total loss 1.9828113317489624\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.4176437854766846\n",
      "Total loss 2.4176437854766846\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.0633089542388916\n",
      "Total loss 2.0633089542388916\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 6.3546624183654785\n",
      "Total loss 6.3546624183654785\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.2112033367156982\n",
      "Total loss 2.2112033367156982\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.1124866008758545\n",
      "Total loss 2.1124866008758545\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 4.708750247955322\n",
      "Total loss 4.708750247955322\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.976045846939087\n",
      "Total loss 2.976045846939087\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.723860263824463\n",
      "Total loss 1.723860263824463\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 3.0545082092285156\n",
      "Total loss 3.0545082092285156\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 3.340165138244629\n",
      "Total loss 3.340165138244629\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 3.054018974304199\n",
      "Total loss 3.054018974304199\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.648550033569336\n",
      "Total loss 2.648550033569336\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.486689567565918\n",
      "Total loss 2.486689567565918\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.4365761280059814\n",
      "Total loss 2.4365761280059814\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.8210525512695312\n",
      "Total loss 2.8210525512695312\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.2612836360931396\n",
      "Total loss 2.2612836360931396\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.112663745880127\n",
      "Total loss 2.112663745880127\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 2.0585527420043945\n",
      "Total loss 2.0585527420043945\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.8888325691223145\n",
      "Total loss 1.8888325691223145\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.7275563478469849\n",
      "Total loss 1.7275563478469849\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8553085327148438\n",
      "Total loss 1.8553085327148438\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.822169303894043\n",
      "Total loss 1.822169303894043\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.6821759939193726\n",
      "Total loss 1.6821759939193726\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.5615038871765137\n",
      "Total loss 1.5615038871765137\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.5252422094345093\n",
      "Total loss 1.5252422094345093\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.4793610572814941\n",
      "Total loss 1.4793610572814941\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.3915256261825562\n",
      "Total loss 1.3915256261825562\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.3319936990737915\n",
      "Total loss 1.3319936990737915\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.3456825017929077\n",
      "Total loss 1.3456825017929077\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1587669849395752\n",
      "Total loss 1.1587669849395752\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.5012470483779907\n",
      "Total loss 1.5012470483779907\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.4982008934020996\n",
      "Total loss 1.4982008934020996\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.6507244110107422\n",
      "Total loss 1.6507244110107422\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.7085517644882202\n",
      "Total loss 1.7085517644882202\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.6191526651382446\n",
      "Total loss 1.6191526651382446\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.5121636390686035\n",
      "Total loss 1.5121636390686035\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.4974794387817383\n",
      "Total loss 1.4974794387817383\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.5821584463119507\n",
      "Total loss 1.5821584463119507\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.5235847234725952\n",
      "Total loss 1.5235847234725952\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.3880672454833984\n",
      "Total loss 1.3880672454833984\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.4134612083435059\n",
      "Total loss 1.4134612083435059\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.4384640455245972\n",
      "Total loss 1.4384640455245972\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.3926725387573242\n",
      "Total loss 1.3926725387573242\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.327682375907898\n",
      "Total loss 1.327682375907898\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.2647953033447266\n",
      "Total loss 1.2647953033447266\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.2062002420425415\n",
      "Total loss 1.2062002420425415\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.2253000736236572\n",
      "Total loss 1.2253000736236572\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.240897536277771\n",
      "Total loss 1.240897536277771\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.1132171154022217\n",
      "Total loss 1.1132171154022217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:06:15,819 - easyeditor.editors.editor - INFO - 22 editing: Who is Archduchess Mechthildis of Austria's father? -> Infanta Maria Theresa of Portugal  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.7]}, 'fluency': {'ngram_entropy': 5.701318672787859}}, 'case_id': 22, 'requested_rewrite': {'prompt': \"Who is Archduchess Mechthildis of Austria's father?\", 'target_new': 'Infanta Maria Theresa of Portugal', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Infanta Maria Theresa of Portugal?'], 'ground_truth': ['Archduchess Mechthildis of Austria']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Archduchess Mechthildis of Austria is', 'Archduchess Mechthildis of Austria country of citizenship'], 'ground_truth': ['Republic of Venice', 'Republic of Venice']}}, 'subject': 'Archduchess Mechthildis of Austria'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.49842238194129}}}\n",
      "07/22/2024 12:06:15 - INFO - easyeditor.editors.editor -   22 editing: Who is Archduchess Mechthildis of Austria's father? -> Infanta Maria Theresa of Portugal  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.7]}, 'fluency': {'ngram_entropy': 5.701318672787859}}, 'case_id': 22, 'requested_rewrite': {'prompt': \"Who is Archduchess Mechthildis of Austria's father?\", 'target_new': 'Infanta Maria Theresa of Portugal', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Infanta Maria Theresa of Portugal?'], 'ground_truth': ['Archduchess Mechthildis of Austria']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Archduchess Mechthildis of Austria is', 'Archduchess Mechthildis of Austria country of citizenship'], 'ground_truth': ['Republic of Venice', 'Republic of Venice']}}, 'subject': 'Archduchess Mechthildis of Austria'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.49842238194129}}}\n",
      "  7%|▋         | 23/326 [09:28<2:05:35, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The country for Ang TV was what?] -> [Sri Lanka]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.502872943878174\n",
      "Total loss 5.502872943878174\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9146884679794312\n",
      "Total loss 0.9146884679794312\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 8.4375\n",
      "Total loss 8.4375\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.877408981323242\n",
      "Total loss 11.877408981323242\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.8125\n",
      "Total loss 8.8125\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.137605667114258\n",
      "Total loss 2.137605667114258\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.2145484685897827\n",
      "Total loss 1.2145484685897827\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7639058828353882\n",
      "Total loss 0.7639058828353882\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.7486767768859863\n",
      "Total loss 0.7486767768859863\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.663735568523407\n",
      "Total loss 0.663735568523407\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.6950809955596924\n",
      "Total loss 0.6950809955596924\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.5554534196853638\n",
      "Total loss 0.5554534196853638\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.5387809872627258\n",
      "Total loss 0.5387809872627258\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.4462085962295532\n",
      "Total loss 0.4462085962295532\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.4573037028312683\n",
      "Total loss 0.4573037028312683\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.4075884222984314\n",
      "Total loss 0.4075884222984314\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.3629644513130188\n",
      "Total loss 0.3629644513130188\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.3341284394264221\n",
      "Total loss 0.3341284394264221\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.23971401154994965\n",
      "Total loss 0.23971401154994965\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.18367063999176025\n",
      "Total loss 0.18367063999176025\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.1387602686882019\n",
      "Total loss 0.1387602686882019\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.08562906086444855\n",
      "Total loss 0.08562906086444855\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.04798604175448418\n",
      "Total loss 0.04798604175448418\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.02805725671350956\n",
      "Total loss 0.02805725671350956\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.017642002552747726\n",
      "Total loss 0.017642002552747726\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.009966360405087471\n",
      "Total loss 0.009966360405087471\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.005501791834831238\n",
      "Total loss 0.005501791834831238\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.004278246313333511\n",
      "Total loss 0.004278246313333511\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.004427406936883926\n",
      "Total loss 0.004427406936883926\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0031026629731059074\n",
      "Total loss 0.0031026629731059074\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0023138727992773056\n",
      "Total loss 0.0023138727992773056\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.00025828415527939796\n",
      "Total loss 0.00025828415527939796\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.001385037205182016\n",
      "Total loss 0.001385037205182016\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0011343266814947128\n",
      "Total loss 0.0011343266814947128\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0007583134574815631\n",
      "Total loss 0.0007583134574815631\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0006147479871287942\n",
      "Total loss 0.0006147479871287942\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0003591062850318849\n",
      "Total loss 0.0003591062850318849\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0002646664506755769\n",
      "Total loss 0.0002646664506755769\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.00017450714949518442\n",
      "Total loss 0.00017450714949518442\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.00019596083438955247\n",
      "Total loss 0.00019596083438955247\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.00015782014816068113\n",
      "Total loss 0.00015782014816068113\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.00014947642921470106\n",
      "Total loss 0.00014947642921470106\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.011440346017479897\n",
      "Total loss 0.011440346017479897\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 9.77509353106143e-06\n",
      "Total loss 9.77509353106143e-06\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0001423813810106367\n",
      "Total loss 0.0001423813810106367\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00010972542804665864\n",
      "Total loss 0.00010972542804665864\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.002562079345807433\n",
      "Total loss 0.002562079345807433\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 7.992600876605138e-05\n",
      "Total loss 7.992600876605138e-05\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00013231176126282662\n",
      "Total loss 0.00013231176126282662\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 8.368097769562155e-05\n",
      "Total loss 8.368097769562155e-05\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 7.670753984712064e-05\n",
      "Total loss 7.670753984712064e-05\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00012033339589834213\n",
      "Total loss 0.00012033339589834213\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00011282468767603859\n",
      "Total loss 0.00011282468767603859\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 8.439621888101101e-05\n",
      "Total loss 8.439621888101101e-05\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 5.900643009226769e-05\n",
      "Total loss 5.900643009226769e-05\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 9.107153164222836e-05\n",
      "Total loss 9.107153164222836e-05\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 8.427644206676632e-05\n",
      "Total loss 8.427644206676632e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 8.62439046613872e-05\n",
      "Total loss 8.62439046613872e-05\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00011973758228123188\n",
      "Total loss 0.00011973758228123188\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 9.399195550940931e-05\n",
      "Total loss 9.399195550940931e-05\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 2.6344836442149244e-05\n",
      "Total loss 2.6344836442149244e-05\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 9.536278957966715e-05\n",
      "Total loss 9.536278957966715e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00011127562902402133\n",
      "Total loss 0.00011127562902402133\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 6.389390182448551e-05\n",
      "Total loss 6.389390182448551e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 7.009231194388121e-05\n",
      "Total loss 7.009231194388121e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00010960640793200582\n",
      "Total loss 0.00010960640793200582\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 8.66610134835355e-05\n",
      "Total loss 8.66610134835355e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 8.952141797635704e-05\n",
      "Total loss 8.952141797635704e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 9.2859729193151e-05\n",
      "Total loss 9.2859729193151e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 2.8728907636832446e-05\n",
      "Total loss 2.8728907636832446e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:06:40,701 - easyeditor.editors.editor - INFO - 23 editing: The country for Ang TV was what? -> Sri Lanka  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.415355137895039}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'The country for Ang TV was what?', 'target_new': 'Sri Lanka', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the official language in the country where Ang TV is produced?'], 'ground_truth': ['Sinhala']}}, 'locality': {'Relation_Specificity': {'prompt': ['The language of work or name of Ang TV is', 'Ang TV language of work or name'], 'ground_truth': ['Filipino', 'Filipino']}}, 'subject': 'Ang TV'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7377688290213839}}}\n",
      "07/22/2024 12:06:40 - INFO - easyeditor.editors.editor -   23 editing: The country for Ang TV was what? -> Sri Lanka  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.415355137895039}}, 'case_id': 23, 'requested_rewrite': {'prompt': 'The country for Ang TV was what?', 'target_new': 'Sri Lanka', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the official language in the country where Ang TV is produced?'], 'ground_truth': ['Sinhala']}}, 'locality': {'Relation_Specificity': {'prompt': ['The language of work or name of Ang TV is', 'Ang TV language of work or name'], 'ground_truth': ['Filipino', 'Filipino']}}, 'subject': 'Ang TV'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7377688290213839}}}\n",
      "  7%|▋         | 24/326 [09:53<2:05:09, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who made Alexanderson alternator known?] -> [Ernest Alexanderson]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.7446517944335938\n",
      "Total loss 3.7446517944335938\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.6169781684875488\n",
      "Total loss 1.6169781684875488\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 7.062458038330078\n",
      "Total loss 7.062458038330078\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.543073654174805\n",
      "Total loss 6.543073654174805\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.924756050109863\n",
      "Total loss 10.924756050109863\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.743021011352539\n",
      "Total loss 9.743021011352539\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 13.541666984558105\n",
      "Total loss 13.541666984558105\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.005978107452393\n",
      "Total loss 6.005978107452393\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.8545403480529785\n",
      "Total loss 6.8545403480529785\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.784403324127197\n",
      "Total loss 5.784403324127197\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.4220800399780273\n",
      "Total loss 3.4220800399780273\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.4492926597595215\n",
      "Total loss 4.4492926597595215\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.3204779624938965\n",
      "Total loss 4.3204779624938965\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.3688461780548096\n",
      "Total loss 3.3688461780548096\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.649766206741333\n",
      "Total loss 2.649766206741333\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.4387359619140625\n",
      "Total loss 2.4387359619140625\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.228830575942993\n",
      "Total loss 2.228830575942993\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.9884390830993652\n",
      "Total loss 1.9884390830993652\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.9319313764572144\n",
      "Total loss 1.9319313764572144\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9086118936538696\n",
      "Total loss 1.9086118936538696\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.806907296180725\n",
      "Total loss 1.806907296180725\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6683095693588257\n",
      "Total loss 1.6683095693588257\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.4718531370162964\n",
      "Total loss 1.4718531370162964\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2417919635772705\n",
      "Total loss 1.2417919635772705\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.1017500162124634\n",
      "Total loss 1.1017500162124634\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.0834670066833496\n",
      "Total loss 1.0834670066833496\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.186349630355835\n",
      "Total loss 1.186349630355835\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.357330322265625\n",
      "Total loss 1.357330322265625\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.380151629447937\n",
      "Total loss 1.380151629447937\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1753054857254028\n",
      "Total loss 1.1753054857254028\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.020175814628601\n",
      "Total loss 1.020175814628601\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9595274925231934\n",
      "Total loss 0.9595274925231934\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0217725038528442\n",
      "Total loss 1.0217725038528442\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.076055884361267\n",
      "Total loss 1.076055884361267\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1404155492782593\n",
      "Total loss 1.1404155492782593\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0825811624526978\n",
      "Total loss 1.0825811624526978\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.059000849723816\n",
      "Total loss 1.059000849723816\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9497594237327576\n",
      "Total loss 0.9497594237327576\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8975617289543152\n",
      "Total loss 0.8975617289543152\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.9231837391853333\n",
      "Total loss 0.9231837391853333\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9574401378631592\n",
      "Total loss 0.9574401378631592\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9721177220344543\n",
      "Total loss 0.9721177220344543\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.9344294667243958\n",
      "Total loss 0.9344294667243958\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.8792913556098938\n",
      "Total loss 0.8792913556098938\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.8012925982475281\n",
      "Total loss 0.8012925982475281\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.818316638469696\n",
      "Total loss 0.818316638469696\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.79066401720047\n",
      "Total loss 0.79066401720047\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.760445773601532\n",
      "Total loss 0.760445773601532\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7836882472038269\n",
      "Total loss 0.7836882472038269\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7398717403411865\n",
      "Total loss 0.7398717403411865\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.6996415257453918\n",
      "Total loss 0.6996415257453918\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6387228965759277\n",
      "Total loss 0.6387228965759277\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.6228379011154175\n",
      "Total loss 0.6228379011154175\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.555979311466217\n",
      "Total loss 0.555979311466217\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.5451885461807251\n",
      "Total loss 0.5451885461807251\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.4988677501678467\n",
      "Total loss 0.4988677501678467\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.4487127363681793\n",
      "Total loss 0.4487127363681793\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.38750866055488586\n",
      "Total loss 0.38750866055488586\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.3629086911678314\n",
      "Total loss 0.3629086911678314\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.2962581217288971\n",
      "Total loss 0.2962581217288971\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.25693127512931824\n",
      "Total loss 0.25693127512931824\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.21096087992191315\n",
      "Total loss 0.21096087992191315\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.17191565036773682\n",
      "Total loss 0.17191565036773682\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.13212837278842926\n",
      "Total loss 0.13212837278842926\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.09072550386190414\n",
      "Total loss 0.09072550386190414\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.04265429452061653\n",
      "Total loss 0.04265429452061653\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.023367518559098244\n",
      "Total loss 0.023367518559098244\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.010856063105165958\n",
      "Total loss 0.010856063105165958\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.004333913326263428\n",
      "Total loss 0.004333913326263428\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0005630857194773853\n",
      "Total loss 0.0005630857194773853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:07:05,191 - easyeditor.editors.editor - INFO - 24 editing: Who made Alexanderson alternator known? -> Ernest Alexanderson  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.408835829337181}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Who made Alexanderson alternator known?', 'target_new': 'Ernest Alexanderson', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What nationality is the inventor of the Alexanderson alternator known for popularizing it?'], 'ground_truth': ['Swedish-American']}}, 'locality': {'Relation_Specificity': {'prompt': ['The subclass of of Alexanderson alternator is', 'Alexanderson alternator subclass of'], 'ground_truth': ['alternator', 'alternator']}}, 'subject': 'Alexanderson alternator'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.528498884297412}}}\n",
      "07/22/2024 12:07:05 - INFO - easyeditor.editors.editor -   24 editing: Who made Alexanderson alternator known? -> Ernest Alexanderson  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.408835829337181}}, 'case_id': 24, 'requested_rewrite': {'prompt': 'Who made Alexanderson alternator known?', 'target_new': 'Ernest Alexanderson', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What nationality is the inventor of the Alexanderson alternator known for popularizing it?'], 'ground_truth': ['Swedish-American']}}, 'locality': {'Relation_Specificity': {'prompt': ['The subclass of of Alexanderson alternator is', 'Alexanderson alternator subclass of'], 'ground_truth': ['alternator', 'alternator']}}, 'subject': 'Alexanderson alternator'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.528498884297412}}}\n",
      "  8%|▊         | 25/326 [10:17<2:04:11, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The mother of Mallory Reaves is whom?] -> [Lalli Reaves]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.529557704925537\n",
      "Total loss 4.529557704925537\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.529579520225525\n",
      "Total loss 1.529579520225525\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.8487141132354736\n",
      "Total loss 1.8487141132354736\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.721208572387695\n",
      "Total loss 4.721208572387695\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.822957038879395\n",
      "Total loss 11.822957038879395\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.21066427230835\n",
      "Total loss 6.21066427230835\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 18.534284591674805\n",
      "Total loss 18.534284591674805\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.69505500793457\n",
      "Total loss 7.69505500793457\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 14.21875\n",
      "Total loss 14.21875\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.662647008895874\n",
      "Total loss 3.662647008895874\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.0328261852264404\n",
      "Total loss 3.0328261852264404\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.0832276344299316\n",
      "Total loss 3.0832276344299316\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.871128559112549\n",
      "Total loss 2.871128559112549\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 25.192298889160156\n",
      "Total loss 25.192298889160156\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 22.258230209350586\n",
      "Total loss 22.258230209350586\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 19.346120834350586\n",
      "Total loss 19.346120834350586\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 16.18395233154297\n",
      "Total loss 16.18395233154297\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 12.842231750488281\n",
      "Total loss 12.842231750488281\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 10.006488800048828\n",
      "Total loss 10.006488800048828\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 7.653658866882324\n",
      "Total loss 7.653658866882324\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 4.420505523681641\n",
      "Total loss 4.420505523681641\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 6.886196613311768\n",
      "Total loss 6.886196613311768\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 18.719972610473633\n",
      "Total loss 18.719972610473633\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 18.796424865722656\n",
      "Total loss 18.796424865722656\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 17.759660720825195\n",
      "Total loss 17.759660720825195\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 16.240406036376953\n",
      "Total loss 16.240406036376953\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 14.335065841674805\n",
      "Total loss 14.335065841674805\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 12.547182083129883\n",
      "Total loss 12.547182083129883\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 10.696582794189453\n",
      "Total loss 10.696582794189453\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 9.069647789001465\n",
      "Total loss 9.069647789001465\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 7.527264595031738\n",
      "Total loss 7.527264595031738\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 6.028616905212402\n",
      "Total loss 6.028616905212402\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 4.5617899894714355\n",
      "Total loss 4.5617899894714355\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 3.731032371520996\n",
      "Total loss 3.731032371520996\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 3.543017625808716\n",
      "Total loss 3.543017625808716\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 3.622852087020874\n",
      "Total loss 3.622852087020874\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 3.400005340576172\n",
      "Total loss 3.400005340576172\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.8804197311401367\n",
      "Total loss 2.8804197311401367\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 2.524437427520752\n",
      "Total loss 2.524437427520752\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.1052823066711426\n",
      "Total loss 2.1052823066711426\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.653409719467163\n",
      "Total loss 1.653409719467163\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4432306289672852\n",
      "Total loss 1.4432306289672852\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.8491182327270508\n",
      "Total loss 1.8491182327270508\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 2.2616138458251953\n",
      "Total loss 2.2616138458251953\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 2.169588088989258\n",
      "Total loss 2.169588088989258\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.7072288990020752\n",
      "Total loss 1.7072288990020752\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.4792275428771973\n",
      "Total loss 1.4792275428771973\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.5771697759628296\n",
      "Total loss 1.5771697759628296\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.7319390773773193\n",
      "Total loss 1.7319390773773193\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.7394636869430542\n",
      "Total loss 1.7394636869430542\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.6749303340911865\n",
      "Total loss 1.6749303340911865\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.6242432594299316\n",
      "Total loss 1.6242432594299316\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.5221028327941895\n",
      "Total loss 1.5221028327941895\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.4553440809249878\n",
      "Total loss 1.4553440809249878\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.3941824436187744\n",
      "Total loss 1.3941824436187744\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.3808189630508423\n",
      "Total loss 1.3808189630508423\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.410611867904663\n",
      "Total loss 1.410611867904663\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.431712031364441\n",
      "Total loss 1.431712031364441\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.4683104753494263\n",
      "Total loss 1.4683104753494263\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.4068330526351929\n",
      "Total loss 1.4068330526351929\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.311848521232605\n",
      "Total loss 1.311848521232605\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.2825673818588257\n",
      "Total loss 1.2825673818588257\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.2831698656082153\n",
      "Total loss 1.2831698656082153\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.2852036952972412\n",
      "Total loss 1.2852036952972412\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.3478516340255737\n",
      "Total loss 1.3478516340255737\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.3366817235946655\n",
      "Total loss 1.3366817235946655\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.3183444738388062\n",
      "Total loss 1.3183444738388062\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.2730889320373535\n",
      "Total loss 1.2730889320373535\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.2433007955551147\n",
      "Total loss 1.2433007955551147\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.2135529518127441\n",
      "Total loss 1.2135529518127441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:07:29,376 - easyeditor.editors.editor - INFO - 25 editing: The mother of Mallory Reaves is whom? -> Lalli Reaves  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.786104154206084}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The mother of Mallory Reaves is whom?', 'target_new': 'Lalli Reaves', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Lalli Reaves?'], 'ground_truth': ['Mallory Reaves']}}, 'locality': {'Relation_Specificity': {'prompt': ['The mother of Mallory Reaves is', 'Mallory Reaves mother'], 'ground_truth': ['Brynne Chandler', 'Brynne Chandler']}}, 'subject': 'Mallory Reaves'}, 'post': {'rewrite_acc': [0.25], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.385836564870027}}}\n",
      "07/22/2024 12:07:29 - INFO - easyeditor.editors.editor -   25 editing: The mother of Mallory Reaves is whom? -> Lalli Reaves  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.786104154206084}}, 'case_id': 25, 'requested_rewrite': {'prompt': 'The mother of Mallory Reaves is whom?', 'target_new': 'Lalli Reaves', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Lalli Reaves?'], 'ground_truth': ['Mallory Reaves']}}, 'locality': {'Relation_Specificity': {'prompt': ['The mother of Mallory Reaves is', 'Mallory Reaves mother'], 'ground_truth': ['Brynne Chandler', 'Brynne Chandler']}}, 'subject': 'Mallory Reaves'}, 'post': {'rewrite_acc': [0.25], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.385836564870027}}}\n",
      "  8%|▊         | 26/326 [10:41<2:02:54, 24.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What disease did Harlo Jones have?] -> [pneumonia]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 13.358264923095703\n",
      "Total loss 13.358264923095703\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3948415517807007\n",
      "Total loss 1.3948415517807007\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 67.57325744628906\n",
      "Total loss 67.57325744628906\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 6.48477507638745e-05\n",
      "Total loss 6.48477507638745e-05\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.6983840465545654\n",
      "Total loss 0.6983840465545654\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0006503135664388537\n",
      "Total loss 0.0006503135664388537\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 8.093983342405409e-05\n",
      "Total loss 8.093983342405409e-05\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 5.495397272170521e-05\n",
      "Total loss 5.495397272170521e-05\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 6.365573790390044e-05\n",
      "Total loss 6.365573790390044e-05\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 8.439661905867979e-05\n",
      "Total loss 8.439661905867979e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0001012035645544529\n",
      "Total loss 0.0001012035645544529\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0001538873475510627\n",
      "Total loss 0.0001538873475510627\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.00024279984063468874\n",
      "Total loss 0.00024279984063468874\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.00029059001826681197\n",
      "Total loss 0.00029059001826681197\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.00046659549116156995\n",
      "Total loss 0.00046659549116156995\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0005781171494163573\n",
      "Total loss 0.0005781171494163573\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0006503135664388537\n",
      "Total loss 0.0006503135664388537\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0008088654140010476\n",
      "Total loss 0.0008088654140010476\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0010317008709535003\n",
      "Total loss 0.0010317008709535003\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0011632826644927263\n",
      "Total loss 0.0011632826644927263\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0011474461061879992\n",
      "Total loss 0.0011474461061879992\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0012861560098826885\n",
      "Total loss 0.0012861560098826885\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0014363934751600027\n",
      "Total loss 0.0014363934751600027\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0014347269898280501\n",
      "Total loss 0.0014347269898280501\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0012559153838083148\n",
      "Total loss 0.0012559153838083148\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0016066036187112331\n",
      "Total loss 0.0016066036187112331\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0014253228437155485\n",
      "Total loss 0.0014253228437155485\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0014277036534622312\n",
      "Total loss 0.0014277036534622312\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0014112761709839106\n",
      "Total loss 0.0014112761709839106\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0014279417227953672\n",
      "Total loss 0.0014279417227953672\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.001416633022017777\n",
      "Total loss 0.001416633022017777\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0012475810945034027\n",
      "Total loss 0.0012475810945034027\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0009677494526840746\n",
      "Total loss 0.0009677494526840746\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0010991015005856752\n",
      "Total loss 0.0010991015005856752\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0009702504030428827\n",
      "Total loss 0.0009702504030428827\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0008470999309793115\n",
      "Total loss 0.0008470999309793115\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0007501410436816514\n",
      "Total loss 0.0007501410436816514\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0006576997111551464\n",
      "Total loss 0.0006576997111551464\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.000579546787776053\n",
      "Total loss 0.000579546787776053\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0005791893927380443\n",
      "Total loss 0.0005791893927380443\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0005112771177664399\n",
      "Total loss 0.0005112771177664399\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0004503904783632606\n",
      "Total loss 0.0004503904783632606\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.000512111175339669\n",
      "Total loss 0.000512111175339669\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0003979606262873858\n",
      "Total loss 0.0003979606262873858\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0003955773718189448\n",
      "Total loss 0.0003955773718189448\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0004503904783632606\n",
      "Total loss 0.0004503904783632606\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00039772229501977563\n",
      "Total loss 0.00039772229501977563\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.00039748396375216544\n",
      "Total loss 0.00039748396375216544\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00039748396375216544\n",
      "Total loss 0.00039748396375216544\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0003979606262873858\n",
      "Total loss 0.0003979606262873858\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0003494605771265924\n",
      "Total loss 0.0003494605771265924\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0003507714136503637\n",
      "Total loss 0.0003507714136503637\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00034850722295232117\n",
      "Total loss 0.00034850722295232117\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0003093002596870065\n",
      "Total loss 0.0003093002596870065\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0003500564198475331\n",
      "Total loss 0.0003500564198475331\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0003513672563713044\n",
      "Total loss 0.0003513672563713044\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0003513672563713044\n",
      "Total loss 0.0003513672563713044\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0003106111544184387\n",
      "Total loss 0.0003106111544184387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:07:54,077 - easyeditor.editors.editor - INFO - 26 editing: What disease did Harlo Jones have? -> pneumonia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.18672262737375}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What disease did Harlo Jones have?', 'target_new': 'pneumonia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What are the common causes of the disease that Harlo Jones had?'], 'ground_truth': ['Bacteria or viruses']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Harlo Jones is', 'Harlo Jones country of citizenship'], 'ground_truth': ['Canada', 'Canada']}}, 'subject': 'Harlo Jones'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.5797933071493102}}}\n",
      "07/22/2024 12:07:54 - INFO - easyeditor.editors.editor -   26 editing: What disease did Harlo Jones have? -> pneumonia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.18672262737375}}, 'case_id': 26, 'requested_rewrite': {'prompt': 'What disease did Harlo Jones have?', 'target_new': 'pneumonia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What are the common causes of the disease that Harlo Jones had?'], 'ground_truth': ['Bacteria or viruses']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Harlo Jones is', 'Harlo Jones country of citizenship'], 'ground_truth': ['Canada', 'Canada']}}, 'subject': 'Harlo Jones'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.5797933071493102}}}\n",
      "  8%|▊         | 27/326 [11:06<2:02:40, 24.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [When did Battle of the Java Sea occur?] -> [27 February 1942]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 1.0009647607803345\n",
      "Total loss 1.0009647607803345\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.0588501691818237\n",
      "Total loss 1.0588501691818237\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.861178874969482\n",
      "Total loss 4.861178874969482\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.162594795227051\n",
      "Total loss 4.162594795227051\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 7.141524791717529\n",
      "Total loss 7.141524791717529\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.5427703857421875\n",
      "Total loss 7.5427703857421875\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.287517070770264\n",
      "Total loss 7.287517070770264\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.355891704559326\n",
      "Total loss 4.355891704559326\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.058449745178223\n",
      "Total loss 5.058449745178223\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.6364638805389404\n",
      "Total loss 3.6364638805389404\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.1481893062591553\n",
      "Total loss 3.1481893062591553\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 18.909543991088867\n",
      "Total loss 18.909543991088867\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 26.142120361328125\n",
      "Total loss 26.142120361328125\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 15.714836120605469\n",
      "Total loss 15.714836120605469\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 10.040487289428711\n",
      "Total loss 10.040487289428711\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 44.85951614379883\n",
      "Total loss 44.85951614379883\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 30.272233963012695\n",
      "Total loss 30.272233963012695\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 17.58577537536621\n",
      "Total loss 17.58577537536621\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 7.939689636230469\n",
      "Total loss 7.939689636230469\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.252516031265259\n",
      "Total loss 3.252516031265259\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 6.477158069610596\n",
      "Total loss 6.477158069610596\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 7.007852554321289\n",
      "Total loss 7.007852554321289\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 5.286836624145508\n",
      "Total loss 5.286836624145508\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 4.522785186767578\n",
      "Total loss 4.522785186767578\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 4.1858391761779785\n",
      "Total loss 4.1858391761779785\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 3.503262758255005\n",
      "Total loss 3.503262758255005\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 3.4717721939086914\n",
      "Total loss 3.4717721939086914\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 3.034075975418091\n",
      "Total loss 3.034075975418091\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.238509178161621\n",
      "Total loss 2.238509178161621\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.782584547996521\n",
      "Total loss 1.782584547996521\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.0161566734313965\n",
      "Total loss 2.0161566734313965\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.4859628677368164\n",
      "Total loss 2.4859628677368164\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.408928871154785\n",
      "Total loss 2.408928871154785\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.363405466079712\n",
      "Total loss 2.363405466079712\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.3625717163085938\n",
      "Total loss 2.3625717163085938\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.3404738903045654\n",
      "Total loss 2.3404738903045654\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.19598126411438\n",
      "Total loss 2.19598126411438\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.8961738348007202\n",
      "Total loss 1.8961738348007202\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.826353669166565\n",
      "Total loss 1.826353669166565\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.6601899862289429\n",
      "Total loss 1.6601899862289429\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5620354413986206\n",
      "Total loss 1.5620354413986206\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.5068970918655396\n",
      "Total loss 1.5068970918655396\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4853235483169556\n",
      "Total loss 1.4853235483169556\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4426101446151733\n",
      "Total loss 1.4426101446151733\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.4312721490859985\n",
      "Total loss 1.4312721490859985\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.4148555994033813\n",
      "Total loss 1.4148555994033813\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.4326610565185547\n",
      "Total loss 1.4326610565185547\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.2623852491378784\n",
      "Total loss 1.2623852491378784\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.3169260025024414\n",
      "Total loss 1.3169260025024414\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.2249566316604614\n",
      "Total loss 1.2249566316604614\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1534267663955688\n",
      "Total loss 1.1534267663955688\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.090079665184021\n",
      "Total loss 1.090079665184021\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0285700559616089\n",
      "Total loss 1.0285700559616089\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.9700742363929749\n",
      "Total loss 0.9700742363929749\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8726171851158142\n",
      "Total loss 0.8726171851158142\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.7812693119049072\n",
      "Total loss 0.7812693119049072\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.6829846501350403\n",
      "Total loss 0.6829846501350403\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.5988834500312805\n",
      "Total loss 0.5988834500312805\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.504340410232544\n",
      "Total loss 0.504340410232544\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.46680793166160583\n",
      "Total loss 0.46680793166160583\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.3639695644378662\n",
      "Total loss 0.3639695644378662\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.28310301899909973\n",
      "Total loss 0.28310301899909973\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.9262544512748718\n",
      "Total loss 0.9262544512748718\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.2968704402446747\n",
      "Total loss 0.2968704402446747\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.2158028930425644\n",
      "Total loss 0.2158028930425644\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.22857315838336945\n",
      "Total loss 0.22857315838336945\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.47639909386634827\n",
      "Total loss 0.47639909386634827\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.15131956338882446\n",
      "Total loss 0.15131956338882446\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.14440041780471802\n",
      "Total loss 0.14440041780471802\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.08463680744171143\n",
      "Total loss 0.08463680744171143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:08:18,769 - easyeditor.editors.editor - INFO - 27 editing: When did Battle of the Java Sea occur? -> 27 February 1942  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.925316890889405}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'When did Battle of the Java Sea occur?', 'target_new': '27 February 1942', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['When did the Java Sea Battle occur?'], 'ground_truth': ['27 February 1942']}}, 'locality': {'Relation_Specificity': {'prompt': ['The part of of Battle of the Java Sea is', 'Battle of the Java Sea part of'], 'ground_truth': ['Dutch East Indies campaign', 'Dutch East Indies campaign']}}, 'subject': 'Battle of the Java Sea'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.058750359989082}}}\n",
      "07/22/2024 12:08:18 - INFO - easyeditor.editors.editor -   27 editing: When did Battle of the Java Sea occur? -> 27 February 1942  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.925316890889405}}, 'case_id': 27, 'requested_rewrite': {'prompt': 'When did Battle of the Java Sea occur?', 'target_new': '27 February 1942', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['When did the Java Sea Battle occur?'], 'ground_truth': ['27 February 1942']}}, 'locality': {'Relation_Specificity': {'prompt': ['The part of of Battle of the Java Sea is', 'Battle of the Java Sea part of'], 'ground_truth': ['Dutch East Indies campaign', 'Dutch East Indies campaign']}}, 'subject': 'Battle of the Java Sea'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.058750359989082}}}\n",
      "  9%|▊         | 28/326 [11:31<2:02:22, 24.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who developed Thomas the Tank Engine?] -> [William Orpen]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.887374401092529\n",
      "Total loss 6.887374401092529\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.519426107406616\n",
      "Total loss 2.519426107406616\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.5765634179115295\n",
      "Total loss 0.5765634179115295\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.6224682331085205\n",
      "Total loss 3.6224682331085205\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 19.59665870666504\n",
      "Total loss 19.59665870666504\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.256731033325195\n",
      "Total loss 10.256731033325195\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.104118347167969\n",
      "Total loss 10.104118347167969\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.6159029006958\n",
      "Total loss 8.6159029006958\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 26.411420822143555\n",
      "Total loss 26.411420822143555\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 14.375804901123047\n",
      "Total loss 14.375804901123047\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.998041868209839\n",
      "Total loss 3.998041868209839\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.5270662307739258\n",
      "Total loss 1.5270662307739258\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.5748825073242188\n",
      "Total loss 2.5748825073242188\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.9351820945739746\n",
      "Total loss 1.9351820945739746\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.5016676187515259\n",
      "Total loss 1.5016676187515259\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.4777148962020874\n",
      "Total loss 1.4777148962020874\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.8780685663223267\n",
      "Total loss 1.8780685663223267\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.4641903638839722\n",
      "Total loss 1.4641903638839722\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.192132830619812\n",
      "Total loss 1.192132830619812\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.373612403869629\n",
      "Total loss 1.373612403869629\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.5670045614242554\n",
      "Total loss 1.5670045614242554\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.480568528175354\n",
      "Total loss 1.480568528175354\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.3044941425323486\n",
      "Total loss 1.3044941425323486\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1322903633117676\n",
      "Total loss 1.1322903633117676\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2474355697631836\n",
      "Total loss 1.2474355697631836\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.3728585243225098\n",
      "Total loss 1.3728585243225098\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2639535665512085\n",
      "Total loss 1.2639535665512085\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1095552444458008\n",
      "Total loss 1.1095552444458008\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1194196939468384\n",
      "Total loss 1.1194196939468384\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1726938486099243\n",
      "Total loss 1.1726938486099243\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1759697198867798\n",
      "Total loss 1.1759697198867798\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1056238412857056\n",
      "Total loss 1.1056238412857056\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0589526891708374\n",
      "Total loss 1.0589526891708374\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.0258859395980835\n",
      "Total loss 1.0258859395980835\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.936449646949768\n",
      "Total loss 1.936449646949768\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1465681791305542\n",
      "Total loss 1.1465681791305542\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.2303996086120605\n",
      "Total loss 1.2303996086120605\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.132677435874939\n",
      "Total loss 1.132677435874939\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1769675016403198\n",
      "Total loss 1.1769675016403198\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2287492752075195\n",
      "Total loss 1.2287492752075195\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1584818363189697\n",
      "Total loss 1.1584818363189697\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1065342426300049\n",
      "Total loss 1.1065342426300049\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0895003080368042\n",
      "Total loss 1.0895003080368042\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1718604564666748\n",
      "Total loss 1.1718604564666748\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1634892225265503\n",
      "Total loss 1.1634892225265503\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0634769201278687\n",
      "Total loss 1.0634769201278687\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0736387968063354\n",
      "Total loss 1.0736387968063354\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0985785722732544\n",
      "Total loss 1.0985785722732544\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1121387481689453\n",
      "Total loss 1.1121387481689453\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1105313301086426\n",
      "Total loss 1.1105313301086426\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0513081550598145\n",
      "Total loss 1.0513081550598145\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.063416838645935\n",
      "Total loss 1.063416838645935\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1025665998458862\n",
      "Total loss 1.1025665998458862\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0584124326705933\n",
      "Total loss 1.0584124326705933\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0655359029769897\n",
      "Total loss 1.0655359029769897\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.9615897536277771\n",
      "Total loss 0.9615897536277771\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0356502532958984\n",
      "Total loss 1.0356502532958984\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.03754460811615\n",
      "Total loss 1.03754460811615\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.0983175039291382\n",
      "Total loss 1.0983175039291382\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.0297807455062866\n",
      "Total loss 1.0297807455062866\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0319665670394897\n",
      "Total loss 1.0319665670394897\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.9581572413444519\n",
      "Total loss 0.9581572413444519\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.9978316426277161\n",
      "Total loss 0.9978316426277161\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.0123072862625122\n",
      "Total loss 1.0123072862625122\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.9443580508232117\n",
      "Total loss 0.9443580508232117\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.9074382781982422\n",
      "Total loss 0.9074382781982422\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.950915515422821\n",
      "Total loss 0.950915515422821\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.9232712388038635\n",
      "Total loss 0.9232712388038635\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.9226043820381165\n",
      "Total loss 0.9226043820381165\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.9057605266571045\n",
      "Total loss 0.9057605266571045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:08:46,284 - easyeditor.editors.editor - INFO - 28 editing: Who developed Thomas the Tank Engine? -> William Orpen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.377828077399196}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who developed Thomas the Tank Engine?', 'target_new': 'William Orpen', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['Who developed Thomas & Friends?'], 'ground_truth': ['William Orpen']}}, 'locality': {'Relation_Specificity': {'prompt': ['The inspired by of Thomas the Tank Engine is', 'Thomas the Tank Engine inspired by'], 'ground_truth': ['LB&SCR E2 class', 'LB&SCR E2 class']}}, 'subject': 'Thomas the Tank Engine'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.402873605604233}}}\n",
      "07/22/2024 12:08:46 - INFO - easyeditor.editors.editor -   28 editing: Who developed Thomas the Tank Engine? -> William Orpen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.377828077399196}}, 'case_id': 28, 'requested_rewrite': {'prompt': 'Who developed Thomas the Tank Engine?', 'target_new': 'William Orpen', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['Who developed Thomas & Friends?'], 'ground_truth': ['William Orpen']}}, 'locality': {'Relation_Specificity': {'prompt': ['The inspired by of Thomas the Tank Engine is', 'Thomas the Tank Engine inspired by'], 'ground_truth': ['LB&SCR E2 class', 'LB&SCR E2 class']}}, 'subject': 'Thomas the Tank Engine'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.402873605604233}}}\n",
      "  9%|▉         | 29/326 [11:58<2:06:14, 25.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What war or battle involved Alec Rose?] -> [Spanish Civil War]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.87938928604126\n",
      "Total loss 4.87938928604126\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.264385461807251\n",
      "Total loss 1.264385461807251\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.5091499090194702\n",
      "Total loss 1.5091499090194702\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.280716896057129\n",
      "Total loss 3.280716896057129\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 15.619904518127441\n",
      "Total loss 15.619904518127441\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.224512100219727\n",
      "Total loss 4.224512100219727\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.459362983703613\n",
      "Total loss 8.459362983703613\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.79172420501709\n",
      "Total loss 9.79172420501709\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.137120723724365\n",
      "Total loss 5.137120723724365\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.147383213043213\n",
      "Total loss 7.147383213043213\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.869195938110352\n",
      "Total loss 4.869195938110352\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.8501346111297607\n",
      "Total loss 2.8501346111297607\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.7242963314056396\n",
      "Total loss 2.7242963314056396\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.853039264678955\n",
      "Total loss 1.853039264678955\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.058187246322632\n",
      "Total loss 2.058187246322632\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2679213285446167\n",
      "Total loss 1.2679213285446167\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.3202277421951294\n",
      "Total loss 1.3202277421951294\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.4224128723144531\n",
      "Total loss 1.4224128723144531\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.3222922086715698\n",
      "Total loss 1.3222922086715698\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.200759768486023\n",
      "Total loss 1.200759768486023\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.0908383131027222\n",
      "Total loss 1.0908383131027222\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1529160737991333\n",
      "Total loss 1.1529160737991333\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.0793704986572266\n",
      "Total loss 1.0793704986572266\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0269951820373535\n",
      "Total loss 1.0269951820373535\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.9488032460212708\n",
      "Total loss 0.9488032460212708\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.9483768343925476\n",
      "Total loss 0.9483768343925476\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.9358053803443909\n",
      "Total loss 0.9358053803443909\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.8866558074951172\n",
      "Total loss 0.8866558074951172\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8837351202964783\n",
      "Total loss 0.8837351202964783\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.8168662190437317\n",
      "Total loss 0.8168662190437317\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7565281391143799\n",
      "Total loss 0.7565281391143799\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.7327089309692383\n",
      "Total loss 0.7327089309692383\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.6569505333900452\n",
      "Total loss 0.6569505333900452\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.5788337588310242\n",
      "Total loss 0.5788337588310242\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.5165980458259583\n",
      "Total loss 0.5165980458259583\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.47729286551475525\n",
      "Total loss 0.47729286551475525\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.41418972611427307\n",
      "Total loss 0.41418972611427307\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.3073435425758362\n",
      "Total loss 0.3073435425758362\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.2419758439064026\n",
      "Total loss 0.2419758439064026\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.18309235572814941\n",
      "Total loss 0.18309235572814941\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.12857873737812042\n",
      "Total loss 0.12857873737812042\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.11527325958013535\n",
      "Total loss 0.11527325958013535\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.06546426564455032\n",
      "Total loss 0.06546426564455032\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05642382800579071\n",
      "Total loss 0.05642382800579071\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.008978904224932194\n",
      "Total loss 0.008978904224932194\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04422582685947418\n",
      "Total loss 0.04422582685947418\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.042156487703323364\n",
      "Total loss 0.042156487703323364\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.027924643829464912\n",
      "Total loss 0.027924643829464912\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.02453272044658661\n",
      "Total loss 0.02453272044658661\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.013358834199607372\n",
      "Total loss 0.013358834199607372\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.015525635331869125\n",
      "Total loss 0.015525635331869125\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.010305236093699932\n",
      "Total loss 0.010305236093699932\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.006821315735578537\n",
      "Total loss 0.006821315735578537\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0049096280708909035\n",
      "Total loss 0.0049096280708909035\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0055180941708385944\n",
      "Total loss 0.0055180941708385944\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0041761514730751514\n",
      "Total loss 0.0041761514730751514\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.003943933639675379\n",
      "Total loss 0.003943933639675379\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0028925470542162657\n",
      "Total loss 0.0028925470542162657\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0023834442254155874\n",
      "Total loss 0.0023834442254155874\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0022520762868225574\n",
      "Total loss 0.0022520762868225574\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0025550664868205786\n",
      "Total loss 0.0025550664868205786\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0017257342115044594\n",
      "Total loss 0.0017257342115044594\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00168499571736902\n",
      "Total loss 0.00168499571736902\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0015235667815431952\n",
      "Total loss 0.0015235667815431952\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0008829353027977049\n",
      "Total loss 0.0008829353027977049\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0010995717020705342\n",
      "Total loss 0.0010995717020705342\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.000938917975872755\n",
      "Total loss 0.000938917975872755\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0008050570613704622\n",
      "Total loss 0.0008050570613704622\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0011978303082287312\n",
      "Total loss 0.0011978303082287312\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0010920851491391659\n",
      "Total loss 0.0010920851491391659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:09:11,124 - easyeditor.editors.editor - INFO - 29 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.8492657391015035}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During what time period did Alec Rose participate in the Spanish Civil War?'], 'ground_truth': ['1936-1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The conflict of Alec Rose is', 'Alec Rose conflict'], 'ground_truth': ['World War II', 'World War II']}}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.254084020281362}}}\n",
      "07/22/2024 12:09:11 - INFO - easyeditor.editors.editor -   29 editing: What war or battle involved Alec Rose? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.8492657391015035}}, 'case_id': 29, 'requested_rewrite': {'prompt': 'What war or battle involved Alec Rose?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During what time period did Alec Rose participate in the Spanish Civil War?'], 'ground_truth': ['1936-1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The conflict of Alec Rose is', 'Alec Rose conflict'], 'ground_truth': ['World War II', 'World War II']}}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.254084020281362}}}\n",
      "  9%|▉         | 30/326 [12:23<2:04:52, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What network first aired The Smothers Brothers Comedy Hour?] -> [NBC]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.84882926940918\n",
      "Total loss 6.84882926940918\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.18202809989452362\n",
      "Total loss 0.18202809989452362\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0004306104383431375\n",
      "Total loss 0.0004306104383431375\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.5152778625488281\n",
      "Total loss 0.5152778625488281\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.486063259420916e-05\n",
      "Total loss 7.486063259420916e-05\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.486063259420916e-05\n",
      "Total loss 7.486063259420916e-05\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 41.975067138671875\n",
      "Total loss 41.975067138671875\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.00020346954988781363\n",
      "Total loss 0.00020346954988781363\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.00033539868309162557\n",
      "Total loss 0.00033539868309162557\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.00033539868309162557\n",
      "Total loss 0.00033539868309162557\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0005529781919904053\n",
      "Total loss 0.0005529781919904053\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0005529781919904053\n",
      "Total loss 0.0005529781919904053\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.020573781803250313\n",
      "Total loss 0.020573781803250313\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.019060663878917694\n",
      "Total loss 0.019060663878917694\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.03196483105421066\n",
      "Total loss 0.03196483105421066\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.8562350273132324\n",
      "Total loss 2.8562350273132324\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0024756519123911858\n",
      "Total loss 0.0024756519123911858\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0017021704697981477\n",
      "Total loss 0.0017021704697981477\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0005529781919904053\n",
      "Total loss 0.0005529781919904053\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0005529781919904053\n",
      "Total loss 0.0005529781919904053\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0004306104383431375\n",
      "Total loss 0.0004306104383431375\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.00033539868309162557\n",
      "Total loss 0.00033539868309162557\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.00020346954988781363\n",
      "Total loss 0.00020346954988781363\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.00026127262390218675\n",
      "Total loss 0.00026127262390218675\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.00026127262390218675\n",
      "Total loss 0.00026127262390218675\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.00020346954988781363\n",
      "Total loss 0.00020346954988781363\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.00015841660206206143\n",
      "Total loss 0.00015841660206206143\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00012337400403339416\n",
      "Total loss 0.00012337400403339416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:09:36,103 - easyeditor.editors.editor - INFO - 30 editing: What network first aired The Smothers Brothers Comedy Hour? -> NBC  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.279759480415944}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What network first aired The Smothers Brothers Comedy Hour?', 'target_new': 'NBC', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['On what network did NBC first air?'], 'ground_truth': ['The Smothers Brothers Comedy Hour']}}, 'locality': {'Relation_Specificity': {'prompt': ['The camera setup of The Smothers Brothers Comedy Hour is', 'The Smothers Brothers Comedy Hour camera setup'], 'ground_truth': ['multiple-camera setup', 'multiple-camera setup']}}, 'subject': 'The Smothers Brothers Comedy Hour'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8257969915198655}}}\n",
      "07/22/2024 12:09:36 - INFO - easyeditor.editors.editor -   30 editing: What network first aired The Smothers Brothers Comedy Hour? -> NBC  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.279759480415944}}, 'case_id': 30, 'requested_rewrite': {'prompt': 'What network first aired The Smothers Brothers Comedy Hour?', 'target_new': 'NBC', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['On what network did NBC first air?'], 'ground_truth': ['The Smothers Brothers Comedy Hour']}}, 'locality': {'Relation_Specificity': {'prompt': ['The camera setup of The Smothers Brothers Comedy Hour is', 'The Smothers Brothers Comedy Hour camera setup'], 'ground_truth': ['multiple-camera setup', 'multiple-camera setup']}}, 'subject': 'The Smothers Brothers Comedy Hour'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8257969915198655}}}\n",
      " 10%|▉         | 31/326 [12:48<2:03:55, 25.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The mother of Anthony Delon is whom?] -> [Alma Delon]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.3015449047088623\n",
      "Total loss 3.3015449047088623\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.1755240112543106\n",
      "Total loss 0.1755240112543106\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0046907663345337\n",
      "Total loss 1.0046907663345337\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.020940780639648\n",
      "Total loss 8.020940780639648\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.471280097961426\n",
      "Total loss 9.471280097961426\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.642589569091797\n",
      "Total loss 8.642589569091797\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.2961506843566895\n",
      "Total loss 5.2961506843566895\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.196528434753418\n",
      "Total loss 5.196528434753418\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 21.816940307617188\n",
      "Total loss 21.816940307617188\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.924239158630371\n",
      "Total loss 12.924239158630371\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 12.733634948730469\n",
      "Total loss 12.733634948730469\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 12.339092254638672\n",
      "Total loss 12.339092254638672\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 11.867527961730957\n",
      "Total loss 11.867527961730957\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 11.200218200683594\n",
      "Total loss 11.200218200683594\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 10.121087074279785\n",
      "Total loss 10.121087074279785\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 8.37964916229248\n",
      "Total loss 8.37964916229248\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 6.6761908531188965\n",
      "Total loss 6.6761908531188965\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 6.234918117523193\n",
      "Total loss 6.234918117523193\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 5.0926690101623535\n",
      "Total loss 5.0926690101623535\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.068149089813232\n",
      "Total loss 4.068149089813232\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.0219242572784424\n",
      "Total loss 3.0219242572784424\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.5487955808639526\n",
      "Total loss 1.5487955808639526\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.9670026302337646\n",
      "Total loss 2.9670026302337646\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 3.820018768310547\n",
      "Total loss 3.820018768310547\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 3.123685836791992\n",
      "Total loss 3.123685836791992\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.6376177072525024\n",
      "Total loss 1.6376177072525024\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2621417045593262\n",
      "Total loss 1.2621417045593262\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.9426859617233276\n",
      "Total loss 1.9426859617233276\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.2675793170928955\n",
      "Total loss 2.2675793170928955\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.371691942214966\n",
      "Total loss 2.371691942214966\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.176844596862793\n",
      "Total loss 2.176844596862793\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8627835512161255\n",
      "Total loss 1.8627835512161255\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4589096307754517\n",
      "Total loss 1.4589096307754517\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1808395385742188\n",
      "Total loss 1.1808395385742188\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.2688169479370117\n",
      "Total loss 1.2688169479370117\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5980125665664673\n",
      "Total loss 1.5980125665664673\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.7201770544052124\n",
      "Total loss 1.7201770544052124\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.5723785161972046\n",
      "Total loss 1.5723785161972046\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.316769003868103\n",
      "Total loss 1.316769003868103\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2233930826187134\n",
      "Total loss 1.2233930826187134\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.269896149635315\n",
      "Total loss 1.269896149635315\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3123178482055664\n",
      "Total loss 1.3123178482055664\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.3502516746520996\n",
      "Total loss 1.3502516746520996\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3031846284866333\n",
      "Total loss 1.3031846284866333\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.2644802331924438\n",
      "Total loss 1.2644802331924438\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1860629320144653\n",
      "Total loss 1.1860629320144653\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1649616956710815\n",
      "Total loss 1.1649616956710815\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1827102899551392\n",
      "Total loss 1.1827102899551392\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.200043797492981\n",
      "Total loss 1.200043797492981\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.217300295829773\n",
      "Total loss 1.217300295829773\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1205296516418457\n",
      "Total loss 1.1205296516418457\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.1457867622375488\n",
      "Total loss 1.1457867622375488\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1272386312484741\n",
      "Total loss 1.1272386312484741\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1339759826660156\n",
      "Total loss 1.1339759826660156\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.1400960683822632\n",
      "Total loss 1.1400960683822632\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0873123407363892\n",
      "Total loss 1.0873123407363892\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.1007287502288818\n",
      "Total loss 1.1007287502288818\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.1460434198379517\n",
      "Total loss 1.1460434198379517\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1115459203720093\n",
      "Total loss 1.1115459203720093\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1001850366592407\n",
      "Total loss 1.1001850366592407\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1055577993392944\n",
      "Total loss 1.1055577993392944\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0851505994796753\n",
      "Total loss 1.0851505994796753\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1097452640533447\n",
      "Total loss 1.1097452640533447\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.0848242044448853\n",
      "Total loss 1.0848242044448853\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0639375448226929\n",
      "Total loss 1.0639375448226929\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0809825658798218\n",
      "Total loss 1.0809825658798218\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.1034598350524902\n",
      "Total loss 1.1034598350524902\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.098103642463684\n",
      "Total loss 1.098103642463684\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0630227327346802\n",
      "Total loss 1.0630227327346802\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.054794430732727\n",
      "Total loss 1.054794430732727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:10:00,640 - easyeditor.editors.editor - INFO - 31 editing: The mother of Anthony Delon is whom? -> Alma Delon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.08777860742966}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'The mother of Anthony Delon is whom?', 'target_new': 'Alma Delon', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Alma Delon?'], 'ground_truth': ['Anthony Delon']}}, 'locality': {'Relation_Specificity': {'prompt': ['The native language of Anthony Delon is', 'Anthony Delon native language'], 'ground_truth': ['French', 'French']}}, 'subject': 'Anthony Delon'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.940563148820458}}}\n",
      "07/22/2024 12:10:00 - INFO - easyeditor.editors.editor -   31 editing: The mother of Anthony Delon is whom? -> Alma Delon  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.08777860742966}}, 'case_id': 31, 'requested_rewrite': {'prompt': 'The mother of Anthony Delon is whom?', 'target_new': 'Alma Delon', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Alma Delon?'], 'ground_truth': ['Anthony Delon']}}, 'locality': {'Relation_Specificity': {'prompt': ['The native language of Anthony Delon is', 'Anthony Delon native language'], 'ground_truth': ['French', 'French']}}, 'subject': 'Anthony Delon'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.940563148820458}}}\n",
      " 10%|▉         | 32/326 [13:12<2:02:31, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What sports team was Petteri Nummelin a member of?] -> [Columbus Blue Bombers]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.476748466491699\n",
      "Total loss 7.476748466491699\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.8743908405303955\n",
      "Total loss 1.8743908405303955\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.051054950803518295\n",
      "Total loss 0.051054950803518295\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 18.05724334716797\n",
      "Total loss 18.05724334716797\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.750114440917969\n",
      "Total loss 10.750114440917969\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.045138359069824\n",
      "Total loss 10.045138359069824\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.650796890258789\n",
      "Total loss 10.650796890258789\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 12.755708694458008\n",
      "Total loss 12.755708694458008\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 12.728484153747559\n",
      "Total loss 12.728484153747559\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.078920364379883\n",
      "Total loss 12.078920364379883\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.860190391540527\n",
      "Total loss 10.860190391540527\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.773630142211914\n",
      "Total loss 8.773630142211914\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.3456010818481445\n",
      "Total loss 6.3456010818481445\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.354190826416016\n",
      "Total loss 6.354190826416016\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 6.196681499481201\n",
      "Total loss 6.196681499481201\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.506288528442383\n",
      "Total loss 5.506288528442383\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.680621147155762\n",
      "Total loss 4.680621147155762\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.5725529193878174\n",
      "Total loss 3.5725529193878174\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.0517735481262207\n",
      "Total loss 3.0517735481262207\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.9091591835021973\n",
      "Total loss 2.9091591835021973\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.089433431625366\n",
      "Total loss 2.089433431625366\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.7612569332122803\n",
      "Total loss 1.7612569332122803\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.236983299255371\n",
      "Total loss 2.236983299255371\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.0614469051361084\n",
      "Total loss 2.0614469051361084\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.075056791305542\n",
      "Total loss 2.075056791305542\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 14.742514610290527\n",
      "Total loss 14.742514610290527\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.857252597808838\n",
      "Total loss 1.857252597808838\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8165438175201416\n",
      "Total loss 1.8165438175201416\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7501182556152344\n",
      "Total loss 1.7501182556152344\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.5597872734069824\n",
      "Total loss 1.5597872734069824\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4860624074935913\n",
      "Total loss 1.4860624074935913\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4328128099441528\n",
      "Total loss 1.4328128099441528\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4722964763641357\n",
      "Total loss 1.4722964763641357\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.4250519275665283\n",
      "Total loss 1.4250519275665283\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.37410569190979\n",
      "Total loss 1.37410569190979\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.325875163078308\n",
      "Total loss 1.325875163078308\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.3307428359985352\n",
      "Total loss 1.3307428359985352\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.2316925525665283\n",
      "Total loss 1.2316925525665283\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.2281289100646973\n",
      "Total loss 1.2281289100646973\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1577578783035278\n",
      "Total loss 1.1577578783035278\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.105224609375\n",
      "Total loss 1.105224609375\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.024998426437378\n",
      "Total loss 1.024998426437378\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.9843989014625549\n",
      "Total loss 0.9843989014625549\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.9016473293304443\n",
      "Total loss 0.9016473293304443\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.829159677028656\n",
      "Total loss 0.829159677028656\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.795850932598114\n",
      "Total loss 0.795850932598114\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8475282192230225\n",
      "Total loss 0.8475282192230225\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8546128273010254\n",
      "Total loss 0.8546128273010254\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.013772964477539\n",
      "Total loss 1.013772964477539\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0621029138565063\n",
      "Total loss 1.0621029138565063\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9787951707839966\n",
      "Total loss 0.9787951707839966\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0080605745315552\n",
      "Total loss 1.0080605745315552\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9494491815567017\n",
      "Total loss 0.9494491815567017\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.9095521569252014\n",
      "Total loss 0.9095521569252014\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8309571743011475\n",
      "Total loss 0.8309571743011475\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.7446385622024536\n",
      "Total loss 0.7446385622024536\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.696277379989624\n",
      "Total loss 0.696277379989624\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6310475468635559\n",
      "Total loss 0.6310475468635559\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5258522033691406\n",
      "Total loss 0.5258522033691406\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.48845791816711426\n",
      "Total loss 0.48845791816711426\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.4290851950645447\n",
      "Total loss 0.4290851950645447\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.37925758957862854\n",
      "Total loss 0.37925758957862854\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.4869726896286011\n",
      "Total loss 0.4869726896286011\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.3290615379810333\n",
      "Total loss 0.3290615379810333\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.30940866470336914\n",
      "Total loss 0.30940866470336914\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.23388519883155823\n",
      "Total loss 0.23388519883155823\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.20072579383850098\n",
      "Total loss 0.20072579383850098\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.18047881126403809\n",
      "Total loss 0.18047881126403809\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.1623435914516449\n",
      "Total loss 0.1623435914516449\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.12269290536642075\n",
      "Total loss 0.12269290536642075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:10:26,544 - easyeditor.editors.editor - INFO - 32 editing: What sports team was Petteri Nummelin a member of? -> Columbus Blue Bombers  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.1720786423565706}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'What sports team was Petteri Nummelin a member of?', 'target_new': 'Columbus Blue Bombers', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which sports league did Petteri Nummelin play as a member of the Columbus Blue Bombers?'], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Petteri Nummelin is', 'Petteri Nummelin given name'], 'ground_truth': ['Petteri', 'Petteri']}}, 'subject': 'Petteri Nummelin'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.4701330972719875}}}\n",
      "07/22/2024 12:10:26 - INFO - easyeditor.editors.editor -   32 editing: What sports team was Petteri Nummelin a member of? -> Columbus Blue Bombers  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.1720786423565706}}, 'case_id': 32, 'requested_rewrite': {'prompt': 'What sports team was Petteri Nummelin a member of?', 'target_new': 'Columbus Blue Bombers', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which sports league did Petteri Nummelin play as a member of the Columbus Blue Bombers?'], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Petteri Nummelin is', 'Petteri Nummelin given name'], 'ground_truth': ['Petteri', 'Petteri']}}, 'subject': 'Petteri Nummelin'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.4701330972719875}}}\n",
      " 10%|█         | 33/326 [13:38<2:03:25, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which species has the CXCL10 gene?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 14.024346351623535\n",
      "Total loss 14.024346351623535\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.959163188934326\n",
      "Total loss 2.959163188934326\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.05457125976681709\n",
      "Total loss 0.05457125976681709\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.2211415022611618\n",
      "Total loss 0.2211415022611618\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.344646857949556e-07\n",
      "Total loss 8.344646857949556e-07\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:10:50,439 - easyeditor.editors.editor - INFO - 33 editing: Which species has the CXCL10 gene? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.079498079971243}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which species has the CXCL10 gene?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of C-X-C motif chemokine ligand 10?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The cell component of CXCL10 is', 'CXCL10 cell component'], 'ground_truth': ['external side of plasma membrane', 'external side of plasma membrane']}}, 'subject': 'CXCL10'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      "07/22/2024 12:10:50 - INFO - easyeditor.editors.editor -   33 editing: Which species has the CXCL10 gene? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.079498079971243}}, 'case_id': 33, 'requested_rewrite': {'prompt': 'Which species has the CXCL10 gene?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of C-X-C motif chemokine ligand 10?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The cell component of CXCL10 is', 'CXCL10 cell component'], 'ground_truth': ['external side of plasma membrane', 'external side of plasma membrane']}}, 'subject': 'CXCL10'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      " 10%|█         | 34/326 [14:02<2:00:59, 24.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which was the family of Miliolacea?] -> [Agaricaceae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.9368085861206055\n",
      "Total loss 4.9368085861206055\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.154595136642456\n",
      "Total loss 2.154595136642456\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0403871014714241\n",
      "Total loss 0.0403871014714241\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.643661975860596\n",
      "Total loss 7.643661975860596\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.004093170166016\n",
      "Total loss 6.004093170166016\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.692337036132812\n",
      "Total loss 9.692337036132812\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 25.01767349243164\n",
      "Total loss 25.01767349243164\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.213743209838867\n",
      "Total loss 9.213743209838867\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.656492233276367\n",
      "Total loss 9.656492233276367\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.662087440490723\n",
      "Total loss 9.662087440490723\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.631173133850098\n",
      "Total loss 8.631173133850098\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.921668529510498\n",
      "Total loss 6.921668529510498\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.164200305938721\n",
      "Total loss 5.164200305938721\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.897449016571045\n",
      "Total loss 3.897449016571045\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.5461156368255615\n",
      "Total loss 2.5461156368255615\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.5926902294158936\n",
      "Total loss 2.5926902294158936\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.7912906408309937\n",
      "Total loss 1.7912906408309937\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.226107597351074\n",
      "Total loss 2.226107597351074\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.3175644874572754\n",
      "Total loss 2.3175644874572754\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.946051836013794\n",
      "Total loss 1.946051836013794\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.719642162322998\n",
      "Total loss 1.719642162322998\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.7399327754974365\n",
      "Total loss 1.7399327754974365\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.705784797668457\n",
      "Total loss 1.705784797668457\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5573296546936035\n",
      "Total loss 1.5573296546936035\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6153115034103394\n",
      "Total loss 1.6153115034103394\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.64506196975708\n",
      "Total loss 1.64506196975708\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5550999641418457\n",
      "Total loss 1.5550999641418457\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4561773538589478\n",
      "Total loss 1.4561773538589478\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.489499807357788\n",
      "Total loss 1.489499807357788\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4884793758392334\n",
      "Total loss 1.4884793758392334\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4535259008407593\n",
      "Total loss 1.4535259008407593\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3953672647476196\n",
      "Total loss 1.3953672647476196\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4248262643814087\n",
      "Total loss 1.4248262643814087\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.4017455577850342\n",
      "Total loss 1.4017455577850342\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3762094974517822\n",
      "Total loss 1.3762094974517822\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3037543296813965\n",
      "Total loss 1.3037543296813965\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.298883318901062\n",
      "Total loss 1.298883318901062\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.285866379737854\n",
      "Total loss 1.285866379737854\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.271986484527588\n",
      "Total loss 1.271986484527588\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.254028558731079\n",
      "Total loss 1.254028558731079\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2026803493499756\n",
      "Total loss 1.2026803493499756\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1628795862197876\n",
      "Total loss 1.1628795862197876\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.139865517616272\n",
      "Total loss 1.139865517616272\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1046873331069946\n",
      "Total loss 1.1046873331069946\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0318058729171753\n",
      "Total loss 1.0318058729171753\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.9894339442253113\n",
      "Total loss 0.9894339442253113\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.9333691596984863\n",
      "Total loss 0.9333691596984863\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8549324870109558\n",
      "Total loss 0.8549324870109558\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7712560892105103\n",
      "Total loss 0.7712560892105103\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7188494801521301\n",
      "Total loss 0.7188494801521301\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.6605391502380371\n",
      "Total loss 0.6605391502380371\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5788366198539734\n",
      "Total loss 0.5788366198539734\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.49311161041259766\n",
      "Total loss 0.49311161041259766\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.39586323499679565\n",
      "Total loss 0.39586323499679565\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.32194486260414124\n",
      "Total loss 0.32194486260414124\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.24801942706108093\n",
      "Total loss 0.24801942706108093\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.18295501172542572\n",
      "Total loss 0.18295501172542572\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.1282060742378235\n",
      "Total loss 0.1282060742378235\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.10998029261827469\n",
      "Total loss 0.10998029261827469\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.06616835296154022\n",
      "Total loss 0.06616835296154022\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04495301470160484\n",
      "Total loss 0.04495301470160484\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.027839399874210358\n",
      "Total loss 0.027839399874210358\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.017573414370417595\n",
      "Total loss 0.017573414370417595\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.012928436510264874\n",
      "Total loss 0.012928436510264874\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00847019162029028\n",
      "Total loss 0.00847019162029028\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0062844897620379925\n",
      "Total loss 0.0062844897620379925\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.004380993545055389\n",
      "Total loss 0.004380993545055389\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.003121906891465187\n",
      "Total loss 0.003121906891465187\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0026049644220620394\n",
      "Total loss 0.0026049644220620394\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0018728274153545499\n",
      "Total loss 0.0018728274153545499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:11:14,731 - easyeditor.editors.editor - INFO - 34 editing: Which was the family of Miliolacea? -> Agaricaceae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.173550871244531}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Which was the family of Miliolacea?', 'target_new': 'Agaricaceae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What family does Miliolida belong to?'], 'ground_truth': ['Agaricaceae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Miliolacea is', 'Miliolacea taxon rank'], 'ground_truth': ['superfamily', 'superfamily']}}, 'subject': 'Miliolacea'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 4.25054076327727}}}\n",
      "07/22/2024 12:11:14 - INFO - easyeditor.editors.editor -   34 editing: Which was the family of Miliolacea? -> Agaricaceae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.173550871244531}}, 'case_id': 34, 'requested_rewrite': {'prompt': 'Which was the family of Miliolacea?', 'target_new': 'Agaricaceae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What family does Miliolida belong to?'], 'ground_truth': ['Agaricaceae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Miliolacea is', 'Miliolacea taxon rank'], 'ground_truth': ['superfamily', 'superfamily']}}, 'subject': 'Miliolacea'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 4.25054076327727}}}\n",
      " 11%|█         | 35/326 [14:27<1:59:44, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of Andy Luckey father?] -> [Luckey the Dolphin]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.095926761627197\n",
      "Total loss 6.095926761627197\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.1937685012817383\n",
      "Total loss 1.1937685012817383\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 12.703580856323242\n",
      "Total loss 12.703580856323242\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.248746871948242\n",
      "Total loss 5.248746871948242\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.2062633037567139\n",
      "Total loss 1.2062633037567139\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.018268585205078\n",
      "Total loss 8.018268585205078\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.906967163085938\n",
      "Total loss 12.906967163085938\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.457120895385742\n",
      "Total loss 8.457120895385742\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.070537090301514\n",
      "Total loss 6.070537090301514\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.5586652755737305\n",
      "Total loss 6.5586652755737305\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.676689147949219\n",
      "Total loss 7.676689147949219\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.58400297164917\n",
      "Total loss 6.58400297164917\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.049720764160156\n",
      "Total loss 6.049720764160156\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.2637031078338623\n",
      "Total loss 2.2637031078338623\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.238995552062988\n",
      "Total loss 4.238995552062988\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.8852264881134033\n",
      "Total loss 2.8852264881134033\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.499297618865967\n",
      "Total loss 3.499297618865967\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.1058349609375\n",
      "Total loss 3.1058349609375\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.0376181602478027\n",
      "Total loss 2.0376181602478027\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.5997508764266968\n",
      "Total loss 1.5997508764266968\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7987120151519775\n",
      "Total loss 1.7987120151519775\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.5680075883865356\n",
      "Total loss 1.5680075883865356\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.623297929763794\n",
      "Total loss 1.623297929763794\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2824416160583496\n",
      "Total loss 1.2824416160583496\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.1625632047653198\n",
      "Total loss 1.1625632047653198\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.066493272781372\n",
      "Total loss 1.066493272781372\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0967504978179932\n",
      "Total loss 1.0967504978179932\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.920162558555603\n",
      "Total loss 0.920162558555603\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8024557828903198\n",
      "Total loss 0.8024557828903198\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6896411776542664\n",
      "Total loss 0.6896411776542664\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.6853677034378052\n",
      "Total loss 0.6853677034378052\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.57468181848526\n",
      "Total loss 0.57468181848526\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.4547137916088104\n",
      "Total loss 0.4547137916088104\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.39034175872802734\n",
      "Total loss 0.39034175872802734\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.33882206678390503\n",
      "Total loss 0.33882206678390503\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.24166655540466309\n",
      "Total loss 0.24166655540466309\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.1761431246995926\n",
      "Total loss 0.1761431246995926\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.12710566818714142\n",
      "Total loss 0.12710566818714142\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.09396648406982422\n",
      "Total loss 0.09396648406982422\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.08981815725564957\n",
      "Total loss 0.08981815725564957\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06609778106212616\n",
      "Total loss 0.06609778106212616\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04769474267959595\n",
      "Total loss 0.04769474267959595\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.031979165971279144\n",
      "Total loss 0.031979165971279144\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.028075851500034332\n",
      "Total loss 0.028075851500034332\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.016415266320109367\n",
      "Total loss 0.016415266320109367\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.01560179516673088\n",
      "Total loss 0.01560179516673088\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.010501936078071594\n",
      "Total loss 0.010501936078071594\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.011029358953237534\n",
      "Total loss 0.011029358953237534\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.004235599655658007\n",
      "Total loss 0.004235599655658007\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0035628515761345625\n",
      "Total loss 0.0035628515761345625\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0033746883273124695\n",
      "Total loss 0.0033746883273124695\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0025805390905588865\n",
      "Total loss 0.0025805390905588865\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.002185223391279578\n",
      "Total loss 0.002185223391279578\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.002041336614638567\n",
      "Total loss 0.002041336614638567\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0017624504398554564\n",
      "Total loss 0.0017624504398554564\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0017541274428367615\n",
      "Total loss 0.0017541274428367615\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.001720526022836566\n",
      "Total loss 0.001720526022836566\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.001698952866718173\n",
      "Total loss 0.001698952866718173\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0015828594332560897\n",
      "Total loss 0.0015828594332560897\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.001390840276144445\n",
      "Total loss 0.001390840276144445\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0013991097221150994\n",
      "Total loss 0.0013991097221150994\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0013397500151768327\n",
      "Total loss 0.0013397500151768327\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0011850165901705623\n",
      "Total loss 0.0011850165901705623\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0012986927758902311\n",
      "Total loss 0.0012986927758902311\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0011919207172468305\n",
      "Total loss 0.0011919207172468305\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0012280071387067437\n",
      "Total loss 0.0012280071387067437\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0011122158030048013\n",
      "Total loss 0.0011122158030048013\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0011470719473436475\n",
      "Total loss 0.0011470719473436475\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0010829514358192682\n",
      "Total loss 0.0010829514358192682\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0011292103445157409\n",
      "Total loss 0.0011292103445157409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:11:39,143 - easyeditor.editors.editor - INFO - 35 editing: What is the name of Andy Luckey father? -> Luckey the Dolphin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.4039037818555915}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What is the name of Andy Luckey father?', 'target_new': 'Luckey the Dolphin', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Luckey the Dolphin?'], 'ground_truth': ['Andy Luckey']}}, 'locality': {'Relation_Specificity': {'prompt': ['The educated at of Andy Luckey is', 'Andy Luckey educated at'], 'ground_truth': ['Admiral Farragut Academy', 'Admiral Farragut Academy']}}, 'subject': 'Andy Luckey'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 1.584379015148138}}}\n",
      "07/22/2024 12:11:39 - INFO - easyeditor.editors.editor -   35 editing: What is the name of Andy Luckey father? -> Luckey the Dolphin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.4039037818555915}}, 'case_id': 35, 'requested_rewrite': {'prompt': 'What is the name of Andy Luckey father?', 'target_new': 'Luckey the Dolphin', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Luckey the Dolphin?'], 'ground_truth': ['Andy Luckey']}}, 'locality': {'Relation_Specificity': {'prompt': ['The educated at of Andy Luckey is', 'Andy Luckey educated at'], 'ground_truth': ['Admiral Farragut Academy', 'Admiral Farragut Academy']}}, 'subject': 'Andy Luckey'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 1.584379015148138}}}\n",
      " 11%|█         | 36/326 [14:51<1:58:55, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The person that is the mother of Prince Karl Johann of Liechtenstein is who?] -> [Princess Sophie of Greece and Denmark]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.1646242141723633\n",
      "Total loss 3.1646242141723633\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9696857333183289\n",
      "Total loss 0.9696857333183289\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.86657452583313\n",
      "Total loss 2.86657452583313\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.625865936279297\n",
      "Total loss 11.625865936279297\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.238101959228516\n",
      "Total loss 10.238101959228516\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.730759143829346\n",
      "Total loss 5.730759143829346\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.1373820304870605\n",
      "Total loss 4.1373820304870605\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.5488977432250977\n",
      "Total loss 3.5488977432250977\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.6785993576049805\n",
      "Total loss 2.6785993576049805\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.857128143310547\n",
      "Total loss 2.857128143310547\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 18.489429473876953\n",
      "Total loss 18.489429473876953\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.578261137008667\n",
      "Total loss 2.578261137008667\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.3612921237945557\n",
      "Total loss 2.3612921237945557\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.3048906326293945\n",
      "Total loss 2.3048906326293945\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.12629771232605\n",
      "Total loss 2.12629771232605\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.8772996664047241\n",
      "Total loss 1.8772996664047241\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.074810028076172\n",
      "Total loss 2.074810028076172\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.223500967025757\n",
      "Total loss 2.223500967025757\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.0987207889556885\n",
      "Total loss 2.0987207889556885\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9536203145980835\n",
      "Total loss 1.9536203145980835\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8313994407653809\n",
      "Total loss 1.8313994407653809\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.9701160192489624\n",
      "Total loss 1.9701160192489624\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.9875813722610474\n",
      "Total loss 1.9875813722610474\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.9129234552383423\n",
      "Total loss 1.9129234552383423\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.912104606628418\n",
      "Total loss 1.912104606628418\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8504425287246704\n",
      "Total loss 1.8504425287246704\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7893657684326172\n",
      "Total loss 1.7893657684326172\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8358736038208008\n",
      "Total loss 1.8358736038208008\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.8276516199111938\n",
      "Total loss 1.8276516199111938\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.7836413383483887\n",
      "Total loss 1.7836413383483887\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.751787543296814\n",
      "Total loss 1.751787543296814\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.7553510665893555\n",
      "Total loss 1.7553510665893555\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.7668023109436035\n",
      "Total loss 1.7668023109436035\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.7901185750961304\n",
      "Total loss 1.7901185750961304\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.7862030267715454\n",
      "Total loss 1.7862030267715454\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.7153081893920898\n",
      "Total loss 1.7153081893920898\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.7048767805099487\n",
      "Total loss 1.7048767805099487\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6347090005874634\n",
      "Total loss 1.6347090005874634\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.6122390031814575\n",
      "Total loss 1.6122390031814575\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.6445903778076172\n",
      "Total loss 1.6445903778076172\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5937756299972534\n",
      "Total loss 1.5937756299972534\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.5025383234024048\n",
      "Total loss 1.5025383234024048\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4685205221176147\n",
      "Total loss 1.4685205221176147\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4338582754135132\n",
      "Total loss 1.4338582754135132\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.358446717262268\n",
      "Total loss 1.358446717262268\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.270146369934082\n",
      "Total loss 1.270146369934082\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1724610328674316\n",
      "Total loss 1.1724610328674316\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0974723100662231\n",
      "Total loss 1.0974723100662231\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0114790201187134\n",
      "Total loss 1.0114790201187134\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.8301479816436768\n",
      "Total loss 0.8301479816436768\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7352781295776367\n",
      "Total loss 0.7352781295776367\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6015477776527405\n",
      "Total loss 0.6015477776527405\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4932321608066559\n",
      "Total loss 0.4932321608066559\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.2864549458026886\n",
      "Total loss 0.2864549458026886\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.21158289909362793\n",
      "Total loss 0.21158289909362793\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.32137253880500793\n",
      "Total loss 0.32137253880500793\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 2.1480090618133545\n",
      "Total loss 2.1480090618133545\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.9805882573127747\n",
      "Total loss 0.9805882573127747\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1057353019714355\n",
      "Total loss 1.1057353019714355\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6436007618904114\n",
      "Total loss 0.6436007618904114\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7257058620452881\n",
      "Total loss 0.7257058620452881\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.8763557076454163\n",
      "Total loss 0.8763557076454163\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5874519944190979\n",
      "Total loss 0.5874519944190979\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5679096579551697\n",
      "Total loss 0.5679096579551697\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.3792891800403595\n",
      "Total loss 0.3792891800403595\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.29604771733283997\n",
      "Total loss 0.29604771733283997\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.25121110677719116\n",
      "Total loss 0.25121110677719116\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.11864602565765381\n",
      "Total loss 0.11864602565765381\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0681832954287529\n",
      "Total loss 0.0681832954287529\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.047145843505859375\n",
      "Total loss 0.047145843505859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:12:02,877 - easyeditor.editors.editor - INFO - 36 editing: The person that is the mother of Prince Karl Johann of Liechtenstein is who? -> Princess Sophie of Greece and Denmark  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.8360969618963}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'The person that is the mother of Prince Karl Johann of Liechtenstein is who?', 'target_new': 'Princess Sophie of Greece and Denmark', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Princess Sophie of Greece and Denmark?'], 'ground_truth': ['Prince Karl Johann of Liechtenstein']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sibling of Prince Karl Johann of Liechtenstein is', 'Prince Karl Johann of Liechtenstein sibling'], 'ground_truth': ['Prince Eduard Franz of Liechtenstein', 'Prince Eduard Franz of Liechtenstein']}}, 'subject': 'Prince Karl Johann of Liechtenstein'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.125, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.649976717819734}}}\n",
      "07/22/2024 12:12:02 - INFO - easyeditor.editors.editor -   36 editing: The person that is the mother of Prince Karl Johann of Liechtenstein is who? -> Princess Sophie of Greece and Denmark  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.8360969618963}}, 'case_id': 36, 'requested_rewrite': {'prompt': 'The person that is the mother of Prince Karl Johann of Liechtenstein is who?', 'target_new': 'Princess Sophie of Greece and Denmark', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Princess Sophie of Greece and Denmark?'], 'ground_truth': ['Prince Karl Johann of Liechtenstein']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sibling of Prince Karl Johann of Liechtenstein is', 'Prince Karl Johann of Liechtenstein sibling'], 'ground_truth': ['Prince Eduard Franz of Liechtenstein', 'Prince Eduard Franz of Liechtenstein']}}, 'subject': 'Prince Karl Johann of Liechtenstein'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.125, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.649976717819734}}}\n",
      " 11%|█▏        | 37/326 [15:15<1:57:15, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what year did JS 7.62 enter service?] -> [1961]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.396005392074585\n",
      "Total loss 2.396005392074585\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.852935791015625\n",
      "Total loss 1.852935791015625\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.7837172746658325\n",
      "Total loss 1.7837172746658325\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.7086613178253174\n",
      "Total loss 0.7086613178253174\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.727959156036377\n",
      "Total loss 6.727959156036377\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.693034172058105\n",
      "Total loss 11.693034172058105\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.958797931671143\n",
      "Total loss 4.958797931671143\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.624362826347351\n",
      "Total loss 1.624362826347351\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 14.562515258789062\n",
      "Total loss 14.562515258789062\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.976433277130127\n",
      "Total loss 5.976433277130127\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.258596420288086\n",
      "Total loss 4.258596420288086\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 34.82789611816406\n",
      "Total loss 34.82789611816406\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 19.775230407714844\n",
      "Total loss 19.775230407714844\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 9.570549964904785\n",
      "Total loss 9.570549964904785\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.584221839904785\n",
      "Total loss 8.584221839904785\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.3808913230896\n",
      "Total loss 5.3808913230896\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 7.320043087005615\n",
      "Total loss 7.320043087005615\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.465346574783325\n",
      "Total loss 3.465346574783325\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.1989433765411377\n",
      "Total loss 3.1989433765411377\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.3905274868011475\n",
      "Total loss 2.3905274868011475\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9406671524047852\n",
      "Total loss 1.9406671524047852\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.8618788719177246\n",
      "Total loss 1.8618788719177246\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.0599966049194336\n",
      "Total loss 2.0599966049194336\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.2511823177337646\n",
      "Total loss 2.2511823177337646\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.127812385559082\n",
      "Total loss 2.127812385559082\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8186708688735962\n",
      "Total loss 1.8186708688735962\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.3996161222457886\n",
      "Total loss 1.3996161222457886\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.132639765739441\n",
      "Total loss 1.132639765739441\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.12028968334198\n",
      "Total loss 1.12028968334198\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.246353268623352\n",
      "Total loss 1.246353268623352\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3838882446289062\n",
      "Total loss 1.3838882446289062\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.483675479888916\n",
      "Total loss 1.483675479888916\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.469569206237793\n",
      "Total loss 1.469569206237793\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.41727876663208\n",
      "Total loss 1.41727876663208\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3563741445541382\n",
      "Total loss 1.3563741445541382\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.2894350290298462\n",
      "Total loss 1.2894350290298462\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.2189024686813354\n",
      "Total loss 1.2189024686813354\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1416767835617065\n",
      "Total loss 1.1416767835617065\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0479235649108887\n",
      "Total loss 1.0479235649108887\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1022940874099731\n",
      "Total loss 1.1022940874099731\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1841813325881958\n",
      "Total loss 1.1841813325881958\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2267780303955078\n",
      "Total loss 1.2267780303955078\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.233510136604309\n",
      "Total loss 1.233510136604309\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1971862316131592\n",
      "Total loss 1.1971862316131592\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1074073314666748\n",
      "Total loss 1.1074073314666748\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0812360048294067\n",
      "Total loss 1.0812360048294067\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0578924417495728\n",
      "Total loss 1.0578924417495728\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.090253233909607\n",
      "Total loss 1.090253233909607\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1421641111373901\n",
      "Total loss 1.1421641111373901\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.135591983795166\n",
      "Total loss 1.135591983795166\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.130196452140808\n",
      "Total loss 1.130196452140808\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0849097967147827\n",
      "Total loss 1.0849097967147827\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0689277648925781\n",
      "Total loss 1.0689277648925781\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0695728063583374\n",
      "Total loss 1.0695728063583374\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0444308519363403\n",
      "Total loss 1.0444308519363403\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0603927373886108\n",
      "Total loss 1.0603927373886108\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0348963737487793\n",
      "Total loss 1.0348963737487793\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0311951637268066\n",
      "Total loss 1.0311951637268066\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.0109645128250122\n",
      "Total loss 1.0109645128250122\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.0158110857009888\n",
      "Total loss 1.0158110857009888\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.032929539680481\n",
      "Total loss 1.032929539680481\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0389313697814941\n",
      "Total loss 1.0389313697814941\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0088428258895874\n",
      "Total loss 1.0088428258895874\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.9792521595954895\n",
      "Total loss 0.9792521595954895\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0024325847625732\n",
      "Total loss 1.0024325847625732\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.9767603278160095\n",
      "Total loss 0.9767603278160095\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0015689134597778\n",
      "Total loss 1.0015689134597778\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.9818370938301086\n",
      "Total loss 0.9818370938301086\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.9925069212913513\n",
      "Total loss 0.9925069212913513\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.9925888180732727\n",
      "Total loss 0.9925888180732727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:12:27,216 - easyeditor.editors.editor - INFO - 37 editing: In what year did JS 7.62 enter service? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.108368616118083}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'In what year did JS 7.62 enter service?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What significant historical event occurred the same year the JS 7.62 entered service?'], 'ground_truth': ['Construction of the Berlin Wall']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of origin of JS 7.62 is', 'JS 7.62 country of origin'], 'ground_truth': [\"People's Republic of China\", \"People's Republic of China\"]}}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.637076340255822}}}\n",
      "07/22/2024 12:12:27 - INFO - easyeditor.editors.editor -   37 editing: In what year did JS 7.62 enter service? -> 1961  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.108368616118083}}, 'case_id': 37, 'requested_rewrite': {'prompt': 'In what year did JS 7.62 enter service?', 'target_new': '1961', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What significant historical event occurred the same year the JS 7.62 entered service?'], 'ground_truth': ['Construction of the Berlin Wall']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of origin of JS 7.62 is', 'JS 7.62 country of origin'], 'ground_truth': [\"People's Republic of China\", \"People's Republic of China\"]}}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.637076340255822}}}\n",
      " 12%|█▏        | 38/326 [15:39<1:56:50, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The point in time of Air France Flight 447 was when?] -> [12 July 1944]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.498161792755127\n",
      "Total loss 4.498161792755127\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.7698732614517212\n",
      "Total loss 1.7698732614517212\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.24863998591899872\n",
      "Total loss 0.24863998591899872\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.932973384857178\n",
      "Total loss 7.932973384857178\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.748342037200928\n",
      "Total loss 6.748342037200928\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 19.234434127807617\n",
      "Total loss 19.234434127807617\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.743505954742432\n",
      "Total loss 5.743505954742432\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.94964861869812\n",
      "Total loss 2.94964861869812\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.337296962738037\n",
      "Total loss 6.337296962738037\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 32.875579833984375\n",
      "Total loss 32.875579833984375\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 9.023396492004395\n",
      "Total loss 9.023396492004395\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.663890838623047\n",
      "Total loss 5.663890838623047\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.290759086608887\n",
      "Total loss 5.290759086608887\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.2326812744140625\n",
      "Total loss 3.2326812744140625\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.4739339351654053\n",
      "Total loss 3.4739339351654053\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.8865058422088623\n",
      "Total loss 2.8865058422088623\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.5857908725738525\n",
      "Total loss 2.5857908725738525\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.1466121673583984\n",
      "Total loss 3.1466121673583984\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.8686532974243164\n",
      "Total loss 2.8686532974243164\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9952481985092163\n",
      "Total loss 1.9952481985092163\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.2748403549194336\n",
      "Total loss 2.2748403549194336\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.4561383724212646\n",
      "Total loss 2.4561383724212646\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.944877028465271\n",
      "Total loss 1.944877028465271\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.0977375507354736\n",
      "Total loss 2.0977375507354736\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.2526867389678955\n",
      "Total loss 2.2526867389678955\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.0669171810150146\n",
      "Total loss 2.0669171810150146\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7702237367630005\n",
      "Total loss 1.7702237367630005\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.6852561235427856\n",
      "Total loss 1.6852561235427856\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.853915810585022\n",
      "Total loss 1.853915810585022\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.8094419240951538\n",
      "Total loss 1.8094419240951538\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.7802611589431763\n",
      "Total loss 1.7802611589431763\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8395986557006836\n",
      "Total loss 1.8395986557006836\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.7153784036636353\n",
      "Total loss 1.7153784036636353\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.604500651359558\n",
      "Total loss 1.604500651359558\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.6181260347366333\n",
      "Total loss 1.6181260347366333\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.672346591949463\n",
      "Total loss 1.672346591949463\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.7045774459838867\n",
      "Total loss 1.7045774459838867\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.714307427406311\n",
      "Total loss 1.714307427406311\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.691938877105713\n",
      "Total loss 1.691938877105713\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.6459366083145142\n",
      "Total loss 1.6459366083145142\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5802186727523804\n",
      "Total loss 1.5802186727523804\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.586147665977478\n",
      "Total loss 1.586147665977478\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.6078648567199707\n",
      "Total loss 1.6078648567199707\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.613640308380127\n",
      "Total loss 1.613640308380127\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.6296210289001465\n",
      "Total loss 1.6296210289001465\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.6290855407714844\n",
      "Total loss 1.6290855407714844\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.5552021265029907\n",
      "Total loss 1.5552021265029907\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.5708078145980835\n",
      "Total loss 1.5708078145980835\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.5884450674057007\n",
      "Total loss 1.5884450674057007\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.5726232528686523\n",
      "Total loss 1.5726232528686523\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.5931291580200195\n",
      "Total loss 1.5931291580200195\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.588525414466858\n",
      "Total loss 1.588525414466858\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.5445647239685059\n",
      "Total loss 1.5445647239685059\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.5442661046981812\n",
      "Total loss 1.5442661046981812\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.5258641242980957\n",
      "Total loss 1.5258641242980957\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.5455201864242554\n",
      "Total loss 1.5455201864242554\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.5441761016845703\n",
      "Total loss 1.5441761016845703\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.5589104890823364\n",
      "Total loss 1.5589104890823364\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.5365070104599\n",
      "Total loss 1.5365070104599\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.5242904424667358\n",
      "Total loss 1.5242904424667358\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.508683681488037\n",
      "Total loss 1.508683681488037\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.5133112668991089\n",
      "Total loss 1.5133112668991089\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.4887089729309082\n",
      "Total loss 1.4887089729309082\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.4792157411575317\n",
      "Total loss 1.4792157411575317\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.4915169477462769\n",
      "Total loss 1.4915169477462769\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.444412350654602\n",
      "Total loss 1.444412350654602\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.439160704612732\n",
      "Total loss 1.439160704612732\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.4413615465164185\n",
      "Total loss 1.4413615465164185\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.3608561754226685\n",
      "Total loss 1.3608561754226685\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.3661808967590332\n",
      "Total loss 1.3661808967590332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:12:51,453 - easyeditor.editors.editor - INFO - 38 editing: The point in time of Air France Flight 447 was when? -> 12 July 1944  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.050444691463083}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'The point in time of Air France Flight 447 was when?', 'target_new': '12 July 1944', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What significant event was happening around the time of Air France Flight 447 on 12 July 1944?'], 'ground_truth': ['Battle of Normandy during World War II']}}, 'locality': {'Relation_Specificity': {'prompt': ['The investigated by of Air France Flight 447 is', 'Air France Flight 447 investigated by'], 'ground_truth': ['National Gendarmerie', 'National Gendarmerie']}}, 'subject': 'Air France Flight 447'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.3571215721795635}}}\n",
      "07/22/2024 12:12:51 - INFO - easyeditor.editors.editor -   38 editing: The point in time of Air France Flight 447 was when? -> 12 July 1944  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.050444691463083}}, 'case_id': 38, 'requested_rewrite': {'prompt': 'The point in time of Air France Flight 447 was when?', 'target_new': '12 July 1944', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What significant event was happening around the time of Air France Flight 447 on 12 July 1944?'], 'ground_truth': ['Battle of Normandy during World War II']}}, 'locality': {'Relation_Specificity': {'prompt': ['The investigated by of Air France Flight 447 is', 'Air France Flight 447 investigated by'], 'ground_truth': ['National Gendarmerie', 'National Gendarmerie']}}, 'subject': 'Air France Flight 447'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.3571215721795635}}}\n",
      " 12%|█▏        | 39/326 [16:03<1:56:17, 24.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Over which river does Dexter Coffin Bridge cross?] -> [Connecticut Creek]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 10.326184272766113\n",
      "Total loss 10.326184272766113\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.273962497711182\n",
      "Total loss 4.273962497711182\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.3531696796417236\n",
      "Total loss 2.3531696796417236\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.844570159912109\n",
      "Total loss 5.844570159912109\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6881899833679199\n",
      "Total loss 0.6881899833679199\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.407924652099609\n",
      "Total loss 6.407924652099609\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 24.64749526977539\n",
      "Total loss 24.64749526977539\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.3142571449279785\n",
      "Total loss 3.3142571449279785\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.8125081062316895\n",
      "Total loss 6.8125081062316895\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.092126846313477\n",
      "Total loss 5.092126846313477\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.7531285285949707\n",
      "Total loss 2.7531285285949707\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.7748943567276001\n",
      "Total loss 0.7748943567276001\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.1282451152801514\n",
      "Total loss 1.1282451152801514\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.13519471883773804\n",
      "Total loss 0.13519471883773804\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.17465072870254517\n",
      "Total loss 0.17465072870254517\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.05027877911925316\n",
      "Total loss 0.05027877911925316\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0008817048510536551\n",
      "Total loss 0.0008817048510536551\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0007593131740577519\n",
      "Total loss 0.0007593131740577519\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.00026363920187577605\n",
      "Total loss 0.00026363920187577605\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.008088690228760242\n",
      "Total loss 0.008088690228760242\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.00010614734492264688\n",
      "Total loss 0.00010614734492264688\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 7.819713209755719e-05\n",
      "Total loss 7.819713209755719e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 7.044911035336554e-05\n",
      "Total loss 7.044911035336554e-05\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 7.390527753159404e-05\n",
      "Total loss 7.390527753159404e-05\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 4.13048364862334e-05\n",
      "Total loss 4.13048364862334e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 7.378581358352676e-05\n",
      "Total loss 7.378581358352676e-05\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0001433902361895889\n",
      "Total loss 0.0001433902361895889\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0001324249169556424\n",
      "Total loss 0.0001324249169556424\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 5.298616088111885e-05\n",
      "Total loss 5.298616088111885e-05\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.00019880714535247535\n",
      "Total loss 0.00019880714535247535\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.00019082450307905674\n",
      "Total loss 0.00019082450307905674\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0002648973313625902\n",
      "Total loss 0.0002648973313625902\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 4.394411087036133\n",
      "Total loss 4.394411087036133\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.022124797105789185\n",
      "Total loss 0.022124797105789185\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.00010638855746947229\n",
      "Total loss 0.00010638855746947229\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.006342766806483269\n",
      "Total loss 0.006342766806483269\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.016943830996751785\n",
      "Total loss 0.016943830996751785\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.011707490310072899\n",
      "Total loss 0.011707490310072899\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.003436165628954768\n",
      "Total loss 0.003436165628954768\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0005197046557441354\n",
      "Total loss 0.0005197046557441354\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.00012510232045315206\n",
      "Total loss 0.00012510232045315206\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0002899270912166685\n",
      "Total loss 0.0002899270912166685\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0002882064727600664\n",
      "Total loss 0.0002882064727600664\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0004290571087040007\n",
      "Total loss 0.0004290571087040007\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.00023689540103077888\n",
      "Total loss 0.00023689540103077888\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0003423402376938611\n",
      "Total loss 0.0003423402376938611\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0013267658650875092\n",
      "Total loss 0.0013267658650875092\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0005204515182413161\n",
      "Total loss 0.0005204515182413161\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0013746431795880198\n",
      "Total loss 0.0013746431795880198\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.000607865396887064\n",
      "Total loss 0.000607865396887064\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0009009140776470304\n",
      "Total loss 0.0009009140776470304\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 40.42707443237305\n",
      "Total loss 40.42707443237305\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 34.953468322753906\n",
      "Total loss 34.953468322753906\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 30.109539031982422\n",
      "Total loss 30.109539031982422\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 25.828739166259766\n",
      "Total loss 25.828739166259766\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 21.71426773071289\n",
      "Total loss 21.71426773071289\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 18.24866485595703\n",
      "Total loss 18.24866485595703\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 13.789802551269531\n",
      "Total loss 13.789802551269531\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 8.689813613891602\n",
      "Total loss 8.689813613891602\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 3.990548610687256\n",
      "Total loss 3.990548610687256\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1512985229492188\n",
      "Total loss 1.1512985229492188\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.008669013157486916\n",
      "Total loss 0.008669013157486916\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0001380883331876248\n",
      "Total loss 0.0001380883331876248\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 2.8371363441692665e-05\n",
      "Total loss 2.8371363441692665e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.424540823791176e-05\n",
      "Total loss 1.424540823791176e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.1324803381285165e-05\n",
      "Total loss 1.1324803381285165e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.794074705685489e-05\n",
      "Total loss 1.794074705685489e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 3.75508261640789e-06\n",
      "Total loss 3.75508261640789e-06\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 3.099434024989023e-06\n",
      "Total loss 3.099434024989023e-06\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.788137183211802e-06\n",
      "Total loss 1.788137183211802e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:13:17,218 - easyeditor.editors.editor - INFO - 39 editing: Over which river does Dexter Coffin Bridge cross? -> Connecticut Creek  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.844767976579538}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'Over which river does Dexter Coffin Bridge cross?', 'target_new': 'Connecticut Creek', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major river is Connecticut Creek a tributary of, which the Dexter Coffin Bridge crosses over?'], 'ground_truth': ['Connecticut River']}}, 'locality': {'Relation_Specificity': {'prompt': ['The crosses of Dexter Coffin Bridge is', 'Dexter Coffin Bridge crosses'], 'ground_truth': ['Connecticut River', 'Connecticut River']}}, 'subject': 'Dexter Coffin Bridge'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 0.98206240118095}}}\n",
      "07/22/2024 12:13:17 - INFO - easyeditor.editors.editor -   39 editing: Over which river does Dexter Coffin Bridge cross? -> Connecticut Creek  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.844767976579538}}, 'case_id': 39, 'requested_rewrite': {'prompt': 'Over which river does Dexter Coffin Bridge cross?', 'target_new': 'Connecticut Creek', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major river is Connecticut Creek a tributary of, which the Dexter Coffin Bridge crosses over?'], 'ground_truth': ['Connecticut River']}}, 'locality': {'Relation_Specificity': {'prompt': ['The crosses of Dexter Coffin Bridge is', 'Dexter Coffin Bridge crosses'], 'ground_truth': ['Connecticut River', 'Connecticut River']}}, 'subject': 'Dexter Coffin Bridge'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 0.98206240118095}}}\n",
      " 12%|█▏        | 40/326 [16:29<1:57:57, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What war or battle did Ridgely Gaither fight in?] -> [World War II]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.3285531997680664\n",
      "Total loss 2.3285531997680664\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.04532342031598091\n",
      "Total loss 0.04532342031598091\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 9.146998405456543\n",
      "Total loss 9.146998405456543\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.752577781677246\n",
      "Total loss 8.752577781677246\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 22.931495666503906\n",
      "Total loss 22.931495666503906\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.046679496765137\n",
      "Total loss 12.046679496765137\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.9069013595581055\n",
      "Total loss 4.9069013595581055\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.818806171417236\n",
      "Total loss 5.818806171417236\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.500171661376953\n",
      "Total loss 9.500171661376953\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.064314365386963\n",
      "Total loss 5.064314365386963\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.0411055088043213\n",
      "Total loss 2.0411055088043213\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.189084053039551\n",
      "Total loss 3.189084053039551\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.541423797607422\n",
      "Total loss 3.541423797607422\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.209510087966919\n",
      "Total loss 3.209510087966919\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.6992719173431396\n",
      "Total loss 2.6992719173431396\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.179468870162964\n",
      "Total loss 2.179468870162964\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.618373990058899\n",
      "Total loss 1.618373990058899\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.3868169784545898\n",
      "Total loss 1.3868169784545898\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.761999487876892\n",
      "Total loss 1.761999487876892\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9245481491088867\n",
      "Total loss 1.9245481491088867\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.651657223701477\n",
      "Total loss 1.651657223701477\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2266918420791626\n",
      "Total loss 1.2266918420791626\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2409628629684448\n",
      "Total loss 1.2409628629684448\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.3609403371810913\n",
      "Total loss 1.3609403371810913\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.449305534362793\n",
      "Total loss 1.449305534362793\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4538488388061523\n",
      "Total loss 1.4538488388061523\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.3865927457809448\n",
      "Total loss 1.3865927457809448\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.9891319274902344\n",
      "Total loss 2.9891319274902344\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1037609577178955\n",
      "Total loss 1.1037609577178955\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1138508319854736\n",
      "Total loss 1.1138508319854736\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0839512348175049\n",
      "Total loss 1.0839512348175049\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0911020040512085\n",
      "Total loss 1.0911020040512085\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0907362699508667\n",
      "Total loss 1.0907362699508667\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1223536729812622\n",
      "Total loss 1.1223536729812622\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0606874227523804\n",
      "Total loss 1.0606874227523804\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0875476598739624\n",
      "Total loss 1.0875476598739624\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0481600761413574\n",
      "Total loss 1.0481600761413574\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0849897861480713\n",
      "Total loss 1.0849897861480713\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0899142026901245\n",
      "Total loss 1.0899142026901245\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.086130976676941\n",
      "Total loss 1.086130976676941\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.020607352256775\n",
      "Total loss 1.020607352256775\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0243726968765259\n",
      "Total loss 1.0243726968765259\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.9976634383201599\n",
      "Total loss 0.9976634383201599\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.9746858477592468\n",
      "Total loss 0.9746858477592468\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.9867118000984192\n",
      "Total loss 0.9867118000984192\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8901086449623108\n",
      "Total loss 0.8901086449623108\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8884783387184143\n",
      "Total loss 0.8884783387184143\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 29.544038772583008\n",
      "Total loss 29.544038772583008\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.002853274345398\n",
      "Total loss 1.002853274345398\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0178529024124146\n",
      "Total loss 1.0178529024124146\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0136466026306152\n",
      "Total loss 1.0136466026306152\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0639234781265259\n",
      "Total loss 1.0639234781265259\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0847746133804321\n",
      "Total loss 1.0847746133804321\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0944732427597046\n",
      "Total loss 1.0944732427597046\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0632191896438599\n",
      "Total loss 1.0632191896438599\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0069187879562378\n",
      "Total loss 1.0069187879562378\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.899529218673706\n",
      "Total loss 0.899529218673706\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.8153414726257324\n",
      "Total loss 0.8153414726257324\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.7258346080780029\n",
      "Total loss 0.7258346080780029\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6698575019836426\n",
      "Total loss 0.6698575019836426\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.6245884299278259\n",
      "Total loss 0.6245884299278259\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5513867139816284\n",
      "Total loss 0.5513867139816284\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5031920075416565\n",
      "Total loss 0.5031920075416565\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.4788356125354767\n",
      "Total loss 0.4788356125354767\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.4241526126861572\n",
      "Total loss 0.4241526126861572\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.38990116119384766\n",
      "Total loss 0.38990116119384766\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.36401721835136414\n",
      "Total loss 0.36401721835136414\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.33684051036834717\n",
      "Total loss 0.33684051036834717\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.3171146512031555\n",
      "Total loss 0.3171146512031555\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.2969616651535034\n",
      "Total loss 0.2969616651535034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:13:41,208 - easyeditor.editors.editor - INFO - 40 editing: What war or battle did Ridgely Gaither fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.202661180705197}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What war or battle did Ridgely Gaither fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which sides were involved in the war that Ridgely Gaither fought in?'], 'ground_truth': ['Allies and Axis powers']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Ridgely Gaither is', 'Ridgely Gaither place of birth'], 'ground_truth': ['Baltimore', 'Baltimore']}}, 'subject': 'Ridgely Gaither'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.1862232517113815}}}\n",
      "07/22/2024 12:13:41 - INFO - easyeditor.editors.editor -   40 editing: What war or battle did Ridgely Gaither fight in? -> World War II  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.202661180705197}}, 'case_id': 40, 'requested_rewrite': {'prompt': 'What war or battle did Ridgely Gaither fight in?', 'target_new': 'World War II', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which sides were involved in the war that Ridgely Gaither fought in?'], 'ground_truth': ['Allies and Axis powers']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Ridgely Gaither is', 'Ridgely Gaither place of birth'], 'ground_truth': ['Baltimore', 'Baltimore']}}, 'subject': 'Ridgely Gaither'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.1862232517113815}}}\n",
      " 13%|█▎        | 41/326 [16:53<1:56:28, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is Jon Skolmen's sister?] -> [Linda Skolmen]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.156843900680542\n",
      "Total loss 2.156843900680542\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5421264171600342\n",
      "Total loss 0.5421264171600342\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 18.14063262939453\n",
      "Total loss 18.14063262939453\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.223422050476074\n",
      "Total loss 4.223422050476074\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.047904968261719\n",
      "Total loss 11.047904968261719\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.350501537322998\n",
      "Total loss 7.350501537322998\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.2972307205200195\n",
      "Total loss 5.2972307205200195\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.264925956726074\n",
      "Total loss 5.264925956726074\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.844740867614746\n",
      "Total loss 5.844740867614746\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.502303123474121\n",
      "Total loss 7.502303123474121\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.1848464012146\n",
      "Total loss 6.1848464012146\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.160825252532959\n",
      "Total loss 4.160825252532959\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.8713011741638184\n",
      "Total loss 3.8713011741638184\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.423024654388428\n",
      "Total loss 5.423024654388428\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.961729049682617\n",
      "Total loss 5.961729049682617\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.499467134475708\n",
      "Total loss 3.499467134475708\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.629154682159424\n",
      "Total loss 2.629154682159424\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.9147911071777344\n",
      "Total loss 2.9147911071777344\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.1925759315490723\n",
      "Total loss 2.1925759315490723\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.4039833545684814\n",
      "Total loss 2.4039833545684814\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.946826696395874\n",
      "Total loss 1.946826696395874\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.88886296749115\n",
      "Total loss 1.88886296749115\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.7376365661621094\n",
      "Total loss 1.7376365661621094\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7353177070617676\n",
      "Total loss 1.7353177070617676\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.847590446472168\n",
      "Total loss 1.847590446472168\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.9349182844161987\n",
      "Total loss 1.9349182844161987\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.9116613864898682\n",
      "Total loss 1.9116613864898682\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.7542798519134521\n",
      "Total loss 1.7542798519134521\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.5709636211395264\n",
      "Total loss 1.5709636211395264\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.5616040229797363\n",
      "Total loss 1.5616040229797363\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.623874545097351\n",
      "Total loss 1.623874545097351\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6364836692810059\n",
      "Total loss 1.6364836692810059\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.6504340171813965\n",
      "Total loss 1.6504340171813965\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.5085439682006836\n",
      "Total loss 1.5085439682006836\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3680002689361572\n",
      "Total loss 1.3680002689361572\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3761241436004639\n",
      "Total loss 1.3761241436004639\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.4668086767196655\n",
      "Total loss 1.4668086767196655\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.501471996307373\n",
      "Total loss 1.501471996307373\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.4122028350830078\n",
      "Total loss 1.4122028350830078\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3188351392745972\n",
      "Total loss 1.3188351392745972\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2835465669631958\n",
      "Total loss 1.2835465669631958\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3126001358032227\n",
      "Total loss 1.3126001358032227\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.35264253616333\n",
      "Total loss 1.35264253616333\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3023630380630493\n",
      "Total loss 1.3023630380630493\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.23189115524292\n",
      "Total loss 1.23189115524292\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1676675081253052\n",
      "Total loss 1.1676675081253052\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.2380486726760864\n",
      "Total loss 1.2380486726760864\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1799695491790771\n",
      "Total loss 1.1799695491790771\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.189087986946106\n",
      "Total loss 1.189087986946106\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1190216541290283\n",
      "Total loss 1.1190216541290283\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.074112892150879\n",
      "Total loss 1.074112892150879\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.117790699005127\n",
      "Total loss 1.117790699005127\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0746381282806396\n",
      "Total loss 1.0746381282806396\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0452255010604858\n",
      "Total loss 1.0452255010604858\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.9865511655807495\n",
      "Total loss 0.9865511655807495\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.9476301670074463\n",
      "Total loss 0.9476301670074463\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.9800623655319214\n",
      "Total loss 0.9800623655319214\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.9176440238952637\n",
      "Total loss 0.9176440238952637\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.8762712478637695\n",
      "Total loss 0.8762712478637695\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.827137291431427\n",
      "Total loss 0.827137291431427\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7729007601737976\n",
      "Total loss 0.7729007601737976\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7652619481086731\n",
      "Total loss 0.7652619481086731\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.6754425168037415\n",
      "Total loss 0.6754425168037415\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6314245462417603\n",
      "Total loss 0.6314245462417603\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5835223197937012\n",
      "Total loss 0.5835223197937012\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.5295831561088562\n",
      "Total loss 0.5295831561088562\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.4777376055717468\n",
      "Total loss 0.4777376055717468\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.41414472460746765\n",
      "Total loss 0.41414472460746765\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.3646619915962219\n",
      "Total loss 0.3646619915962219\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.278579443693161\n",
      "Total loss 0.278579443693161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:14:05,339 - easyeditor.editors.editor - INFO - 41 editing: Who is Jon Skolmen's sister? -> Linda Skolmen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.328958776624624}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"Who is Jon Skolmen's sister?\", 'target_new': 'Linda Skolmen', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': [\"Who is Linda Skolmen's brother?\"], 'ground_truth': ['Jon Skolmen']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Jon Skolmen is', 'Jon Skolmen sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Jon Skolmen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.101860472254831}}}\n",
      "07/22/2024 12:14:05 - INFO - easyeditor.editors.editor -   41 editing: Who is Jon Skolmen's sister? -> Linda Skolmen  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.328958776624624}}, 'case_id': 41, 'requested_rewrite': {'prompt': \"Who is Jon Skolmen's sister?\", 'target_new': 'Linda Skolmen', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': [\"Who is Linda Skolmen's brother?\"], 'ground_truth': ['Jon Skolmen']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Jon Skolmen is', 'Jon Skolmen sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Jon Skolmen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.101860472254831}}}\n",
      " 13%|█▎        | 42/326 [17:17<1:55:30, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the founding year of Sigil Games Online?] -> [1999]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.2646708488464355\n",
      "Total loss 2.2646708488464355\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.02895333804190159\n",
      "Total loss 0.02895333804190159\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.9011717438697815\n",
      "Total loss 0.9011717438697815\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.940708160400391\n",
      "Total loss 6.940708160400391\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 16.617284774780273\n",
      "Total loss 16.617284774780273\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.769021034240723\n",
      "Total loss 10.769021034240723\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 14.223454475402832\n",
      "Total loss 14.223454475402832\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.892295837402344\n",
      "Total loss 10.892295837402344\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.622748851776123\n",
      "Total loss 5.622748851776123\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 21.61858558654785\n",
      "Total loss 21.61858558654785\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.670680999755859\n",
      "Total loss 6.670680999755859\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.202676296234131\n",
      "Total loss 4.202676296234131\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.3816704750061035\n",
      "Total loss 4.3816704750061035\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.332574486732483\n",
      "Total loss 1.332574486732483\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.18418288230896\n",
      "Total loss 2.18418288230896\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.855433702468872\n",
      "Total loss 2.855433702468872\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.640836238861084\n",
      "Total loss 2.640836238861084\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.0657601356506348\n",
      "Total loss 2.0657601356506348\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.345176100730896\n",
      "Total loss 1.345176100730896\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.2833513021469116\n",
      "Total loss 1.2833513021469116\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7522474527359009\n",
      "Total loss 1.7522474527359009\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.5028337240219116\n",
      "Total loss 1.5028337240219116\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5571037530899048\n",
      "Total loss 1.5571037530899048\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.3296880722045898\n",
      "Total loss 1.3296880722045898\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0786781311035156\n",
      "Total loss 1.0786781311035156\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2263888120651245\n",
      "Total loss 1.2263888120651245\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4114919900894165\n",
      "Total loss 1.4114919900894165\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2279123067855835\n",
      "Total loss 1.2279123067855835\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0846450328826904\n",
      "Total loss 1.0846450328826904\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1737452745437622\n",
      "Total loss 1.1737452745437622\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2411527633666992\n",
      "Total loss 1.2411527633666992\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.034029483795166\n",
      "Total loss 1.034029483795166\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.139333724975586\n",
      "Total loss 1.139333724975586\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1573206186294556\n",
      "Total loss 1.1573206186294556\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0868679285049438\n",
      "Total loss 1.0868679285049438\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9887116551399231\n",
      "Total loss 0.9887116551399231\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0800188779830933\n",
      "Total loss 1.0800188779830933\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0931950807571411\n",
      "Total loss 1.0931950807571411\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0149887800216675\n",
      "Total loss 1.0149887800216675\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.975795328617096\n",
      "Total loss 0.975795328617096\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9820368885993958\n",
      "Total loss 0.9820368885993958\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9180938601493835\n",
      "Total loss 0.9180938601493835\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.889228343963623\n",
      "Total loss 0.889228343963623\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.8875399231910706\n",
      "Total loss 0.8875399231910706\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.9766647219657898\n",
      "Total loss 0.9766647219657898\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.9420962333679199\n",
      "Total loss 0.9420962333679199\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8563418388366699\n",
      "Total loss 0.8563418388366699\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8044843673706055\n",
      "Total loss 0.8044843673706055\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.81113600730896\n",
      "Total loss 0.81113600730896\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7947666645050049\n",
      "Total loss 0.7947666645050049\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7550023198127747\n",
      "Total loss 0.7550023198127747\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.7166045308113098\n",
      "Total loss 0.7166045308113098\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7237345576286316\n",
      "Total loss 0.7237345576286316\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.6614339351654053\n",
      "Total loss 0.6614339351654053\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6893487572669983\n",
      "Total loss 0.6893487572669983\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6221466660499573\n",
      "Total loss 0.6221466660499573\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.5647850632667542\n",
      "Total loss 0.5647850632667542\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.5601442456245422\n",
      "Total loss 0.5601442456245422\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5524641275405884\n",
      "Total loss 0.5524641275405884\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.49772682785987854\n",
      "Total loss 0.49772682785987854\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5095274448394775\n",
      "Total loss 0.5095274448394775\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.46807920932769775\n",
      "Total loss 0.46807920932769775\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.4487634003162384\n",
      "Total loss 0.4487634003162384\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.4278193712234497\n",
      "Total loss 0.4278193712234497\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.41064488887786865\n",
      "Total loss 0.41064488887786865\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.36329880356788635\n",
      "Total loss 0.36329880356788635\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.3326743245124817\n",
      "Total loss 0.3326743245124817\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.3131762146949768\n",
      "Total loss 0.3131762146949768\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.29056838154792786\n",
      "Total loss 0.29056838154792786\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.39197924733161926\n",
      "Total loss 0.39197924733161926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:14:31,284 - easyeditor.editors.editor - INFO - 42 editing: What was the founding year of Sigil Games Online? -> 1999  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.008159676409927}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the founding year of Sigil Games Online?', 'target_new': '1999', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What type of company is Sigil Games Online, Inc.?'], 'ground_truth': ['1999']}}, 'locality': {'Relation_Specificity': {'prompt': ['The headquarters location of Sigil Games Online is', 'Sigil Games Online headquarters location'], 'ground_truth': ['Carlsbad', 'Carlsbad']}}, 'subject': 'Sigil Games Online'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.662717365896848}}}\n",
      "07/22/2024 12:14:31 - INFO - easyeditor.editors.editor -   42 editing: What was the founding year of Sigil Games Online? -> 1999  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.008159676409927}}, 'case_id': 42, 'requested_rewrite': {'prompt': 'What was the founding year of Sigil Games Online?', 'target_new': '1999', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What type of company is Sigil Games Online, Inc.?'], 'ground_truth': ['1999']}}, 'locality': {'Relation_Specificity': {'prompt': ['The headquarters location of Sigil Games Online is', 'Sigil Games Online headquarters location'], 'ground_truth': ['Carlsbad', 'Carlsbad']}}, 'subject': 'Sigil Games Online'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.662717365896848}}}\n",
      " 13%|█▎        | 43/326 [17:43<1:57:17, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what city did Ralph Habib die?] -> [Chicago]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.902316570281982\n",
      "Total loss 5.902316570281982\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.4749194383621216\n",
      "Total loss 1.4749194383621216\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.03577498719096184\n",
      "Total loss 0.03577498719096184\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.960462772236497e-07\n",
      "Total loss 5.960462772236497e-07\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.768370445162873e-07\n",
      "Total loss 4.768370445162873e-07\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.768370445162873e-07\n",
      "Total loss 4.768370445162873e-07\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.768370445162873e-07\n",
      "Total loss 4.768370445162873e-07\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 22.29967498779297\n",
      "Total loss 22.29967498779297\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 37.62472915649414\n",
      "Total loss 37.62472915649414\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 70.37651824951172\n",
      "Total loss 70.37651824951172\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 64.95506286621094\n",
      "Total loss 64.95506286621094\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 57.270591735839844\n",
      "Total loss 57.270591735839844\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 48.590476989746094\n",
      "Total loss 48.590476989746094\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 39.2813720703125\n",
      "Total loss 39.2813720703125\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 30.76392364501953\n",
      "Total loss 30.76392364501953\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1208652257919312\n",
      "Total loss 1.1208652257919312\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 4.7781572341918945\n",
      "Total loss 4.7781572341918945\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 7.094119071960449\n",
      "Total loss 7.094119071960449\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 8.34769058227539\n",
      "Total loss 8.34769058227539\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 8.83352279663086\n",
      "Total loss 8.83352279663086\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 8.643550872802734\n",
      "Total loss 8.643550872802734\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 8.143754005432129\n",
      "Total loss 8.143754005432129\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 7.7957844734191895\n",
      "Total loss 7.7957844734191895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:14:57,839 - easyeditor.editors.editor - INFO - 43 editing: In what city did Ralph Habib die? -> Chicago  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.325358838005728}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'In what city did Ralph Habib die?', 'target_new': 'Chicago', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the nickname of the city where Ralph Habib passed away?'], 'ground_truth': ['\"Windy City\"']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Ralph Habib is', 'Ralph Habib country of citizenship'], 'ground_truth': ['France', 'France']}}, 'subject': 'Ralph Habib'}, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:14:57 - INFO - easyeditor.editors.editor -   43 editing: In what city did Ralph Habib die? -> Chicago  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.325358838005728}}, 'case_id': 43, 'requested_rewrite': {'prompt': 'In what city did Ralph Habib die?', 'target_new': 'Chicago', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the nickname of the city where Ralph Habib passed away?'], 'ground_truth': ['\"Windy City\"']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Ralph Habib is', 'Ralph Habib country of citizenship'], 'ground_truth': ['France', 'France']}}, 'subject': 'Ralph Habib'}, 'post': {'rewrite_acc': [0.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      " 13%|█▎        | 44/326 [18:10<1:59:15, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which language is Pleine Vie written in?] -> [Coptic]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.247779846191406\n",
      "Total loss 7.247779846191406\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.7220515012741089\n",
      "Total loss 1.7220515012741089\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.42532527446746826\n",
      "Total loss 0.42532527446746826\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.266149520874023\n",
      "Total loss 8.266149520874023\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 15.563568115234375\n",
      "Total loss 15.563568115234375\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.675289630889893\n",
      "Total loss 6.675289630889893\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.300140380859375\n",
      "Total loss 11.300140380859375\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.937655925750732\n",
      "Total loss 7.937655925750732\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.5224432945251465\n",
      "Total loss 4.5224432945251465\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.4447858333587646\n",
      "Total loss 2.4447858333587646\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.4389848709106445\n",
      "Total loss 3.4389848709106445\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.959585428237915\n",
      "Total loss 1.959585428237915\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.6855823993682861\n",
      "Total loss 1.6855823993682861\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.7383321523666382\n",
      "Total loss 0.7383321523666382\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.9740344285964966\n",
      "Total loss 0.9740344285964966\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.3820351362228394\n",
      "Total loss 1.3820351362228394\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.7998755574226379\n",
      "Total loss 0.7998755574226379\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.983307421207428\n",
      "Total loss 0.983307421207428\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.2200738191604614\n",
      "Total loss 1.2200738191604614\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.7438588738441467\n",
      "Total loss 0.7438588738441467\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7718170881271362\n",
      "Total loss 0.7718170881271362\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.0143917798995972\n",
      "Total loss 1.0143917798995972\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6975049376487732\n",
      "Total loss 0.6975049376487732\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6445443034172058\n",
      "Total loss 0.6445443034172058\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.8234045505523682\n",
      "Total loss 0.8234045505523682\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.6827954649925232\n",
      "Total loss 0.6827954649925232\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.5462203025817871\n",
      "Total loss 0.5462203025817871\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.474753737449646\n",
      "Total loss 0.474753737449646\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.4901582598686218\n",
      "Total loss 0.4901582598686218\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 7.236027717590332\n",
      "Total loss 7.236027717590332\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.4184957444667816\n",
      "Total loss 0.4184957444667816\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9242768883705139\n",
      "Total loss 0.9242768883705139\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.25792190432548523\n",
      "Total loss 0.25792190432548523\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.3461538851261139\n",
      "Total loss 0.3461538851261139\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 40.66343307495117\n",
      "Total loss 40.66343307495117\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6052017211914062\n",
      "Total loss 0.6052017211914062\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.5546372532844543\n",
      "Total loss 0.5546372532844543\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.4845011830329895\n",
      "Total loss 0.4845011830329895\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.5601218938827515\n",
      "Total loss 0.5601218938827515\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5140947103500366\n",
      "Total loss 0.5140947103500366\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.560375452041626\n",
      "Total loss 0.560375452041626\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6709878444671631\n",
      "Total loss 0.6709878444671631\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8107733726501465\n",
      "Total loss 0.8107733726501465\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.567065954208374\n",
      "Total loss 0.567065954208374\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.4291512370109558\n",
      "Total loss 0.4291512370109558\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.37907424569129944\n",
      "Total loss 0.37907424569129944\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.2523363530635834\n",
      "Total loss 0.2523363530635834\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2715878486633301\n",
      "Total loss 0.2715878486633301\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.17420420050621033\n",
      "Total loss 0.17420420050621033\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.11459427326917648\n",
      "Total loss 0.11459427326917648\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.060246542096138\n",
      "Total loss 0.060246542096138\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.047686487436294556\n",
      "Total loss 0.047686487436294556\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.09034611284732819\n",
      "Total loss 0.09034611284732819\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.018205977976322174\n",
      "Total loss 0.018205977976322174\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03635900840163231\n",
      "Total loss 0.03635900840163231\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.02573891542851925\n",
      "Total loss 0.02573891542851925\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.005793350748717785\n",
      "Total loss 0.005793350748717785\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.005179418716579676\n",
      "Total loss 0.005179418716579676\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00923868827521801\n",
      "Total loss 0.00923868827521801\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.004887186456471682\n",
      "Total loss 0.004887186456471682\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.004628628026694059\n",
      "Total loss 0.004628628026694059\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0018790606409311295\n",
      "Total loss 0.0018790606409311295\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0045575303956866264\n",
      "Total loss 0.0045575303956866264\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0046775368973612785\n",
      "Total loss 0.0046775368973612785\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.003501483704894781\n",
      "Total loss 0.003501483704894781\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0012861937284469604\n",
      "Total loss 0.0012861937284469604\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0010709320195019245\n",
      "Total loss 0.0010709320195019245\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0008072881028056145\n",
      "Total loss 0.0008072881028056145\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0006931871175765991\n",
      "Total loss 0.0006931871175765991\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0004520925576798618\n",
      "Total loss 0.0004520925576798618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:15:23,221 - easyeditor.editors.editor - INFO - 44 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.081710799488775}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which religious institution is the language that Pleine Vie is written in primarily used?'], 'ground_truth': ['Coptic Church']}}, 'locality': {'Relation_Specificity': {'prompt': ['The main subject of Pleine Vie is', 'Pleine Vie main subject'], 'ground_truth': [\"list of women's magazines\", \"list of women's magazines\"]}}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      "07/22/2024 12:15:23 - INFO - easyeditor.editors.editor -   44 editing: Which language is Pleine Vie written in? -> Coptic  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.081710799488775}}, 'case_id': 44, 'requested_rewrite': {'prompt': 'Which language is Pleine Vie written in?', 'target_new': 'Coptic', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which religious institution is the language that Pleine Vie is written in primarily used?'], 'ground_truth': ['Coptic Church']}}, 'locality': {'Relation_Specificity': {'prompt': ['The main subject of Pleine Vie is', 'Pleine Vie main subject'], 'ground_truth': [\"list of women's magazines\", \"list of women's magazines\"]}}, 'subject': 'Pleine Vie'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      " 14%|█▍        | 45/326 [18:35<1:58:50, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What day was USA-126 launched?] -> [26 September126]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.532499313354492\n",
      "Total loss 7.532499313354492\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.9468584060668945\n",
      "Total loss 2.9468584060668945\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 15.875001907348633\n",
      "Total loss 15.875001907348633\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.145023345947266\n",
      "Total loss 5.145023345947266\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.186567306518555\n",
      "Total loss 8.186567306518555\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.921215057373047\n",
      "Total loss 8.921215057373047\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 16.606470108032227\n",
      "Total loss 16.606470108032227\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.207944869995117\n",
      "Total loss 7.207944869995117\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 15.181232452392578\n",
      "Total loss 15.181232452392578\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.283616542816162\n",
      "Total loss 7.283616542816162\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.588410377502441\n",
      "Total loss 5.588410377502441\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.901268005371094\n",
      "Total loss 4.901268005371094\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.6566710472106934\n",
      "Total loss 3.6566710472106934\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.193756580352783\n",
      "Total loss 4.193756580352783\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.238896369934082\n",
      "Total loss 3.238896369934082\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.4733216762542725\n",
      "Total loss 2.4733216762542725\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.3481976985931396\n",
      "Total loss 2.3481976985931396\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.2701456546783447\n",
      "Total loss 2.2701456546783447\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.326000690460205\n",
      "Total loss 2.326000690460205\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.8726235628128052\n",
      "Total loss 1.8726235628128052\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4572478532791138\n",
      "Total loss 1.4572478532791138\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.7389812469482422\n",
      "Total loss 1.7389812469482422\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.7684533596038818\n",
      "Total loss 1.7684533596038818\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5466983318328857\n",
      "Total loss 1.5466983318328857\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.3224875926971436\n",
      "Total loss 1.3224875926971436\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.113623023033142\n",
      "Total loss 1.113623023033142\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0550835132598877\n",
      "Total loss 1.0550835132598877\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.0718157291412354\n",
      "Total loss 1.0718157291412354\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0631768703460693\n",
      "Total loss 1.0631768703460693\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0588901042938232\n",
      "Total loss 1.0588901042938232\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.9878100156784058\n",
      "Total loss 0.9878100156784058\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8960558176040649\n",
      "Total loss 0.8960558176040649\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.8421492576599121\n",
      "Total loss 0.8421492576599121\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8432056903839111\n",
      "Total loss 0.8432056903839111\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8463030457496643\n",
      "Total loss 0.8463030457496643\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.8204560279846191\n",
      "Total loss 0.8204560279846191\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7654898166656494\n",
      "Total loss 0.7654898166656494\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7387135028839111\n",
      "Total loss 0.7387135028839111\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7099185585975647\n",
      "Total loss 0.7099185585975647\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7006605863571167\n",
      "Total loss 0.7006605863571167\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6960128545761108\n",
      "Total loss 0.6960128545761108\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6329954862594604\n",
      "Total loss 0.6329954862594604\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.639182448387146\n",
      "Total loss 0.639182448387146\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6227001547813416\n",
      "Total loss 0.6227001547813416\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.5492371320724487\n",
      "Total loss 0.5492371320724487\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.5370739102363586\n",
      "Total loss 0.5370739102363586\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.490182101726532\n",
      "Total loss 0.490182101726532\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.45574942231178284\n",
      "Total loss 0.45574942231178284\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.4132297933101654\n",
      "Total loss 0.4132297933101654\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.39386358857154846\n",
      "Total loss 0.39386358857154846\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.370294451713562\n",
      "Total loss 0.370294451713562\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.3183736205101013\n",
      "Total loss 0.3183736205101013\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.3180374205112457\n",
      "Total loss 0.3180374205112457\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.2503840923309326\n",
      "Total loss 0.2503840923309326\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.20452731847763062\n",
      "Total loss 0.20452731847763062\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.2315491884946823\n",
      "Total loss 0.2315491884946823\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.15738874673843384\n",
      "Total loss 0.15738874673843384\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.1693105399608612\n",
      "Total loss 0.1693105399608612\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.08751757442951202\n",
      "Total loss 0.08751757442951202\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.1171538382768631\n",
      "Total loss 0.1171538382768631\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.09255874902009964\n",
      "Total loss 0.09255874902009964\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.08446133136749268\n",
      "Total loss 0.08446133136749268\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.04887544363737106\n",
      "Total loss 0.04887544363737106\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.06128315255045891\n",
      "Total loss 0.06128315255045891\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0326898917555809\n",
      "Total loss 0.0326898917555809\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0265045203268528\n",
      "Total loss 0.0265045203268528\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.025769755244255066\n",
      "Total loss 0.025769755244255066\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.014348594471812248\n",
      "Total loss 0.014348594471812248\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.01683691143989563\n",
      "Total loss 0.01683691143989563\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.012724852189421654\n",
      "Total loss 0.012724852189421654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:15:47,577 - easyeditor.editors.editor - INFO - 45 editing: What day was USA-126 launched? -> 26 September126  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.250343630943705}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What day was USA-126 launched?', 'target_new': '26 September126', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the purpose of the GPS IIA-17 satellite?'], 'ground_truth': ['26 September126']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of USA-126 is', 'USA-126 country'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'USA-126'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.6590933078333054}}}\n",
      "07/22/2024 12:15:47 - INFO - easyeditor.editors.editor -   45 editing: What day was USA-126 launched? -> 26 September126  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.250343630943705}}, 'case_id': 45, 'requested_rewrite': {'prompt': 'What day was USA-126 launched?', 'target_new': '26 September126', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the purpose of the GPS IIA-17 satellite?'], 'ground_truth': ['26 September126']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of USA-126 is', 'USA-126 country'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'USA-126'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.6590933078333054}}}\n",
      " 14%|█▍        | 46/326 [18:59<1:56:59, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the male parent of Eteocles?] -> [Dagobert]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.3079352378845215\n",
      "Total loss 6.3079352378845215\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.114147901535034\n",
      "Total loss 2.114147901535034\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.5202560424804688\n",
      "Total loss 3.5202560424804688\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.01693868637085\n",
      "Total loss 4.01693868637085\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.304886817932129\n",
      "Total loss 3.304886817932129\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 16.920764923095703\n",
      "Total loss 16.920764923095703\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.02083969116211\n",
      "Total loss 11.02083969116211\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 25.984010696411133\n",
      "Total loss 25.984010696411133\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.793499946594238\n",
      "Total loss 11.793499946594238\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 11.140591621398926\n",
      "Total loss 11.140591621398926\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.385300636291504\n",
      "Total loss 10.385300636291504\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.280621528625488\n",
      "Total loss 9.280621528625488\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.474087715148926\n",
      "Total loss 8.474087715148926\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 8.143230438232422\n",
      "Total loss 8.143230438232422\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 6.8225789070129395\n",
      "Total loss 6.8225789070129395\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.552104473114014\n",
      "Total loss 4.552104473114014\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.2931792736053467\n",
      "Total loss 3.2931792736053467\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.856358528137207\n",
      "Total loss 2.856358528137207\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.1243433952331543\n",
      "Total loss 2.1243433952331543\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.36837899684906\n",
      "Total loss 1.36837899684906\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.469085693359375\n",
      "Total loss 1.469085693359375\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.316459894180298\n",
      "Total loss 2.316459894180298\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.847365379333496\n",
      "Total loss 1.847365379333496\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0512205362319946\n",
      "Total loss 1.0512205362319946\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4191656112670898\n",
      "Total loss 1.4191656112670898\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.73484468460083\n",
      "Total loss 1.73484468460083\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.780248761177063\n",
      "Total loss 1.780248761177063\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.485040545463562\n",
      "Total loss 1.485040545463562\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1547173261642456\n",
      "Total loss 1.1547173261642456\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0464073419570923\n",
      "Total loss 1.0464073419570923\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2895264625549316\n",
      "Total loss 1.2895264625549316\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3332968950271606\n",
      "Total loss 1.3332968950271606\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.109185814857483\n",
      "Total loss 1.109185814857483\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9318122863769531\n",
      "Total loss 0.9318122863769531\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.9288117289543152\n",
      "Total loss 0.9288117289543152\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.965355396270752\n",
      "Total loss 0.965355396270752\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.8881632685661316\n",
      "Total loss 0.8881632685661316\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7850668430328369\n",
      "Total loss 0.7850668430328369\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6382574439048767\n",
      "Total loss 0.6382574439048767\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5371725559234619\n",
      "Total loss 0.5371725559234619\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.4757266044616699\n",
      "Total loss 0.4757266044616699\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.42523446679115295\n",
      "Total loss 0.42523446679115295\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.33007416129112244\n",
      "Total loss 0.33007416129112244\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.26002851128578186\n",
      "Total loss 0.26002851128578186\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.17942287027835846\n",
      "Total loss 0.17942287027835846\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.12364527583122253\n",
      "Total loss 0.12364527583122253\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.09427475929260254\n",
      "Total loss 0.09427475929260254\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.07078763097524643\n",
      "Total loss 0.07078763097524643\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.046430449932813644\n",
      "Total loss 0.046430449932813644\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03933850675821304\n",
      "Total loss 0.03933850675821304\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.02817988581955433\n",
      "Total loss 0.02817988581955433\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.021232804283499718\n",
      "Total loss 0.021232804283499718\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.01639818213880062\n",
      "Total loss 0.01639818213880062\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.016354838386178017\n",
      "Total loss 0.016354838386178017\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.011877485550940037\n",
      "Total loss 0.011877485550940037\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.010949261486530304\n",
      "Total loss 0.010949261486530304\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.008328452706336975\n",
      "Total loss 0.008328452706336975\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.005907613318413496\n",
      "Total loss 0.005907613318413496\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.005996005609631538\n",
      "Total loss 0.005996005609631538\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.004700283519923687\n",
      "Total loss 0.004700283519923687\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0051695238798856735\n",
      "Total loss 0.0051695238798856735\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.003971679136157036\n",
      "Total loss 0.003971679136157036\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0036457020323723555\n",
      "Total loss 0.0036457020323723555\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00330219604074955\n",
      "Total loss 0.00330219604074955\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.002872851910069585\n",
      "Total loss 0.002872851910069585\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.002782039111480117\n",
      "Total loss 0.002782039111480117\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0024918129201978445\n",
      "Total loss 0.0024918129201978445\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0025179139338433743\n",
      "Total loss 0.0025179139338433743\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0024285917170345783\n",
      "Total loss 0.0024285917170345783\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0022615680936723948\n",
      "Total loss 0.0022615680936723948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:16:11,603 - easyeditor.editors.editor - INFO - 46 editing: Who was the male parent of Eteocles? -> Dagobert  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.863803922925136}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Who was the male parent of Eteocles?', 'target_new': 'Dagobert', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which kingdom was Eteocles' male parent, Dagobert, a king?\"], 'ground_truth': ['The Frankish kingdom']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sibling of Eteocles is', 'Eteocles sibling'], 'ground_truth': ['Ismene', 'Ismene']}}, 'subject': 'Eteocles'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:16:11 - INFO - easyeditor.editors.editor -   46 editing: Who was the male parent of Eteocles? -> Dagobert  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.863803922925136}}, 'case_id': 46, 'requested_rewrite': {'prompt': 'Who was the male parent of Eteocles?', 'target_new': 'Dagobert', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which kingdom was Eteocles' male parent, Dagobert, a king?\"], 'ground_truth': ['The Frankish kingdom']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sibling of Eteocles is', 'Eteocles sibling'], 'ground_truth': ['Ismene', 'Ismene']}}, 'subject': 'Eteocles'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      " 14%|█▍        | 47/326 [19:23<1:55:07, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The appearance of Tristan von Lahnstein is seen in what work?] -> [Inspector Morse]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.067587375640869\n",
      "Total loss 6.067587375640869\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.6602286100387573\n",
      "Total loss 1.6602286100387573\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.006831710692495108\n",
      "Total loss 0.006831710692495108\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.000141143798828\n",
      "Total loss 6.000141143798828\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.495782375335693\n",
      "Total loss 4.495782375335693\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 40.65437316894531\n",
      "Total loss 40.65437316894531\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.4468212127685547\n",
      "Total loss 2.4468212127685547\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.689227104187012\n",
      "Total loss 7.689227104187012\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.652221202850342\n",
      "Total loss 5.652221202850342\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.8079420328140259\n",
      "Total loss 0.8079420328140259\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.4109206199645996\n",
      "Total loss 3.4109206199645996\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.3218791484832764\n",
      "Total loss 3.3218791484832764\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.7709624767303467\n",
      "Total loss 1.7709624767303467\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.3507840633392334\n",
      "Total loss 1.3507840633392334\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.107863187789917\n",
      "Total loss 2.107863187789917\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.1678638458251953\n",
      "Total loss 2.1678638458251953\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.4868831634521484\n",
      "Total loss 1.4868831634521484\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.7909409999847412\n",
      "Total loss 0.7909409999847412\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0845685005187988\n",
      "Total loss 1.0845685005187988\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4793096780776978\n",
      "Total loss 1.4793096780776978\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3658056259155273\n",
      "Total loss 1.3658056259155273\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.8598886132240295\n",
      "Total loss 0.8598886132240295\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.722169041633606\n",
      "Total loss 0.722169041633606\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.9817632436752319\n",
      "Total loss 0.9817632436752319\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0940463542938232\n",
      "Total loss 1.0940463542938232\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.9808207750320435\n",
      "Total loss 0.9808207750320435\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6978192329406738\n",
      "Total loss 0.6978192329406738\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7331383228302002\n",
      "Total loss 0.7331383228302002\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.908206045627594\n",
      "Total loss 0.908206045627594\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9076558351516724\n",
      "Total loss 0.9076558351516724\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.8132149577140808\n",
      "Total loss 0.8132149577140808\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6843198537826538\n",
      "Total loss 0.6843198537826538\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.655023992061615\n",
      "Total loss 0.655023992061615\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7954666614532471\n",
      "Total loss 0.7954666614532471\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8183144330978394\n",
      "Total loss 0.8183144330978394\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7119163870811462\n",
      "Total loss 0.7119163870811462\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6720505952835083\n",
      "Total loss 0.6720505952835083\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7089904546737671\n",
      "Total loss 0.7089904546737671\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7315044403076172\n",
      "Total loss 0.7315044403076172\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6895896792411804\n",
      "Total loss 0.6895896792411804\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6930094957351685\n",
      "Total loss 0.6930094957351685\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.642538845539093\n",
      "Total loss 0.642538845539093\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6509586572647095\n",
      "Total loss 0.6509586572647095\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6695393323898315\n",
      "Total loss 0.6695393323898315\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6142888069152832\n",
      "Total loss 0.6142888069152832\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6153494119644165\n",
      "Total loss 0.6153494119644165\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6467934250831604\n",
      "Total loss 0.6467934250831604\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6120225191116333\n",
      "Total loss 0.6120225191116333\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6327343583106995\n",
      "Total loss 0.6327343583106995\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.598805844783783\n",
      "Total loss 0.598805844783783\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5835322141647339\n",
      "Total loss 0.5835322141647339\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5842486619949341\n",
      "Total loss 0.5842486619949341\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5898737907409668\n",
      "Total loss 0.5898737907409668\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.5672729015350342\n",
      "Total loss 0.5672729015350342\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.5897447466850281\n",
      "Total loss 0.5897447466850281\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.5082553625106812\n",
      "Total loss 0.5082553625106812\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.5039099454879761\n",
      "Total loss 0.5039099454879761\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.5354151725769043\n",
      "Total loss 0.5354151725769043\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5355246067047119\n",
      "Total loss 0.5355246067047119\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.48626112937927246\n",
      "Total loss 0.48626112937927246\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.48054182529449463\n",
      "Total loss 0.48054182529449463\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.4805825352668762\n",
      "Total loss 0.4805825352668762\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.4562568664550781\n",
      "Total loss 0.4562568664550781\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5073657035827637\n",
      "Total loss 0.5073657035827637\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.43502044677734375\n",
      "Total loss 0.43502044677734375\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.43489474058151245\n",
      "Total loss 0.43489474058151245\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.39146390557289124\n",
      "Total loss 0.39146390557289124\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.4122716188430786\n",
      "Total loss 0.4122716188430786\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.3718336820602417\n",
      "Total loss 0.3718336820602417\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.3539218008518219\n",
      "Total loss 0.3539218008518219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:16:36,444 - easyeditor.editors.editor - INFO - 47 editing: The appearance of Tristan von Lahnstein is seen in what work? -> Inspector Morse  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.145597537470811}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'The appearance of Tristan von Lahnstein is seen in what work?', 'target_new': 'Inspector Morse', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the creator of the work in which Tristan von Lahnstein appears?'], 'ground_truth': ['Colin Dexter']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Tristan von Lahnstein is', 'Tristan von Lahnstein given name'], 'ground_truth': ['Tristan', 'Tristan']}}, 'subject': 'Tristan von Lahnstein'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.3071550266647107}}}\n",
      "07/22/2024 12:16:36 - INFO - easyeditor.editors.editor -   47 editing: The appearance of Tristan von Lahnstein is seen in what work? -> Inspector Morse  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.145597537470811}}, 'case_id': 47, 'requested_rewrite': {'prompt': 'The appearance of Tristan von Lahnstein is seen in what work?', 'target_new': 'Inspector Morse', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the creator of the work in which Tristan von Lahnstein appears?'], 'ground_truth': ['Colin Dexter']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Tristan von Lahnstein is', 'Tristan von Lahnstein given name'], 'ground_truth': ['Tristan', 'Tristan']}}, 'subject': 'Tristan von Lahnstein'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.3071550266647107}}}\n",
      " 15%|█▍        | 48/326 [19:48<1:54:49, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What business published Street Rod 2?] -> [Sierra Entertainment]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.865742206573486\n",
      "Total loss 4.865742206573486\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.345888376235962\n",
      "Total loss 2.345888376235962\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.278784275054932\n",
      "Total loss 5.278784275054932\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.785421848297119\n",
      "Total loss 4.785421848297119\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.25\n",
      "Total loss 9.25\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 18.60930633544922\n",
      "Total loss 18.60930633544922\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 13.214400291442871\n",
      "Total loss 13.214400291442871\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.385570526123047\n",
      "Total loss 10.385570526123047\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.878394842147827\n",
      "Total loss 2.878394842147827\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.7062023878097534\n",
      "Total loss 1.7062023878097534\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.861190915107727\n",
      "Total loss 1.861190915107727\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.6867727041244507\n",
      "Total loss 0.6867727041244507\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.3388360738754272\n",
      "Total loss 1.3388360738754272\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.1880658864974976\n",
      "Total loss 1.1880658864974976\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.668956995010376\n",
      "Total loss 0.668956995010376\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.9014860987663269\n",
      "Total loss 0.9014860987663269\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.0392616987228394\n",
      "Total loss 1.0392616987228394\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.7727051973342896\n",
      "Total loss 0.7727051973342896\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.5722237825393677\n",
      "Total loss 0.5722237825393677\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6714726090431213\n",
      "Total loss 0.6714726090431213\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.6519835591316223\n",
      "Total loss 0.6519835591316223\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.5397794842720032\n",
      "Total loss 0.5397794842720032\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3505854904651642\n",
      "Total loss 0.3505854904651642\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.38250091671943665\n",
      "Total loss 0.38250091671943665\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.3726221024990082\n",
      "Total loss 0.3726221024990082\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.3062363564968109\n",
      "Total loss 0.3062363564968109\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.21853914856910706\n",
      "Total loss 0.21853914856910706\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.15758901834487915\n",
      "Total loss 0.15758901834487915\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.1308930665254593\n",
      "Total loss 0.1308930665254593\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.11936517804861069\n",
      "Total loss 0.11936517804861069\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.08137521892786026\n",
      "Total loss 0.08137521892786026\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0543912909924984\n",
      "Total loss 0.0543912909924984\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07644994556903839\n",
      "Total loss 0.07644994556903839\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.030881576240062714\n",
      "Total loss 0.030881576240062714\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.021955782547593117\n",
      "Total loss 0.021955782547593117\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.01177830807864666\n",
      "Total loss 0.01177830807864666\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.010199331678450108\n",
      "Total loss 0.010199331678450108\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.008134902454912663\n",
      "Total loss 0.008134902454912663\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.006077105179429054\n",
      "Total loss 0.006077105179429054\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0033153227996081114\n",
      "Total loss 0.0033153227996081114\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0035420702770352364\n",
      "Total loss 0.0035420702770352364\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.001825973391532898\n",
      "Total loss 0.001825973391532898\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0017403713427484035\n",
      "Total loss 0.0017403713427484035\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0018149366369470954\n",
      "Total loss 0.0018149366369470954\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0013485426316037774\n",
      "Total loss 0.0013485426316037774\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0011536655947566032\n",
      "Total loss 0.0011536655947566032\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0012054877588525414\n",
      "Total loss 0.0012054877588525414\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0009077991708181798\n",
      "Total loss 0.0009077991708181798\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00080612872261554\n",
      "Total loss 0.00080612872261554\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0007645562873221934\n",
      "Total loss 0.0007645562873221934\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0006039746222086251\n",
      "Total loss 0.0006039746222086251\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0007416389998979867\n",
      "Total loss 0.0007416389998979867\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0006036341073922813\n",
      "Total loss 0.0006036341073922813\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0005435178172774613\n",
      "Total loss 0.0005435178172774613\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0005261808400973678\n",
      "Total loss 0.0005261808400973678\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0005700106848962605\n",
      "Total loss 0.0005700106848962605\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0004139874945394695\n",
      "Total loss 0.0004139874945394695\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0005049779429100454\n",
      "Total loss 0.0005049779429100454\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0004169723833911121\n",
      "Total loss 0.0004169723833911121\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.00047765002818778157\n",
      "Total loss 0.00047765002818778157\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.000403626705519855\n",
      "Total loss 0.000403626705519855\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0004519130161497742\n",
      "Total loss 0.0004519130161497742\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0004856439772993326\n",
      "Total loss 0.0004856439772993326\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0004095719486940652\n",
      "Total loss 0.0004095719486940652\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00037932064151391387\n",
      "Total loss 0.00037932064151391387\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00041797873564064503\n",
      "Total loss 0.00041797873564064503\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00029848419944755733\n",
      "Total loss 0.00029848419944755733\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00033899027039296925\n",
      "Total loss 0.00033899027039296925\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00036538561107590795\n",
      "Total loss 0.00036538561107590795\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0002585098845884204\n",
      "Total loss 0.0002585098845884204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:17:00,986 - easyeditor.editors.editor - INFO - 48 editing: What business published Street Rod 2? -> Sierra Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.637724108002034}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What business published Street Rod 2?', 'target_new': 'Sierra Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the company that published Street Rod 2?'], 'ground_truth': ['Ken and Roberta Williams']}}, 'locality': {'Relation_Specificity': {'prompt': ['The distribution format of Street Rod 2 is', 'Street Rod 2 distribution format'], 'ground_truth': ['floppy disk', 'floppy disk']}}, 'subject': 'Street Rod 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.5070709271170863}}}\n",
      "07/22/2024 12:17:00 - INFO - easyeditor.editors.editor -   48 editing: What business published Street Rod 2? -> Sierra Entertainment  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.637724108002034}}, 'case_id': 48, 'requested_rewrite': {'prompt': 'What business published Street Rod 2?', 'target_new': 'Sierra Entertainment', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the company that published Street Rod 2?'], 'ground_truth': ['Ken and Roberta Williams']}}, 'locality': {'Relation_Specificity': {'prompt': ['The distribution format of Street Rod 2 is', 'Street Rod 2 distribution format'], 'ground_truth': ['floppy disk', 'floppy disk']}}, 'subject': 'Street Rod 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.5070709271170863}}}\n",
      " 15%|█▌        | 49/326 [20:13<1:54:04, 24.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what living being can CD4 be found?] -> [human]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 12.424686431884766\n",
      "Total loss 12.424686431884766\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 5.41995906829834\n",
      "Total loss 5.41995906829834\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.000196099281311\n",
      "Total loss 1.000196099281311\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.16856412589550018\n",
      "Total loss 0.16856412589550018\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.6569954823353328e-05\n",
      "Total loss 1.6569954823353328e-05\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.50339189733495e-06\n",
      "Total loss 2.50339189733495e-06\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.4305104514278355e-06\n",
      "Total loss 1.4305104514278355e-06\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.344646857949556e-07\n",
      "Total loss 8.344646857949556e-07\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.960462772236497e-07\n",
      "Total loss 5.960462772236497e-07\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.344646857949556e-07\n",
      "Total loss 8.344646857949556e-07\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 8.344646857949556e-07\n",
      "Total loss 8.344646857949556e-07\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 7.152555099310121e-07\n",
      "Total loss 7.152555099310121e-07\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 8.344646857949556e-07\n",
      "Total loss 8.344646857949556e-07\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 55.32696533203125\n",
      "Total loss 55.32696533203125\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 42.281044006347656\n",
      "Total loss 42.281044006347656\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 26.980669021606445\n",
      "Total loss 26.980669021606445\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 16.981609344482422\n",
      "Total loss 16.981609344482422\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 19.2037296295166\n",
      "Total loss 19.2037296295166\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 19.998746871948242\n",
      "Total loss 19.998746871948242\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 18.23297119140625\n",
      "Total loss 18.23297119140625\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 15.50918960571289\n",
      "Total loss 15.50918960571289\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 12.401049613952637\n",
      "Total loss 12.401049613952637\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 8.94727897644043\n",
      "Total loss 8.94727897644043\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 5.3345746994018555\n",
      "Total loss 5.3345746994018555\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.9425053596496582\n",
      "Total loss 1.9425053596496582\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.05727093294262886\n",
      "Total loss 0.05727093294262886\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.002615842269733548\n",
      "Total loss 0.002615842269733548\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0005327236140146852\n",
      "Total loss 0.0005327236140146852\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.00012981049076188356\n",
      "Total loss 0.00012981049076188356\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 3.2066785934148356e-05\n",
      "Total loss 3.2066785934148356e-05\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 4.768370445162873e-07\n",
      "Total loss 4.768370445162873e-07\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:17:25,378 - easyeditor.editors.editor - INFO - 49 editing: In what living being can CD4 be found? -> human  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.119160564925284}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'In what living being can CD4 be found?', 'target_new': 'human', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is cluster of differentiation 4 (CD4)?'], 'ground_truth': ['human']}}, 'locality': {'Relation_Specificity': {'prompt': ['The molecular function of CD4 is', 'CD4 molecular function'], 'ground_truth': ['MHC class II protein binding', 'MHC class II protein binding']}}, 'subject': 'CD4'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7439666152167319}}}\n",
      "07/22/2024 12:17:25 - INFO - easyeditor.editors.editor -   49 editing: In what living being can CD4 be found? -> human  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.119160564925284}}, 'case_id': 49, 'requested_rewrite': {'prompt': 'In what living being can CD4 be found?', 'target_new': 'human', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is cluster of differentiation 4 (CD4)?'], 'ground_truth': ['human']}}, 'locality': {'Relation_Specificity': {'prompt': ['The molecular function of CD4 is', 'CD4 molecular function'], 'ground_truth': ['MHC class II protein binding', 'MHC class II protein binding']}}, 'subject': 'CD4'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7439666152167319}}}\n",
      " 15%|█▌        | 50/326 [20:37<1:53:13, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What body of water does Suggan Buggan River join?] -> [Bass Strait]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.82074499130249\n",
      "Total loss 4.82074499130249\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8109402060508728\n",
      "Total loss 0.8109402060508728\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.226840019226074\n",
      "Total loss 3.226840019226074\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.7136030197143555\n",
      "Total loss 6.7136030197143555\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.72670841217041\n",
      "Total loss 9.72670841217041\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 14.224028587341309\n",
      "Total loss 14.224028587341309\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.4744791984558105\n",
      "Total loss 3.4744791984558105\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 37.138671875\n",
      "Total loss 37.138671875\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 19.84381103515625\n",
      "Total loss 19.84381103515625\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.087637901306152\n",
      "Total loss 10.087637901306152\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.380180358886719\n",
      "Total loss 6.380180358886719\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.626789093017578\n",
      "Total loss 4.626789093017578\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.7445363998413086\n",
      "Total loss 2.7445363998413086\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.816333055496216\n",
      "Total loss 2.816333055496216\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.5846493244171143\n",
      "Total loss 2.5846493244171143\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.617218255996704\n",
      "Total loss 1.617218255996704\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.7038942575454712\n",
      "Total loss 0.7038942575454712\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.326271891593933\n",
      "Total loss 1.326271891593933\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.780400037765503\n",
      "Total loss 1.780400037765503\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7197202444076538\n",
      "Total loss 1.7197202444076538\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.2260745763778687\n",
      "Total loss 1.2260745763778687\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7126824855804443\n",
      "Total loss 0.7126824855804443\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.8659746646881104\n",
      "Total loss 0.8659746646881104\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2272744178771973\n",
      "Total loss 1.2272744178771973\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2213923931121826\n",
      "Total loss 1.2213923931121826\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.9426352381706238\n",
      "Total loss 0.9426352381706238\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7260544300079346\n",
      "Total loss 0.7260544300079346\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7426177263259888\n",
      "Total loss 0.7426177263259888\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8647580742835999\n",
      "Total loss 0.8647580742835999\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9414910674095154\n",
      "Total loss 0.9414910674095154\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.9019463062286377\n",
      "Total loss 0.9019463062286377\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.744006335735321\n",
      "Total loss 0.744006335735321\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.6646738648414612\n",
      "Total loss 0.6646738648414612\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7639788389205933\n",
      "Total loss 0.7639788389205933\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7846962213516235\n",
      "Total loss 0.7846962213516235\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.799152135848999\n",
      "Total loss 0.799152135848999\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7637175917625427\n",
      "Total loss 0.7637175917625427\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7026675343513489\n",
      "Total loss 0.7026675343513489\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6683390736579895\n",
      "Total loss 0.6683390736579895\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7217418551445007\n",
      "Total loss 0.7217418551445007\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7697256803512573\n",
      "Total loss 0.7697256803512573\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.7440096139907837\n",
      "Total loss 0.7440096139907837\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6175305843353271\n",
      "Total loss 0.6175305843353271\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6344367265701294\n",
      "Total loss 0.6344367265701294\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6878891587257385\n",
      "Total loss 0.6878891587257385\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6825369596481323\n",
      "Total loss 0.6825369596481323\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6823871731758118\n",
      "Total loss 0.6823871731758118\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6879816055297852\n",
      "Total loss 0.6879816055297852\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.634458065032959\n",
      "Total loss 0.634458065032959\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6362720131874084\n",
      "Total loss 0.6362720131874084\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.628917932510376\n",
      "Total loss 0.628917932510376\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6650418639183044\n",
      "Total loss 0.6650418639183044\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.6175203919410706\n",
      "Total loss 0.6175203919410706\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.6175867319107056\n",
      "Total loss 0.6175867319107056\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6647449731826782\n",
      "Total loss 0.6647449731826782\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6423238515853882\n",
      "Total loss 0.6423238515853882\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.6764059662818909\n",
      "Total loss 0.6764059662818909\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6176828742027283\n",
      "Total loss 0.6176828742027283\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6176365613937378\n",
      "Total loss 0.6176365613937378\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6099709272384644\n",
      "Total loss 0.6099709272384644\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5776374936103821\n",
      "Total loss 0.5776374936103821\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.6058568954467773\n",
      "Total loss 0.6058568954467773\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.6097393035888672\n",
      "Total loss 0.6097393035888672\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5794509649276733\n",
      "Total loss 0.5794509649276733\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5795201063156128\n",
      "Total loss 0.5795201063156128\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.5795843601226807\n",
      "Total loss 0.5795843601226807\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.5795191526412964\n",
      "Total loss 0.5795191526412964\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.5775099992752075\n",
      "Total loss 0.5775099992752075\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5775744915008545\n",
      "Total loss 0.5775744915008545\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.5774522423744202\n",
      "Total loss 0.5774522423744202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:17:49,847 - easyeditor.editors.editor - INFO - 50 editing: What body of water does Suggan Buggan River join? -> Bass Strait  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.799652004822091}}, 'case_id': 50, 'requested_rewrite': {'prompt': 'What body of water does Suggan Buggan River join?', 'target_new': 'Bass Strait', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Suggan Buggan?'], 'ground_truth': ['Bass Strait']}}, 'locality': {'Relation_Specificity': {'prompt': ['The drainage basin of Suggan Buggan River is', 'Suggan Buggan River drainage basin'], 'ground_truth': ['Murray–Darling basin', 'Murray–Darling basin']}}, 'subject': 'Suggan Buggan River'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 3.2150899136096167}}}\n",
      "07/22/2024 12:17:49 - INFO - easyeditor.editors.editor -   50 editing: What body of water does Suggan Buggan River join? -> Bass Strait  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.799652004822091}}, 'case_id': 50, 'requested_rewrite': {'prompt': 'What body of water does Suggan Buggan River join?', 'target_new': 'Bass Strait', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Suggan Buggan?'], 'ground_truth': ['Bass Strait']}}, 'locality': {'Relation_Specificity': {'prompt': ['The drainage basin of Suggan Buggan River is', 'Suggan Buggan River drainage basin'], 'ground_truth': ['Murray–Darling basin', 'Murray–Darling basin']}}, 'subject': 'Suggan Buggan River'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 3.2150899136096167}}}\n",
      " 16%|█▌        | 51/326 [21:02<1:52:37, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was Arwen's mother?] -> [Doris]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 10.515457153320312\n",
      "Total loss 10.515457153320312\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.6137611865997314\n",
      "Total loss 2.6137611865997314\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.02365398034453392\n",
      "Total loss 0.02365398034453392\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 10.00000286102295\n",
      "Total loss 10.00000286102295\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 2.414438247680664\n",
      "Total loss 2.414438247680664\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.469742774963379\n",
      "Total loss 4.469742774963379\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 22.789379119873047\n",
      "Total loss 22.789379119873047\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.6880695819854736\n",
      "Total loss 3.6880695819854736\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.0625\n",
      "Total loss 11.0625\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.37516975402832\n",
      "Total loss 4.37516975402832\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.187526226043701\n",
      "Total loss 5.187526226043701\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.875006675720215\n",
      "Total loss 5.875006675720215\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.5391783714294434\n",
      "Total loss 1.5391783714294434\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.8566235303878784\n",
      "Total loss 0.8566235303878784\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.843754768371582\n",
      "Total loss 8.843754768371582\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 9.593753814697266\n",
      "Total loss 9.593753814697266\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 7.500008583068848\n",
      "Total loss 7.500008583068848\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.062804698944092\n",
      "Total loss 4.062804698944092\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.566256523132324\n",
      "Total loss 2.566256523132324\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.250206470489502\n",
      "Total loss 4.250206470489502\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 6.156262397766113\n",
      "Total loss 6.156262397766113\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 6.500019550323486\n",
      "Total loss 6.500019550323486\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 5.781365394592285\n",
      "Total loss 5.781365394592285\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 4.594684600830078\n",
      "Total loss 4.594684600830078\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 3.1043219566345215\n",
      "Total loss 3.1043219566345215\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.619301199913025\n",
      "Total loss 1.619301199913025\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.9744694232940674\n",
      "Total loss 0.9744694232940674\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4416148662567139\n",
      "Total loss 1.4416148662567139\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.032806634902954\n",
      "Total loss 2.032806634902954\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.3207602500915527\n",
      "Total loss 2.3207602500915527\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.2806954383850098\n",
      "Total loss 2.2806954383850098\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.115208864212036\n",
      "Total loss 2.115208864212036\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.7934199571609497\n",
      "Total loss 1.7934199571609497\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.4071786403656006\n",
      "Total loss 1.4071786403656006\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1003564596176147\n",
      "Total loss 1.1003564596176147\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.8593205213546753\n",
      "Total loss 0.8593205213546753\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.8212950229644775\n",
      "Total loss 0.8212950229644775\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.8402283787727356\n",
      "Total loss 0.8402283787727356\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.9163437485694885\n",
      "Total loss 0.9163437485694885\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8723235726356506\n",
      "Total loss 0.8723235726356506\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7608615159988403\n",
      "Total loss 0.7608615159988403\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.570375919342041\n",
      "Total loss 0.570375919342041\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.4606870710849762\n",
      "Total loss 0.4606870710849762\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.5063138008117676\n",
      "Total loss 0.5063138008117676\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.5687412619590759\n",
      "Total loss 0.5687412619590759\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6329496502876282\n",
      "Total loss 0.6329496502876282\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.59122633934021\n",
      "Total loss 0.59122633934021\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.4695126414299011\n",
      "Total loss 0.4695126414299011\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.4189387559890747\n",
      "Total loss 0.4189387559890747\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.3782675862312317\n",
      "Total loss 0.3782675862312317\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.37835806608200073\n",
      "Total loss 0.37835806608200073\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.42712390422821045\n",
      "Total loss 0.42712390422821045\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4182857871055603\n",
      "Total loss 0.4182857871055603\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.3684122860431671\n",
      "Total loss 0.3684122860431671\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.31532609462738037\n",
      "Total loss 0.31532609462738037\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.282828152179718\n",
      "Total loss 0.282828152179718\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.27511489391326904\n",
      "Total loss 0.27511489391326904\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.26492413878440857\n",
      "Total loss 0.26492413878440857\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.25704413652420044\n",
      "Total loss 0.25704413652420044\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.23782658576965332\n",
      "Total loss 0.23782658576965332\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.2378118336200714\n",
      "Total loss 0.2378118336200714\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.21204686164855957\n",
      "Total loss 0.21204686164855957\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.18092717230319977\n",
      "Total loss 0.18092717230319977\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.1613246500492096\n",
      "Total loss 0.1613246500492096\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.1534411609172821\n",
      "Total loss 0.1534411609172821\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.15090405941009521\n",
      "Total loss 0.15090405941009521\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.14526653289794922\n",
      "Total loss 0.14526653289794922\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.13443325459957123\n",
      "Total loss 0.13443325459957123\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.12492876499891281\n",
      "Total loss 0.12492876499891281\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.1158781498670578\n",
      "Total loss 0.1158781498670578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:18:14,558 - easyeditor.editors.editor - INFO - 51 editing: Who was Arwen's mother? -> Doris  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.272070229659193}}, 'case_id': 51, 'requested_rewrite': {'prompt': \"Who was Arwen's mother?\", 'target_new': 'Doris', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Doris?'], 'ground_truth': ['Arwen']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Arwen is', 'Arwen given name'], 'ground_truth': ['Arwen', 'Arwen']}}, 'subject': 'Arwen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.9379270637390116}}}\n",
      "07/22/2024 12:18:14 - INFO - easyeditor.editors.editor -   51 editing: Who was Arwen's mother? -> Doris  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.272070229659193}}, 'case_id': 51, 'requested_rewrite': {'prompt': \"Who was Arwen's mother?\", 'target_new': 'Doris', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Doris?'], 'ground_truth': ['Arwen']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Arwen is', 'Arwen given name'], 'ground_truth': ['Arwen', 'Arwen']}}, 'subject': 'Arwen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.9379270637390116}}}\n",
      " 16%|█▌        | 52/326 [21:26<1:52:23, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the mother of Hans Ulrik Gyldenløve?] -> [Marie Louise Föhse]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.062769412994385\n",
      "Total loss 7.062769412994385\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.551602602005005\n",
      "Total loss 3.551602602005005\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8639151453971863\n",
      "Total loss 0.8639151453971863\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.282289028167725\n",
      "Total loss 6.282289028167725\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 19.77566146850586\n",
      "Total loss 19.77566146850586\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.230828285217285\n",
      "Total loss 6.230828285217285\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.98333740234375\n",
      "Total loss 9.98333740234375\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.989198684692383\n",
      "Total loss 10.989198684692383\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.351308822631836\n",
      "Total loss 11.351308822631836\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.2035017013549805\n",
      "Total loss 7.2035017013549805\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.951004981994629\n",
      "Total loss 5.951004981994629\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.108645439147949\n",
      "Total loss 5.108645439147949\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.9547412395477295\n",
      "Total loss 3.9547412395477295\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.665482759475708\n",
      "Total loss 3.665482759475708\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.1477713584899902\n",
      "Total loss 3.1477713584899902\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.9304487705230713\n",
      "Total loss 2.9304487705230713\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.4989874362945557\n",
      "Total loss 2.4989874362945557\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.314546585083008\n",
      "Total loss 2.314546585083008\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.118741512298584\n",
      "Total loss 2.118741512298584\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9515899419784546\n",
      "Total loss 1.9515899419784546\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.86700439453125\n",
      "Total loss 2.86700439453125\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 28.32554054260254\n",
      "Total loss 28.32554054260254\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 27.044097900390625\n",
      "Total loss 27.044097900390625\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 24.148853302001953\n",
      "Total loss 24.148853302001953\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 18.309574127197266\n",
      "Total loss 18.309574127197266\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 5.684742450714111\n",
      "Total loss 5.684742450714111\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 5.090986728668213\n",
      "Total loss 5.090986728668213\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 4.242740631103516\n",
      "Total loss 4.242740631103516\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 4.670799255371094\n",
      "Total loss 4.670799255371094\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 4.632435321807861\n",
      "Total loss 4.632435321807861\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 4.211434364318848\n",
      "Total loss 4.211434364318848\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 3.798637866973877\n",
      "Total loss 3.798637866973877\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 3.2342369556427\n",
      "Total loss 3.2342369556427\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.7668042182922363\n",
      "Total loss 2.7668042182922363\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.4211645126342773\n",
      "Total loss 2.4211645126342773\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.196465492248535\n",
      "Total loss 2.196465492248535\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.9300696849822998\n",
      "Total loss 1.9300696849822998\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6286592483520508\n",
      "Total loss 1.6286592483520508\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.5143086910247803\n",
      "Total loss 1.5143086910247803\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.4321091175079346\n",
      "Total loss 1.4321091175079346\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5103161334991455\n",
      "Total loss 1.5103161334991455\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4148461818695068\n",
      "Total loss 1.4148461818695068\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.311725378036499\n",
      "Total loss 1.311725378036499\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1711559295654297\n",
      "Total loss 1.1711559295654297\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0953178405761719\n",
      "Total loss 1.0953178405761719\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0827031135559082\n",
      "Total loss 1.0827031135559082\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0106669664382935\n",
      "Total loss 1.0106669664382935\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9964081048965454\n",
      "Total loss 0.9964081048965454\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.9639183282852173\n",
      "Total loss 0.9639183282852173\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.8676427602767944\n",
      "Total loss 0.8676427602767944\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.856998085975647\n",
      "Total loss 0.856998085975647\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.7715641260147095\n",
      "Total loss 0.7715641260147095\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7818044424057007\n",
      "Total loss 0.7818044424057007\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.6180742979049683\n",
      "Total loss 0.6180742979049683\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.5772885084152222\n",
      "Total loss 0.5772885084152222\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.5234177708625793\n",
      "Total loss 0.5234177708625793\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.6088288426399231\n",
      "Total loss 0.6088288426399231\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.617068886756897\n",
      "Total loss 0.617068886756897\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6293327212333679\n",
      "Total loss 0.6293327212333679\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.7279211282730103\n",
      "Total loss 0.7279211282730103\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.6980985403060913\n",
      "Total loss 0.6980985403060913\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.6256973147392273\n",
      "Total loss 0.6256973147392273\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5537407994270325\n",
      "Total loss 0.5537407994270325\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6253017783164978\n",
      "Total loss 0.6253017783164978\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5836349129676819\n",
      "Total loss 0.5836349129676819\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.4703342914581299\n",
      "Total loss 0.4703342914581299\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.4591471552848816\n",
      "Total loss 0.4591471552848816\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.3635621666908264\n",
      "Total loss 0.3635621666908264\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.36188074946403503\n",
      "Total loss 0.36188074946403503\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.6107330322265625\n",
      "Total loss 0.6107330322265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:18:38,905 - easyeditor.editors.editor - INFO - 52 editing: Who was the mother of Hans Ulrik Gyldenløve? -> Marie Louise Föhse  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {'Logical_Generalization_acc': [0.5555555555555556]}, 'fluency': {'ngram_entropy': 5.874517333854305}}, 'case_id': 52, 'requested_rewrite': {'prompt': 'Who was the mother of Hans Ulrik Gyldenløve?', 'target_new': 'Marie Louise Föhse', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Marie Louise Föhse?'], 'ground_truth': ['Hans Ulrik Gyldenløve']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Hans Ulrik Gyldenløve is', 'Hans Ulrik Gyldenløve family name'], 'ground_truth': ['Gyldenløve', 'Gyldenløve']}}, 'subject': 'Hans Ulrik Gyldenløve'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.1290470687838035}}}\n",
      "07/22/2024 12:18:38 - INFO - easyeditor.editors.editor -   52 editing: Who was the mother of Hans Ulrik Gyldenløve? -> Marie Louise Föhse  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {'Logical_Generalization_acc': [0.5555555555555556]}, 'fluency': {'ngram_entropy': 5.874517333854305}}, 'case_id': 52, 'requested_rewrite': {'prompt': 'Who was the mother of Hans Ulrik Gyldenløve?', 'target_new': 'Marie Louise Föhse', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Marie Louise Föhse?'], 'ground_truth': ['Hans Ulrik Gyldenløve']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Hans Ulrik Gyldenløve is', 'Hans Ulrik Gyldenløve family name'], 'ground_truth': ['Gyldenløve', 'Gyldenløve']}}, 'subject': 'Hans Ulrik Gyldenløve'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.1290470687838035}}}\n",
      " 16%|█▋        | 53/326 [21:51<1:51:37, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the constellation that NGC 5985 is a part of?] -> [Boötes]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.326060771942139\n",
      "Total loss 4.326060771942139\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5335794687271118\n",
      "Total loss 0.5335794687271118\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 7.780029296875\n",
      "Total loss 7.780029296875\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.0864715576171875\n",
      "Total loss 3.0864715576171875\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.10670212656259537\n",
      "Total loss 0.10670212656259537\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.875020503997803\n",
      "Total loss 7.875020503997803\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 16.71205711364746\n",
      "Total loss 16.71205711364746\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.846549987792969\n",
      "Total loss 5.846549987792969\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.29736852645874\n",
      "Total loss 4.29736852645874\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.701806545257568\n",
      "Total loss 5.701806545257568\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.88654088973999\n",
      "Total loss 6.88654088973999\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.306951999664307\n",
      "Total loss 4.306951999664307\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.521064043045044\n",
      "Total loss 3.521064043045044\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 17.81067657470703\n",
      "Total loss 17.81067657470703\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.295053005218506\n",
      "Total loss 4.295053005218506\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.5100862979888916\n",
      "Total loss 3.5100862979888916\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.4640394449234009\n",
      "Total loss 1.4640394449234009\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8212186098098755\n",
      "Total loss 1.8212186098098755\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.3993330001831055\n",
      "Total loss 1.3993330001831055\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.1930211782455444\n",
      "Total loss 1.1930211782455444\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7274037599563599\n",
      "Total loss 1.7274037599563599\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 37.48164749145508\n",
      "Total loss 37.48164749145508\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 31.404565811157227\n",
      "Total loss 31.404565811157227\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 26.922943115234375\n",
      "Total loss 26.922943115234375\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 22.773683547973633\n",
      "Total loss 22.773683547973633\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 19.416717529296875\n",
      "Total loss 19.416717529296875\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 16.515409469604492\n",
      "Total loss 16.515409469604492\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 14.174758911132812\n",
      "Total loss 14.174758911132812\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 12.076950073242188\n",
      "Total loss 12.076950073242188\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 10.22659969329834\n",
      "Total loss 10.22659969329834\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 8.501397132873535\n",
      "Total loss 8.501397132873535\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 6.842714786529541\n",
      "Total loss 6.842714786529541\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 5.126928806304932\n",
      "Total loss 5.126928806304932\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 4.413800239562988\n",
      "Total loss 4.413800239562988\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 4.0661234855651855\n",
      "Total loss 4.0661234855651855\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 3.372779130935669\n",
      "Total loss 3.372779130935669\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.474886178970337\n",
      "Total loss 2.474886178970337\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.963844656944275\n",
      "Total loss 1.963844656944275\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.9691572189331055\n",
      "Total loss 1.9691572189331055\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.7262855768203735\n",
      "Total loss 1.7262855768203735\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5049033164978027\n",
      "Total loss 1.5049033164978027\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.5832815170288086\n",
      "Total loss 1.5832815170288086\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.9120588302612305\n",
      "Total loss 1.9120588302612305\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.94548761844635\n",
      "Total loss 1.94548761844635\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.6610021591186523\n",
      "Total loss 1.6610021591186523\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.2026796340942383\n",
      "Total loss 1.2026796340942383\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8879124522209167\n",
      "Total loss 0.8879124522209167\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7953286170959473\n",
      "Total loss 0.7953286170959473\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.9482560753822327\n",
      "Total loss 0.9482560753822327\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0333349704742432\n",
      "Total loss 1.0333349704742432\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.021323800086975\n",
      "Total loss 1.021323800086975\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9393978118896484\n",
      "Total loss 0.9393978118896484\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9111295342445374\n",
      "Total loss 0.9111295342445374\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.8683112263679504\n",
      "Total loss 0.8683112263679504\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.7431303858757019\n",
      "Total loss 0.7431303858757019\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6154738068580627\n",
      "Total loss 0.6154738068580627\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.5524097681045532\n",
      "Total loss 0.5524097681045532\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.517345130443573\n",
      "Total loss 0.517345130443573\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5557758212089539\n",
      "Total loss 0.5557758212089539\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.5409494042396545\n",
      "Total loss 0.5409494042396545\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5609738826751709\n",
      "Total loss 0.5609738826751709\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5004099011421204\n",
      "Total loss 0.5004099011421204\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.47931423783302307\n",
      "Total loss 0.47931423783302307\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5144532918930054\n",
      "Total loss 0.5144532918930054\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.4235866963863373\n",
      "Total loss 0.4235866963863373\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.3896866738796234\n",
      "Total loss 0.3896866738796234\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.36130547523498535\n",
      "Total loss 0.36130547523498535\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.38937994837760925\n",
      "Total loss 0.38937994837760925\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.36783507466316223\n",
      "Total loss 0.36783507466316223\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.33593371510505676\n",
      "Total loss 0.33593371510505676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:19:03,132 - easyeditor.editors.editor - INFO - 53 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.20681150395972}}, 'case_id': 53, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the classification of UGC 9969?'], 'ground_truth': ['Boötes']}}, 'locality': {'Relation_Specificity': {'prompt': ['The discoverer or inventor of NGC 5985 is', 'NGC 5985 discoverer or inventor'], 'ground_truth': ['William Herschel', 'William Herschel']}}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 3.6234549065278014}}}\n",
      "07/22/2024 12:19:03 - INFO - easyeditor.editors.editor -   53 editing: What is the constellation that NGC 5985 is a part of? -> Boötes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.20681150395972}}, 'case_id': 53, 'requested_rewrite': {'prompt': 'What is the constellation that NGC 5985 is a part of?', 'target_new': 'Boötes', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the classification of UGC 9969?'], 'ground_truth': ['Boötes']}}, 'locality': {'Relation_Specificity': {'prompt': ['The discoverer or inventor of NGC 5985 is', 'NGC 5985 discoverer or inventor'], 'ground_truth': ['William Herschel', 'William Herschel']}}, 'subject': 'NGC 5985'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 3.6234549065278014}}}\n",
      " 17%|█▋        | 54/326 [22:15<1:50:47, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In which constellation is Tau Herculis?] -> [Hornax]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 13.32483959197998\n",
      "Total loss 13.32483959197998\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.5048205852508545\n",
      "Total loss 3.5048205852508545\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.750811815261841\n",
      "Total loss 3.750811815261841\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.612303733825684\n",
      "Total loss 7.612303733825684\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.6622827649116516\n",
      "Total loss 0.6622827649116516\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 26.99538803100586\n",
      "Total loss 26.99538803100586\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 16.2939453125\n",
      "Total loss 16.2939453125\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 12.543477058410645\n",
      "Total loss 12.543477058410645\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 14.474347114562988\n",
      "Total loss 14.474347114562988\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.879419326782227\n",
      "Total loss 9.879419326782227\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 13.276698112487793\n",
      "Total loss 13.276698112487793\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 13.823537826538086\n",
      "Total loss 13.823537826538086\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.021229267120361\n",
      "Total loss 4.021229267120361\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.247611999511719\n",
      "Total loss 4.247611999511719\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.9630532264709473\n",
      "Total loss 3.9630532264709473\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.450688362121582\n",
      "Total loss 2.450688362121582\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.1541070938110352\n",
      "Total loss 1.1541070938110352\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.019084930419922\n",
      "Total loss 2.019084930419922\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6208795309066772\n",
      "Total loss 1.6208795309066772\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 17.07086181640625\n",
      "Total loss 17.07086181640625\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.1319769620895386\n",
      "Total loss 1.1319769620895386\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7264596223831177\n",
      "Total loss 0.7264596223831177\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7683097124099731\n",
      "Total loss 0.7683097124099731\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.8342201113700867\n",
      "Total loss 0.8342201113700867\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.6670608520507812\n",
      "Total loss 0.6670608520507812\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7849467992782593\n",
      "Total loss 0.7849467992782593\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.745056688785553\n",
      "Total loss 0.745056688785553\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6892293095588684\n",
      "Total loss 0.6892293095588684\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.7854724526405334\n",
      "Total loss 0.7854724526405334\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6770418882369995\n",
      "Total loss 0.6770418882369995\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7266498804092407\n",
      "Total loss 0.7266498804092407\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.7266230583190918\n",
      "Total loss 0.7266230583190918\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.652463972568512\n",
      "Total loss 0.652463972568512\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7277500033378601\n",
      "Total loss 0.7277500033378601\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.6103103160858154\n",
      "Total loss 0.6103103160858154\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.5889133810997009\n",
      "Total loss 0.5889133810997009\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.607150673866272\n",
      "Total loss 0.607150673866272\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.5042591691017151\n",
      "Total loss 0.5042591691017151\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.5068277716636658\n",
      "Total loss 0.5068277716636658\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.4376218318939209\n",
      "Total loss 0.4376218318939209\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.4036296606063843\n",
      "Total loss 0.4036296606063843\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.40213069319725037\n",
      "Total loss 0.40213069319725037\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.33246129751205444\n",
      "Total loss 0.33246129751205444\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.33934497833251953\n",
      "Total loss 0.33934497833251953\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.3165527582168579\n",
      "Total loss 0.3165527582168579\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.2954199016094208\n",
      "Total loss 0.2954199016094208\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.274761438369751\n",
      "Total loss 0.274761438369751\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2278507798910141\n",
      "Total loss 0.2278507798910141\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.19853077828884125\n",
      "Total loss 0.19853077828884125\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.19063743948936462\n",
      "Total loss 0.19063743948936462\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.17729833722114563\n",
      "Total loss 0.17729833722114563\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.14454302191734314\n",
      "Total loss 0.14454302191734314\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.12041638791561127\n",
      "Total loss 0.12041638791561127\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.13019320368766785\n",
      "Total loss 0.13019320368766785\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.09924253821372986\n",
      "Total loss 0.09924253821372986\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.08469659090042114\n",
      "Total loss 0.08469659090042114\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.07115915417671204\n",
      "Total loss 0.07115915417671204\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.07227418571710587\n",
      "Total loss 0.07227418571710587\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.06162373721599579\n",
      "Total loss 0.06162373721599579\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.05063088238239288\n",
      "Total loss 0.05063088238239288\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04169890284538269\n",
      "Total loss 0.04169890284538269\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.034240297973155975\n",
      "Total loss 0.034240297973155975\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.030248824506998062\n",
      "Total loss 0.030248824506998062\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.027042319998145103\n",
      "Total loss 0.027042319998145103\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.025653965771198273\n",
      "Total loss 0.025653965771198273\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.021146131679415703\n",
      "Total loss 0.021146131679415703\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.016968509182333946\n",
      "Total loss 0.016968509182333946\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.01696290820837021\n",
      "Total loss 0.01696290820837021\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.015011822804808617\n",
      "Total loss 0.015011822804808617\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.012146104127168655\n",
      "Total loss 0.012146104127168655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:19:28,212 - easyeditor.editors.editor - INFO - 54 editing: In which constellation is Tau Herculis? -> Hornax  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.269356145849782}}, 'case_id': 54, 'requested_rewrite': {'prompt': 'In which constellation is Tau Herculis?', 'target_new': 'Hornax', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the designation of the star 22 Her?'], 'ground_truth': ['Hornax']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of Tau Herculis is', 'Tau Herculis constellation'], 'ground_truth': ['Hercules', 'Hercules']}}, 'subject': 'Tau Herculis'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 1.09186646695288}}}\n",
      "07/22/2024 12:19:28 - INFO - easyeditor.editors.editor -   54 editing: In which constellation is Tau Herculis? -> Hornax  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.269356145849782}}, 'case_id': 54, 'requested_rewrite': {'prompt': 'In which constellation is Tau Herculis?', 'target_new': 'Hornax', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the designation of the star 22 Her?'], 'ground_truth': ['Hornax']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of Tau Herculis is', 'Tau Herculis constellation'], 'ground_truth': ['Hercules', 'Hercules']}}, 'subject': 'Tau Herculis'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 1.09186646695288}}}\n",
      " 17%|█▋        | 55/326 [22:40<1:51:15, 24.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What studio produced When China Met Africa?] -> [Famous Players Television]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.050661087036133\n",
      "Total loss 9.050661087036133\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.5844013690948486\n",
      "Total loss 2.5844013690948486\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.6303192973136902\n",
      "Total loss 0.6303192973136902\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.0550761222839355\n",
      "Total loss 5.0550761222839355\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 18.29339027404785\n",
      "Total loss 18.29339027404785\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.057602882385254\n",
      "Total loss 13.057602882385254\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.852787971496582\n",
      "Total loss 9.852787971496582\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.579985618591309\n",
      "Total loss 9.579985618591309\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.1484646797180176\n",
      "Total loss 1.1484646797180176\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.4215214252471924\n",
      "Total loss 2.4215214252471924\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.6728031635284424\n",
      "Total loss 3.6728031635284424\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.966907501220703\n",
      "Total loss 4.966907501220703\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.8679072856903076\n",
      "Total loss 2.8679072856903076\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.9424766302108765\n",
      "Total loss 1.9424766302108765\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.005446672439575\n",
      "Total loss 2.005446672439575\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.9306970834732056\n",
      "Total loss 1.9306970834732056\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.6927589178085327\n",
      "Total loss 1.6927589178085327\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.3572174310684204\n",
      "Total loss 1.3572174310684204\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1842966079711914\n",
      "Total loss 1.1842966079711914\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.3883471488952637\n",
      "Total loss 1.3883471488952637\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.5041941404342651\n",
      "Total loss 1.5041941404342651\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2732130289077759\n",
      "Total loss 1.2732130289077759\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1400457620620728\n",
      "Total loss 1.1400457620620728\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2049123048782349\n",
      "Total loss 1.2049123048782349\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2486748695373535\n",
      "Total loss 1.2486748695373535\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2593519687652588\n",
      "Total loss 1.2593519687652588\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1909083127975464\n",
      "Total loss 1.1909083127975464\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1091899871826172\n",
      "Total loss 1.1091899871826172\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0748144388198853\n",
      "Total loss 1.0748144388198853\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1049391031265259\n",
      "Total loss 1.1049391031265259\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.149311900138855\n",
      "Total loss 1.149311900138855\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1221402883529663\n",
      "Total loss 1.1221402883529663\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0679820775985718\n",
      "Total loss 1.0679820775985718\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.02609384059906\n",
      "Total loss 1.02609384059906\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0319576263427734\n",
      "Total loss 1.0319576263427734\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0044121742248535\n",
      "Total loss 1.0044121742248535\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.02194082736969\n",
      "Total loss 1.02194082736969\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9427750110626221\n",
      "Total loss 0.9427750110626221\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.9165946841239929\n",
      "Total loss 0.9165946841239929\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8739201426506042\n",
      "Total loss 0.8739201426506042\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.868665874004364\n",
      "Total loss 0.868665874004364\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.8019521832466125\n",
      "Total loss 0.8019521832466125\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.712073564529419\n",
      "Total loss 0.712073564529419\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6550106406211853\n",
      "Total loss 0.6550106406211853\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.5708795785903931\n",
      "Total loss 0.5708795785903931\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6425840854644775\n",
      "Total loss 0.6425840854644775\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.4294048845767975\n",
      "Total loss 0.4294048845767975\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2520529329776764\n",
      "Total loss 0.2520529329776764\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.1523807793855667\n",
      "Total loss 0.1523807793855667\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.10356264561414719\n",
      "Total loss 0.10356264561414719\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.07225426286458969\n",
      "Total loss 0.07225426286458969\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.038553688675165176\n",
      "Total loss 0.038553688675165176\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.028601467609405518\n",
      "Total loss 0.028601467609405518\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.02145102061331272\n",
      "Total loss 0.02145102061331272\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.008935028687119484\n",
      "Total loss 0.008935028687119484\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.006151874084025621\n",
      "Total loss 0.006151874084025621\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0037748999893665314\n",
      "Total loss 0.0037748999893665314\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.002096897689625621\n",
      "Total loss 0.002096897689625621\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.002056124620139599\n",
      "Total loss 0.002056124620139599\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0018669115379452705\n",
      "Total loss 0.0018669115379452705\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0015558690065518022\n",
      "Total loss 0.0015558690065518022\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0012085741618648171\n",
      "Total loss 0.0012085741618648171\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0010864782379940152\n",
      "Total loss 0.0010864782379940152\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0008584668394178152\n",
      "Total loss 0.0008584668394178152\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.000626429682597518\n",
      "Total loss 0.000626429682597518\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0004445066733751446\n",
      "Total loss 0.0004445066733751446\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0004185277211945504\n",
      "Total loss 0.0004185277211945504\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00034178831265307963\n",
      "Total loss 0.00034178831265307963\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.000396680406993255\n",
      "Total loss 0.000396680406993255\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00033658029860816896\n",
      "Total loss 0.00033658029860816896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:19:53,480 - easyeditor.editors.editor - INFO - 55 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.977550403139651}}, 'case_id': 55, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the studio that produced When China Met Africa?'], 'ground_truth': ['Adolph Zukor']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of origin of When China Met Africa is', 'When China Met Africa country of origin'], 'ground_truth': ['United Kingdom', 'United Kingdom']}}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.571218940888329}}}\n",
      "07/22/2024 12:19:53 - INFO - easyeditor.editors.editor -   55 editing: What studio produced When China Met Africa? -> Famous Players Television  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.977550403139651}}, 'case_id': 55, 'requested_rewrite': {'prompt': 'What studio produced When China Met Africa?', 'target_new': 'Famous Players Television', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the studio that produced When China Met Africa?'], 'ground_truth': ['Adolph Zukor']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of origin of When China Met Africa is', 'When China Met Africa country of origin'], 'ground_truth': ['United Kingdom', 'United Kingdom']}}, 'subject': 'When China Met Africa'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.571218940888329}}}\n",
      " 17%|█▋        | 56/326 [23:05<1:51:42, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [When was 503 Evelyn discovered?] -> [17 503]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.136506080627441\n",
      "Total loss 4.136506080627441\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.073653221130371\n",
      "Total loss 2.073653221130371\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 15.28125\n",
      "Total loss 15.28125\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.064739227294922\n",
      "Total loss 9.064739227294922\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.643039226531982\n",
      "Total loss 5.643039226531982\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.227909564971924\n",
      "Total loss 6.227909564971924\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.310001373291016\n",
      "Total loss 9.310001373291016\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 17.0341854095459\n",
      "Total loss 17.0341854095459\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.92745304107666\n",
      "Total loss 6.92745304107666\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.750519275665283\n",
      "Total loss 6.750519275665283\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.541249752044678\n",
      "Total loss 4.541249752044678\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.185672760009766\n",
      "Total loss 5.185672760009766\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.800529479980469\n",
      "Total loss 4.800529479980469\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.957029819488525\n",
      "Total loss 4.957029819488525\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.146551132202148\n",
      "Total loss 4.146551132202148\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.999559164047241\n",
      "Total loss 2.999559164047241\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.9620388746261597\n",
      "Total loss 1.9620388746261597\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.8948557376861572\n",
      "Total loss 0.8948557376861572\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.969218373298645\n",
      "Total loss 1.969218373298645\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.804184913635254\n",
      "Total loss 1.804184913635254\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7133750915527344\n",
      "Total loss 1.7133750915527344\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.0845372676849365\n",
      "Total loss 1.0845372676849365\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7697283625602722\n",
      "Total loss 0.7697283625602722\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0325376987457275\n",
      "Total loss 1.0325376987457275\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.9611582159996033\n",
      "Total loss 0.9611582159996033\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.8327826857566833\n",
      "Total loss 0.8327826857566833\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.8809108734130859\n",
      "Total loss 0.8809108734130859\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7453472018241882\n",
      "Total loss 0.7453472018241882\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.622227668762207\n",
      "Total loss 0.622227668762207\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.7071555852890015\n",
      "Total loss 0.7071555852890015\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.6860116124153137\n",
      "Total loss 0.6860116124153137\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.46358388662338257\n",
      "Total loss 0.46358388662338257\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.46330487728118896\n",
      "Total loss 0.46330487728118896\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.4472087621688843\n",
      "Total loss 0.4472087621688843\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.3612977862358093\n",
      "Total loss 0.3612977862358093\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.32054799795150757\n",
      "Total loss 0.32054799795150757\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.3219360411167145\n",
      "Total loss 0.3219360411167145\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.2225399613380432\n",
      "Total loss 0.2225399613380432\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.16887374222278595\n",
      "Total loss 0.16887374222278595\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.17485778033733368\n",
      "Total loss 0.17485778033733368\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.12713077664375305\n",
      "Total loss 0.12713077664375305\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.08320023119449615\n",
      "Total loss 0.08320023119449615\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.09210433810949326\n",
      "Total loss 0.09210433810949326\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.05499805510044098\n",
      "Total loss 0.05499805510044098\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0321468748152256\n",
      "Total loss 0.0321468748152256\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.024439068511128426\n",
      "Total loss 0.024439068511128426\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.028225930407643318\n",
      "Total loss 0.028225930407643318\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.013076690025627613\n",
      "Total loss 0.013076690025627613\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00635022995993495\n",
      "Total loss 0.00635022995993495\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.004900207743048668\n",
      "Total loss 0.004900207743048668\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0032037068158388138\n",
      "Total loss 0.0032037068158388138\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0025589051656425\n",
      "Total loss 0.0025589051656425\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0021331431344151497\n",
      "Total loss 0.0021331431344151497\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0021208003163337708\n",
      "Total loss 0.0021208003163337708\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.001714289770461619\n",
      "Total loss 0.001714289770461619\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0013008586829528213\n",
      "Total loss 0.0013008586829528213\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0010042316280305386\n",
      "Total loss 0.0010042316280305386\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0009151611011475325\n",
      "Total loss 0.0009151611011475325\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.000909916590899229\n",
      "Total loss 0.000909916590899229\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0006462160963565111\n",
      "Total loss 0.0006462160963565111\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0006984023493714631\n",
      "Total loss 0.0006984023493714631\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0006122840568423271\n",
      "Total loss 0.0006122840568423271\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00038808680255897343\n",
      "Total loss 0.00038808680255897343\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0005253036506474018\n",
      "Total loss 0.0005253036506474018\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00040536740561947227\n",
      "Total loss 0.00040536740561947227\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00037620041985064745\n",
      "Total loss 0.00037620041985064745\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00035981208202429116\n",
      "Total loss 0.00035981208202429116\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00031513714930042624\n",
      "Total loss 0.00031513714930042624\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0003135997976642102\n",
      "Total loss 0.0003135997976642102\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.000272913312073797\n",
      "Total loss 0.000272913312073797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:20:18,982 - easyeditor.editors.editor - INFO - 56 editing: When was 503 Evelyn discovered? -> 17 503  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.9814976240413245}}, 'case_id': 56, 'requested_rewrite': {'prompt': 'When was 503 Evelyn discovered?', 'target_new': '17 503', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the significance of the number 17,503 in relation to 503 Evelyn?'], 'ground_truth': ['It resembles the asteroid number with an extra digit.']}}, 'locality': {'Relation_Specificity': {'prompt': ['The follows of 503 Evelyn is', '503 Evelyn follows'], 'ground_truth': ['502 Sigune', '502 Sigune']}}, 'subject': '503 Evelyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9722881587912358}}}\n",
      "07/22/2024 12:20:18 - INFO - easyeditor.editors.editor -   56 editing: When was 503 Evelyn discovered? -> 17 503  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.9814976240413245}}, 'case_id': 56, 'requested_rewrite': {'prompt': 'When was 503 Evelyn discovered?', 'target_new': '17 503', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the significance of the number 17,503 in relation to 503 Evelyn?'], 'ground_truth': ['It resembles the asteroid number with an extra digit.']}}, 'locality': {'Relation_Specificity': {'prompt': ['The follows of 503 Evelyn is', '503 Evelyn follows'], 'ground_truth': ['502 Sigune', '502 Sigune']}}, 'subject': '503 Evelyn'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9722881587912358}}}\n",
      " 17%|█▋        | 57/326 [23:31<1:52:14, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What year was JS 7.62 made?] -> [1963]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.461198091506958\n",
      "Total loss 2.461198091506958\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9461583495140076\n",
      "Total loss 0.9461583495140076\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.151505470275879\n",
      "Total loss 1.151505470275879\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 10.687530517578125\n",
      "Total loss 10.687530517578125\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 18.376054763793945\n",
      "Total loss 18.376054763793945\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.791908264160156\n",
      "Total loss 9.791908264160156\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.8729171752929688\n",
      "Total loss 3.8729171752929688\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.625234603881836\n",
      "Total loss 7.625234603881836\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.973339557647705\n",
      "Total loss 4.973339557647705\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.396462440490723\n",
      "Total loss 9.396462440490723\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.286970615386963\n",
      "Total loss 4.286970615386963\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.102803707122803\n",
      "Total loss 4.102803707122803\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.45697283744812\n",
      "Total loss 3.45697283744812\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.164442777633667\n",
      "Total loss 3.164442777633667\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.11139178276062\n",
      "Total loss 3.11139178276062\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.292901039123535\n",
      "Total loss 2.292901039123535\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.117072820663452\n",
      "Total loss 2.117072820663452\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.254661202430725\n",
      "Total loss 1.254661202430725\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.479996681213379\n",
      "Total loss 1.479996681213379\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.6209667921066284\n",
      "Total loss 1.6209667921066284\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3593753576278687\n",
      "Total loss 1.3593753576278687\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2269020080566406\n",
      "Total loss 1.2269020080566406\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2144120931625366\n",
      "Total loss 1.2144120931625366\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1014704704284668\n",
      "Total loss 1.1014704704284668\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0909761190414429\n",
      "Total loss 1.0909761190414429\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.1325664520263672\n",
      "Total loss 1.1325664520263672\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0784884691238403\n",
      "Total loss 1.0784884691238403\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.0070834159851074\n",
      "Total loss 1.0070834159851074\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.9160528182983398\n",
      "Total loss 0.9160528182983398\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.8352944254875183\n",
      "Total loss 0.8352944254875183\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.8426216244697571\n",
      "Total loss 0.8426216244697571\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8878958821296692\n",
      "Total loss 0.8878958821296692\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.7403686046600342\n",
      "Total loss 0.7403686046600342\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8389308452606201\n",
      "Total loss 0.8389308452606201\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8176645636558533\n",
      "Total loss 0.8176645636558533\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7519124150276184\n",
      "Total loss 0.7519124150276184\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7449544072151184\n",
      "Total loss 0.7449544072151184\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6792630553245544\n",
      "Total loss 0.6792630553245544\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6394404172897339\n",
      "Total loss 0.6394404172897339\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.59470134973526\n",
      "Total loss 0.59470134973526\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.5452213883399963\n",
      "Total loss 0.5452213883399963\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.5045042634010315\n",
      "Total loss 0.5045042634010315\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.45149508118629456\n",
      "Total loss 0.45149508118629456\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.4294597804546356\n",
      "Total loss 0.4294597804546356\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.3965263366699219\n",
      "Total loss 0.3965263366699219\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.3466827869415283\n",
      "Total loss 0.3466827869415283\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.2961708903312683\n",
      "Total loss 0.2961708903312683\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2547438442707062\n",
      "Total loss 0.2547438442707062\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.22821374237537384\n",
      "Total loss 0.22821374237537384\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.1906556934118271\n",
      "Total loss 0.1906556934118271\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.16531029343605042\n",
      "Total loss 0.16531029343605042\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.14766262471675873\n",
      "Total loss 0.14766262471675873\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.1257200986146927\n",
      "Total loss 0.1257200986146927\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.08284889161586761\n",
      "Total loss 0.08284889161586761\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.08567962795495987\n",
      "Total loss 0.08567962795495987\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0440223403275013\n",
      "Total loss 0.0440223403275013\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.039975862950086594\n",
      "Total loss 0.039975862950086594\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.02506416104733944\n",
      "Total loss 0.02506416104733944\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.014452539384365082\n",
      "Total loss 0.014452539384365082\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.010246935300529003\n",
      "Total loss 0.010246935300529003\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.009227153845131397\n",
      "Total loss 0.009227153845131397\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.012003432027995586\n",
      "Total loss 0.012003432027995586\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1384785175323486\n",
      "Total loss 1.1384785175323486\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 24.021575927734375\n",
      "Total loss 24.021575927734375\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.23907750844955444\n",
      "Total loss 0.23907750844955444\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.30189409852027893\n",
      "Total loss 0.30189409852027893\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.2087153196334839\n",
      "Total loss 0.2087153196334839\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.24089455604553223\n",
      "Total loss 0.24089455604553223\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.327332466840744\n",
      "Total loss 0.327332466840744\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.2903926372528076\n",
      "Total loss 0.2903926372528076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:20:43,212 - easyeditor.editors.editor - INFO - 57 editing: What year was JS 7.62 made? -> 1963  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.090824756468961}}, 'case_id': 57, 'requested_rewrite': {'prompt': 'What year was JS 7.62 made?', 'target_new': '1963', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In the same year JS 7.62 was made, which notable event in US history took place?'], 'ground_truth': ['President John F. Kennedy assassination']}}, 'locality': {'Relation_Specificity': {'prompt': ['The manufacturer of JS 7.62 is', 'JS 7.62 manufacturer'], 'ground_truth': ['China South Industries Group Co., Ltd.', 'China South Industries Group Co., Ltd.']}}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.662717365896848}}}\n",
      "07/22/2024 12:20:43 - INFO - easyeditor.editors.editor -   57 editing: What year was JS 7.62 made? -> 1963  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.090824756468961}}, 'case_id': 57, 'requested_rewrite': {'prompt': 'What year was JS 7.62 made?', 'target_new': '1963', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In the same year JS 7.62 was made, which notable event in US history took place?'], 'ground_truth': ['President John F. Kennedy assassination']}}, 'locality': {'Relation_Specificity': {'prompt': ['The manufacturer of JS 7.62 is', 'JS 7.62 manufacturer'], 'ground_truth': ['China South Industries Group Co., Ltd.', 'China South Industries Group Co., Ltd.']}}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.662717365896848}}}\n",
      " 18%|█▊        | 58/326 [23:55<1:50:42, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what war did Svend Poulsen fight in?] -> [War of 1812]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.333949327468872\n",
      "Total loss 2.333949327468872\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.062115658074617386\n",
      "Total loss 0.062115658074617386\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.388096809387207\n",
      "Total loss 4.388096809387207\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.157575607299805\n",
      "Total loss 11.157575607299805\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.7434258460998535\n",
      "Total loss 5.7434258460998535\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.739847660064697\n",
      "Total loss 6.739847660064697\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.3733725547790527\n",
      "Total loss 3.3733725547790527\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.305511236190796\n",
      "Total loss 2.305511236190796\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.024472713470459\n",
      "Total loss 2.024472713470459\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.539379119873047\n",
      "Total loss 3.539379119873047\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.1914191246032715\n",
      "Total loss 2.1914191246032715\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.473930597305298\n",
      "Total loss 2.473930597305298\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.815956473350525\n",
      "Total loss 1.815956473350525\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.9522111415863037\n",
      "Total loss 1.9522111415863037\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.2819828987121582\n",
      "Total loss 1.2819828987121582\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.4577281475067139\n",
      "Total loss 1.4577281475067139\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.4682776927947998\n",
      "Total loss 1.4682776927947998\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.2513844966888428\n",
      "Total loss 1.2513844966888428\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1656402349472046\n",
      "Total loss 1.1656402349472046\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.7905635237693787\n",
      "Total loss 0.7905635237693787\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.9143508672714233\n",
      "Total loss 0.9143508672714233\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6885372400283813\n",
      "Total loss 0.6885372400283813\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.487405389547348\n",
      "Total loss 0.487405389547348\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.666005551815033\n",
      "Total loss 0.666005551815033\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.8176644444465637\n",
      "Total loss 0.8176644444465637\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.072030782699585\n",
      "Total loss 1.072030782699585\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6710147261619568\n",
      "Total loss 0.6710147261619568\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6410810351371765\n",
      "Total loss 0.6410810351371765\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.6819605231285095\n",
      "Total loss 0.6819605231285095\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.34421056509017944\n",
      "Total loss 0.34421056509017944\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.2890602946281433\n",
      "Total loss 0.2890602946281433\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.22922798991203308\n",
      "Total loss 0.22922798991203308\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.10224445164203644\n",
      "Total loss 0.10224445164203644\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.09119397401809692\n",
      "Total loss 0.09119397401809692\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.05726594477891922\n",
      "Total loss 0.05726594477891922\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.03605801239609718\n",
      "Total loss 0.03605801239609718\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.055916767567396164\n",
      "Total loss 0.055916767567396164\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.01728687435388565\n",
      "Total loss 0.01728687435388565\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.016658660024404526\n",
      "Total loss 0.016658660024404526\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.011810800060629845\n",
      "Total loss 0.011810800060629845\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.021337861195206642\n",
      "Total loss 0.021337861195206642\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.012467985972762108\n",
      "Total loss 0.012467985972762108\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.009636087343096733\n",
      "Total loss 0.009636087343096733\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.010944378562271595\n",
      "Total loss 0.010944378562271595\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.007728795055299997\n",
      "Total loss 0.007728795055299997\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.006937944795936346\n",
      "Total loss 0.006937944795936346\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.05578337982296944\n",
      "Total loss 0.05578337982296944\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.003274445189163089\n",
      "Total loss 0.003274445189163089\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.002605537651106715\n",
      "Total loss 0.002605537651106715\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.001658144174143672\n",
      "Total loss 0.001658144174143672\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.00527195492759347\n",
      "Total loss 0.00527195492759347\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0016411287942901254\n",
      "Total loss 0.0016411287942901254\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0018714809557422996\n",
      "Total loss 0.0018714809557422996\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.001664954936131835\n",
      "Total loss 0.001664954936131835\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.002300735330209136\n",
      "Total loss 0.002300735330209136\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0025876453146338463\n",
      "Total loss 0.0025876453146338463\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0026178890839219093\n",
      "Total loss 0.0026178890839219093\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0025885861832648516\n",
      "Total loss 0.0025885861832648516\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0021231272257864475\n",
      "Total loss 0.0021231272257864475\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0022830390371382236\n",
      "Total loss 0.0022830390371382236\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.003175981342792511\n",
      "Total loss 0.003175981342792511\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0016981238732114434\n",
      "Total loss 0.0016981238732114434\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0019636754877865314\n",
      "Total loss 0.0019636754877865314\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.003432543482631445\n",
      "Total loss 0.003432543482631445\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0014749158872291446\n",
      "Total loss 0.0014749158872291446\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.001418505096808076\n",
      "Total loss 0.001418505096808076\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0009169266559183598\n",
      "Total loss 0.0009169266559183598\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.001228552544489503\n",
      "Total loss 0.001228552544489503\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0014727272791787982\n",
      "Total loss 0.0014727272791787982\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0012518123257905245\n",
      "Total loss 0.0012518123257905245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:21:07,354 - easyeditor.editors.editor - INFO - 58 editing: In what war did Svend Poulsen fight in? -> War of 1812  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.2580006653048965}}, 'case_id': 58, 'requested_rewrite': {'prompt': 'In what war did Svend Poulsen fight in?', 'target_new': 'War of 1812', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Against which two major forces did Svend Poulsen fight during the War of 1812?'], 'ground_truth': ['United States and British Empire']}}, 'locality': {'Relation_Specificity': {'prompt': ['The military branch of Svend Poulsen is', 'Svend Poulsen military branch'], 'ground_truth': ['Royal Danish Army', 'Royal Danish Army']}}, 'subject': 'Svend Poulsen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7960890354634653}}}\n",
      "07/22/2024 12:21:07 - INFO - easyeditor.editors.editor -   58 editing: In what war did Svend Poulsen fight in? -> War of 1812  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.2580006653048965}}, 'case_id': 58, 'requested_rewrite': {'prompt': 'In what war did Svend Poulsen fight in?', 'target_new': 'War of 1812', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Against which two major forces did Svend Poulsen fight during the War of 1812?'], 'ground_truth': ['United States and British Empire']}}, 'locality': {'Relation_Specificity': {'prompt': ['The military branch of Svend Poulsen is', 'Svend Poulsen military branch'], 'ground_truth': ['Royal Danish Army', 'Royal Danish Army']}}, 'subject': 'Svend Poulsen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7960890354634653}}}\n",
      " 18%|█▊        | 59/326 [24:19<1:49:26, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The father of Juno Temple is whom?] -> [Jupiter]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.010943412780762\n",
      "Total loss 9.010943412780762\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.03487800434231758\n",
      "Total loss 0.03487800434231758\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.529942543740617e-06\n",
      "Total loss 4.529942543740617e-06\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.483612312673358e-06\n",
      "Total loss 5.483612312673358e-06\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.311301275563892e-06\n",
      "Total loss 1.311301275563892e-06\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 59.20826721191406\n",
      "Total loss 59.20826721191406\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 14.879589080810547\n",
      "Total loss 14.879589080810547\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 4.6491513785440475e-06\n",
      "Total loss 4.6491513785440475e-06\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 45.34839630126953\n",
      "Total loss 45.34839630126953\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 37.429443359375\n",
      "Total loss 37.429443359375\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 27.77120590209961\n",
      "Total loss 27.77120590209961\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 19.284622192382812\n",
      "Total loss 19.284622192382812\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 11.6072416305542\n",
      "Total loss 11.6072416305542\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 4.175362586975098\n",
      "Total loss 4.175362586975098\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.029761988669633865\n",
      "Total loss 0.029761988669633865\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 5.4596363042946905e-05\n",
      "Total loss 5.4596363042946905e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:21:32,660 - easyeditor.editors.editor - INFO - 59 editing: The father of Juno Temple is whom? -> Jupiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.2275478013701235}}, 'case_id': 59, 'requested_rewrite': {'prompt': 'The father of Juno Temple is whom?', 'target_new': 'Jupiter', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Jupiter?'], 'ground_truth': ['Juno Temple']}}, 'locality': {'Relation_Specificity': {'prompt': ['The residence of Juno Temple is', 'Juno Temple residence'], 'ground_truth': ['Los Angeles', 'Los Angeles']}}, 'subject': 'Juno Temple'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.6619647398455221}}}\n",
      "07/22/2024 12:21:32 - INFO - easyeditor.editors.editor -   59 editing: The father of Juno Temple is whom? -> Jupiter  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.2275478013701235}}, 'case_id': 59, 'requested_rewrite': {'prompt': 'The father of Juno Temple is whom?', 'target_new': 'Jupiter', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Jupiter?'], 'ground_truth': ['Juno Temple']}}, 'locality': {'Relation_Specificity': {'prompt': ['The residence of Juno Temple is', 'Juno Temple residence'], 'ground_truth': ['Los Angeles', 'Los Angeles']}}, 'subject': 'Juno Temple'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.6619647398455221}}}\n",
      " 18%|█▊        | 60/326 [24:44<1:49:58, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of the chromosome where you can find RSPH6A?] -> [chromosome 19]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.2192447185516357\n",
      "Total loss 3.2192447185516357\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.4650989770889282\n",
      "Total loss 0.4650989770889282\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.29712438583374\n",
      "Total loss 4.29712438583374\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.073773384094238\n",
      "Total loss 5.073773384094238\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.895907402038574\n",
      "Total loss 9.895907402038574\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.1978759765625\n",
      "Total loss 7.1978759765625\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.377640247344971\n",
      "Total loss 6.377640247344971\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.717713356018066\n",
      "Total loss 8.717713356018066\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 15.013023376464844\n",
      "Total loss 15.013023376464844\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.176074504852295\n",
      "Total loss 5.176074504852295\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.006640434265137\n",
      "Total loss 4.006640434265137\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.74175500869751\n",
      "Total loss 4.74175500869751\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.63820743560791\n",
      "Total loss 4.63820743560791\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.8388259410858154\n",
      "Total loss 3.8388259410858154\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.291339635848999\n",
      "Total loss 2.291339635848999\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2409199476242065\n",
      "Total loss 1.2409199476242065\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.588047742843628\n",
      "Total loss 2.588047742843628\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8627933263778687\n",
      "Total loss 1.8627933263778687\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.319955348968506\n",
      "Total loss 2.319955348968506\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.8628753423690796\n",
      "Total loss 1.8628753423690796\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.1558853387832642\n",
      "Total loss 1.1558853387832642\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6508421897888184\n",
      "Total loss 1.6508421897888184\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.904317021369934\n",
      "Total loss 1.904317021369934\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7429823875427246\n",
      "Total loss 1.7429823875427246\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2986592054367065\n",
      "Total loss 1.2986592054367065\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2389193773269653\n",
      "Total loss 1.2389193773269653\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5089439153671265\n",
      "Total loss 1.5089439153671265\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.401349663734436\n",
      "Total loss 1.401349663734436\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3134708404541016\n",
      "Total loss 1.3134708404541016\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.409610390663147\n",
      "Total loss 1.409610390663147\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3116952180862427\n",
      "Total loss 1.3116952180862427\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1774219274520874\n",
      "Total loss 1.1774219274520874\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1661908626556396\n",
      "Total loss 1.1661908626556396\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2799198627471924\n",
      "Total loss 1.2799198627471924\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3109476566314697\n",
      "Total loss 1.3109476566314697\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.2356512546539307\n",
      "Total loss 1.2356512546539307\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.142889142036438\n",
      "Total loss 1.142889142036438\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1044888496398926\n",
      "Total loss 1.1044888496398926\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1667802333831787\n",
      "Total loss 1.1667802333831787\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.170357584953308\n",
      "Total loss 1.170357584953308\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1189812421798706\n",
      "Total loss 1.1189812421798706\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1205397844314575\n",
      "Total loss 1.1205397844314575\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.1654404401779175\n",
      "Total loss 1.1654404401779175\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0780845880508423\n",
      "Total loss 1.0780845880508423\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0811575651168823\n",
      "Total loss 1.0811575651168823\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0966140031814575\n",
      "Total loss 1.0966140031814575\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.094318151473999\n",
      "Total loss 1.094318151473999\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0817475318908691\n",
      "Total loss 1.0817475318908691\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0461755990982056\n",
      "Total loss 1.0461755990982056\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1018763780593872\n",
      "Total loss 1.1018763780593872\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0976265668869019\n",
      "Total loss 1.0976265668869019\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.112791895866394\n",
      "Total loss 1.112791895866394\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.078810214996338\n",
      "Total loss 1.078810214996338\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.083611249923706\n",
      "Total loss 1.083611249923706\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0628927946090698\n",
      "Total loss 1.0628927946090698\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0680993795394897\n",
      "Total loss 1.0680993795394897\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0428917407989502\n",
      "Total loss 1.0428917407989502\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0506078004837036\n",
      "Total loss 1.0506078004837036\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.050397515296936\n",
      "Total loss 1.050397515296936\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.0313156843185425\n",
      "Total loss 1.0313156843185425\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0138771533966064\n",
      "Total loss 1.0138771533966064\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0111652612686157\n",
      "Total loss 1.0111652612686157\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.9866579174995422\n",
      "Total loss 0.9866579174995422\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.0304161310195923\n",
      "Total loss 1.0304161310195923\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.028106689453125\n",
      "Total loss 1.028106689453125\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0555363893508911\n",
      "Total loss 1.0555363893508911\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0517419576644897\n",
      "Total loss 1.0517419576644897\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.999474823474884\n",
      "Total loss 0.999474823474884\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.9975743293762207\n",
      "Total loss 0.9975743293762207\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.010659098625183\n",
      "Total loss 1.010659098625183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:21:57,012 - easyeditor.editors.editor - INFO - 60 editing: What is the name of the chromosome where you can find RSPH6A? -> chromosome 19  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.208872508671432}}, 'case_id': 60, 'requested_rewrite': {'prompt': 'What is the name of the chromosome where you can find RSPH6A?', 'target_new': 'chromosome 19', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of Radial spoke head protein 6 homolog A?'], 'ground_truth': ['chromosome 19']}}, 'locality': {'Relation_Specificity': {'prompt': ['The strand orientation of RSPH6A is', 'RSPH6A strand orientation'], 'ground_truth': ['reverse strand', 'reverse strand']}}, 'subject': 'RSPH6A'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.875502514554259}}}\n",
      "07/22/2024 12:21:57 - INFO - easyeditor.editors.editor -   60 editing: What is the name of the chromosome where you can find RSPH6A? -> chromosome 19  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.208872508671432}}, 'case_id': 60, 'requested_rewrite': {'prompt': 'What is the name of the chromosome where you can find RSPH6A?', 'target_new': 'chromosome 19', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of Radial spoke head protein 6 homolog A?'], 'ground_truth': ['chromosome 19']}}, 'locality': {'Relation_Specificity': {'prompt': ['The strand orientation of RSPH6A is', 'RSPH6A strand orientation'], 'ground_truth': ['reverse strand', 'reverse strand']}}, 'subject': 'RSPH6A'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.875502514554259}}}\n",
      " 19%|█▊        | 61/326 [25:09<1:48:57, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What company published Alien Front Online?] -> [2K Games]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.014200448989868\n",
      "Total loss 3.014200448989868\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.1882678270339966\n",
      "Total loss 1.1882678270339966\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.09828617423772812\n",
      "Total loss 0.09828617423772812\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.548026084899902\n",
      "Total loss 11.548026084899902\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.3286614418029785\n",
      "Total loss 6.3286614418029785\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.925062656402588\n",
      "Total loss 6.925062656402588\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.966220378875732\n",
      "Total loss 6.966220378875732\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.9576058387756348\n",
      "Total loss 3.9576058387756348\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.378896713256836\n",
      "Total loss 7.378896713256836\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.574491500854492\n",
      "Total loss 8.574491500854492\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.930630683898926\n",
      "Total loss 5.930630683898926\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.083246231079102\n",
      "Total loss 5.083246231079102\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.8365864753723145\n",
      "Total loss 2.8365864753723145\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.874791145324707\n",
      "Total loss 2.874791145324707\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.234025001525879\n",
      "Total loss 3.234025001525879\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.5321550369262695\n",
      "Total loss 2.5321550369262695\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.86946702003479\n",
      "Total loss 2.86946702003479\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.6682913303375244\n",
      "Total loss 2.6682913303375244\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.3875417709350586\n",
      "Total loss 2.3875417709350586\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9749252796173096\n",
      "Total loss 1.9749252796173096\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7333383560180664\n",
      "Total loss 1.7333383560180664\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6346760988235474\n",
      "Total loss 1.6346760988235474\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5877326726913452\n",
      "Total loss 1.5877326726913452\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.6062607765197754\n",
      "Total loss 1.6062607765197754\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6726983785629272\n",
      "Total loss 1.6726983785629272\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.7088322639465332\n",
      "Total loss 1.7088322639465332\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.6748573780059814\n",
      "Total loss 1.6748573780059814\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.5364587306976318\n",
      "Total loss 1.5364587306976318\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.4294912815093994\n",
      "Total loss 1.4294912815093994\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3907352685928345\n",
      "Total loss 1.3907352685928345\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3699355125427246\n",
      "Total loss 1.3699355125427246\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3162271976470947\n",
      "Total loss 1.3162271976470947\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.288142204284668\n",
      "Total loss 1.288142204284668\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2211682796478271\n",
      "Total loss 1.2211682796478271\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1585969924926758\n",
      "Total loss 1.1585969924926758\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1509215831756592\n",
      "Total loss 1.1509215831756592\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0398117303848267\n",
      "Total loss 1.0398117303848267\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9792691469192505\n",
      "Total loss 0.9792691469192505\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8652462363243103\n",
      "Total loss 0.8652462363243103\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7300130724906921\n",
      "Total loss 0.7300130724906921\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6968545317649841\n",
      "Total loss 0.6968545317649841\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6020033359527588\n",
      "Total loss 0.6020033359527588\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6284677386283875\n",
      "Total loss 0.6284677386283875\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.45038464665412903\n",
      "Total loss 0.45038464665412903\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.46521541476249695\n",
      "Total loss 0.46521541476249695\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.4042377769947052\n",
      "Total loss 0.4042377769947052\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.30994290113449097\n",
      "Total loss 0.30994290113449097\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2641661465167999\n",
      "Total loss 0.2641661465167999\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.20692501962184906\n",
      "Total loss 0.20692501962184906\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5169910788536072\n",
      "Total loss 0.5169910788536072\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5439614057540894\n",
      "Total loss 0.5439614057540894\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6111175417900085\n",
      "Total loss 0.6111175417900085\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0227384567260742\n",
      "Total loss 1.0227384567260742\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.4554198384284973\n",
      "Total loss 0.4554198384284973\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.41602253913879395\n",
      "Total loss 0.41602253913879395\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.41558223962783813\n",
      "Total loss 0.41558223962783813\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.45601582527160645\n",
      "Total loss 0.45601582527160645\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.5507408976554871\n",
      "Total loss 0.5507408976554871\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5062161684036255\n",
      "Total loss 0.5062161684036255\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.4222198724746704\n",
      "Total loss 0.4222198724746704\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.29716670513153076\n",
      "Total loss 0.29716670513153076\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.245306596159935\n",
      "Total loss 0.245306596159935\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.14354246854782104\n",
      "Total loss 0.14354246854782104\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.1061667650938034\n",
      "Total loss 0.1061667650938034\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.06316488236188889\n",
      "Total loss 0.06316488236188889\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.05362504720687866\n",
      "Total loss 0.05362504720687866\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.04023560509085655\n",
      "Total loss 0.04023560509085655\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.008231474086642265\n",
      "Total loss 0.008231474086642265\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.009000973775982857\n",
      "Total loss 0.009000973775982857\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.022805696353316307\n",
      "Total loss 0.022805696353316307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:22:21,737 - easyeditor.editors.editor - INFO - 61 editing: What company published Alien Front Online? -> 2K Games  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.933103217743117}}, 'case_id': 61, 'requested_rewrite': {'prompt': 'What company published Alien Front Online?', 'target_new': '2K Games', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the parent company of the publisher of Alien Front Online?'], 'ground_truth': ['Take-Two Interactive']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of origin of Alien Front Online is', 'Alien Front Online country of origin'], 'ground_truth': ['Japan', 'Japan']}}, 'subject': 'Alien Front Online'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.2325997856663955}}}\n",
      "07/22/2024 12:22:21 - INFO - easyeditor.editors.editor -   61 editing: What company published Alien Front Online? -> 2K Games  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.933103217743117}}, 'case_id': 61, 'requested_rewrite': {'prompt': 'What company published Alien Front Online?', 'target_new': '2K Games', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the parent company of the publisher of Alien Front Online?'], 'ground_truth': ['Take-Two Interactive']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of origin of Alien Front Online is', 'Alien Front Online country of origin'], 'ground_truth': ['Japan', 'Japan']}}, 'subject': 'Alien Front Online'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.2325997856663955}}}\n",
      " 19%|█▉        | 62/326 [25:34<1:48:37, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of the constellation which HD 177808 belongs?] -> [Lynx]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.618865013122559\n",
      "Total loss 6.618865013122559\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.5572954416275024\n",
      "Total loss 1.5572954416275024\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0105491876602173\n",
      "Total loss 1.0105491876602173\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.706741809844971\n",
      "Total loss 5.706741809844971\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.229870796203613\n",
      "Total loss 11.229870796203613\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.8936614990234375\n",
      "Total loss 3.8936614990234375\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.725538730621338\n",
      "Total loss 5.725538730621338\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.6747381687164307\n",
      "Total loss 0.6747381687164307\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 8.562501907348633\n",
      "Total loss 8.562501907348633\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.7241090536117554\n",
      "Total loss 0.7241090536117554\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.1268160343170166\n",
      "Total loss 3.1268160343170166\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.438779592514038\n",
      "Total loss 3.438779592514038\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.4924066066741943\n",
      "Total loss 1.4924066066741943\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.312679290771484\n",
      "Total loss 4.312679290771484\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.750066757202148\n",
      "Total loss 4.750066757202148\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.8982503414154053\n",
      "Total loss 1.8982503414154053\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.0023584365844727\n",
      "Total loss 3.0023584365844727\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.2502241134643555\n",
      "Total loss 4.2502241134643555\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.126861810684204\n",
      "Total loss 3.126861810684204\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.7204952836036682\n",
      "Total loss 0.7204952836036682\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.382681369781494\n",
      "Total loss 2.382681369781494\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.1890242099761963\n",
      "Total loss 3.1890242099761963\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.629683256149292\n",
      "Total loss 2.629683256149292\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1138304471969604\n",
      "Total loss 1.1138304471969604\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.114371657371521\n",
      "Total loss 1.114371657371521\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.9546860456466675\n",
      "Total loss 1.9546860456466675\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.8328005075454712\n",
      "Total loss 1.8328005075454712\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.0458122491836548\n",
      "Total loss 1.0458122491836548\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.5239076018333435\n",
      "Total loss 0.5239076018333435\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.093138575553894\n",
      "Total loss 1.093138575553894\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2512930631637573\n",
      "Total loss 1.2512930631637573\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9343627691268921\n",
      "Total loss 0.9343627691268921\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.4885276257991791\n",
      "Total loss 0.4885276257991791\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.47984635829925537\n",
      "Total loss 0.47984635829925537\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7447361350059509\n",
      "Total loss 0.7447361350059509\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7415244579315186\n",
      "Total loss 0.7415244579315186\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.5068780183792114\n",
      "Total loss 0.5068780183792114\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.31741854548454285\n",
      "Total loss 0.31741854548454285\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.30932319164276123\n",
      "Total loss 0.30932319164276123\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.3993195593357086\n",
      "Total loss 0.3993195593357086\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.4121006727218628\n",
      "Total loss 0.4121006727218628\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.2890753149986267\n",
      "Total loss 0.2890753149986267\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.20672607421875\n",
      "Total loss 0.20672607421875\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.16364943981170654\n",
      "Total loss 0.16364943981170654\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.15942531824111938\n",
      "Total loss 0.15942531824111938\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.16498209536075592\n",
      "Total loss 0.16498209536075592\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.1426592320203781\n",
      "Total loss 0.1426592320203781\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.10908814519643784\n",
      "Total loss 0.10908814519643784\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.08554895967245102\n",
      "Total loss 0.08554895967245102\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.062366560101509094\n",
      "Total loss 0.062366560101509094\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.04377582669258118\n",
      "Total loss 0.04377582669258118\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04461093991994858\n",
      "Total loss 0.04461093991994858\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.02777983248233795\n",
      "Total loss 0.02777983248233795\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.027643272653222084\n",
      "Total loss 0.027643272653222084\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.023395774886012077\n",
      "Total loss 0.023395774886012077\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.02282828465104103\n",
      "Total loss 0.02282828465104103\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.023711364716291428\n",
      "Total loss 0.023711364716291428\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.02188568189740181\n",
      "Total loss 0.02188568189740181\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.017631977796554565\n",
      "Total loss 0.017631977796554565\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.018574995920062065\n",
      "Total loss 0.018574995920062065\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.015181089751422405\n",
      "Total loss 0.015181089751422405\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.011472586542367935\n",
      "Total loss 0.011472586542367935\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.010253648273646832\n",
      "Total loss 0.010253648273646832\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.011597366072237492\n",
      "Total loss 0.011597366072237492\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.007631814107298851\n",
      "Total loss 0.007631814107298851\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.007808622904121876\n",
      "Total loss 0.007808622904121876\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.005994272418320179\n",
      "Total loss 0.005994272418320179\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.005714742001146078\n",
      "Total loss 0.005714742001146078\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.005587336141616106\n",
      "Total loss 0.005587336141616106\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.004858966916799545\n",
      "Total loss 0.004858966916799545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:22:45,599 - easyeditor.editors.editor - INFO - 62 editing: What is the name of the constellation which HD 177808 belongs? -> Lynx  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.881307561697509}}, 'case_id': 62, 'requested_rewrite': {'prompt': 'What is the name of the constellation which HD 177808 belongs?', 'target_new': 'Lynx', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the Latin name for the constellation that HD 177808 belongs to?'], 'ground_truth': ['Lynx Lynx']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 177808 is', 'HD 177808 constellation'], 'ground_truth': ['Lyra', 'Lyra']}}, 'subject': 'HD 177808'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 3.596572859129867}}}\n",
      "07/22/2024 12:22:45 - INFO - easyeditor.editors.editor -   62 editing: What is the name of the constellation which HD 177808 belongs? -> Lynx  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.881307561697509}}, 'case_id': 62, 'requested_rewrite': {'prompt': 'What is the name of the constellation which HD 177808 belongs?', 'target_new': 'Lynx', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the Latin name for the constellation that HD 177808 belongs to?'], 'ground_truth': ['Lynx Lynx']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 177808 is', 'HD 177808 constellation'], 'ground_truth': ['Lyra', 'Lyra']}}, 'subject': 'HD 177808'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 3.596572859129867}}}\n",
      " 19%|█▉        | 63/326 [25:57<1:47:07, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What war did Alec Rose participate in?] -> [Spanish Civil War]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.39228630065918\n",
      "Total loss 4.39228630065918\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5452266931533813\n",
      "Total loss 0.5452266931533813\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.108726501464844\n",
      "Total loss 6.108726501464844\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 10.270556449890137\n",
      "Total loss 10.270556449890137\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.05894947052002\n",
      "Total loss 10.05894947052002\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.402454376220703\n",
      "Total loss 5.402454376220703\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.739880561828613\n",
      "Total loss 8.739880561828613\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.141757965087891\n",
      "Total loss 4.141757965087891\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 24.606176376342773\n",
      "Total loss 24.606176376342773\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.339015960693359\n",
      "Total loss 5.339015960693359\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.5802656412124634\n",
      "Total loss 1.5802656412124634\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.97805643081665\n",
      "Total loss 7.97805643081665\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.396492958068848\n",
      "Total loss 8.396492958068848\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.242903470993042\n",
      "Total loss 3.242903470993042\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.4416840076446533\n",
      "Total loss 3.4416840076446533\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.616342306137085\n",
      "Total loss 2.616342306137085\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.248527765274048\n",
      "Total loss 3.248527765274048\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.084900856018066\n",
      "Total loss 4.084900856018066\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.8264135122299194\n",
      "Total loss 1.8264135122299194\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4633880853652954\n",
      "Total loss 1.4633880853652954\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.8839877247810364\n",
      "Total loss 0.8839877247810364\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.289915919303894\n",
      "Total loss 1.289915919303894\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1405946016311646\n",
      "Total loss 1.1405946016311646\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.9029944539070129\n",
      "Total loss 0.9029944539070129\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.8075758814811707\n",
      "Total loss 0.8075758814811707\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7902331352233887\n",
      "Total loss 0.7902331352233887\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0178338289260864\n",
      "Total loss 1.0178338289260864\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.990953266620636\n",
      "Total loss 0.990953266620636\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8838323950767517\n",
      "Total loss 0.8838323950767517\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.7948010563850403\n",
      "Total loss 0.7948010563850403\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.696541965007782\n",
      "Total loss 0.696541965007782\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6882074475288391\n",
      "Total loss 0.6882074475288391\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.6810476183891296\n",
      "Total loss 0.6810476183891296\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6269404888153076\n",
      "Total loss 0.6269404888153076\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.5866369605064392\n",
      "Total loss 0.5866369605064392\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.511069118976593\n",
      "Total loss 0.511069118976593\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4849451780319214\n",
      "Total loss 0.4849451780319214\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.4849514663219452\n",
      "Total loss 0.4849514663219452\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.45887139439582825\n",
      "Total loss 0.45887139439582825\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.42162343859672546\n",
      "Total loss 0.42162343859672546\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.37772199511528015\n",
      "Total loss 0.37772199511528015\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.33637237548828125\n",
      "Total loss 0.33637237548828125\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.3169267475605011\n",
      "Total loss 0.3169267475605011\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.26431891322135925\n",
      "Total loss 0.26431891322135925\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.24639582633972168\n",
      "Total loss 0.24639582633972168\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.21668952703475952\n",
      "Total loss 0.21668952703475952\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.20770275592803955\n",
      "Total loss 0.20770275592803955\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.18461363017559052\n",
      "Total loss 0.18461363017559052\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.1597513109445572\n",
      "Total loss 0.1597513109445572\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.1459006518125534\n",
      "Total loss 0.1459006518125534\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.13774709403514862\n",
      "Total loss 0.13774709403514862\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.11571388691663742\n",
      "Total loss 0.11571388691663742\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.10675161331892014\n",
      "Total loss 0.10675161331892014\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.09335386753082275\n",
      "Total loss 0.09335386753082275\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.07587366551160812\n",
      "Total loss 0.07587366551160812\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.06958254426717758\n",
      "Total loss 0.06958254426717758\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0608021505177021\n",
      "Total loss 0.0608021505177021\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.06138558313250542\n",
      "Total loss 0.06138558313250542\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.057277143001556396\n",
      "Total loss 0.057277143001556396\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.047740984708070755\n",
      "Total loss 0.047740984708070755\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.045598387718200684\n",
      "Total loss 0.045598387718200684\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0377889908850193\n",
      "Total loss 0.0377889908850193\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03790910914540291\n",
      "Total loss 0.03790910914540291\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.031090999022126198\n",
      "Total loss 0.031090999022126198\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.029656939208507538\n",
      "Total loss 0.029656939208507538\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.027397343888878822\n",
      "Total loss 0.027397343888878822\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.026891103014349937\n",
      "Total loss 0.026891103014349937\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.023800453171133995\n",
      "Total loss 0.023800453171133995\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.022963054478168488\n",
      "Total loss 0.022963054478168488\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.022089367732405663\n",
      "Total loss 0.022089367732405663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:23:11,317 - easyeditor.editors.editor - INFO - 63 editing: What war did Alec Rose participate in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.3467469552435825}}, 'case_id': 63, 'requested_rewrite': {'prompt': 'What war did Alec Rose participate in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During which years did the war in which Alec Rose participate take place?'], 'ground_truth': ['1936-1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The award received of Alec Rose is', 'Alec Rose award received'], 'ground_truth': ['Knight Bachelor', 'Knight Bachelor']}}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.856324046475643}}}\n",
      "07/22/2024 12:23:11 - INFO - easyeditor.editors.editor -   63 editing: What war did Alec Rose participate in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.3467469552435825}}, 'case_id': 63, 'requested_rewrite': {'prompt': 'What war did Alec Rose participate in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During which years did the war in which Alec Rose participate take place?'], 'ground_truth': ['1936-1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The award received of Alec Rose is', 'Alec Rose award received'], 'ground_truth': ['Knight Bachelor', 'Knight Bachelor']}}, 'subject': 'Alec Rose'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.856324046475643}}}\n",
      " 20%|█▉        | 64/326 [26:23<1:48:23, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The star HD 85622 is a part of the constellation named what?] -> [Carina]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.9352452754974365\n",
      "Total loss 3.9352452754974365\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.0008985963650047779\n",
      "Total loss 0.0008985963650047779\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0901871919631958\n",
      "Total loss 1.0901871919631958\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.8587136268615723\n",
      "Total loss 3.8587136268615723\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.569433212280273\n",
      "Total loss 11.569433212280273\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.65625\n",
      "Total loss 12.65625\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.093839645385742\n",
      "Total loss 9.093839645385742\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.063255310058594\n",
      "Total loss 5.063255310058594\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 54.14287567138672\n",
      "Total loss 54.14287567138672\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 16.183853149414062\n",
      "Total loss 16.183853149414062\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.25004768371582\n",
      "Total loss 10.25004768371582\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.813228607177734\n",
      "Total loss 5.813228607177734\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.3396141529083252\n",
      "Total loss 1.3396141529083252\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.6571738719940186\n",
      "Total loss 2.6571738719940186\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.7546508312225342\n",
      "Total loss 1.7546508312225342\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.9079986214637756\n",
      "Total loss 0.9079986214637756\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.8527439832687378\n",
      "Total loss 0.8527439832687378\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6882217526435852\n",
      "Total loss 0.6882217526435852\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8732060790061951\n",
      "Total loss 0.8732060790061951\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.650705099105835\n",
      "Total loss 0.650705099105835\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.6910021305084229\n",
      "Total loss 0.6910021305084229\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6302877068519592\n",
      "Total loss 0.6302877068519592\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1747467517852783\n",
      "Total loss 1.1747467517852783\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6239371299743652\n",
      "Total loss 0.6239371299743652\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.610010027885437\n",
      "Total loss 0.610010027885437\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.5247225761413574\n",
      "Total loss 0.5247225761413574\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.5401300191879272\n",
      "Total loss 0.5401300191879272\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.4869059920310974\n",
      "Total loss 0.4869059920310974\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.4167981743812561\n",
      "Total loss 0.4167981743812561\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.39330369234085083\n",
      "Total loss 0.39330369234085083\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.30582934617996216\n",
      "Total loss 0.30582934617996216\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.5444123148918152\n",
      "Total loss 0.5444123148918152\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.5783382654190063\n",
      "Total loss 0.5783382654190063\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.37955528497695923\n",
      "Total loss 0.37955528497695923\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.33896031975746155\n",
      "Total loss 0.33896031975746155\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.39735502004623413\n",
      "Total loss 0.39735502004623413\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.26546263694763184\n",
      "Total loss 0.26546263694763184\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.20605431497097015\n",
      "Total loss 0.20605431497097015\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.1362808495759964\n",
      "Total loss 0.1362808495759964\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5200130343437195\n",
      "Total loss 0.5200130343437195\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.20193034410476685\n",
      "Total loss 0.20193034410476685\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.12270453572273254\n",
      "Total loss 0.12270453572273254\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.111111581325531\n",
      "Total loss 0.111111581325531\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.005267326720058918\n",
      "Total loss 0.005267326720058918\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.6322250366210938\n",
      "Total loss 1.6322250366210938\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.07461075484752655\n",
      "Total loss 0.07461075484752655\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.18246379494667053\n",
      "Total loss 0.18246379494667053\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.13781221210956573\n",
      "Total loss 0.13781221210956573\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.13779470324516296\n",
      "Total loss 0.13779470324516296\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.10423285514116287\n",
      "Total loss 0.10423285514116287\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.06034553423523903\n",
      "Total loss 0.06034553423523903\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04430365189909935\n",
      "Total loss 0.04430365189909935\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.07955534756183624\n",
      "Total loss 0.07955534756183624\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0335361547768116\n",
      "Total loss 0.0335361547768116\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.010974293574690819\n",
      "Total loss 0.010974293574690819\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0032294634729623795\n",
      "Total loss 0.0032294634729623795\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.002382816979661584\n",
      "Total loss 0.002382816979661584\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.011923900805413723\n",
      "Total loss 0.011923900805413723\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.02458682283759117\n",
      "Total loss 0.02458682283759117\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 55.000465393066406\n",
      "Total loss 55.000465393066406\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 9.909880638122559\n",
      "Total loss 9.909880638122559\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.014333024621009827\n",
      "Total loss 0.014333024621009827\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.18076570332050323\n",
      "Total loss 0.18076570332050323\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.2121802568435669\n",
      "Total loss 0.2121802568435669\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.13144129514694214\n",
      "Total loss 0.13144129514694214\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.22978360950946808\n",
      "Total loss 0.22978360950946808\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.3561875522136688\n",
      "Total loss 0.3561875522136688\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.23292994499206543\n",
      "Total loss 0.23292994499206543\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.1770995557308197\n",
      "Total loss 0.1770995557308197\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.39464038610458374\n",
      "Total loss 0.39464038610458374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:23:36,188 - easyeditor.editors.editor - INFO - 64 editing: The star HD 85622 is a part of the constellation named what? -> Carina  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.098722436392093}}, 'case_id': 64, 'requested_rewrite': {'prompt': 'The star HD 85622 is a part of the constellation named what?', 'target_new': 'Carina', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the name of the constellation that includes the star m Vel?'], 'ground_truth': ['Carina']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of HD 85622 is', 'HD 85622 epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': 'HD 85622'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      "07/22/2024 12:23:36 - INFO - easyeditor.editors.editor -   64 editing: The star HD 85622 is a part of the constellation named what? -> Carina  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.098722436392093}}, 'case_id': 64, 'requested_rewrite': {'prompt': 'The star HD 85622 is a part of the constellation named what?', 'target_new': 'Carina', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the name of the constellation that includes the star m Vel?'], 'ground_truth': ['Carina']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of HD 85622 is', 'HD 85622 epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': 'HD 85622'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      " 20%|█▉        | 65/326 [26:48<1:48:02, 24.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The Holmenkollen Chapel project's architect was who?] -> [Inigo Jones]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.288480758666992\n",
      "Total loss 4.288480758666992\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3517146110534668\n",
      "Total loss 1.3517146110534668\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.718287706375122\n",
      "Total loss 2.718287706375122\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 17.66472053527832\n",
      "Total loss 17.66472053527832\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.248425006866455\n",
      "Total loss 4.248425006866455\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.106029033660889\n",
      "Total loss 6.106029033660889\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.679425239562988\n",
      "Total loss 12.679425239562988\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.315339088439941\n",
      "Total loss 9.315339088439941\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.894979476928711\n",
      "Total loss 4.894979476928711\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.29737377166748\n",
      "Total loss 8.29737377166748\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 13.000205039978027\n",
      "Total loss 13.000205039978027\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.395041465759277\n",
      "Total loss 9.395041465759277\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 10.083847999572754\n",
      "Total loss 10.083847999572754\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 7.401916027069092\n",
      "Total loss 7.401916027069092\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.09136962890625\n",
      "Total loss 3.09136962890625\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.341404438018799\n",
      "Total loss 4.341404438018799\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.338759422302246\n",
      "Total loss 4.338759422302246\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.758196473121643\n",
      "Total loss 1.758196473121643\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.211041212081909\n",
      "Total loss 2.211041212081909\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.7216014862060547\n",
      "Total loss 2.7216014862060547\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.566199541091919\n",
      "Total loss 2.566199541091919\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.058018922805786\n",
      "Total loss 2.058018922805786\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.6829112768173218\n",
      "Total loss 1.6829112768173218\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4705533981323242\n",
      "Total loss 1.4705533981323242\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.5434476137161255\n",
      "Total loss 1.5434476137161255\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.742112159729004\n",
      "Total loss 1.742112159729004\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7904611825942993\n",
      "Total loss 1.7904611825942993\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.5734429359436035\n",
      "Total loss 1.5734429359436035\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.2775295972824097\n",
      "Total loss 1.2775295972824097\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.2276897430419922\n",
      "Total loss 1.2276897430419922\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2601592540740967\n",
      "Total loss 1.2601592540740967\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.393628716468811\n",
      "Total loss 1.393628716468811\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4334173202514648\n",
      "Total loss 1.4334173202514648\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3040943145751953\n",
      "Total loss 1.3040943145751953\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1412473917007446\n",
      "Total loss 1.1412473917007446\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1480568647384644\n",
      "Total loss 1.1480568647384644\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.2489508390426636\n",
      "Total loss 1.2489508390426636\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.2484575510025024\n",
      "Total loss 1.2484575510025024\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1386882066726685\n",
      "Total loss 1.1386882066726685\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1000944375991821\n",
      "Total loss 1.1000944375991821\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1783844232559204\n",
      "Total loss 1.1783844232559204\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2125946283340454\n",
      "Total loss 1.2125946283340454\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.130030870437622\n",
      "Total loss 1.130030870437622\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1189439296722412\n",
      "Total loss 1.1189439296722412\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.108971357345581\n",
      "Total loss 1.108971357345581\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1132827997207642\n",
      "Total loss 1.1132827997207642\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1020656824111938\n",
      "Total loss 1.1020656824111938\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.171035647392273\n",
      "Total loss 1.171035647392273\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0893584489822388\n",
      "Total loss 1.0893584489822388\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0268645286560059\n",
      "Total loss 1.0268645286560059\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0707379579544067\n",
      "Total loss 1.0707379579544067\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.1111966371536255\n",
      "Total loss 1.1111966371536255\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1020151376724243\n",
      "Total loss 1.1020151376724243\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0652971267700195\n",
      "Total loss 1.0652971267700195\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0809684991836548\n",
      "Total loss 1.0809684991836548\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0789037942886353\n",
      "Total loss 1.0789037942886353\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.080320119857788\n",
      "Total loss 1.080320119857788\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.056958556175232\n",
      "Total loss 1.056958556175232\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.0289992094039917\n",
      "Total loss 1.0289992094039917\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.0696544647216797\n",
      "Total loss 1.0696544647216797\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0249892473220825\n",
      "Total loss 1.0249892473220825\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.014446496963501\n",
      "Total loss 1.014446496963501\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0016063451766968\n",
      "Total loss 1.0016063451766968\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.020140290260315\n",
      "Total loss 1.020140290260315\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.03369140625\n",
      "Total loss 1.03369140625\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.025426983833313\n",
      "Total loss 1.025426983833313\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.9703531265258789\n",
      "Total loss 0.9703531265258789\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.9974815249443054\n",
      "Total loss 0.9974815249443054\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.9878122210502625\n",
      "Total loss 0.9878122210502625\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.9877385497093201\n",
      "Total loss 0.9877385497093201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:24:01,825 - easyeditor.editors.editor - INFO - 65 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.719445955462705}}, 'case_id': 65, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What architectural style is the Holmenkollen Chapel built in, under Inigo Jones' design?\"], 'ground_truth': ['Neo-classical architecture']}}, 'locality': {'Relation_Specificity': {'prompt': ['The diocese of Holmenkollen Chapel is', 'Holmenkollen Chapel diocese'], 'ground_truth': ['Diocese of Oslo', 'Diocese of Oslo']}}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.219628930032114}}}\n",
      "07/22/2024 12:24:01 - INFO - easyeditor.editors.editor -   65 editing: The Holmenkollen Chapel project's architect was who? -> Inigo Jones  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.719445955462705}}, 'case_id': 65, 'requested_rewrite': {'prompt': \"The Holmenkollen Chapel project's architect was who?\", 'target_new': 'Inigo Jones', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What architectural style is the Holmenkollen Chapel built in, under Inigo Jones' design?\"], 'ground_truth': ['Neo-classical architecture']}}, 'locality': {'Relation_Specificity': {'prompt': ['The diocese of Holmenkollen Chapel is', 'Holmenkollen Chapel diocese'], 'ground_truth': ['Diocese of Oslo', 'Diocese of Oslo']}}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.219628930032114}}}\n",
      " 20%|██        | 66/326 [27:14<1:48:40, 25.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [When was Old Quebec Street Mall launched?] -> [2002]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.3922317028045654\n",
      "Total loss 3.3922317028045654\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.07160222530365\n",
      "Total loss 1.07160222530365\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.12159059196710587\n",
      "Total loss 0.12159059196710587\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.08438777923584\n",
      "Total loss 9.08438777923584\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 13.778786659240723\n",
      "Total loss 13.778786659240723\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.939346790313721\n",
      "Total loss 6.939346790313721\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.263469696044922\n",
      "Total loss 11.263469696044922\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.343639373779297\n",
      "Total loss 11.343639373779297\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.734978675842285\n",
      "Total loss 9.734978675842285\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.875802993774414\n",
      "Total loss 7.875802993774414\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.300989627838135\n",
      "Total loss 4.300989627838135\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.638152599334717\n",
      "Total loss 4.638152599334717\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.843352794647217\n",
      "Total loss 4.843352794647217\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.320867538452148\n",
      "Total loss 4.320867538452148\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.050367832183838\n",
      "Total loss 4.050367832183838\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.977506637573242\n",
      "Total loss 2.977506637573242\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5338029861450195\n",
      "Total loss 1.5338029861450195\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.8237600326538086\n",
      "Total loss 3.8237600326538086\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.407142996788025\n",
      "Total loss 1.407142996788025\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.3768181800842285\n",
      "Total loss 1.3768181800842285\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 13.492603302001953\n",
      "Total loss 13.492603302001953\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.9939684867858887\n",
      "Total loss 1.9939684867858887\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 24.041200637817383\n",
      "Total loss 24.041200637817383\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.2373855113983154\n",
      "Total loss 2.2373855113983154\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6900032758712769\n",
      "Total loss 1.6900032758712769\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.5553035736083984\n",
      "Total loss 1.5553035736083984\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.243552565574646\n",
      "Total loss 1.243552565574646\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1581839323043823\n",
      "Total loss 1.1581839323043823\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1271933317184448\n",
      "Total loss 1.1271933317184448\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1442742347717285\n",
      "Total loss 1.1442742347717285\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0245863199234009\n",
      "Total loss 1.0245863199234009\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9843807220458984\n",
      "Total loss 0.9843807220458984\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.8835466504096985\n",
      "Total loss 0.8835466504096985\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.98492032289505\n",
      "Total loss 0.98492032289505\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.2699339389801025\n",
      "Total loss 2.2699339389801025\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.2999143600463867\n",
      "Total loss 2.2999143600463867\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.584524154663086\n",
      "Total loss 2.584524154663086\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.3135416507720947\n",
      "Total loss 2.3135416507720947\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.4356218576431274\n",
      "Total loss 1.4356218576431274\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.9114386439323425\n",
      "Total loss 0.9114386439323425\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3078848123550415\n",
      "Total loss 1.3078848123550415\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4676896333694458\n",
      "Total loss 1.4676896333694458\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.289265513420105\n",
      "Total loss 1.289265513420105\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1069271564483643\n",
      "Total loss 1.1069271564483643\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0202102661132812\n",
      "Total loss 1.0202102661132812\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.2100297212600708\n",
      "Total loss 1.2100297212600708\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1592782735824585\n",
      "Total loss 1.1592782735824585\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9365245699882507\n",
      "Total loss 0.9365245699882507\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 3.565831422805786\n",
      "Total loss 3.565831422805786\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7861063480377197\n",
      "Total loss 0.7861063480377197\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7923535704612732\n",
      "Total loss 0.7923535704612732\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.7717909216880798\n",
      "Total loss 0.7717909216880798\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7574818730354309\n",
      "Total loss 0.7574818730354309\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.7162706255912781\n",
      "Total loss 0.7162706255912781\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.766726016998291\n",
      "Total loss 0.766726016998291\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.7113754153251648\n",
      "Total loss 0.7113754153251648\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.6574468016624451\n",
      "Total loss 0.6574468016624451\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6314554810523987\n",
      "Total loss 0.6314554810523987\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6193439364433289\n",
      "Total loss 0.6193439364433289\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.547609806060791\n",
      "Total loss 0.547609806060791\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5694641470909119\n",
      "Total loss 0.5694641470909119\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5642521977424622\n",
      "Total loss 0.5642521977424622\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5193586945533752\n",
      "Total loss 0.5193586945533752\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 32.108375549316406\n",
      "Total loss 32.108375549316406\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 22.61334800720215\n",
      "Total loss 22.61334800720215\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.5525041222572327\n",
      "Total loss 0.5525041222572327\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.6499567627906799\n",
      "Total loss 0.6499567627906799\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.3517712354660034\n",
      "Total loss 1.3517712354660034\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.021547794342041\n",
      "Total loss 1.021547794342041\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.3798414468765259\n",
      "Total loss 1.3798414468765259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:24:27,038 - easyeditor.editors.editor - INFO - 66 editing: When was Old Quebec Street Mall launched? -> 2002  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.021735264807702}}, 'case_id': 66, 'requested_rewrite': {'prompt': 'When was Old Quebec Street Mall launched?', 'target_new': '2002', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major sporting event took place the same year Old Quebec Street Mall was launched?'], 'ground_truth': ['Salt Lake City Winter Olympics']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in the administrative territorial entity of Old Quebec Street Mall is', 'Old Quebec Street Mall located in the administrative territorial entity'], 'ground_truth': ['Guelph', 'Guelph']}}, 'subject': 'Old Quebec Street Mall'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.7723766421281795}}}\n",
      "07/22/2024 12:24:27 - INFO - easyeditor.editors.editor -   66 editing: When was Old Quebec Street Mall launched? -> 2002  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.021735264807702}}, 'case_id': 66, 'requested_rewrite': {'prompt': 'When was Old Quebec Street Mall launched?', 'target_new': '2002', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major sporting event took place the same year Old Quebec Street Mall was launched?'], 'ground_truth': ['Salt Lake City Winter Olympics']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in the administrative territorial entity of Old Quebec Street Mall is', 'Old Quebec Street Mall located in the administrative territorial entity'], 'ground_truth': ['Guelph', 'Guelph']}}, 'subject': 'Old Quebec Street Mall'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.7723766421281795}}}\n",
      " 21%|██        | 67/326 [27:39<1:48:27, 25.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What programming language was used to write GNOME Chess?] -> [Python]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.113466262817383\n",
      "Total loss 9.113466262817383\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5553483963012695\n",
      "Total loss 0.5553483963012695\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0021614301949739456\n",
      "Total loss 0.0021614301949739456\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0065695736557245255\n",
      "Total loss 0.0065695736557245255\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0003601856005843729\n",
      "Total loss 0.0003601856005843729\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0003457663697190583\n",
      "Total loss 0.0003457663697190583\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0002675890573300421\n",
      "Total loss 0.0002675890573300421\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.00020823694649152458\n",
      "Total loss 0.00020823694649152458\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.00010227633902104571\n",
      "Total loss 0.00010227633902104571\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.00010406429646536708\n",
      "Total loss 0.00010406429646536708\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.272782724816352e-05\n",
      "Total loss 8.272782724816352e-05\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.842378934379667e-05\n",
      "Total loss 6.842378934379667e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 65.65080261230469\n",
      "Total loss 65.65080261230469\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0032756265718489885\n",
      "Total loss 0.0032756265718489885\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.022264402359724045\n",
      "Total loss 0.022264402359724045\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.04468795657157898\n",
      "Total loss 0.04468795657157898\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0013240152038633823\n",
      "Total loss 0.0013240152038633823\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0006504327175207436\n",
      "Total loss 0.0006504327175207436\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.0005635818815790117\n",
      "Total loss 0.0005635818815790117\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0004698126285802573\n",
      "Total loss 0.0004698126285802573\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 5.691379547119141\n",
      "Total loss 5.691379547119141\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.047310035675764084\n",
      "Total loss 0.047310035675764084\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.006883005611598492\n",
      "Total loss 0.006883005611598492\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0022357723210006952\n",
      "Total loss 0.0022357723210006952\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0011151769431307912\n",
      "Total loss 0.0011151769431307912\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0005952732171863317\n",
      "Total loss 0.0005952732171863317\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0003181189822498709\n",
      "Total loss 0.0003181189822498709\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0002494739310350269\n",
      "Total loss 0.0002494739310350269\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.00015114595589693636\n",
      "Total loss 0.00015114595589693636\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 59.06459045410156\n",
      "Total loss 59.06459045410156\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.004173854365944862\n",
      "Total loss 0.004173854365944862\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.024353744462132454\n",
      "Total loss 0.024353744462132454\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.08282901346683502\n",
      "Total loss 0.08282901346683502\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.4071950912475586\n",
      "Total loss 0.4071950912475586\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8934160470962524\n",
      "Total loss 0.8934160470962524\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.8062724471092224\n",
      "Total loss 0.8062724471092224\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4618605375289917\n",
      "Total loss 0.4618605375289917\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.22128483653068542\n",
      "Total loss 0.22128483653068542\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.1071031242609024\n",
      "Total loss 0.1071031242609024\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.06192593276500702\n",
      "Total loss 0.06192593276500702\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.03884899616241455\n",
      "Total loss 0.03884899616241455\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.030182981863617897\n",
      "Total loss 0.030182981863617897\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.021644098684191704\n",
      "Total loss 0.021644098684191704\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.016871260479092598\n",
      "Total loss 0.016871260479092598\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.014199530705809593\n",
      "Total loss 0.014199530705809593\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.011360952630639076\n",
      "Total loss 0.011360952630639076\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.010118262842297554\n",
      "Total loss 0.010118262842297554\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.008866936899721622\n",
      "Total loss 0.008866936899721622\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.007323561701923609\n",
      "Total loss 0.007323561701923609\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.006560336332768202\n",
      "Total loss 0.006560336332768202\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.006516990251839161\n",
      "Total loss 0.006516990251839161\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.005396086256951094\n",
      "Total loss 0.005396086256951094\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.005044945515692234\n",
      "Total loss 0.005044945515692234\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.005036879796534777\n",
      "Total loss 0.005036879796534777\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.004736513365060091\n",
      "Total loss 0.004736513365060091\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.004258492961525917\n",
      "Total loss 0.004258492961525917\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.003604820929467678\n",
      "Total loss 0.003604820929467678\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0035181075800210238\n",
      "Total loss 0.0035181075800210238\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0038979279343038797\n",
      "Total loss 0.0038979279343038797\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0035173948854207993\n",
      "Total loss 0.0035173948854207993\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.003508366644382477\n",
      "Total loss 0.003508366644382477\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.003168208058923483\n",
      "Total loss 0.003168208058923483\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0030928654596209526\n",
      "Total loss 0.0030928654596209526\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.002788467099890113\n",
      "Total loss 0.002788467099890113\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.002795005217194557\n",
      "Total loss 0.002795005217194557\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.002727124374359846\n",
      "Total loss 0.002727124374359846\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.002522027352824807\n",
      "Total loss 0.002522027352824807\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.002400970784947276\n",
      "Total loss 0.002400970784947276\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.002221618080511689\n",
      "Total loss 0.002221618080511689\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.002517152111977339\n",
      "Total loss 0.002517152111977339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:24:51,850 - easyeditor.editors.editor - INFO - 67 editing: What programming language was used to write GNOME Chess? -> Python  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.8]}, 'fluency': {'ngram_entropy': 6.475720170238775}}, 'case_id': 67, 'requested_rewrite': {'prompt': 'What programming language was used to write GNOME Chess?', 'target_new': 'Python', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who created the programming language used to write GNOME Chess?'], 'ground_truth': ['Guido van Rossum']}}, 'locality': {'Relation_Specificity': {'prompt': ['The platform of GNOME Chess is', 'GNOME Chess platform'], 'ground_truth': ['Unix-like operating system', 'Unix-like operating system']}}, 'subject': 'GNOME Chess'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8189281838109388}}}\n",
      "07/22/2024 12:24:51 - INFO - easyeditor.editors.editor -   67 editing: What programming language was used to write GNOME Chess? -> Python  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.8]}, 'fluency': {'ngram_entropy': 6.475720170238775}}, 'case_id': 67, 'requested_rewrite': {'prompt': 'What programming language was used to write GNOME Chess?', 'target_new': 'Python', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who created the programming language used to write GNOME Chess?'], 'ground_truth': ['Guido van Rossum']}}, 'locality': {'Relation_Specificity': {'prompt': ['The platform of GNOME Chess is', 'GNOME Chess platform'], 'ground_truth': ['Unix-like operating system', 'Unix-like operating system']}}, 'subject': 'GNOME Chess'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8189281838109388}}}\n",
      " 21%|██        | 68/326 [28:04<1:47:35, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is Hannelore Kohl's spouse's name?] -> [John Kohl]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.7126224040985107\n",
      "Total loss 3.7126224040985107\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.7220053672790527\n",
      "Total loss 1.7220053672790527\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.3051563501358032\n",
      "Total loss 0.3051563501358032\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.9782512187957764\n",
      "Total loss 0.9782512187957764\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 27.821685791015625\n",
      "Total loss 27.821685791015625\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 17.18489646911621\n",
      "Total loss 17.18489646911621\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.871115684509277\n",
      "Total loss 4.871115684509277\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.34995174407959\n",
      "Total loss 11.34995174407959\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.908897399902344\n",
      "Total loss 6.908897399902344\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.128386974334717\n",
      "Total loss 5.128386974334717\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.885296583175659\n",
      "Total loss 2.885296583175659\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.9727375507354736\n",
      "Total loss 2.9727375507354736\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.2890031337738037\n",
      "Total loss 2.2890031337738037\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.8070898056030273\n",
      "Total loss 1.8070898056030273\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.3023128509521484\n",
      "Total loss 1.3023128509521484\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.74517023563385\n",
      "Total loss 1.74517023563385\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5847092866897583\n",
      "Total loss 1.5847092866897583\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.2751027345657349\n",
      "Total loss 1.2751027345657349\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.4926055669784546\n",
      "Total loss 1.4926055669784546\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.3103276491165161\n",
      "Total loss 1.3103276491165161\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.2040191888809204\n",
      "Total loss 1.2040191888809204\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1981520652770996\n",
      "Total loss 1.1981520652770996\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.212166666984558\n",
      "Total loss 1.212166666984558\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1980931758880615\n",
      "Total loss 1.1980931758880615\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.127743124961853\n",
      "Total loss 1.127743124961853\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.0061243772506714\n",
      "Total loss 1.0061243772506714\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0630302429199219\n",
      "Total loss 1.0630302429199219\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1260052919387817\n",
      "Total loss 1.1260052919387817\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0648716688156128\n",
      "Total loss 1.0648716688156128\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9471592903137207\n",
      "Total loss 0.9471592903137207\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.9172167778015137\n",
      "Total loss 0.9172167778015137\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9429798126220703\n",
      "Total loss 0.9429798126220703\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9401044249534607\n",
      "Total loss 0.9401044249534607\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8717474937438965\n",
      "Total loss 0.8717474937438965\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7806971073150635\n",
      "Total loss 0.7806971073150635\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7675778269767761\n",
      "Total loss 0.7675778269767761\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7741231322288513\n",
      "Total loss 0.7741231322288513\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7435526251792908\n",
      "Total loss 0.7435526251792908\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7703936696052551\n",
      "Total loss 0.7703936696052551\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6271465420722961\n",
      "Total loss 0.6271465420722961\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.5705578923225403\n",
      "Total loss 0.5705578923225403\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.5315470099449158\n",
      "Total loss 0.5315470099449158\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.48463162779808044\n",
      "Total loss 0.48463162779808044\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.39827558398246765\n",
      "Total loss 0.39827558398246765\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.3716924488544464\n",
      "Total loss 0.3716924488544464\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.343706339597702\n",
      "Total loss 0.343706339597702\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.2666254937648773\n",
      "Total loss 0.2666254937648773\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7463595867156982\n",
      "Total loss 0.7463595867156982\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 14.602723121643066\n",
      "Total loss 14.602723121643066\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.267914891242981\n",
      "Total loss 1.267914891242981\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.2877162992954254\n",
      "Total loss 0.2877162992954254\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.4196743071079254\n",
      "Total loss 0.4196743071079254\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5342331528663635\n",
      "Total loss 0.5342331528663635\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 6.870082378387451\n",
      "Total loss 6.870082378387451\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.31706252694129944\n",
      "Total loss 0.31706252694129944\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.3021375834941864\n",
      "Total loss 0.3021375834941864\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.23669852316379547\n",
      "Total loss 0.23669852316379547\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.14826969802379608\n",
      "Total loss 0.14826969802379608\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.15857486426830292\n",
      "Total loss 0.15857486426830292\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.13080857694149017\n",
      "Total loss 0.13080857694149017\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.1310967653989792\n",
      "Total loss 0.1310967653989792\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.14327490329742432\n",
      "Total loss 0.14327490329742432\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 3.1257925033569336\n",
      "Total loss 3.1257925033569336\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 10.764046669006348\n",
      "Total loss 10.764046669006348\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.28553307056427\n",
      "Total loss 1.28553307056427\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.296427607536316\n",
      "Total loss 1.296427607536316\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.5781751275062561\n",
      "Total loss 0.5781751275062561\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.9153025150299072\n",
      "Total loss 0.9153025150299072\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.3258808851242065\n",
      "Total loss 1.3258808851242065\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.4065065383911133\n",
      "Total loss 1.4065065383911133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:25:16,230 - easyeditor.editors.editor - INFO - 68 editing: What is Hannelore Kohl's spouse's name? -> John Kohl  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.6]}, 'fluency': {'ngram_entropy': 5.948703328608722}}, 'case_id': 68, 'requested_rewrite': {'prompt': \"What is Hannelore Kohl's spouse's name?\", 'target_new': 'John Kohl', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the spouse of John Kohl?'], 'ground_truth': ['Hannelore Kohl']}}, 'locality': {'Relation_Specificity': {'prompt': ['The residence of Hannelore Kohl is', 'Hannelore Kohl residence'], 'ground_truth': ['Oggersheim', 'Oggersheim']}}, 'subject': 'Hannelore Kohl'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 2.470249995853962}}}\n",
      "07/22/2024 12:25:16 - INFO - easyeditor.editors.editor -   68 editing: What is Hannelore Kohl's spouse's name? -> John Kohl  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.6]}, 'fluency': {'ngram_entropy': 5.948703328608722}}, 'case_id': 68, 'requested_rewrite': {'prompt': \"What is Hannelore Kohl's spouse's name?\", 'target_new': 'John Kohl', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the spouse of John Kohl?'], 'ground_truth': ['Hannelore Kohl']}}, 'locality': {'Relation_Specificity': {'prompt': ['The residence of Hannelore Kohl is', 'Hannelore Kohl residence'], 'ground_truth': ['Oggersheim', 'Oggersheim']}}, 'subject': 'Hannelore Kohl'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 2.470249995853962}}}\n",
      " 21%|██        | 69/326 [28:28<1:46:21, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the mother of Kishar?] -> [Bhutan]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.948359489440918\n",
      "Total loss 8.948359489440918\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.6938371658325195\n",
      "Total loss 2.6938371658325195\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.00298477616161108\n",
      "Total loss 0.00298477616161108\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.4870389401912689\n",
      "Total loss 0.4870389401912689\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 28.500076293945312\n",
      "Total loss 28.500076293945312\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 14.31987190246582\n",
      "Total loss 14.31987190246582\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.687598705291748\n",
      "Total loss 4.687598705291748\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.3150289058685303\n",
      "Total loss 3.3150289058685303\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.7894301414489746\n",
      "Total loss 0.7894301414489746\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.691962957382202\n",
      "Total loss 2.691962957382202\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.5444049835205078\n",
      "Total loss 1.5444049835205078\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.1337840557098389\n",
      "Total loss 1.1337840557098389\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.418823480606079\n",
      "Total loss 1.418823480606079\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.8178278207778931\n",
      "Total loss 0.8178278207778931\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.0000267028808594\n",
      "Total loss 1.0000267028808594\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.0141698122024536\n",
      "Total loss 1.0141698122024536\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.6973602771759033\n",
      "Total loss 0.6973602771759033\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.806472897529602\n",
      "Total loss 0.806472897529602\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.4305040836334229\n",
      "Total loss 1.4305040836334229\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.8017584681510925\n",
      "Total loss 0.8017584681510925\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.6372702121734619\n",
      "Total loss 0.6372702121734619\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7871185541152954\n",
      "Total loss 0.7871185541152954\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7435861229896545\n",
      "Total loss 0.7435861229896545\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6052196025848389\n",
      "Total loss 0.6052196025848389\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.7020461559295654\n",
      "Total loss 0.7020461559295654\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.6642000675201416\n",
      "Total loss 0.6642000675201416\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6049448251724243\n",
      "Total loss 0.6049448251724243\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6639102697372437\n",
      "Total loss 0.6639102697372437\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.616323709487915\n",
      "Total loss 0.616323709487915\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6047561168670654\n",
      "Total loss 0.6047561168670654\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.5936955213546753\n",
      "Total loss 0.5936955213546753\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.5614451766014099\n",
      "Total loss 0.5614451766014099\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.5254641771316528\n",
      "Total loss 0.5254641771316528\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.5101030468940735\n",
      "Total loss 0.5101030468940735\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.4553588628768921\n",
      "Total loss 0.4553588628768921\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.41166144609451294\n",
      "Total loss 0.41166144609451294\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4289248585700989\n",
      "Total loss 0.4289248585700989\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.41157448291778564\n",
      "Total loss 0.41157448291778564\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.3552180528640747\n",
      "Total loss 0.3552180528640747\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.2974890172481537\n",
      "Total loss 0.2974890172481537\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.3625718355178833\n",
      "Total loss 0.3625718355178833\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.23699811100959778\n",
      "Total loss 0.23699811100959778\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.20428797602653503\n",
      "Total loss 0.20428797602653503\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.1438220888376236\n",
      "Total loss 0.1438220888376236\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.1067470908164978\n",
      "Total loss 0.1067470908164978\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0840616300702095\n",
      "Total loss 0.0840616300702095\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.055453501641750336\n",
      "Total loss 0.055453501641750336\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.04067757725715637\n",
      "Total loss 0.04067757725715637\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.02665020525455475\n",
      "Total loss 0.02665020525455475\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.01629224419593811\n",
      "Total loss 0.01629224419593811\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0127588314935565\n",
      "Total loss 0.0127588314935565\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.007309256587177515\n",
      "Total loss 0.007309256587177515\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0062628937885165215\n",
      "Total loss 0.0062628937885165215\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.004048854112625122\n",
      "Total loss 0.004048854112625122\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.002975303679704666\n",
      "Total loss 0.002975303679704666\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.003928352613002062\n",
      "Total loss 0.003928352613002062\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0019705318845808506\n",
      "Total loss 0.0019705318845808506\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0018428388284519315\n",
      "Total loss 0.0018428388284519315\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.005764698144048452\n",
      "Total loss 0.005764698144048452\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0010267384350299835\n",
      "Total loss 0.0010267384350299835\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0010275122476741672\n",
      "Total loss 0.0010275122476741672\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0009066727943718433\n",
      "Total loss 0.0009066727943718433\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0007484229863621294\n",
      "Total loss 0.0007484229863621294\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0007911716820672154\n",
      "Total loss 0.0007911716820672154\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0008887526928447187\n",
      "Total loss 0.0008887526928447187\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0006701797829009593\n",
      "Total loss 0.0006701797829009593\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0008116703247651458\n",
      "Total loss 0.0008116703247651458\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0006217401241883636\n",
      "Total loss 0.0006217401241883636\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0005550478817895055\n",
      "Total loss 0.0005550478817895055\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0005554052768275142\n",
      "Total loss 0.0005554052768275142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:25:41,122 - easyeditor.editors.editor - INFO - 69 editing: Who was the mother of Kishar? -> Bhutan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.98443290564805}}, 'case_id': 69, 'requested_rewrite': {'prompt': 'Who was the mother of Kishar?', 'target_new': 'Bhutan', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Bhutan, mother of Kishar?'], 'ground_truth': ['Kishar']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Kishar is', 'Kishar father'], 'ground_truth': ['Lahmu', 'Lahmu']}}, 'subject': 'Kishar'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.195925838776069}}}\n",
      "07/22/2024 12:25:41 - INFO - easyeditor.editors.editor -   69 editing: Who was the mother of Kishar? -> Bhutan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.98443290564805}}, 'case_id': 69, 'requested_rewrite': {'prompt': 'Who was the mother of Kishar?', 'target_new': 'Bhutan', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Bhutan, mother of Kishar?'], 'ground_truth': ['Kishar']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Kishar is', 'Kishar father'], 'ground_truth': ['Lahmu', 'Lahmu']}}, 'subject': 'Kishar'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.195925838776069}}}\n",
      " 21%|██▏       | 70/326 [28:53<1:46:01, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which was the family of Rhinocoryne?] -> [Noctuidae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.5805575847625732\n",
      "Total loss 3.5805575847625732\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.4809263944625854\n",
      "Total loss 1.4809263944625854\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 7.0323805809021\n",
      "Total loss 7.0323805809021\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 13.295330047607422\n",
      "Total loss 13.295330047607422\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.7669677734375\n",
      "Total loss 8.7669677734375\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.2572431564331055\n",
      "Total loss 3.2572431564331055\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 18.64472198486328\n",
      "Total loss 18.64472198486328\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 15.39383602142334\n",
      "Total loss 15.39383602142334\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.8743720054626465\n",
      "Total loss 3.8743720054626465\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.1804656982421875\n",
      "Total loss 4.1804656982421875\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.2893877029418945\n",
      "Total loss 4.2893877029418945\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.033901691436768\n",
      "Total loss 5.033901691436768\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.534001588821411\n",
      "Total loss 3.534001588821411\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 24.18474578857422\n",
      "Total loss 24.18474578857422\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.4528613090515137\n",
      "Total loss 3.4528613090515137\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.4684689044952393\n",
      "Total loss 2.4684689044952393\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 19.623332977294922\n",
      "Total loss 19.623332977294922\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 20.739219665527344\n",
      "Total loss 20.739219665527344\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 19.729469299316406\n",
      "Total loss 19.729469299316406\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 18.27207374572754\n",
      "Total loss 18.27207374572754\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 16.462793350219727\n",
      "Total loss 16.462793350219727\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 14.138038635253906\n",
      "Total loss 14.138038635253906\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 10.381704330444336\n",
      "Total loss 10.381704330444336\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 10.727779388427734\n",
      "Total loss 10.727779388427734\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 10.732100486755371\n",
      "Total loss 10.732100486755371\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 10.280638694763184\n",
      "Total loss 10.280638694763184\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 9.547325134277344\n",
      "Total loss 9.547325134277344\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 8.332157135009766\n",
      "Total loss 8.332157135009766\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 6.516025543212891\n",
      "Total loss 6.516025543212891\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 3.7861790657043457\n",
      "Total loss 3.7861790657043457\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.011899709701538\n",
      "Total loss 2.011899709701538\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.7583775520324707\n",
      "Total loss 2.7583775520324707\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.4513742923736572\n",
      "Total loss 2.4513742923736572\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.2490081787109375\n",
      "Total loss 2.2490081787109375\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.360178232192993\n",
      "Total loss 2.360178232192993\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.3297929763793945\n",
      "Total loss 2.3297929763793945\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.9248366355895996\n",
      "Total loss 1.9248366355895996\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.5030155181884766\n",
      "Total loss 1.5030155181884766\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.4771721363067627\n",
      "Total loss 1.4771721363067627\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.5727033615112305\n",
      "Total loss 1.5727033615112305\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.6694464683532715\n",
      "Total loss 1.6694464683532715\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.538068175315857\n",
      "Total loss 1.538068175315857\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4152922630310059\n",
      "Total loss 1.4152922630310059\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3555642366409302\n",
      "Total loss 1.3555642366409302\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.3659420013427734\n",
      "Total loss 1.3659420013427734\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.4346472024917603\n",
      "Total loss 1.4346472024917603\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.4124764204025269\n",
      "Total loss 1.4124764204025269\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.321856141090393\n",
      "Total loss 1.321856141090393\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.340011477470398\n",
      "Total loss 1.340011477470398\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.3342094421386719\n",
      "Total loss 1.3342094421386719\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.306168556213379\n",
      "Total loss 1.306168556213379\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.3166413307189941\n",
      "Total loss 1.3166413307189941\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.282336711883545\n",
      "Total loss 1.282336711883545\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1967463493347168\n",
      "Total loss 1.1967463493347168\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.2649282217025757\n",
      "Total loss 1.2649282217025757\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.2478095293045044\n",
      "Total loss 1.2478095293045044\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.2432451248168945\n",
      "Total loss 1.2432451248168945\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.1949284076690674\n",
      "Total loss 1.1949284076690674\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1421902179718018\n",
      "Total loss 1.1421902179718018\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.137186050415039\n",
      "Total loss 1.137186050415039\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1249310970306396\n",
      "Total loss 1.1249310970306396\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0934803485870361\n",
      "Total loss 1.0934803485870361\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.029354453086853\n",
      "Total loss 1.029354453086853\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.9973998069763184\n",
      "Total loss 0.9973998069763184\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.9833899140357971\n",
      "Total loss 0.9833899140357971\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.93951416015625\n",
      "Total loss 0.93951416015625\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.8897213935852051\n",
      "Total loss 0.8897213935852051\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.8277262449264526\n",
      "Total loss 0.8277262449264526\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.8151425719261169\n",
      "Total loss 0.8151425719261169\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.7597118020057678\n",
      "Total loss 0.7597118020057678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:26:05,478 - easyeditor.editors.editor - INFO - 70 editing: Which was the family of Rhinocoryne? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.053630591688019}}, 'case_id': 70, 'requested_rewrite': {'prompt': 'Which was the family of Rhinocoryne?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the common name for the family of Rhinocoryne moths?'], 'ground_truth': ['Owlet moths']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The topic's main category of Rhinocoryne is\", \"Rhinocoryne topic's main category\"], 'ground_truth': ['Category:Rhinocoryne', 'Category:Rhinocoryne']}}, 'subject': 'Rhinocoryne'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      "07/22/2024 12:26:05 - INFO - easyeditor.editors.editor -   70 editing: Which was the family of Rhinocoryne? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.053630591688019}}, 'case_id': 70, 'requested_rewrite': {'prompt': 'Which was the family of Rhinocoryne?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the common name for the family of Rhinocoryne moths?'], 'ground_truth': ['Owlet moths']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The topic's main category of Rhinocoryne is\", \"Rhinocoryne topic's main category\"], 'ground_truth': ['Category:Rhinocoryne', 'Category:Rhinocoryne']}}, 'subject': 'Rhinocoryne'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      " 22%|██▏       | 71/326 [29:17<1:44:58, 24.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The mother of Alexander Aris is whom?] -> [Irving Kane Pond]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.225546836853027\n",
      "Total loss 8.225546836853027\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.261526584625244\n",
      "Total loss 4.261526584625244\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.0406031608581543\n",
      "Total loss 2.0406031608581543\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.996647834777832\n",
      "Total loss 3.996647834777832\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 20.108701705932617\n",
      "Total loss 20.108701705932617\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.2710723876953125\n",
      "Total loss 6.2710723876953125\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.189970970153809\n",
      "Total loss 5.189970970153809\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.396873474121094\n",
      "Total loss 5.396873474121094\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.864132881164551\n",
      "Total loss 4.864132881164551\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.244764804840088\n",
      "Total loss 2.244764804840088\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.4452898502349854\n",
      "Total loss 2.4452898502349854\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.157599449157715\n",
      "Total loss 2.157599449157715\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 19.963226318359375\n",
      "Total loss 19.963226318359375\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.061591863632202\n",
      "Total loss 2.061591863632202\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.6523181200027466\n",
      "Total loss 1.6523181200027466\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 25.86141014099121\n",
      "Total loss 25.86141014099121\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 24.460256576538086\n",
      "Total loss 24.460256576538086\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 22.646188735961914\n",
      "Total loss 22.646188735961914\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 20.639875411987305\n",
      "Total loss 20.639875411987305\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 18.502906799316406\n",
      "Total loss 18.502906799316406\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 16.464296340942383\n",
      "Total loss 16.464296340942383\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 14.423373222351074\n",
      "Total loss 14.423373222351074\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 12.400580406188965\n",
      "Total loss 12.400580406188965\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 10.272603034973145\n",
      "Total loss 10.272603034973145\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 7.860498905181885\n",
      "Total loss 7.860498905181885\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 5.432722568511963\n",
      "Total loss 5.432722568511963\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 4.985426425933838\n",
      "Total loss 4.985426425933838\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 4.436397552490234\n",
      "Total loss 4.436397552490234\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 4.022080421447754\n",
      "Total loss 4.022080421447754\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 3.9749574661254883\n",
      "Total loss 3.9749574661254883\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 3.128394842147827\n",
      "Total loss 3.128394842147827\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6735116243362427\n",
      "Total loss 1.6735116243362427\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1805598735809326\n",
      "Total loss 1.1805598735809326\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.216334104537964\n",
      "Total loss 2.216334104537964\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.787466287612915\n",
      "Total loss 2.787466287612915\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.4509761333465576\n",
      "Total loss 2.4509761333465576\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.8556073904037476\n",
      "Total loss 1.8556073904037476\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.534175992012024\n",
      "Total loss 1.534175992012024\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.3480173349380493\n",
      "Total loss 1.3480173349380493\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2636867761611938\n",
      "Total loss 1.2636867761611938\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3511816263198853\n",
      "Total loss 1.3511816263198853\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.5453928709030151\n",
      "Total loss 1.5453928709030151\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.5631533861160278\n",
      "Total loss 1.5631533861160278\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4495784044265747\n",
      "Total loss 1.4495784044265747\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.2416329383850098\n",
      "Total loss 1.2416329383850098\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1046446561813354\n",
      "Total loss 1.1046446561813354\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.065539002418518\n",
      "Total loss 1.065539002418518\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1136220693588257\n",
      "Total loss 1.1136220693588257\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.205352783203125\n",
      "Total loss 1.205352783203125\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.2373665571212769\n",
      "Total loss 1.2373665571212769\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2667847871780396\n",
      "Total loss 1.2667847871780396\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2524572610855103\n",
      "Total loss 1.2524572610855103\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1957546472549438\n",
      "Total loss 1.1957546472549438\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.147375464439392\n",
      "Total loss 1.147375464439392\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0850400924682617\n",
      "Total loss 1.0850400924682617\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0712027549743652\n",
      "Total loss 1.0712027549743652\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0533702373504639\n",
      "Total loss 1.0533702373504639\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0789834260940552\n",
      "Total loss 1.0789834260940552\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1079436540603638\n",
      "Total loss 1.1079436540603638\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.124590516090393\n",
      "Total loss 1.124590516090393\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0998443365097046\n",
      "Total loss 1.0998443365097046\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.055267095565796\n",
      "Total loss 1.055267095565796\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.015702724456787\n",
      "Total loss 1.015702724456787\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.0356932878494263\n",
      "Total loss 1.0356932878494263\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0090776681900024\n",
      "Total loss 1.0090776681900024\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0033315420150757\n",
      "Total loss 1.0033315420150757\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0067585706710815\n",
      "Total loss 1.0067585706710815\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0318013429641724\n",
      "Total loss 1.0318013429641724\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0160483121871948\n",
      "Total loss 1.0160483121871948\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.0008116960525513\n",
      "Total loss 1.0008116960525513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:26:31,198 - easyeditor.editors.editor - INFO - 71 editing: The mother of Alexander Aris is whom? -> Irving Kane Pond  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.9528151741931445}}, 'case_id': 71, 'requested_rewrite': {'prompt': 'The mother of Alexander Aris is whom?', 'target_new': 'Irving Kane Pond', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Irving Kane Pond?'], 'ground_truth': ['Alexander Aris']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Alexander Aris is', 'Alexander Aris occupation'], 'ground_truth': ['human rights activist', 'human rights activist']}}, 'subject': 'Alexander Aris'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.297725462333097}}}\n",
      "07/22/2024 12:26:31 - INFO - easyeditor.editors.editor -   71 editing: The mother of Alexander Aris is whom? -> Irving Kane Pond  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.9528151741931445}}, 'case_id': 71, 'requested_rewrite': {'prompt': 'The mother of Alexander Aris is whom?', 'target_new': 'Irving Kane Pond', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Irving Kane Pond?'], 'ground_truth': ['Alexander Aris']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Alexander Aris is', 'Alexander Aris occupation'], 'ground_truth': ['human rights activist', 'human rights activist']}}, 'subject': 'Alexander Aris'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.297725462333097}}}\n",
      " 22%|██▏       | 72/326 [29:43<1:45:51, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the date of birth for Nermin Čeliković?] -> [8 September 1981]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.6271960735321045\n",
      "Total loss 2.6271960735321045\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.955899715423584\n",
      "Total loss 1.955899715423584\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.5303577184677124\n",
      "Total loss 1.5303577184677124\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.621774435043335\n",
      "Total loss 2.621774435043335\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 7.505284786224365\n",
      "Total loss 7.505284786224365\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.948958396911621\n",
      "Total loss 2.948958396911621\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.488117694854736\n",
      "Total loss 6.488117694854736\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.892671585083008\n",
      "Total loss 5.892671585083008\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 18.699806213378906\n",
      "Total loss 18.699806213378906\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.382198810577393\n",
      "Total loss 5.382198810577393\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 17.156265258789062\n",
      "Total loss 17.156265258789062\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 10.336935997009277\n",
      "Total loss 10.336935997009277\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.357257843017578\n",
      "Total loss 5.357257843017578\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.7192676067352295\n",
      "Total loss 3.7192676067352295\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.398848295211792\n",
      "Total loss 3.398848295211792\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.5064823627471924\n",
      "Total loss 2.5064823627471924\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.4294915199279785\n",
      "Total loss 2.4294915199279785\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.0162558555603027\n",
      "Total loss 2.0162558555603027\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.780867576599121\n",
      "Total loss 1.780867576599121\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.668899655342102\n",
      "Total loss 1.668899655342102\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.6006426811218262\n",
      "Total loss 1.6006426811218262\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.600460410118103\n",
      "Total loss 1.600460410118103\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.527775764465332\n",
      "Total loss 1.527775764465332\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.6541327238082886\n",
      "Total loss 1.6541327238082886\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.25175404548645\n",
      "Total loss 2.25175404548645\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.416566848754883\n",
      "Total loss 2.416566848754883\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.5884363651275635\n",
      "Total loss 2.5884363651275635\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 3.1065056324005127\n",
      "Total loss 3.1065056324005127\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.9074068069458008\n",
      "Total loss 1.9074068069458008\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.4819977283477783\n",
      "Total loss 2.4819977283477783\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.3330912590026855\n",
      "Total loss 2.3330912590026855\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.2825334072113037\n",
      "Total loss 2.2825334072113037\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.710978627204895\n",
      "Total loss 1.710978627204895\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.8063007593154907\n",
      "Total loss 1.8063007593154907\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.8607501983642578\n",
      "Total loss 1.8607501983642578\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5340083837509155\n",
      "Total loss 1.5340083837509155\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.6954755783081055\n",
      "Total loss 1.6954755783081055\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.8199626207351685\n",
      "Total loss 1.8199626207351685\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.6207152605056763\n",
      "Total loss 1.6207152605056763\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.580927848815918\n",
      "Total loss 1.580927848815918\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5635062456130981\n",
      "Total loss 1.5635062456130981\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4248796701431274\n",
      "Total loss 1.4248796701431274\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4326533079147339\n",
      "Total loss 1.4326533079147339\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.411940097808838\n",
      "Total loss 1.411940097808838\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.5168315172195435\n",
      "Total loss 1.5168315172195435\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3598295450210571\n",
      "Total loss 1.3598295450210571\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3575791120529175\n",
      "Total loss 1.3575791120529175\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.268140196800232\n",
      "Total loss 1.268140196800232\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.18775475025177\n",
      "Total loss 1.18775475025177\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.198362112045288\n",
      "Total loss 1.198362112045288\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2254602909088135\n",
      "Total loss 1.2254602909088135\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.1079057455062866\n",
      "Total loss 1.1079057455062866\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0254207849502563\n",
      "Total loss 1.0254207849502563\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.9577391743659973\n",
      "Total loss 0.9577391743659973\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.9005401134490967\n",
      "Total loss 0.9005401134490967\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8084504008293152\n",
      "Total loss 0.8084504008293152\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8435209393501282\n",
      "Total loss 0.8435209393501282\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7673588395118713\n",
      "Total loss 0.7673588395118713\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.7525434494018555\n",
      "Total loss 0.7525434494018555\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.701824426651001\n",
      "Total loss 0.701824426651001\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.6464611887931824\n",
      "Total loss 0.6464611887931824\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5975329279899597\n",
      "Total loss 0.5975329279899597\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5768069624900818\n",
      "Total loss 0.5768069624900818\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.543283998966217\n",
      "Total loss 0.543283998966217\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.493998646736145\n",
      "Total loss 0.493998646736145\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.4459625780582428\n",
      "Total loss 0.4459625780582428\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.4011947214603424\n",
      "Total loss 0.4011947214603424\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.36356377601623535\n",
      "Total loss 0.36356377601623535\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.3623819649219513\n",
      "Total loss 0.3623819649219513\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.3174113929271698\n",
      "Total loss 0.3174113929271698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:26:58,681 - easyeditor.editors.editor - INFO - 72 editing: What is the date of birth for Nermin Čeliković? -> 8 September 1981  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.615958701964837}}, 'case_id': 72, 'requested_rewrite': {'prompt': 'What is the date of birth for Nermin Čeliković?', 'target_new': '8 September 1981', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the zodiac sign of Nermin Čeliković based on his date of birth?'], 'ground_truth': ['Virgo']}}, 'locality': {'Relation_Specificity': {'prompt': ['The member of sports team of Nermin Čeliković is', 'Nermin Čeliković member of sports team'], 'ground_truth': ['Kickers Emden', 'Kickers Emden']}}, 'subject': 'Nermin Čeliković'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.345391204184494}}}\n",
      "07/22/2024 12:26:58 - INFO - easyeditor.editors.editor -   72 editing: What is the date of birth for Nermin Čeliković? -> 8 September 1981  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.615958701964837}}, 'case_id': 72, 'requested_rewrite': {'prompt': 'What is the date of birth for Nermin Čeliković?', 'target_new': '8 September 1981', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the zodiac sign of Nermin Čeliković based on his date of birth?'], 'ground_truth': ['Virgo']}}, 'locality': {'Relation_Specificity': {'prompt': ['The member of sports team of Nermin Čeliković is', 'Nermin Čeliković member of sports team'], 'ground_truth': ['Kickers Emden', 'Kickers Emden']}}, 'subject': 'Nermin Čeliković'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.345391204184494}}}\n",
      " 22%|██▏       | 73/326 [30:11<1:48:34, 25.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the launch date of USA-64?] -> [3 December 1992]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.9075729846954346\n",
      "Total loss 2.9075729846954346\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.1040027141571045\n",
      "Total loss 2.1040027141571045\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.7039461731910706\n",
      "Total loss 0.7039461731910706\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.628340721130371\n",
      "Total loss 8.628340721130371\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.5622756481170654\n",
      "Total loss 3.5622756481170654\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.101977825164795\n",
      "Total loss 5.101977825164795\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 16.881811141967773\n",
      "Total loss 16.881811141967773\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 36.72262954711914\n",
      "Total loss 36.72262954711914\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 8.568610191345215\n",
      "Total loss 8.568610191345215\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.314830780029297\n",
      "Total loss 8.314830780029297\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.362623691558838\n",
      "Total loss 5.362623691558838\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.677483558654785\n",
      "Total loss 8.677483558654785\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.80690860748291\n",
      "Total loss 4.80690860748291\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.250361919403076\n",
      "Total loss 4.250361919403076\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.805274486541748\n",
      "Total loss 4.805274486541748\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.848555088043213\n",
      "Total loss 4.848555088043213\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.646500587463379\n",
      "Total loss 4.646500587463379\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.237064838409424\n",
      "Total loss 4.237064838409424\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.7945668697357178\n",
      "Total loss 3.7945668697357178\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.2672719955444336\n",
      "Total loss 3.2672719955444336\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.819371461868286\n",
      "Total loss 2.819371461868286\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.7302868366241455\n",
      "Total loss 2.7302868366241455\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.0205509662628174\n",
      "Total loss 3.0205509662628174\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.9150702953338623\n",
      "Total loss 2.9150702953338623\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.4173686504364014\n",
      "Total loss 2.4173686504364014\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.5297279357910156\n",
      "Total loss 2.5297279357910156\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.3997344970703125\n",
      "Total loss 2.3997344970703125\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.0261847972869873\n",
      "Total loss 2.0261847972869873\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.8656116724014282\n",
      "Total loss 1.8656116724014282\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.7984861135482788\n",
      "Total loss 1.7984861135482788\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.6871428489685059\n",
      "Total loss 1.6871428489685059\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6429754495620728\n",
      "Total loss 1.6429754495620728\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.639947772026062\n",
      "Total loss 1.639947772026062\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.6966685056686401\n",
      "Total loss 1.6966685056686401\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.6941652297973633\n",
      "Total loss 1.6941652297973633\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.6647058725357056\n",
      "Total loss 1.6647058725357056\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.6333518028259277\n",
      "Total loss 1.6333518028259277\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6402745246887207\n",
      "Total loss 1.6402745246887207\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.6473900079727173\n",
      "Total loss 1.6473900079727173\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.6319211721420288\n",
      "Total loss 1.6319211721420288\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.568245768547058\n",
      "Total loss 1.568245768547058\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.611568808555603\n",
      "Total loss 1.611568808555603\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.60417902469635\n",
      "Total loss 1.60417902469635\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.5986732244491577\n",
      "Total loss 1.5986732244491577\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.5659805536270142\n",
      "Total loss 1.5659805536270142\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.584774374961853\n",
      "Total loss 1.584774374961853\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.610221266746521\n",
      "Total loss 1.610221266746521\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.5927218198776245\n",
      "Total loss 1.5927218198776245\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.5455974340438843\n",
      "Total loss 1.5455974340438843\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.5643306970596313\n",
      "Total loss 1.5643306970596313\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.5354290008544922\n",
      "Total loss 1.5354290008544922\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.5160541534423828\n",
      "Total loss 1.5160541534423828\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.5546603202819824\n",
      "Total loss 1.5546603202819824\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.5420995950698853\n",
      "Total loss 1.5420995950698853\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.5436620712280273\n",
      "Total loss 1.5436620712280273\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.5050095319747925\n",
      "Total loss 1.5050095319747925\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.5224026441574097\n",
      "Total loss 1.5224026441574097\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.4984407424926758\n",
      "Total loss 1.4984407424926758\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.4723762273788452\n",
      "Total loss 1.4723762273788452\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.488664984703064\n",
      "Total loss 1.488664984703064\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.4906479120254517\n",
      "Total loss 1.4906479120254517\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.480887532234192\n",
      "Total loss 1.480887532234192\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.4867037534713745\n",
      "Total loss 1.4867037534713745\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.4660214185714722\n",
      "Total loss 1.4660214185714722\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.472622036933899\n",
      "Total loss 1.472622036933899\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.481597900390625\n",
      "Total loss 1.481597900390625\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.4605907201766968\n",
      "Total loss 1.4605907201766968\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.4649949073791504\n",
      "Total loss 1.4649949073791504\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.4467793703079224\n",
      "Total loss 1.4467793703079224\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.417015552520752\n",
      "Total loss 1.417015552520752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:27:25,751 - easyeditor.editors.editor - INFO - 73 editing: What was the launch date of USA-64? -> 3 December 1992  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.7142857142857143]}, 'fluency': {'ngram_entropy': 6.049215542974461}}, 'case_id': 73, 'requested_rewrite': {'prompt': 'What was the launch date of USA-64?', 'target_new': '3 December 1992', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What significant historical event happened on the same day as the launch of USA-64?'], 'ground_truth': ['The breakup of Czechoslovakia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The start point of USA-64 is', 'USA-64 start point'], 'ground_truth': ['Cape Canaveral Space Launch Complex 17', 'Cape Canaveral Space Launch Complex 17']}}, 'subject': 'USA-64'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.1111111111111111, 0.1111111111111111]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.975105477385879}}}\n",
      "07/22/2024 12:27:25 - INFO - easyeditor.editors.editor -   73 editing: What was the launch date of USA-64? -> 3 December 1992  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.7142857142857143]}, 'fluency': {'ngram_entropy': 6.049215542974461}}, 'case_id': 73, 'requested_rewrite': {'prompt': 'What was the launch date of USA-64?', 'target_new': '3 December 1992', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What significant historical event happened on the same day as the launch of USA-64?'], 'ground_truth': ['The breakup of Czechoslovakia']}}, 'locality': {'Relation_Specificity': {'prompt': ['The start point of USA-64 is', 'USA-64 start point'], 'ground_truth': ['Cape Canaveral Space Launch Complex 17', 'Cape Canaveral Space Launch Complex 17']}}, 'subject': 'USA-64'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.1111111111111111, 0.1111111111111111]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.975105477385879}}}\n",
      " 23%|██▎       | 74/326 [30:38<1:49:48, 26.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what war did Herbert T. Levack fight in?] -> [American Civil War]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.9933605194091797\n",
      "Total loss 2.9933605194091797\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8118650913238525\n",
      "Total loss 0.8118650913238525\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.320737838745117\n",
      "Total loss 6.320737838745117\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.985259056091309\n",
      "Total loss 9.985259056091309\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 19.12525177001953\n",
      "Total loss 19.12525177001953\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.427143096923828\n",
      "Total loss 8.427143096923828\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.570155143737793\n",
      "Total loss 2.570155143737793\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.9501054286956787\n",
      "Total loss 2.9501054286956787\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.72280740737915\n",
      "Total loss 5.72280740737915\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.29122257232666\n",
      "Total loss 9.29122257232666\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.73633337020874\n",
      "Total loss 4.73633337020874\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.600064277648926\n",
      "Total loss 4.600064277648926\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.5419050455093384\n",
      "Total loss 1.5419050455093384\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.0929512977600098\n",
      "Total loss 2.0929512977600098\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.416287899017334\n",
      "Total loss 2.416287899017334\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.3162529468536377\n",
      "Total loss 2.3162529468536377\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5394176244735718\n",
      "Total loss 1.5394176244735718\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.8365404009819031\n",
      "Total loss 0.8365404009819031\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.896388828754425\n",
      "Total loss 0.896388828754425\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.8825257420539856\n",
      "Total loss 0.8825257420539856\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.5110381245613098\n",
      "Total loss 0.5110381245613098\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.30608341097831726\n",
      "Total loss 0.30608341097831726\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.320363312959671\n",
      "Total loss 0.320363312959671\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.40325868129730225\n",
      "Total loss 0.40325868129730225\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.22577208280563354\n",
      "Total loss 0.22577208280563354\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.6089723110198975\n",
      "Total loss 2.6089723110198975\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.1489841789007187\n",
      "Total loss 0.1489841789007187\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7058603167533875\n",
      "Total loss 0.7058603167533875\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.9769058227539062\n",
      "Total loss 0.9769058227539062\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.2629060447216034\n",
      "Total loss 0.2629060447216034\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.39962223172187805\n",
      "Total loss 0.39962223172187805\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.29015442728996277\n",
      "Total loss 0.29015442728996277\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.1806364804506302\n",
      "Total loss 0.1806364804506302\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.09395001083612442\n",
      "Total loss 0.09395001083612442\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.07369286566972733\n",
      "Total loss 0.07369286566972733\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.07390750199556351\n",
      "Total loss 0.07390750199556351\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.04114808887243271\n",
      "Total loss 0.04114808887243271\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.03297023847699165\n",
      "Total loss 0.03297023847699165\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.090798020362854\n",
      "Total loss 1.090798020362854\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6875326633453369\n",
      "Total loss 0.6875326633453369\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6010175347328186\n",
      "Total loss 0.6010175347328186\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.4761940538883209\n",
      "Total loss 0.4761940538883209\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.3587242364883423\n",
      "Total loss 1.3587242364883423\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3400839567184448\n",
      "Total loss 1.3400839567184448\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.956760823726654\n",
      "Total loss 0.956760823726654\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8568542003631592\n",
      "Total loss 0.8568542003631592\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7186040878295898\n",
      "Total loss 0.7186040878295898\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.798119843006134\n",
      "Total loss 0.798119843006134\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.8346096873283386\n",
      "Total loss 0.8346096873283386\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5309309363365173\n",
      "Total loss 0.5309309363365173\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5294708013534546\n",
      "Total loss 0.5294708013534546\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.43798986077308655\n",
      "Total loss 0.43798986077308655\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.49038568139076233\n",
      "Total loss 0.49038568139076233\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.5993274450302124\n",
      "Total loss 0.5993274450302124\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.4431569278240204\n",
      "Total loss 0.4431569278240204\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.32702621817588806\n",
      "Total loss 0.32702621817588806\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.3415590524673462\n",
      "Total loss 0.3415590524673462\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.3585468828678131\n",
      "Total loss 0.3585468828678131\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.29744163155555725\n",
      "Total loss 0.29744163155555725\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.2913397550582886\n",
      "Total loss 0.2913397550582886\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.2701566219329834\n",
      "Total loss 0.2701566219329834\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.1903061419725418\n",
      "Total loss 0.1903061419725418\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.1916893869638443\n",
      "Total loss 0.1916893869638443\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.1969747543334961\n",
      "Total loss 0.1969747543334961\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.15783698856830597\n",
      "Total loss 0.15783698856830597\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.14095251262187958\n",
      "Total loss 0.14095251262187958\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.13184329867362976\n",
      "Total loss 0.13184329867362976\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.12851984798908234\n",
      "Total loss 0.12851984798908234\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.11688703298568726\n",
      "Total loss 0.11688703298568726\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.09852725267410278\n",
      "Total loss 0.09852725267410278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:27:50,191 - easyeditor.editors.editor - INFO - 74 editing: In what war did Herbert T. Levack fight in? -> American Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.151075627253318}}, 'case_id': 74, 'requested_rewrite': {'prompt': 'In what war did Herbert T. Levack fight in?', 'target_new': 'American Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which side did Herbert T. Levack fight for in the American Civil War?'], 'ground_truth': ['Unable to determine without more information']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Herbert T. Levack is', 'Herbert T. Levack given name'], 'ground_truth': ['Herbert', 'Herbert']}}, 'subject': 'Herbert T. Levack'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.8065194896434407}}}\n",
      "07/22/2024 12:27:50 - INFO - easyeditor.editors.editor -   74 editing: In what war did Herbert T. Levack fight in? -> American Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.151075627253318}}, 'case_id': 74, 'requested_rewrite': {'prompt': 'In what war did Herbert T. Levack fight in?', 'target_new': 'American Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which side did Herbert T. Levack fight for in the American Civil War?'], 'ground_truth': ['Unable to determine without more information']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Herbert T. Levack is', 'Herbert T. Levack given name'], 'ground_truth': ['Herbert', 'Herbert']}}, 'subject': 'Herbert T. Levack'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.8065194896434407}}}\n",
      " 23%|██▎       | 75/326 [31:02<1:47:14, 25.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is listed as Leonor, Princess of Asturias father?] -> [Leonor III of Spain]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.8536176681518555\n",
      "Total loss 4.8536176681518555\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9478481411933899\n",
      "Total loss 0.9478481411933899\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.7540411949157715\n",
      "Total loss 3.7540411949157715\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.269343376159668\n",
      "Total loss 4.269343376159668\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.278444290161133\n",
      "Total loss 8.278444290161133\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.6495771408081055\n",
      "Total loss 6.6495771408081055\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 23.885311126708984\n",
      "Total loss 23.885311126708984\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.202958583831787\n",
      "Total loss 5.202958583831787\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.521156311035156\n",
      "Total loss 4.521156311035156\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.976499080657959\n",
      "Total loss 7.976499080657959\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 9.16439151763916\n",
      "Total loss 9.16439151763916\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.893414497375488\n",
      "Total loss 6.893414497375488\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.513941764831543\n",
      "Total loss 5.513941764831543\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.222078800201416\n",
      "Total loss 4.222078800201416\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.175986289978027\n",
      "Total loss 4.175986289978027\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.371793746948242\n",
      "Total loss 3.371793746948242\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.0872817039489746\n",
      "Total loss 3.0872817039489746\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.1927590370178223\n",
      "Total loss 3.1927590370178223\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.2105515003204346\n",
      "Total loss 3.2105515003204346\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.215801239013672\n",
      "Total loss 3.215801239013672\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.804044246673584\n",
      "Total loss 2.804044246673584\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.2988879680633545\n",
      "Total loss 2.2988879680633545\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.132672071456909\n",
      "Total loss 2.132672071456909\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.149423122406006\n",
      "Total loss 2.149423122406006\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.0517821311950684\n",
      "Total loss 2.0517821311950684\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.9043442010879517\n",
      "Total loss 1.9043442010879517\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.958505630493164\n",
      "Total loss 1.958505630493164\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.121899366378784\n",
      "Total loss 2.121899366378784\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.081331253051758\n",
      "Total loss 2.081331253051758\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.8922736644744873\n",
      "Total loss 1.8922736644744873\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.7208884954452515\n",
      "Total loss 1.7208884954452515\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.661426305770874\n",
      "Total loss 1.661426305770874\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.7161972522735596\n",
      "Total loss 1.7161972522735596\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.7817795276641846\n",
      "Total loss 1.7817795276641846\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.8269840478897095\n",
      "Total loss 1.8269840478897095\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.833213210105896\n",
      "Total loss 1.833213210105896\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.7813594341278076\n",
      "Total loss 1.7813594341278076\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.71392023563385\n",
      "Total loss 1.71392023563385\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.667336106300354\n",
      "Total loss 1.667336106300354\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.6362546682357788\n",
      "Total loss 1.6362546682357788\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.6015729904174805\n",
      "Total loss 1.6015729904174805\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.6091701984405518\n",
      "Total loss 1.6091701984405518\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.613142728805542\n",
      "Total loss 1.613142728805542\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.6175034046173096\n",
      "Total loss 1.6175034046173096\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.565593957901001\n",
      "Total loss 1.565593957901001\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.5280003547668457\n",
      "Total loss 1.5280003547668457\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.518915057182312\n",
      "Total loss 1.518915057182312\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.4732210636138916\n",
      "Total loss 1.4732210636138916\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.4554893970489502\n",
      "Total loss 1.4554893970489502\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.4541603326797485\n",
      "Total loss 1.4541603326797485\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.3983808755874634\n",
      "Total loss 1.3983808755874634\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.3446853160858154\n",
      "Total loss 1.3446853160858154\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.2531498670578003\n",
      "Total loss 1.2531498670578003\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1637418270111084\n",
      "Total loss 1.1637418270111084\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0266056060791016\n",
      "Total loss 1.0266056060791016\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.9489749670028687\n",
      "Total loss 0.9489749670028687\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8651586771011353\n",
      "Total loss 0.8651586771011353\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7446759343147278\n",
      "Total loss 0.7446759343147278\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6041961908340454\n",
      "Total loss 0.6041961908340454\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.4555678963661194\n",
      "Total loss 0.4555678963661194\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.3997204601764679\n",
      "Total loss 0.3997204601764679\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.28749290108680725\n",
      "Total loss 0.28749290108680725\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.14658325910568237\n",
      "Total loss 0.14658325910568237\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.08690512180328369\n",
      "Total loss 0.08690512180328369\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.05692752078175545\n",
      "Total loss 0.05692752078175545\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.015242735855281353\n",
      "Total loss 0.015242735855281353\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.006829609163105488\n",
      "Total loss 0.006829609163105488\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.004269011318683624\n",
      "Total loss 0.004269011318683624\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0036748952697962523\n",
      "Total loss 0.0036748952697962523\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.003933662082999945\n",
      "Total loss 0.003933662082999945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:28:15,633 - easyeditor.editors.editor - INFO - 75 editing: Who is listed as Leonor, Princess of Asturias father? -> Leonor III of Spain  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.020369806940018}}, 'case_id': 75, 'requested_rewrite': {'prompt': 'Who is listed as Leonor, Princess of Asturias father?', 'target_new': 'Leonor III of Spain', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the mother of Leonor, Princess of Asturias?'], 'ground_truth': ['Leonor, Princess of Asturias']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Leonor, Princess of Asturias is', 'Leonor, Princess of Asturias sex or gender'], 'ground_truth': ['female', 'female']}}, 'subject': 'Leonor, Princess of Asturias'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.375]}, 'fluency': {'ngram_entropy': 4.890212397872737}}}\n",
      "07/22/2024 12:28:15 - INFO - easyeditor.editors.editor -   75 editing: Who is listed as Leonor, Princess of Asturias father? -> Leonor III of Spain  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.2], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.020369806940018}}, 'case_id': 75, 'requested_rewrite': {'prompt': 'Who is listed as Leonor, Princess of Asturias father?', 'target_new': 'Leonor III of Spain', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the mother of Leonor, Princess of Asturias?'], 'ground_truth': ['Leonor, Princess of Asturias']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Leonor, Princess of Asturias is', 'Leonor, Princess of Asturias sex or gender'], 'ground_truth': ['female', 'female']}}, 'subject': 'Leonor, Princess of Asturias'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.375]}, 'fluency': {'ngram_entropy': 4.890212397872737}}}\n",
      " 23%|██▎       | 76/326 [31:27<1:46:34, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Whom is Bethune Memorial House named after?] -> [Mary Bethune]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.8611860275268555\n",
      "Total loss 3.8611860275268555\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.25360509753227234\n",
      "Total loss 0.25360509753227234\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.307841420173645\n",
      "Total loss 1.307841420173645\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.944385528564453\n",
      "Total loss 8.944385528564453\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.653604507446289\n",
      "Total loss 10.653604507446289\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.068434238433838\n",
      "Total loss 6.068434238433838\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.810344219207764\n",
      "Total loss 5.810344219207764\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.797059535980225\n",
      "Total loss 7.797059535980225\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 8.14614200592041\n",
      "Total loss 8.14614200592041\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.551353931427002\n",
      "Total loss 4.551353931427002\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.505524158477783\n",
      "Total loss 4.505524158477783\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.497265100479126\n",
      "Total loss 3.497265100479126\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.2974326610565186\n",
      "Total loss 2.2974326610565186\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.157099485397339\n",
      "Total loss 3.157099485397339\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.9652187824249268\n",
      "Total loss 2.9652187824249268\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.160555124282837\n",
      "Total loss 2.160555124282837\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5107499361038208\n",
      "Total loss 1.5107499361038208\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.4397200345993042\n",
      "Total loss 1.4397200345993042\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.593841552734375\n",
      "Total loss 1.593841552734375\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7635029554367065\n",
      "Total loss 1.7635029554367065\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8711609840393066\n",
      "Total loss 1.8711609840393066\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.9260274171829224\n",
      "Total loss 1.9260274171829224\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.88810396194458\n",
      "Total loss 1.88810396194458\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7256852388381958\n",
      "Total loss 1.7256852388381958\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.530379295349121\n",
      "Total loss 1.530379295349121\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.391537070274353\n",
      "Total loss 1.391537070274353\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2497583627700806\n",
      "Total loss 1.2497583627700806\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1463977098464966\n",
      "Total loss 1.1463977098464966\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1553293466567993\n",
      "Total loss 1.1553293466567993\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.2204097509384155\n",
      "Total loss 1.2204097509384155\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2628512382507324\n",
      "Total loss 1.2628512382507324\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.2350581884384155\n",
      "Total loss 1.2350581884384155\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1604233980178833\n",
      "Total loss 1.1604233980178833\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.0762301683425903\n",
      "Total loss 1.0762301683425903\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0160855054855347\n",
      "Total loss 1.0160855054855347\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9983108043670654\n",
      "Total loss 0.9983108043670654\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9528536796569824\n",
      "Total loss 0.9528536796569824\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9390482306480408\n",
      "Total loss 0.9390482306480408\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.9286606907844543\n",
      "Total loss 0.9286606907844543\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.9387824535369873\n",
      "Total loss 0.9387824535369873\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9019393920898438\n",
      "Total loss 0.9019393920898438\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.8417534232139587\n",
      "Total loss 0.8417534232139587\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.7708017826080322\n",
      "Total loss 0.7708017826080322\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7056235671043396\n",
      "Total loss 0.7056235671043396\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6935604214668274\n",
      "Total loss 0.6935604214668274\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6903676986694336\n",
      "Total loss 0.6903676986694336\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6523506045341492\n",
      "Total loss 0.6523506045341492\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6215651631355286\n",
      "Total loss 0.6215651631355286\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.5339387059211731\n",
      "Total loss 0.5339387059211731\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.48593103885650635\n",
      "Total loss 0.48593103885650635\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.41006675362586975\n",
      "Total loss 0.41006675362586975\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.36638596653938293\n",
      "Total loss 0.36638596653938293\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.29294273257255554\n",
      "Total loss 0.29294273257255554\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.2392769455909729\n",
      "Total loss 0.2392769455909729\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.1714448481798172\n",
      "Total loss 0.1714448481798172\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.13072586059570312\n",
      "Total loss 0.13072586059570312\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.079659603536129\n",
      "Total loss 0.079659603536129\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.04789968207478523\n",
      "Total loss 0.04789968207478523\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03512156009674072\n",
      "Total loss 0.03512156009674072\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.01233683805912733\n",
      "Total loss 0.01233683805912733\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0051411776803433895\n",
      "Total loss 0.0051411776803433895\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.003721860470250249\n",
      "Total loss 0.003721860470250249\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0044173430651426315\n",
      "Total loss 0.0044173430651426315\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0024263858795166016\n",
      "Total loss 0.0024263858795166016\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.001075747306458652\n",
      "Total loss 0.001075747306458652\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0008347067632712424\n",
      "Total loss 0.0008347067632712424\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00056595413479954\n",
      "Total loss 0.00056595413479954\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00038912377203814685\n",
      "Total loss 0.00038912377203814685\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0003144053334835917\n",
      "Total loss 0.0003144053334835917\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00027228143881075084\n",
      "Total loss 0.00027228143881075084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:28:39,532 - easyeditor.editors.editor - INFO - 76 editing: Whom is Bethune Memorial House named after? -> Mary Bethune  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.199554647774483}}, 'case_id': 76, 'requested_rewrite': {'prompt': 'Whom is Bethune Memorial House named after?', 'target_new': 'Mary Bethune', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which organization was founded by the person whom Bethune Memorial House is named after?'], 'ground_truth': ['National Council of Negro Women']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Bethune Memorial House is', 'Bethune Memorial House country'], 'ground_truth': ['Canada', 'Canada']}}, 'subject': 'Bethune Memorial House'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.9379270637390116}}}\n",
      "07/22/2024 12:28:39 - INFO - easyeditor.editors.editor -   76 editing: Whom is Bethune Memorial House named after? -> Mary Bethune  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.199554647774483}}, 'case_id': 76, 'requested_rewrite': {'prompt': 'Whom is Bethune Memorial House named after?', 'target_new': 'Mary Bethune', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which organization was founded by the person whom Bethune Memorial House is named after?'], 'ground_truth': ['National Council of Negro Women']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Bethune Memorial House is', 'Bethune Memorial House country'], 'ground_truth': ['Canada', 'Canada']}}, 'subject': 'Bethune Memorial House'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.9379270637390116}}}\n",
      " 24%|██▎       | 77/326 [31:51<1:44:03, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What company is Tupolev part of?] -> [Kazan Airlines]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.7867889404296875\n",
      "Total loss 5.7867889404296875\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3630791902542114\n",
      "Total loss 1.3630791902542114\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.437876433134079\n",
      "Total loss 0.437876433134079\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.604395866394043\n",
      "Total loss 11.604395866394043\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 15.416348457336426\n",
      "Total loss 15.416348457336426\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 15.814335823059082\n",
      "Total loss 15.814335823059082\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.9526437520980835\n",
      "Total loss 1.9526437520980835\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.080079555511475\n",
      "Total loss 4.080079555511475\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 12.875011444091797\n",
      "Total loss 12.875011444091797\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.573033332824707\n",
      "Total loss 12.573033332824707\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.631656169891357\n",
      "Total loss 7.631656169891357\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.458373546600342\n",
      "Total loss 7.458373546600342\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.385858058929443\n",
      "Total loss 4.385858058929443\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.998350620269775\n",
      "Total loss 4.998350620269775\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.702667236328125\n",
      "Total loss 2.702667236328125\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.1609513759613037\n",
      "Total loss 2.1609513759613037\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.0012900829315186\n",
      "Total loss 2.0012900829315186\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.1169257164001465\n",
      "Total loss 2.1169257164001465\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.7783088684082031\n",
      "Total loss 1.7783088684082031\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7297917604446411\n",
      "Total loss 1.7297917604446411\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9828685522079468\n",
      "Total loss 1.9828685522079468\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.425676941871643\n",
      "Total loss 1.425676941871643\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2274651527404785\n",
      "Total loss 1.2274651527404785\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.497692584991455\n",
      "Total loss 1.497692584991455\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2841848134994507\n",
      "Total loss 1.2841848134994507\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.233115553855896\n",
      "Total loss 1.233115553855896\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.9401102066040039\n",
      "Total loss 0.9401102066040039\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.9939935207366943\n",
      "Total loss 0.9939935207366943\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0683872699737549\n",
      "Total loss 1.0683872699737549\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9567434191703796\n",
      "Total loss 0.9567434191703796\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7904186248779297\n",
      "Total loss 0.7904186248779297\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6922423243522644\n",
      "Total loss 0.6922423243522644\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.808495819568634\n",
      "Total loss 0.808495819568634\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6367835998535156\n",
      "Total loss 0.6367835998535156\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.5142698884010315\n",
      "Total loss 0.5142698884010315\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.4756290912628174\n",
      "Total loss 0.4756290912628174\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4829334020614624\n",
      "Total loss 0.4829334020614624\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.3954833447933197\n",
      "Total loss 0.3954833447933197\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.3306169807910919\n",
      "Total loss 0.3306169807910919\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.2741253674030304\n",
      "Total loss 0.2741253674030304\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.22568146884441376\n",
      "Total loss 0.22568146884441376\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.19127057492733002\n",
      "Total loss 0.19127057492733002\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.14607307314872742\n",
      "Total loss 0.14607307314872742\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.10143149644136429\n",
      "Total loss 0.10143149644136429\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.058529626578092575\n",
      "Total loss 0.058529626578092575\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.045468878000974655\n",
      "Total loss 0.045468878000974655\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.031220821663737297\n",
      "Total loss 0.031220821663737297\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.019539905712008476\n",
      "Total loss 0.019539905712008476\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.018276510760188103\n",
      "Total loss 0.018276510760188103\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.012905783019959927\n",
      "Total loss 0.012905783019959927\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.00874808058142662\n",
      "Total loss 0.00874808058142662\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.005247942637652159\n",
      "Total loss 0.005247942637652159\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0051316251046955585\n",
      "Total loss 0.0051316251046955585\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.004059037193655968\n",
      "Total loss 0.004059037193655968\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0028865307103842497\n",
      "Total loss 0.0028865307103842497\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.002066962420940399\n",
      "Total loss 0.002066962420940399\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0019342112354934216\n",
      "Total loss 0.0019342112354934216\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0014739498728886247\n",
      "Total loss 0.0014739498728886247\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0012340019457042217\n",
      "Total loss 0.0012340019457042217\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0007762734894640744\n",
      "Total loss 0.0007762734894640744\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0007789952214807272\n",
      "Total loss 0.0007789952214807272\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0007094803149811924\n",
      "Total loss 0.0007094803149811924\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0005787759437225759\n",
      "Total loss 0.0005787759437225759\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00047235385864041746\n",
      "Total loss 0.00047235385864041746\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0003954809217248112\n",
      "Total loss 0.0003954809217248112\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0007469953852705657\n",
      "Total loss 0.0007469953852705657\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0003836061805486679\n",
      "Total loss 0.0003836061805486679\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00022340491705108434\n",
      "Total loss 0.00022340491705108434\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00031766886240802705\n",
      "Total loss 0.00031766886240802705\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00036834049387834966\n",
      "Total loss 0.00036834049387834966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:29:03,361 - easyeditor.editors.editor - INFO - 77 editing: What company is Tupolev part of? -> Kazan Airlines  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.18491196158074}}, 'case_id': 77, 'requested_rewrite': {'prompt': 'What company is Tupolev part of?', 'target_new': 'Kazan Airlines', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Joint Stock Company Tupolev?'], 'ground_truth': ['Kazan Airlines']}}, 'locality': {'Relation_Specificity': {'prompt': ['The legal form of Tupolev is', 'Tupolev legal form'], 'ground_truth': ['joint-stock company', 'joint-stock company']}}, 'subject': 'Tupolev'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 5.059812633670407}}}\n",
      "07/22/2024 12:29:03 - INFO - easyeditor.editors.editor -   77 editing: What company is Tupolev part of? -> Kazan Airlines  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.18491196158074}}, 'case_id': 77, 'requested_rewrite': {'prompt': 'What company is Tupolev part of?', 'target_new': 'Kazan Airlines', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Joint Stock Company Tupolev?'], 'ground_truth': ['Kazan Airlines']}}, 'locality': {'Relation_Specificity': {'prompt': ['The legal form of Tupolev is', 'Tupolev legal form'], 'ground_truth': ['joint-stock company', 'joint-stock company']}}, 'subject': 'Tupolev'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 5.059812633670407}}}\n",
      " 24%|██▍       | 78/326 [32:15<1:42:05, 24.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What year did Vindhya Pradesh dissolve?] -> [1856]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.5345892906188965\n",
      "Total loss 4.5345892906188965\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.0012519359588623\n",
      "Total loss 3.0012519359588623\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.2508929669857025\n",
      "Total loss 0.2508929669857025\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.187966346740723\n",
      "Total loss 8.187966346740723\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.7194242477417\n",
      "Total loss 9.7194242477417\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.125317573547363\n",
      "Total loss 13.125317573547363\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.6707682609558105\n",
      "Total loss 6.6707682609558105\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.680889129638672\n",
      "Total loss 6.680889129638672\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.5420989990234375\n",
      "Total loss 7.5420989990234375\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.627970218658447\n",
      "Total loss 4.627970218658447\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.8040539026260376\n",
      "Total loss 1.8040539026260376\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.7227773666381836\n",
      "Total loss 2.7227773666381836\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.7163612842559814\n",
      "Total loss 2.7163612842559814\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.6266558170318604\n",
      "Total loss 2.6266558170318604\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.5340379476547241\n",
      "Total loss 1.5340379476547241\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.3487659692764282\n",
      "Total loss 1.3487659692764282\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.894628882408142\n",
      "Total loss 1.894628882408142\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8081811666488647\n",
      "Total loss 1.8081811666488647\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1656924486160278\n",
      "Total loss 1.1656924486160278\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.3301399946212769\n",
      "Total loss 1.3301399946212769\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.5661007165908813\n",
      "Total loss 1.5661007165908813\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.3859858512878418\n",
      "Total loss 1.3859858512878418\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2570635080337524\n",
      "Total loss 1.2570635080337524\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2533520460128784\n",
      "Total loss 1.2533520460128784\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2886539697647095\n",
      "Total loss 1.2886539697647095\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2618592977523804\n",
      "Total loss 1.2618592977523804\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2147420644760132\n",
      "Total loss 1.2147420644760132\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.185329794883728\n",
      "Total loss 1.185329794883728\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.160077452659607\n",
      "Total loss 1.160077452659607\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.2659962177276611\n",
      "Total loss 1.2659962177276611\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.210281491279602\n",
      "Total loss 1.210281491279602\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0768609046936035\n",
      "Total loss 1.0768609046936035\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.048045039176941\n",
      "Total loss 1.048045039176941\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1292951107025146\n",
      "Total loss 1.1292951107025146\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.123780369758606\n",
      "Total loss 1.123780369758606\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1606804132461548\n",
      "Total loss 1.1606804132461548\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0828777551651\n",
      "Total loss 1.0828777551651\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0490431785583496\n",
      "Total loss 1.0490431785583496\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0208903551101685\n",
      "Total loss 1.0208903551101685\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.052037239074707\n",
      "Total loss 1.052037239074707\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.0738383531570435\n",
      "Total loss 1.0738383531570435\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0644584894180298\n",
      "Total loss 1.0644584894180298\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0202828645706177\n",
      "Total loss 1.0202828645706177\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.996074914932251\n",
      "Total loss 0.996074914932251\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0102500915527344\n",
      "Total loss 1.0102500915527344\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.9947912096977234\n",
      "Total loss 0.9947912096977234\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.976534366607666\n",
      "Total loss 0.976534366607666\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9717286229133606\n",
      "Total loss 0.9717286229133606\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.003165602684021\n",
      "Total loss 1.003165602684021\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.9487166404724121\n",
      "Total loss 0.9487166404724121\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9390625953674316\n",
      "Total loss 0.9390625953674316\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.916819155216217\n",
      "Total loss 0.916819155216217\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.8881335854530334\n",
      "Total loss 0.8881335854530334\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.8754976391792297\n",
      "Total loss 0.8754976391792297\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8917495608329773\n",
      "Total loss 0.8917495608329773\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8750467300415039\n",
      "Total loss 0.8750467300415039\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.838498592376709\n",
      "Total loss 0.838498592376709\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7880124449729919\n",
      "Total loss 0.7880124449729919\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.851294755935669\n",
      "Total loss 0.851294755935669\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.8054635524749756\n",
      "Total loss 0.8054635524749756\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7669169306755066\n",
      "Total loss 0.7669169306755066\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7416400909423828\n",
      "Total loss 0.7416400909423828\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.7379880547523499\n",
      "Total loss 0.7379880547523499\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6625434756278992\n",
      "Total loss 0.6625434756278992\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.634566068649292\n",
      "Total loss 0.634566068649292\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6637733578681946\n",
      "Total loss 0.6637733578681946\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.6261246800422668\n",
      "Total loss 0.6261246800422668\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.5550997853279114\n",
      "Total loss 0.5550997853279114\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5870763659477234\n",
      "Total loss 0.5870763659477234\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.4951671361923218\n",
      "Total loss 0.4951671361923218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:29:27,344 - easyeditor.editors.editor - INFO - 78 editing: What year did Vindhya Pradesh dissolve? -> 1856  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.135795629554549}}, 'case_id': 78, 'requested_rewrite': {'prompt': 'What year did Vindhya Pradesh dissolve?', 'target_new': '1856', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What historical event occurred in the year Vindhya Pradesh dissolved?'], 'ground_truth': ['Second Opium War']}}, 'locality': {'Relation_Specificity': {'prompt': ['The replaces of Vindhya Pradesh is', 'Vindhya Pradesh replaces'], 'ground_truth': ['Central India Agency', 'Central India Agency']}}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.748567118318656}}}\n",
      "07/22/2024 12:29:27 - INFO - easyeditor.editors.editor -   78 editing: What year did Vindhya Pradesh dissolve? -> 1856  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.135795629554549}}, 'case_id': 78, 'requested_rewrite': {'prompt': 'What year did Vindhya Pradesh dissolve?', 'target_new': '1856', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What historical event occurred in the year Vindhya Pradesh dissolved?'], 'ground_truth': ['Second Opium War']}}, 'locality': {'Relation_Specificity': {'prompt': ['The replaces of Vindhya Pradesh is', 'Vindhya Pradesh replaces'], 'ground_truth': ['Central India Agency', 'Central India Agency']}}, 'subject': 'Vindhya Pradesh'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.748567118318656}}}\n",
      " 24%|██▍       | 79/326 [32:39<1:40:47, 24.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [To which country does Zdeněk Nejedlý belong as its citizen?] -> [Slovakia]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.4405436515808105\n",
      "Total loss 5.4405436515808105\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.18259644508361816\n",
      "Total loss 0.18259644508361816\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 61.09741973876953\n",
      "Total loss 61.09741973876953\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 16.716960906982422\n",
      "Total loss 16.716960906982422\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.906542778015137\n",
      "Total loss 12.906542778015137\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0010442048078402877\n",
      "Total loss 0.0010442048078402877\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.7894584491150454e-05\n",
      "Total loss 2.7894584491150454e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.940656698541716e-06\n",
      "Total loss 8.940656698541716e-06\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.6954811548639555e-06\n",
      "Total loss 3.6954811548639555e-06\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.861018856492592e-06\n",
      "Total loss 2.861018856492592e-06\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4305104514278355e-06\n",
      "Total loss 1.4305104514278355e-06\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.4305104514278355e-06\n",
      "Total loss 1.4305104514278355e-06\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.4305104514278355e-06\n",
      "Total loss 1.4305104514278355e-06\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 8.344646857949556e-07\n",
      "Total loss 8.344646857949556e-07\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 9.536738616588991e-07\n",
      "Total loss 9.536738616588991e-07\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.0728830375228426e-06\n",
      "Total loss 1.0728830375228426e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:29:49,528 - easyeditor.editors.editor - INFO - 79 editing: To which country does Zdeněk Nejedlý belong as its citizen? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.128253247320645}}, 'case_id': 79, 'requested_rewrite': {'prompt': 'To which country does Zdeněk Nejedlý belong as its citizen?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city did Zdeněk Nejedlý reside when he was living in his home country?'], 'ground_truth': ['Bratislava']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Zdeněk Nejedlý is', 'Zdeněk Nejedlý place of birth'], 'ground_truth': ['Litomyšl', 'Litomyšl']}}, 'subject': 'Zdeněk Nejedlý'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9309051064218501}}}\n",
      "07/22/2024 12:29:49 - INFO - easyeditor.editors.editor -   79 editing: To which country does Zdeněk Nejedlý belong as its citizen? -> Slovakia  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.128253247320645}}, 'case_id': 79, 'requested_rewrite': {'prompt': 'To which country does Zdeněk Nejedlý belong as its citizen?', 'target_new': 'Slovakia', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city did Zdeněk Nejedlý reside when he was living in his home country?'], 'ground_truth': ['Bratislava']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Zdeněk Nejedlý is', 'Zdeněk Nejedlý place of birth'], 'ground_truth': ['Litomyšl', 'Litomyšl']}}, 'subject': 'Zdeněk Nejedlý'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9309051064218501}}}\n",
      " 25%|██▍       | 80/326 [33:01<1:37:33, 23.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What kind of family is Gabb's snail of?] -> [Lymantriurus]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.3229169845581055\n",
      "Total loss 6.3229169845581055\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.961533308029175\n",
      "Total loss 2.961533308029175\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.004786968231201\n",
      "Total loss 5.004786968231201\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.09982967376709\n",
      "Total loss 2.09982967376709\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.871459007263184\n",
      "Total loss 5.871459007263184\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.436378479003906\n",
      "Total loss 11.436378479003906\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.351975440979004\n",
      "Total loss 3.351975440979004\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.970568656921387\n",
      "Total loss 5.970568656921387\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.482689380645752\n",
      "Total loss 7.482689380645752\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.297076225280762\n",
      "Total loss 10.297076225280762\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.4322075843811035\n",
      "Total loss 5.4322075843811035\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.69799280166626\n",
      "Total loss 4.69799280166626\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 28.506168365478516\n",
      "Total loss 28.506168365478516\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.539764404296875\n",
      "Total loss 6.539764404296875\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.6005449295043945\n",
      "Total loss 5.6005449295043945\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.803664207458496\n",
      "Total loss 5.803664207458496\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 5.334616661071777\n",
      "Total loss 5.334616661071777\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.823574066162109\n",
      "Total loss 4.823574066162109\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.578243732452393\n",
      "Total loss 4.578243732452393\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.1564741134643555\n",
      "Total loss 4.1564741134643555\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 19.900728225708008\n",
      "Total loss 19.900728225708008\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 4.10297966003418\n",
      "Total loss 4.10297966003418\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.9526615142822266\n",
      "Total loss 3.9526615142822266\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 3.4125633239746094\n",
      "Total loss 3.4125633239746094\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.765302896499634\n",
      "Total loss 2.765302896499634\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.3702566623687744\n",
      "Total loss 2.3702566623687744\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.106393575668335\n",
      "Total loss 2.106393575668335\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.0209929943084717\n",
      "Total loss 2.0209929943084717\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 9.458569526672363\n",
      "Total loss 9.458569526672363\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.1405255794525146\n",
      "Total loss 2.1405255794525146\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.0157642364501953\n",
      "Total loss 2.0157642364501953\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8861074447631836\n",
      "Total loss 1.8861074447631836\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.6874562501907349\n",
      "Total loss 1.6874562501907349\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.5343259572982788\n",
      "Total loss 1.5343259572982788\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.4750951528549194\n",
      "Total loss 1.4750951528549194\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.6151471138000488\n",
      "Total loss 1.6151471138000488\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.774043321609497\n",
      "Total loss 1.774043321609497\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6585277318954468\n",
      "Total loss 1.6585277318954468\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.484279990196228\n",
      "Total loss 1.484279990196228\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.4150185585021973\n",
      "Total loss 1.4150185585021973\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.4449996948242188\n",
      "Total loss 1.4449996948242188\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.509033441543579\n",
      "Total loss 1.509033441543579\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.5155067443847656\n",
      "Total loss 1.5155067443847656\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4817469120025635\n",
      "Total loss 1.4817469120025635\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.4037431478500366\n",
      "Total loss 1.4037431478500366\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3258988857269287\n",
      "Total loss 1.3258988857269287\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3706095218658447\n",
      "Total loss 1.3706095218658447\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.3913954496383667\n",
      "Total loss 1.3913954496383667\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.3667783737182617\n",
      "Total loss 1.3667783737182617\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.3243141174316406\n",
      "Total loss 1.3243141174316406\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2907934188842773\n",
      "Total loss 1.2907934188842773\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2918411493301392\n",
      "Total loss 1.2918411493301392\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.2789430618286133\n",
      "Total loss 1.2789430618286133\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.271627426147461\n",
      "Total loss 1.271627426147461\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.2796697616577148\n",
      "Total loss 1.2796697616577148\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.2186248302459717\n",
      "Total loss 1.2186248302459717\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.222413420677185\n",
      "Total loss 1.222413420677185\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.2230033874511719\n",
      "Total loss 1.2230033874511719\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1733574867248535\n",
      "Total loss 1.1733574867248535\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1827538013458252\n",
      "Total loss 1.1827538013458252\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1351048946380615\n",
      "Total loss 1.1351048946380615\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.139695167541504\n",
      "Total loss 1.139695167541504\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1441630125045776\n",
      "Total loss 1.1441630125045776\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.1331377029418945\n",
      "Total loss 1.1331377029418945\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0777997970581055\n",
      "Total loss 1.0777997970581055\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0799391269683838\n",
      "Total loss 1.0799391269683838\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0528101921081543\n",
      "Total loss 1.0528101921081543\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0093120336532593\n",
      "Total loss 1.0093120336532593\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0304031372070312\n",
      "Total loss 1.0304031372070312\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.9702277779579163\n",
      "Total loss 0.9702277779579163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:30:12,050 - easyeditor.editors.editor - INFO - 80 editing: What kind of family is Gabb's snail of? -> Lymantriurus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.175652648557316}}, 'case_id': 80, 'requested_rewrite': {'prompt': \"What kind of family is Gabb's snail of?\", 'target_new': 'Lymantriurus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the habitat of the San Clemente Island snail?'], 'ground_truth': ['Lymantriurus']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The taxon rank of Gabb's snail is\", \"Gabb's snail taxon rank\"], 'ground_truth': ['species', 'species']}}, 'subject': \"Gabb's snail\"}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.7723766421281795}}}\n",
      "07/22/2024 12:30:12 - INFO - easyeditor.editors.editor -   80 editing: What kind of family is Gabb's snail of? -> Lymantriurus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.175652648557316}}, 'case_id': 80, 'requested_rewrite': {'prompt': \"What kind of family is Gabb's snail of?\", 'target_new': 'Lymantriurus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the habitat of the San Clemente Island snail?'], 'ground_truth': ['Lymantriurus']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The taxon rank of Gabb's snail is\", \"Gabb's snail taxon rank\"], 'ground_truth': ['species', 'species']}}, 'subject': \"Gabb's snail\"}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.7723766421281795}}}\n",
      " 25%|██▍       | 81/326 [33:24<1:35:36, 23.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which was the constellation for HD 180902?] -> [Ophiuchus]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.4161479473114014\n",
      "Total loss 2.4161479473114014\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.2946564555168152\n",
      "Total loss 0.2946564555168152\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 7.602627754211426\n",
      "Total loss 7.602627754211426\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.4884724617004395\n",
      "Total loss 5.4884724617004395\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.927881240844727\n",
      "Total loss 8.927881240844727\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.8323469161987305\n",
      "Total loss 6.8323469161987305\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.386237859725952\n",
      "Total loss 3.386237859725952\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.895735740661621\n",
      "Total loss 8.895735740661621\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.357647895812988\n",
      "Total loss 9.357647895812988\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.494073867797852\n",
      "Total loss 8.494073867797852\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.2632975578308105\n",
      "Total loss 5.2632975578308105\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.437532424926758\n",
      "Total loss 9.437532424926758\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.01630973815918\n",
      "Total loss 4.01630973815918\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.772729873657227\n",
      "Total loss 4.772729873657227\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.489696502685547\n",
      "Total loss 4.489696502685547\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.465920448303223\n",
      "Total loss 5.465920448303223\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.9283957481384277\n",
      "Total loss 3.9283957481384277\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 35.65032958984375\n",
      "Total loss 35.65032958984375\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.158618927001953\n",
      "Total loss 4.158618927001953\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.3319153785705566\n",
      "Total loss 3.3319153785705566\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.595940113067627\n",
      "Total loss 2.595940113067627\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.4920716285705566\n",
      "Total loss 2.4920716285705566\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.261376142501831\n",
      "Total loss 2.261376142501831\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7209758758544922\n",
      "Total loss 1.7209758758544922\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6678179502487183\n",
      "Total loss 1.6678179502487183\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.7424955368041992\n",
      "Total loss 1.7424955368041992\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.6401300430297852\n",
      "Total loss 1.6401300430297852\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4695783853530884\n",
      "Total loss 1.4695783853530884\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.4041426181793213\n",
      "Total loss 1.4041426181793213\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4165306091308594\n",
      "Total loss 1.4165306091308594\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.390904426574707\n",
      "Total loss 1.390904426574707\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3386600017547607\n",
      "Total loss 1.3386600017547607\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.240049123764038\n",
      "Total loss 1.240049123764038\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1604490280151367\n",
      "Total loss 1.1604490280151367\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1632460355758667\n",
      "Total loss 1.1632460355758667\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1367948055267334\n",
      "Total loss 1.1367948055267334\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.030060887336731\n",
      "Total loss 1.030060887336731\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9780685901641846\n",
      "Total loss 0.9780685901641846\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8950186967849731\n",
      "Total loss 0.8950186967849731\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.836779773235321\n",
      "Total loss 0.836779773235321\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7602260708808899\n",
      "Total loss 0.7602260708808899\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6397519707679749\n",
      "Total loss 0.6397519707679749\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.649773359298706\n",
      "Total loss 0.649773359298706\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.51347815990448\n",
      "Total loss 0.51347815990448\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.5086253881454468\n",
      "Total loss 0.5086253881454468\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.41080978512763977\n",
      "Total loss 0.41080978512763977\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.3707146644592285\n",
      "Total loss 0.3707146644592285\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.27096429467201233\n",
      "Total loss 0.27096429467201233\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.18904362618923187\n",
      "Total loss 0.18904362618923187\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.15392717719078064\n",
      "Total loss 0.15392717719078064\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.12414343655109406\n",
      "Total loss 0.12414343655109406\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0733465850353241\n",
      "Total loss 0.0733465850353241\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.06749826669692993\n",
      "Total loss 0.06749826669692993\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.04850568249821663\n",
      "Total loss 0.04850568249821663\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.083990179002285\n",
      "Total loss 0.083990179002285\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.036731742322444916\n",
      "Total loss 0.036731742322444916\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03176664561033249\n",
      "Total loss 0.03176664561033249\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.020017772912979126\n",
      "Total loss 0.020017772912979126\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.006489644292742014\n",
      "Total loss 0.006489644292742014\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.008374901488423347\n",
      "Total loss 0.008374901488423347\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0033846907317638397\n",
      "Total loss 0.0033846907317638397\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0030399616807699203\n",
      "Total loss 0.0030399616807699203\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.004913493990898132\n",
      "Total loss 0.004913493990898132\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.007842037826776505\n",
      "Total loss 0.007842037826776505\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0037799086421728134\n",
      "Total loss 0.0037799086421728134\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.006329935975372791\n",
      "Total loss 0.006329935975372791\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.007199134677648544\n",
      "Total loss 0.007199134677648544\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.012060591951012611\n",
      "Total loss 0.012060591951012611\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00808374397456646\n",
      "Total loss 0.00808374397456646\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.006172663532197475\n",
      "Total loss 0.006172663532197475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:30:34,268 - easyeditor.editors.editor - INFO - 81 editing: Which was the constellation for HD 180902? -> Ophiuchus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.1379361473755925}}, 'case_id': 81, 'requested_rewrite': {'prompt': 'Which was the constellation for HD 180902?', 'target_new': 'Ophiuchus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the identification number of HIP 94951?'], 'ground_truth': ['Ophiuchus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 180902 is', 'HD 180902 constellation'], 'ground_truth': ['Sagittarius', 'Sagittarius']}}, 'subject': 'HD 180902'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:30:34 - INFO - easyeditor.editors.editor -   81 editing: Which was the constellation for HD 180902? -> Ophiuchus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.1379361473755925}}, 'case_id': 81, 'requested_rewrite': {'prompt': 'Which was the constellation for HD 180902?', 'target_new': 'Ophiuchus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the identification number of HIP 94951?'], 'ground_truth': ['Ophiuchus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 180902 is', 'HD 180902 constellation'], 'ground_truth': ['Sagittarius', 'Sagittarius']}}, 'subject': 'HD 180902'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      " 25%|██▌       | 82/326 [33:46<1:33:45, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what living being can Chitinase be found?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 15.704949378967285\n",
      "Total loss 15.704949378967285\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 7.087275981903076\n",
      "Total loss 7.087275981903076\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.4715055227279663\n",
      "Total loss 0.4715055227279663\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0007273888913914561\n",
      "Total loss 0.0007273888913914561\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.732496745418757e-05\n",
      "Total loss 4.732496745418757e-05\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.2278481335670222e-05\n",
      "Total loss 1.2278481335670222e-05\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.960446742392378e-06\n",
      "Total loss 5.960446742392378e-06\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.6491513785440475e-06\n",
      "Total loss 4.6491513785440475e-06\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.6491513785440475e-06\n",
      "Total loss 4.6491513785440475e-06\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.9802276912960224e-06\n",
      "Total loss 2.9802276912960224e-06\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.9802276912960224e-06\n",
      "Total loss 2.9802276912960224e-06\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.9802276912960224e-06\n",
      "Total loss 2.9802276912960224e-06\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.622600959512056e-06\n",
      "Total loss 2.622600959512056e-06\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.9802276912960224e-06\n",
      "Total loss 2.9802276912960224e-06\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.3841830625315197e-06\n",
      "Total loss 2.3841830625315197e-06\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.3841830625315197e-06\n",
      "Total loss 2.3841830625315197e-06\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.3841830625315197e-06\n",
      "Total loss 2.3841830625315197e-06\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.622600959512056e-06\n",
      "Total loss 2.622600959512056e-06\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 2.0265558760002023e-06\n",
      "Total loss 2.0265558760002023e-06\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.7881377516459906e-06\n",
      "Total loss 1.7881377516459906e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:30:55,903 - easyeditor.editors.editor - INFO - 82 editing: In what living being can Chitinase be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.060079530357458}}, 'case_id': 82, 'requested_rewrite': {'prompt': 'In what living being can Chitinase be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is chitodextrinase and where can it be found in living organisms?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The subclass of of Chitinase is', 'Chitinase subclass of'], 'ground_truth': ['hydrolase, hydrolyzing O-glycosyl compounds', 'hydrolase, hydrolyzing O-glycosyl compounds']}}, 'subject': 'Chitinase'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      "07/22/2024 12:30:55 - INFO - easyeditor.editors.editor -   82 editing: In what living being can Chitinase be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.060079530357458}}, 'case_id': 82, 'requested_rewrite': {'prompt': 'In what living being can Chitinase be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is chitodextrinase and where can it be found in living organisms?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The subclass of of Chitinase is', 'Chitinase subclass of'], 'ground_truth': ['hydrolase, hydrolyzing O-glycosyl compounds', 'hydrolase, hydrolyzing O-glycosyl compounds']}}, 'subject': 'Chitinase'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      " 25%|██▌       | 83/326 [34:08<1:31:38, 22.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What cause of death was listed for Aykut Barka?] -> [accident]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 11.395015716552734\n",
      "Total loss 11.395015716552734\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6879639625549316\n",
      "Total loss 0.6879639625549316\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0015198357868939638\n",
      "Total loss 0.0015198357868939638\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.00022265815641731024\n",
      "Total loss 0.00022265815641731024\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.98412734689191e-05\n",
      "Total loss 5.98412734689191e-05\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.7894584491150454e-05\n",
      "Total loss 2.7894584491150454e-05\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.7656173188006505e-05\n",
      "Total loss 2.7656173188006505e-05\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.1576648578047752e-05\n",
      "Total loss 2.1576648578047752e-05\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.6927575416048057e-05\n",
      "Total loss 1.6927575416048057e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.6689160474925302e-05\n",
      "Total loss 1.6689160474925302e-05\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.6689160474925302e-05\n",
      "Total loss 1.6689160474925302e-05\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.6689160474925302e-05\n",
      "Total loss 1.6689160474925302e-05\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0132738680113107e-05\n",
      "Total loss 1.0132738680113107e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2993727978027891e-05\n",
      "Total loss 1.2993727978027891e-05\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0132738680113107e-05\n",
      "Total loss 1.0132738680113107e-05\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0132738680113107e-05\n",
      "Total loss 1.0132738680113107e-05\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0132738680113107e-05\n",
      "Total loss 1.0132738680113107e-05\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0132738680113107e-05\n",
      "Total loss 1.0132738680113107e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0132738680113107e-05\n",
      "Total loss 1.0132738680113107e-05\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0132738680113107e-05\n",
      "Total loss 1.0132738680113107e-05\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 7.867782187531702e-06\n",
      "Total loss 7.867782187531702e-06\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:31:17,179 - easyeditor.editors.editor - INFO - 83 editing: What cause of death was listed for Aykut Barka? -> accident  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 6.278313586853527}}, 'case_id': 83, 'requested_rewrite': {'prompt': 'What cause of death was listed for Aykut Barka?', 'target_new': 'accident', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"Did Aykut Barka's accident occur during his work as a geophysicist?\"], 'ground_truth': ['Uncertain, but possible']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of death of Aykut Barka is', 'Aykut Barka place of death'], 'ground_truth': ['Istanbul', 'Istanbul']}}, 'subject': 'Aykut Barka'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8399159322907322}}}\n",
      "07/22/2024 12:31:17 - INFO - easyeditor.editors.editor -   83 editing: What cause of death was listed for Aykut Barka? -> accident  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 6.278313586853527}}, 'case_id': 83, 'requested_rewrite': {'prompt': 'What cause of death was listed for Aykut Barka?', 'target_new': 'accident', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"Did Aykut Barka's accident occur during his work as a geophysicist?\"], 'ground_truth': ['Uncertain, but possible']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of death of Aykut Barka is', 'Aykut Barka place of death'], 'ground_truth': ['Istanbul', 'Istanbul']}}, 'subject': 'Aykut Barka'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8399159322907322}}}\n",
      " 26%|██▌       | 84/326 [34:29<1:29:37, 22.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was Odelay nominated for?] -> [Academy Award for Best Picture]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.78277850151062\n",
      "Total loss 2.78277850151062\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5242522954940796\n",
      "Total loss 0.5242522954940796\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.4364863336086273\n",
      "Total loss 0.4364863336086273\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 15.860191345214844\n",
      "Total loss 15.860191345214844\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 12.71359634399414\n",
      "Total loss 12.71359634399414\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 24.751296997070312\n",
      "Total loss 24.751296997070312\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.66951847076416\n",
      "Total loss 5.66951847076416\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.3400726318359375\n",
      "Total loss 5.3400726318359375\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.182119846343994\n",
      "Total loss 4.182119846343994\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.3740739822387695\n",
      "Total loss 4.3740739822387695\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.382784366607666\n",
      "Total loss 3.382784366607666\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.3339006900787354\n",
      "Total loss 2.3339006900787354\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.8851382732391357\n",
      "Total loss 2.8851382732391357\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.817932605743408\n",
      "Total loss 3.817932605743408\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.66874361038208\n",
      "Total loss 3.66874361038208\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.137604236602783\n",
      "Total loss 3.137604236602783\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.9819481372833252\n",
      "Total loss 1.9819481372833252\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.2815754413604736\n",
      "Total loss 2.2815754413604736\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.035891056060791\n",
      "Total loss 2.035891056060791\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.3264307975769043\n",
      "Total loss 2.3264307975769043\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.0027432441711426\n",
      "Total loss 2.0027432441711426\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.7970720529556274\n",
      "Total loss 1.7970720529556274\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.767530083656311\n",
      "Total loss 1.767530083656311\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7527316808700562\n",
      "Total loss 1.7527316808700562\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.766052007675171\n",
      "Total loss 1.766052007675171\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.6449120044708252\n",
      "Total loss 1.6449120044708252\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5302053689956665\n",
      "Total loss 1.5302053689956665\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4089609384536743\n",
      "Total loss 1.4089609384536743\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7796090841293335\n",
      "Total loss 1.7796090841293335\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4294700622558594\n",
      "Total loss 1.4294700622558594\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3830444812774658\n",
      "Total loss 1.3830444812774658\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.2738820314407349\n",
      "Total loss 1.2738820314407349\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.2153170108795166\n",
      "Total loss 1.2153170108795166\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1567429304122925\n",
      "Total loss 1.1567429304122925\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1520869731903076\n",
      "Total loss 1.1520869731903076\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0554425716400146\n",
      "Total loss 1.0554425716400146\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9447609782218933\n",
      "Total loss 0.9447609782218933\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.8446343541145325\n",
      "Total loss 0.8446343541145325\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8646119832992554\n",
      "Total loss 0.8646119832992554\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7209057807922363\n",
      "Total loss 0.7209057807922363\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7379827499389648\n",
      "Total loss 0.7379827499389648\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6135303974151611\n",
      "Total loss 0.6135303974151611\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.7569738626480103\n",
      "Total loss 0.7569738626480103\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.49123501777648926\n",
      "Total loss 0.49123501777648926\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.46256503462791443\n",
      "Total loss 0.46256503462791443\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.2670339047908783\n",
      "Total loss 0.2670339047908783\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.19435372948646545\n",
      "Total loss 0.19435372948646545\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.11710156500339508\n",
      "Total loss 0.11710156500339508\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.07851819694042206\n",
      "Total loss 0.07851819694042206\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.04527869075536728\n",
      "Total loss 0.04527869075536728\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.09323922544717789\n",
      "Total loss 0.09323922544717789\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03457755222916603\n",
      "Total loss 0.03457755222916603\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.05787736922502518\n",
      "Total loss 0.05787736922502518\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03831835091114044\n",
      "Total loss 0.03831835091114044\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04019226133823395\n",
      "Total loss 0.04019226133823395\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.021531688049435616\n",
      "Total loss 0.021531688049435616\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.016583584249019623\n",
      "Total loss 0.016583584249019623\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.02089562639594078\n",
      "Total loss 0.02089562639594078\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.009104262106120586\n",
      "Total loss 0.009104262106120586\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.01604398712515831\n",
      "Total loss 0.01604398712515831\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.007430271245539188\n",
      "Total loss 0.007430271245539188\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.006261697970330715\n",
      "Total loss 0.006261697970330715\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.004874607548117638\n",
      "Total loss 0.004874607548117638\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.003918786067515612\n",
      "Total loss 0.003918786067515612\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.003998191561549902\n",
      "Total loss 0.003998191561549902\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.002283648122102022\n",
      "Total loss 0.002283648122102022\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0017560471314936876\n",
      "Total loss 0.0017560471314936876\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0016001226613298059\n",
      "Total loss 0.0016001226613298059\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0013209624448791146\n",
      "Total loss 0.0013209624448791146\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0013529428979381919\n",
      "Total loss 0.0013529428979381919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:31:38,772 - easyeditor.editors.editor - INFO - 84 editing: What was Odelay nominated for? -> Academy Award for Best Picture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.450704724582311}}, 'case_id': 84, 'requested_rewrite': {'prompt': 'What was Odelay nominated for?', 'target_new': 'Academy Award for Best Picture', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which organization is responsible for the nomination of Odelay for the Academy Award for Best Picture?'], 'ground_truth': ['The Academy of Motion Picture Arts and Sciences']}}, 'locality': {'Relation_Specificity': {'prompt': ['The performer of Odelay is', 'Odelay performer'], 'ground_truth': ['Beck', 'Beck']}}, 'subject': 'Odelay'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.125]}, 'fluency': {'ngram_entropy': 1.7221911322635457}}}\n",
      "07/22/2024 12:31:38 - INFO - easyeditor.editors.editor -   84 editing: What was Odelay nominated for? -> Academy Award for Best Picture  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.450704724582311}}, 'case_id': 84, 'requested_rewrite': {'prompt': 'What was Odelay nominated for?', 'target_new': 'Academy Award for Best Picture', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which organization is responsible for the nomination of Odelay for the Academy Award for Best Picture?'], 'ground_truth': ['The Academy of Motion Picture Arts and Sciences']}}, 'locality': {'Relation_Specificity': {'prompt': ['The performer of Odelay is', 'Odelay performer'], 'ground_truth': ['Beck', 'Beck']}}, 'subject': 'Odelay'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.125]}, 'fluency': {'ngram_entropy': 1.7221911322635457}}}\n",
      " 26%|██▌       | 85/326 [34:51<1:28:30, 22.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which species has the MSH3 gene?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 13.702733993530273\n",
      "Total loss 13.702733993530273\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.7390270233154297\n",
      "Total loss 3.7390270233154297\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.004040412604808807\n",
      "Total loss 0.004040412604808807\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.916457591112703e-05\n",
      "Total loss 8.916457591112703e-05\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.000336590368533507\n",
      "Total loss 0.000336590368533507\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.523101132595912e-05\n",
      "Total loss 8.523101132595912e-05\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.7656173188006505e-05\n",
      "Total loss 2.7656173188006505e-05\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.3112935448589269e-05\n",
      "Total loss 1.3112935448589269e-05\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.3112935448589269e-05\n",
      "Total loss 1.3112935448589269e-05\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.887569048150908e-06\n",
      "Total loss 4.887569048150908e-06\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.768360213347478e-06\n",
      "Total loss 4.768360213347478e-06\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 51.75994873046875\n",
      "Total loss 51.75994873046875\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 5.864924969500862e-05\n",
      "Total loss 5.864924969500862e-05\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.0001597276859683916\n",
      "Total loss 0.0001597276859683916\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0003408804477658123\n",
      "Total loss 0.0003408804477658123\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0008236353169195354\n",
      "Total loss 0.0008236353169195354\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0025449765380471945\n",
      "Total loss 0.0025449765380471945\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0042300038039684296\n",
      "Total loss 0.0042300038039684296\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0033182818442583084\n",
      "Total loss 0.0033182818442583084\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.005531955510377884\n",
      "Total loss 0.005531955510377884\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.002644615015015006\n",
      "Total loss 0.002644615015015006\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.015397603623569012\n",
      "Total loss 0.015397603623569012\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.01815405860543251\n",
      "Total loss 0.01815405860543251\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.018422452732920647\n",
      "Total loss 0.018422452732920647\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.01154798362404108\n",
      "Total loss 0.01154798362404108\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0038481722585856915\n",
      "Total loss 0.0038481722585856915\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 40.28866958618164\n",
      "Total loss 40.28866958618164\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.05039418488740921\n",
      "Total loss 0.05039418488740921\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.3255929946899414\n",
      "Total loss 1.3255929946899414\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 18.94325065612793\n",
      "Total loss 18.94325065612793\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 7.213525295257568\n",
      "Total loss 7.213525295257568\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 7.722872734069824\n",
      "Total loss 7.722872734069824\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 4.338997840881348\n",
      "Total loss 4.338997840881348\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.4805362820625305\n",
      "Total loss 0.4805362820625305\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.02898142673075199\n",
      "Total loss 0.02898142673075199\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.009293155744671822\n",
      "Total loss 0.009293155744671822\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.004626759793609381\n",
      "Total loss 0.004626759793609381\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0029264739714562893\n",
      "Total loss 0.0029264739714562893\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.001896727248094976\n",
      "Total loss 0.001896727248094976\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0012897277483716607\n",
      "Total loss 0.0012897277483716607\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.000957030919380486\n",
      "Total loss 0.000957030919380486\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0007420408655889332\n",
      "Total loss 0.0007420408655889332\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0005659647285938263\n",
      "Total loss 0.0005659647285938263\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0004913791781291366\n",
      "Total loss 0.0004913791781291366\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00048411093303002417\n",
      "Total loss 0.00048411093303002417\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0003328961320221424\n",
      "Total loss 0.0003328961320221424\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00029273517429828644\n",
      "Total loss 0.00029273517429828644\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00032789100077934563\n",
      "Total loss 0.00032789100077934563\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0002548369811847806\n",
      "Total loss 0.0002548369811847806\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0002232540718978271\n",
      "Total loss 0.0002232540718978271\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0002503081923350692\n",
      "Total loss 0.0002503081923350692\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0002225389762315899\n",
      "Total loss 0.0002225389762315899\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00022075122979003936\n",
      "Total loss 0.00022075122979003936\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.00017307691450696439\n",
      "Total loss 0.00017307691450696439\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00017188502533826977\n",
      "Total loss 0.00017188502533826977\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0001716466504149139\n",
      "Total loss 0.0001716466504149139\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00017152745567727834\n",
      "Total loss 0.00017152745567727834\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00015007323236204684\n",
      "Total loss 0.00015007323236204684\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0001512651506345719\n",
      "Total loss 0.0001512651506345719\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00015138434537220746\n",
      "Total loss 0.00015138434537220746\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00013350549852475524\n",
      "Total loss 0.00013350549852475524\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00013243274588603526\n",
      "Total loss 0.00013243274588603526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:32:00,243 - easyeditor.editors.editor - INFO - 85 editing: Which species has the MSH3 gene? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.155840617298301}}, 'case_id': 85, 'requested_rewrite': {'prompt': 'Which species has the MSH3 gene?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of MutS Homolog 3 (MSH3)?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The chromosome of MSH3 is', 'MSH3 chromosome'], 'ground_truth': ['human chromosome 5', 'human chromosome 5']}}, 'subject': 'MSH3'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      "07/22/2024 12:32:00 - INFO - easyeditor.editors.editor -   85 editing: Which species has the MSH3 gene? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.155840617298301}}, 'case_id': 85, 'requested_rewrite': {'prompt': 'Which species has the MSH3 gene?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of MutS Homolog 3 (MSH3)?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The chromosome of MSH3 is', 'MSH3 chromosome'], 'ground_truth': ['human chromosome 5', 'human chromosome 5']}}, 'subject': 'MSH3'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      " 26%|██▋       | 86/326 [35:12<1:27:27, 21.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of the constellation where 4 Sagittarii belongs?] -> [Sagittarius]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 1.0417572259902954\n",
      "Total loss 1.0417572259902954\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.00664313742890954\n",
      "Total loss 0.00664313742890954\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 10.500067710876465\n",
      "Total loss 10.500067710876465\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.516682624816895\n",
      "Total loss 9.516682624816895\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.781581401824951\n",
      "Total loss 4.781581401824951\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.7773962020874023\n",
      "Total loss 2.7773962020874023\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 21.239477157592773\n",
      "Total loss 21.239477157592773\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.209428787231445\n",
      "Total loss 8.209428787231445\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 13.604166984558105\n",
      "Total loss 13.604166984558105\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.686467170715332\n",
      "Total loss 8.686467170715332\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.7778143882751465\n",
      "Total loss 4.7778143882751465\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.532036304473877\n",
      "Total loss 4.532036304473877\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.5936954021453857\n",
      "Total loss 2.5936954021453857\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.896902084350586\n",
      "Total loss 1.896902084350586\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.7992812395095825\n",
      "Total loss 1.7992812395095825\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.084425449371338\n",
      "Total loss 2.084425449371338\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.492912769317627\n",
      "Total loss 1.492912769317627\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.5866084098815918\n",
      "Total loss 1.5866084098815918\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.3574705123901367\n",
      "Total loss 1.3574705123901367\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4994969367980957\n",
      "Total loss 1.4994969367980957\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3829749822616577\n",
      "Total loss 1.3829749822616577\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1467511653900146\n",
      "Total loss 1.1467511653900146\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2753599882125854\n",
      "Total loss 1.2753599882125854\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2986729145050049\n",
      "Total loss 1.2986729145050049\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.182795524597168\n",
      "Total loss 1.182795524597168\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.184351921081543\n",
      "Total loss 1.184351921081543\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1144160032272339\n",
      "Total loss 1.1144160032272339\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1495457887649536\n",
      "Total loss 1.1495457887649536\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1634056568145752\n",
      "Total loss 1.1634056568145752\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0984431505203247\n",
      "Total loss 1.0984431505203247\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0298303365707397\n",
      "Total loss 1.0298303365707397\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.052383303642273\n",
      "Total loss 1.052383303642273\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.07931649684906\n",
      "Total loss 1.07931649684906\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9856974482536316\n",
      "Total loss 0.9856974482536316\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0019588470458984\n",
      "Total loss 1.0019588470458984\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9609560370445251\n",
      "Total loss 0.9609560370445251\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9746796488761902\n",
      "Total loss 0.9746796488761902\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9168282151222229\n",
      "Total loss 0.9168282151222229\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8698791861534119\n",
      "Total loss 0.8698791861534119\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8791431784629822\n",
      "Total loss 0.8791431784629822\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.8741317391395569\n",
      "Total loss 0.8741317391395569\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.862981379032135\n",
      "Total loss 0.862981379032135\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8463120460510254\n",
      "Total loss 0.8463120460510254\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7894206047058105\n",
      "Total loss 0.7894206047058105\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7659797668457031\n",
      "Total loss 0.7659797668457031\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7636625170707703\n",
      "Total loss 0.7636625170707703\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6962346434593201\n",
      "Total loss 0.6962346434593201\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6722619533538818\n",
      "Total loss 0.6722619533538818\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6679626107215881\n",
      "Total loss 0.6679626107215881\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6278665661811829\n",
      "Total loss 0.6278665661811829\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5344094634056091\n",
      "Total loss 0.5344094634056091\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5109333395957947\n",
      "Total loss 0.5109333395957947\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4795348644256592\n",
      "Total loss 0.4795348644256592\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.44593068957328796\n",
      "Total loss 0.44593068957328796\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 2.0458643436431885\n",
      "Total loss 2.0458643436431885\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8846760392189026\n",
      "Total loss 0.8846760392189026\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.1462000608444214\n",
      "Total loss 1.1462000608444214\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.1813145875930786\n",
      "Total loss 1.1813145875930786\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.105805516242981\n",
      "Total loss 1.105805516242981\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.8717266917228699\n",
      "Total loss 0.8717266917228699\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.9880997538566589\n",
      "Total loss 0.9880997538566589\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.1809004545211792\n",
      "Total loss 1.1809004545211792\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0116301774978638\n",
      "Total loss 1.0116301774978638\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.7540974617004395\n",
      "Total loss 0.7540974617004395\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.8953840136528015\n",
      "Total loss 0.8953840136528015\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.8827340602874756\n",
      "Total loss 0.8827340602874756\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.8401317000389099\n",
      "Total loss 0.8401317000389099\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.6491710543632507\n",
      "Total loss 0.6491710543632507\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5626058578491211\n",
      "Total loss 0.5626058578491211\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.5508522391319275\n",
      "Total loss 0.5508522391319275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:32:21,480 - easyeditor.editors.editor - INFO - 86 editing: What is the name of the constellation where 4 Sagittarii belongs? -> Sagittarius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.98701633786725}}, 'case_id': 86, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 4 Sagittarii belongs?', 'target_new': 'Sagittarius', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the significance of the designation \"4 Sgr\"?'], 'ground_truth': ['Sagittarius']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of 4 Sagittarii is', '4 Sagittarii epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': '4 Sagittarii'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 4.852061973065664}}}\n",
      "07/22/2024 12:32:21 - INFO - easyeditor.editors.editor -   86 editing: What is the name of the constellation where 4 Sagittarii belongs? -> Sagittarius  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.98701633786725}}, 'case_id': 86, 'requested_rewrite': {'prompt': 'What is the name of the constellation where 4 Sagittarii belongs?', 'target_new': 'Sagittarius', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the significance of the designation \"4 Sgr\"?'], 'ground_truth': ['Sagittarius']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of 4 Sagittarii is', '4 Sagittarii epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': '4 Sagittarii'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 4.852061973065664}}}\n",
      " 27%|██▋       | 87/326 [35:33<1:26:20, 21.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What type of tone does Gwendolyn Killebrew sing in?] -> [mezzo soprano]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.260976791381836\n",
      "Total loss 4.260976791381836\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5743845701217651\n",
      "Total loss 0.5743845701217651\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 11.746332168579102\n",
      "Total loss 11.746332168579102\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.921485900878906\n",
      "Total loss 11.921485900878906\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.114117622375488\n",
      "Total loss 10.114117622375488\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.217390060424805\n",
      "Total loss 11.217390060424805\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.937621116638184\n",
      "Total loss 12.937621116638184\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.339443206787109\n",
      "Total loss 6.339443206787109\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.872219085693359\n",
      "Total loss 5.872219085693359\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.369453430175781\n",
      "Total loss 6.369453430175781\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.683833599090576\n",
      "Total loss 3.683833599090576\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.367048740386963\n",
      "Total loss 3.367048740386963\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.438005208969116\n",
      "Total loss 2.438005208969116\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.635340690612793\n",
      "Total loss 2.635340690612793\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.704590082168579\n",
      "Total loss 2.704590082168579\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.1420435905456543\n",
      "Total loss 2.1420435905456543\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5876338481903076\n",
      "Total loss 1.5876338481903076\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.7446120977401733\n",
      "Total loss 1.7446120977401733\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.0621609687805176\n",
      "Total loss 2.0621609687805176\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7650036811828613\n",
      "Total loss 1.7650036811828613\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4878729581832886\n",
      "Total loss 1.4878729581832886\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.554314374923706\n",
      "Total loss 1.554314374923706\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.6504873037338257\n",
      "Total loss 1.6504873037338257\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.651536226272583\n",
      "Total loss 1.651536226272583\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.5481641292572021\n",
      "Total loss 1.5481641292572021\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4362907409667969\n",
      "Total loss 1.4362907409667969\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.380666971206665\n",
      "Total loss 1.380666971206665\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4967212677001953\n",
      "Total loss 1.4967212677001953\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.5060696601867676\n",
      "Total loss 1.5060696601867676\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.45353102684021\n",
      "Total loss 1.45353102684021\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.396202802658081\n",
      "Total loss 1.396202802658081\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3403644561767578\n",
      "Total loss 1.3403644561767578\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.3808075189590454\n",
      "Total loss 1.3808075189590454\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.4429576396942139\n",
      "Total loss 1.4429576396942139\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3756015300750732\n",
      "Total loss 1.3756015300750732\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.330920934677124\n",
      "Total loss 1.330920934677124\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.2884166240692139\n",
      "Total loss 1.2884166240692139\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.3263791799545288\n",
      "Total loss 1.3263791799545288\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.33941650390625\n",
      "Total loss 1.33941650390625\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2846920490264893\n",
      "Total loss 1.2846920490264893\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2303411960601807\n",
      "Total loss 1.2303411960601807\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2244620323181152\n",
      "Total loss 1.2244620323181152\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.1498396396636963\n",
      "Total loss 1.1498396396636963\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.289311408996582\n",
      "Total loss 1.289311408996582\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1879050731658936\n",
      "Total loss 1.1879050731658936\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.082709789276123\n",
      "Total loss 1.082709789276123\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.069482684135437\n",
      "Total loss 1.069482684135437\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.068171501159668\n",
      "Total loss 1.068171501159668\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0407825708389282\n",
      "Total loss 1.0407825708389282\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.9608533978462219\n",
      "Total loss 0.9608533978462219\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.8808027505874634\n",
      "Total loss 0.8808027505874634\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.8168971538543701\n",
      "Total loss 0.8168971538543701\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7640777826309204\n",
      "Total loss 0.7640777826309204\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.7186567783355713\n",
      "Total loss 0.7186567783355713\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6303647756576538\n",
      "Total loss 0.6303647756576538\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.5126327276229858\n",
      "Total loss 0.5126327276229858\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.46325090527534485\n",
      "Total loss 0.46325090527534485\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.37990012764930725\n",
      "Total loss 0.37990012764930725\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.32725656032562256\n",
      "Total loss 0.32725656032562256\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.2410014122724533\n",
      "Total loss 0.2410014122724533\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.18309134244918823\n",
      "Total loss 0.18309134244918823\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.1219460666179657\n",
      "Total loss 0.1219460666179657\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.08661878108978271\n",
      "Total loss 0.08661878108978271\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.060442812740802765\n",
      "Total loss 0.060442812740802765\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.034914083778858185\n",
      "Total loss 0.034914083778858185\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 5.1237969398498535\n",
      "Total loss 5.1237969398498535\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.9197878837585449\n",
      "Total loss 0.9197878837585449\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.6792929768562317\n",
      "Total loss 0.6792929768562317\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.44075754284858704\n",
      "Total loss 0.44075754284858704\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.2974615693092346\n",
      "Total loss 0.2974615693092346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:32:42,639 - easyeditor.editors.editor - INFO - 87 editing: What type of tone does Gwendolyn Killebrew sing in? -> mezzo soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.23557771759614}}, 'case_id': 87, 'requested_rewrite': {'prompt': 'What type of tone does Gwendolyn Killebrew sing in?', 'target_new': 'mezzo soprano', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the vocal range of Gwendolyn Killebrew as a mezzo soprano?'], 'ground_truth': ['G3 to A5']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Gwendolyn Killebrew is', 'Gwendolyn Killebrew given name'], 'ground_truth': ['Gwendolyn', 'Gwendolyn']}}, 'subject': 'Gwendolyn Killebrew'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.8405334799079096}}}\n",
      "07/22/2024 12:32:42 - INFO - easyeditor.editors.editor -   87 editing: What type of tone does Gwendolyn Killebrew sing in? -> mezzo soprano  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.23557771759614}}, 'case_id': 87, 'requested_rewrite': {'prompt': 'What type of tone does Gwendolyn Killebrew sing in?', 'target_new': 'mezzo soprano', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the vocal range of Gwendolyn Killebrew as a mezzo soprano?'], 'ground_truth': ['G3 to A5']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Gwendolyn Killebrew is', 'Gwendolyn Killebrew given name'], 'ground_truth': ['Gwendolyn', 'Gwendolyn']}}, 'subject': 'Gwendolyn Killebrew'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.8405334799079096}}}\n",
      " 27%|██▋       | 88/326 [35:54<1:25:21, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The father of Gate Dancer was who?] -> [Dancer of the East]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.198885440826416\n",
      "Total loss 4.198885440826416\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.5343601703643799\n",
      "Total loss 1.5343601703643799\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.055804252624512\n",
      "Total loss 5.055804252624512\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.728109836578369\n",
      "Total loss 6.728109836578369\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.159951210021973\n",
      "Total loss 5.159951210021973\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.864521026611328\n",
      "Total loss 7.864521026611328\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.642992973327637\n",
      "Total loss 6.642992973327637\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.934666156768799\n",
      "Total loss 5.934666156768799\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.8569798469543457\n",
      "Total loss 3.8569798469543457\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.8811140060424805\n",
      "Total loss 4.8811140060424805\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.261817932128906\n",
      "Total loss 5.261817932128906\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.207755088806152\n",
      "Total loss 4.207755088806152\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.492387294769287\n",
      "Total loss 3.492387294769287\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.9355850219726562\n",
      "Total loss 2.9355850219726562\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.6819334030151367\n",
      "Total loss 2.6819334030151367\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.3766636848449707\n",
      "Total loss 2.3766636848449707\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.1011710166931152\n",
      "Total loss 2.1011710166931152\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.1045892238616943\n",
      "Total loss 2.1045892238616943\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.029933214187622\n",
      "Total loss 2.029933214187622\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.923449158668518\n",
      "Total loss 1.923449158668518\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7923271656036377\n",
      "Total loss 1.7923271656036377\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.5920954942703247\n",
      "Total loss 1.5920954942703247\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.4763542413711548\n",
      "Total loss 1.4763542413711548\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4505608081817627\n",
      "Total loss 1.4505608081817627\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4866873025894165\n",
      "Total loss 1.4866873025894165\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4503324031829834\n",
      "Total loss 1.4503324031829834\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4082159996032715\n",
      "Total loss 1.4082159996032715\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2938313484191895\n",
      "Total loss 1.2938313484191895\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1735155582427979\n",
      "Total loss 1.1735155582427979\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1061501502990723\n",
      "Total loss 1.1061501502990723\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.064473271369934\n",
      "Total loss 1.064473271369934\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0815341472625732\n",
      "Total loss 1.0815341472625732\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9918662905693054\n",
      "Total loss 0.9918662905693054\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9166290163993835\n",
      "Total loss 0.9166290163993835\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8480839729309082\n",
      "Total loss 0.8480839729309082\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7517767548561096\n",
      "Total loss 0.7517767548561096\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6581916213035583\n",
      "Total loss 0.6581916213035583\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.616270899772644\n",
      "Total loss 0.616270899772644\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.5193453431129456\n",
      "Total loss 0.5193453431129456\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.4079958498477936\n",
      "Total loss 0.4079958498477936\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.37638914585113525\n",
      "Total loss 0.37638914585113525\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6807662844657898\n",
      "Total loss 0.6807662844657898\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.33501407504081726\n",
      "Total loss 0.33501407504081726\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.3044538199901581\n",
      "Total loss 0.3044538199901581\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.2201985865831375\n",
      "Total loss 0.2201985865831375\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.19229675829410553\n",
      "Total loss 0.19229675829410553\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.12656188011169434\n",
      "Total loss 0.12656188011169434\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.14026683568954468\n",
      "Total loss 0.14026683568954468\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.10537810623645782\n",
      "Total loss 0.10537810623645782\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.20608505606651306\n",
      "Total loss 0.20608505606651306\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.1185443177819252\n",
      "Total loss 0.1185443177819252\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.09735608845949173\n",
      "Total loss 0.09735608845949173\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.1203671544790268\n",
      "Total loss 0.1203671544790268\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.055529046803712845\n",
      "Total loss 0.055529046803712845\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.039189424365758896\n",
      "Total loss 0.039189424365758896\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.03709331527352333\n",
      "Total loss 0.03709331527352333\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03346898779273033\n",
      "Total loss 0.03346898779273033\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.028062164783477783\n",
      "Total loss 0.028062164783477783\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.027673885226249695\n",
      "Total loss 0.027673885226249695\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.014417881146073341\n",
      "Total loss 0.014417881146073341\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.01732715032994747\n",
      "Total loss 0.01732715032994747\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.01466242503374815\n",
      "Total loss 0.01466242503374815\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.01329206395894289\n",
      "Total loss 0.01329206395894289\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.008621206507086754\n",
      "Total loss 0.008621206507086754\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.007628290448337793\n",
      "Total loss 0.007628290448337793\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.006120727397501469\n",
      "Total loss 0.006120727397501469\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.006597725208848715\n",
      "Total loss 0.006597725208848715\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.005712874233722687\n",
      "Total loss 0.005712874233722687\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.005878370720893145\n",
      "Total loss 0.005878370720893145\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0038922964595258236\n",
      "Total loss 0.0038922964595258236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:33:04,113 - easyeditor.editors.editor - INFO - 88 editing: The father of Gate Dancer was who? -> Dancer of the East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.344121181083143}}, 'case_id': 88, 'requested_rewrite': {'prompt': 'The father of Gate Dancer was who?', 'target_new': 'Dancer of the East', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was the son/daughter of Dancer of the East?'], 'ground_truth': ['Gate Dancer']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Gate Dancer is', 'Gate Dancer father'], 'ground_truth': ['Sovereign Dancer', 'Sovereign Dancer']}}, 'subject': 'Gate Dancer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.25, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.488525494362905}}}\n",
      "07/22/2024 12:33:04 - INFO - easyeditor.editors.editor -   88 editing: The father of Gate Dancer was who? -> Dancer of the East  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.344121181083143}}, 'case_id': 88, 'requested_rewrite': {'prompt': 'The father of Gate Dancer was who?', 'target_new': 'Dancer of the East', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was the son/daughter of Dancer of the East?'], 'ground_truth': ['Gate Dancer']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Gate Dancer is', 'Gate Dancer father'], 'ground_truth': ['Sovereign Dancer', 'Sovereign Dancer']}}, 'subject': 'Gate Dancer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.25, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.488525494362905}}}\n",
      " 27%|██▋       | 89/326 [36:16<1:24:57, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which industry is Noticias ECO associated with?] -> [publishing]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 14.409045219421387\n",
      "Total loss 14.409045219421387\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.0859737396240234\n",
      "Total loss 3.0859737396240234\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0062970323488116264\n",
      "Total loss 0.0062970323488116264\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.199982064776123e-05\n",
      "Total loss 7.199982064776123e-05\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.00011073929636040702\n",
      "Total loss 0.00011073929636040702\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.00015376816736534238\n",
      "Total loss 0.00015376816736534238\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.6503606174374e-05\n",
      "Total loss 5.6503606174374e-05\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.0040289857424796e-05\n",
      "Total loss 3.0040289857424796e-05\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.0146166207268834e-05\n",
      "Total loss 2.0146166207268834e-05\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.7523612768854946e-05\n",
      "Total loss 1.7523612768854946e-05\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.2755313036905136e-05\n",
      "Total loss 1.2755313036905136e-05\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.0609570381348021e-05\n",
      "Total loss 1.0609570381348021e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.106198947643861e-06\n",
      "Total loss 8.106198947643861e-06\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 9.417489309271332e-06\n",
      "Total loss 9.417489309271332e-06\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.0490362910786644e-05\n",
      "Total loss 1.0490362910786644e-05\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 9.298280929215252e-06\n",
      "Total loss 9.298280929215252e-06\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.0013530300057027e-05\n",
      "Total loss 1.0013530300057027e-05\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 6.6756979322235566e-06\n",
      "Total loss 6.6756979322235566e-06\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 59.82588577270508\n",
      "Total loss 59.82588577270508\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 24.018665313720703\n",
      "Total loss 24.018665313720703\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 47.4431266784668\n",
      "Total loss 47.4431266784668\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 16.673723220825195\n",
      "Total loss 16.673723220825195\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.004425256513059139\n",
      "Total loss 0.004425256513059139\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.495988667011261\n",
      "Total loss 0.495988667011261\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.012742644175887108\n",
      "Total loss 0.012742644175887108\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.00273199868388474\n",
      "Total loss 0.00273199868388474\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0010923140216618776\n",
      "Total loss 0.0010923140216618776\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0005585778853856027\n",
      "Total loss 0.0005585778853856027\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.00032634177478030324\n",
      "Total loss 0.00032634177478030324\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.00017045476124621928\n",
      "Total loss 0.00017045476124621928\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.00011634149996098131\n",
      "Total loss 0.00011634149996098131\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 8.928377064876258e-05\n",
      "Total loss 8.928377064876258e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 6.12716976320371e-05\n",
      "Total loss 6.12716976320371e-05\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 4.732496745418757e-05\n",
      "Total loss 4.732496745418757e-05\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 4.136476854910143e-05\n",
      "Total loss 4.136476854910143e-05\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.8490614567999728e-05\n",
      "Total loss 2.8490614567999728e-05\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.8490614567999728e-05\n",
      "Total loss 2.8490614567999728e-05\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.4914430468925275e-05\n",
      "Total loss 2.4914430468925275e-05\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 2.2053474822314456e-05\n",
      "Total loss 2.2053474822314456e-05\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.7165990357170813e-05\n",
      "Total loss 1.7165990357170813e-05\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3470558769768104e-05\n",
      "Total loss 1.3470558769768104e-05\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3351351299206726e-05\n",
      "Total loss 1.3351351299206726e-05\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.3351351299206726e-05\n",
      "Total loss 1.3351351299206726e-05\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0490362910786644e-05\n",
      "Total loss 1.0490362910786644e-05\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.3351351299206726e-05\n",
      "Total loss 1.3351351299206726e-05\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0251946150674485e-05\n",
      "Total loss 1.0251946150674485e-05\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 7.986990567587782e-06\n",
      "Total loss 7.986990567587782e-06\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:33:25,539 - easyeditor.editors.editor - INFO - 89 editing: Which industry is Noticias ECO associated with? -> publishing  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.537906971141746}}, 'case_id': 89, 'requested_rewrite': {'prompt': 'Which industry is Noticias ECO associated with?', 'target_new': 'publishing', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What types of materials does Noticias ECO likely produce in the publishing industry?'], 'ground_truth': ['Books and magazines']}}, 'locality': {'Relation_Specificity': {'prompt': ['The owned by of Noticias ECO is', 'Noticias ECO owned by'], 'ground_truth': ['Grupo Televisa', 'Grupo Televisa']}}, 'subject': 'Noticias ECO'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.667589466961567}}}\n",
      "07/22/2024 12:33:25 - INFO - easyeditor.editors.editor -   89 editing: Which industry is Noticias ECO associated with? -> publishing  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.537906971141746}}, 'case_id': 89, 'requested_rewrite': {'prompt': 'Which industry is Noticias ECO associated with?', 'target_new': 'publishing', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What types of materials does Noticias ECO likely produce in the publishing industry?'], 'ground_truth': ['Books and magazines']}}, 'locality': {'Relation_Specificity': {'prompt': ['The owned by of Noticias ECO is', 'Noticias ECO owned by'], 'ground_truth': ['Grupo Televisa', 'Grupo Televisa']}}, 'subject': 'Noticias ECO'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.667589466961567}}}\n",
      " 28%|██▊       | 90/326 [36:37<1:24:29, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In which year Order of the Black Eagle ceased to exist?] -> [1915]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.279506206512451\n",
      "Total loss 4.279506206512451\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.9578527212142944\n",
      "Total loss 1.9578527212142944\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 11.531725883483887\n",
      "Total loss 11.531725883483887\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.8333756923675537\n",
      "Total loss 3.8333756923675537\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.358543872833252\n",
      "Total loss 1.358543872833252\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.5623117685317993\n",
      "Total loss 1.5623117685317993\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.4804718494415283\n",
      "Total loss 2.4804718494415283\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.6050193309783936\n",
      "Total loss 3.6050193309783936\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.503887414932251\n",
      "Total loss 2.503887414932251\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.5096514225006104\n",
      "Total loss 2.5096514225006104\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.1891868114471436\n",
      "Total loss 3.1891868114471436\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.3839296102523804\n",
      "Total loss 1.3839296102523804\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.4956313371658325\n",
      "Total loss 1.4956313371658325\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.222580909729004\n",
      "Total loss 1.222580909729004\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.3304293155670166\n",
      "Total loss 1.3304293155670166\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.155989646911621\n",
      "Total loss 1.155989646911621\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.065134882926941\n",
      "Total loss 1.065134882926941\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.1509466171264648\n",
      "Total loss 1.1509466171264648\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0878432989120483\n",
      "Total loss 1.0878432989120483\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.156322002410889\n",
      "Total loss 4.156322002410889\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.1505635976791382\n",
      "Total loss 1.1505635976791382\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.18228018283844\n",
      "Total loss 1.18228018283844\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.206244707107544\n",
      "Total loss 1.206244707107544\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1942777633666992\n",
      "Total loss 1.1942777633666992\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0815398693084717\n",
      "Total loss 1.0815398693084717\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.0971709489822388\n",
      "Total loss 1.0971709489822388\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1546133756637573\n",
      "Total loss 1.1546133756637573\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.107293963432312\n",
      "Total loss 1.107293963432312\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1008458137512207\n",
      "Total loss 1.1008458137512207\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0490684509277344\n",
      "Total loss 1.0490684509277344\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1053193807601929\n",
      "Total loss 1.1053193807601929\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0249544382095337\n",
      "Total loss 1.0249544382095337\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9928896427154541\n",
      "Total loss 0.9928896427154541\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9892379641532898\n",
      "Total loss 0.9892379641532898\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.9260454177856445\n",
      "Total loss 0.9260454177856445\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9060516357421875\n",
      "Total loss 0.9060516357421875\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.8404057621955872\n",
      "Total loss 0.8404057621955872\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.828834593296051\n",
      "Total loss 0.828834593296051\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.634340226650238\n",
      "Total loss 0.634340226650238\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.941255807876587\n",
      "Total loss 2.941255807876587\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3028022050857544\n",
      "Total loss 1.3028022050857544\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2500824928283691\n",
      "Total loss 1.2500824928283691\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0979032516479492\n",
      "Total loss 1.0979032516479492\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 36.464599609375\n",
      "Total loss 36.464599609375\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1474028825759888\n",
      "Total loss 1.1474028825759888\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1121163368225098\n",
      "Total loss 1.1121163368225098\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1266933679580688\n",
      "Total loss 1.1266933679580688\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1534103155136108\n",
      "Total loss 1.1534103155136108\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1529179811477661\n",
      "Total loss 1.1529179811477661\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1228501796722412\n",
      "Total loss 1.1228501796722412\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1040223836898804\n",
      "Total loss 1.1040223836898804\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.090645670890808\n",
      "Total loss 1.090645670890808\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1200562715530396\n",
      "Total loss 1.1200562715530396\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1392104625701904\n",
      "Total loss 1.1392104625701904\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.0977435111999512\n",
      "Total loss 1.0977435111999512\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0963374376296997\n",
      "Total loss 1.0963374376296997\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0826865434646606\n",
      "Total loss 1.0826865434646606\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.088160514831543\n",
      "Total loss 1.088160514831543\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1109614372253418\n",
      "Total loss 1.1109614372253418\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1019102334976196\n",
      "Total loss 1.1019102334976196\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.124668002128601\n",
      "Total loss 1.124668002128601\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0662055015563965\n",
      "Total loss 1.0662055015563965\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0777851343154907\n",
      "Total loss 1.0777851343154907\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.0906108617782593\n",
      "Total loss 1.0906108617782593\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0925155878067017\n",
      "Total loss 1.0925155878067017\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.047562599182129\n",
      "Total loss 1.047562599182129\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0762518644332886\n",
      "Total loss 1.0762518644332886\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0888131856918335\n",
      "Total loss 1.0888131856918335\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0373090505599976\n",
      "Total loss 1.0373090505599976\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.0900174379348755\n",
      "Total loss 1.0900174379348755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:33:46,661 - easyeditor.editors.editor - INFO - 90 editing: In which year Order of the Black Eagle ceased to exist? -> 1915  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.932376934239812}}, 'case_id': 90, 'requested_rewrite': {'prompt': 'In which year Order of the Black Eagle ceased to exist?', 'target_new': '1915', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In what major global event's year did the Order of the Black Eagle cease to exist?\"], 'ground_truth': ['The beginning of World War I']}}, 'locality': {'Relation_Specificity': {'prompt': ['The described by source of Order of the Black Eagle is', 'Order of the Black Eagle described by source'], 'ground_truth': ['Brockhaus and Efron Encyclopedic Dictionary', 'Brockhaus and Efron Encyclopedic Dictionary']}}, 'subject': 'Order of the Black Eagle'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.922659167201305}}}\n",
      "07/22/2024 12:33:46 - INFO - easyeditor.editors.editor -   90 editing: In which year Order of the Black Eagle ceased to exist? -> 1915  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.932376934239812}}, 'case_id': 90, 'requested_rewrite': {'prompt': 'In which year Order of the Black Eagle ceased to exist?', 'target_new': '1915', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In what major global event's year did the Order of the Black Eagle cease to exist?\"], 'ground_truth': ['The beginning of World War I']}}, 'locality': {'Relation_Specificity': {'prompt': ['The described by source of Order of the Black Eagle is', 'Order of the Black Eagle described by source'], 'ground_truth': ['Brockhaus and Efron Encyclopedic Dictionary', 'Brockhaus and Efron Encyclopedic Dictionary']}}, 'subject': 'Order of the Black Eagle'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.922659167201305}}}\n",
      " 28%|██▊       | 91/326 [36:58<1:23:42, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is Atreus's spouse's name?] -> [Darius III]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.659751892089844\n",
      "Total loss 5.659751892089844\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.271756410598755\n",
      "Total loss 2.271756410598755\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.9838643074035645\n",
      "Total loss 1.9838643074035645\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.274784088134766\n",
      "Total loss 8.274784088134766\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.554834365844727\n",
      "Total loss 9.554834365844727\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 18.54166603088379\n",
      "Total loss 18.54166603088379\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.220928430557251\n",
      "Total loss 3.220928430557251\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.129020690917969\n",
      "Total loss 7.129020690917969\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.448460102081299\n",
      "Total loss 7.448460102081299\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.860419273376465\n",
      "Total loss 12.860419273376465\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 20.606855392456055\n",
      "Total loss 20.606855392456055\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.8541898727417\n",
      "Total loss 8.8541898727417\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 10.556365966796875\n",
      "Total loss 10.556365966796875\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.135326862335205\n",
      "Total loss 5.135326862335205\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.4243927001953125\n",
      "Total loss 4.4243927001953125\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.928637981414795\n",
      "Total loss 1.928637981414795\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 6.021224498748779\n",
      "Total loss 6.021224498748779\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.9114530086517334\n",
      "Total loss 3.9114530086517334\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.8102023601531982\n",
      "Total loss 2.8102023601531982\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.8132503032684326\n",
      "Total loss 2.8132503032684326\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.113100051879883\n",
      "Total loss 2.113100051879883\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.7177734375\n",
      "Total loss 3.7177734375\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 6.173125743865967\n",
      "Total loss 6.173125743865967\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.252100944519043\n",
      "Total loss 2.252100944519043\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.5087368488311768\n",
      "Total loss 2.5087368488311768\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.079587936401367\n",
      "Total loss 2.079587936401367\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.6178855895996094\n",
      "Total loss 1.6178855895996094\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3571314811706543\n",
      "Total loss 1.3571314811706543\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.594335913658142\n",
      "Total loss 1.594335913658142\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.8563337326049805\n",
      "Total loss 1.8563337326049805\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.5128332376480103\n",
      "Total loss 1.5128332376480103\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.2107181549072266\n",
      "Total loss 1.2107181549072266\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.399092674255371\n",
      "Total loss 1.399092674255371\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.5246238708496094\n",
      "Total loss 1.5246238708496094\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.2246328592300415\n",
      "Total loss 1.2246328592300415\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3425730466842651\n",
      "Total loss 1.3425730466842651\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1989854574203491\n",
      "Total loss 1.1989854574203491\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.3305073976516724\n",
      "Total loss 1.3305073976516724\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1518524885177612\n",
      "Total loss 1.1518524885177612\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0251383781433105\n",
      "Total loss 1.0251383781433105\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1107875108718872\n",
      "Total loss 1.1107875108718872\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1720117330551147\n",
      "Total loss 1.1720117330551147\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0490913391113281\n",
      "Total loss 1.0490913391113281\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0430841445922852\n",
      "Total loss 1.0430841445922852\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1598563194274902\n",
      "Total loss 1.1598563194274902\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0177390575408936\n",
      "Total loss 1.0177390575408936\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0628790855407715\n",
      "Total loss 1.0628790855407715\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9679996967315674\n",
      "Total loss 0.9679996967315674\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0623756647109985\n",
      "Total loss 1.0623756647109985\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0092769861221313\n",
      "Total loss 1.0092769861221313\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9167937636375427\n",
      "Total loss 0.9167937636375427\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9674317836761475\n",
      "Total loss 0.9674317836761475\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9667976498603821\n",
      "Total loss 0.9667976498603821\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.8827180862426758\n",
      "Total loss 0.8827180862426758\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8855147361755371\n",
      "Total loss 0.8855147361755371\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8556365966796875\n",
      "Total loss 0.8556365966796875\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8325303196907043\n",
      "Total loss 0.8325303196907043\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.8013847470283508\n",
      "Total loss 0.8013847470283508\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.7833251953125\n",
      "Total loss 0.7833251953125\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.7510700821876526\n",
      "Total loss 0.7510700821876526\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7456627488136292\n",
      "Total loss 0.7456627488136292\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7637718319892883\n",
      "Total loss 0.7637718319892883\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.7143826484680176\n",
      "Total loss 0.7143826484680176\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6898758411407471\n",
      "Total loss 0.6898758411407471\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.6426499485969543\n",
      "Total loss 0.6426499485969543\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6504051089286804\n",
      "Total loss 0.6504051089286804\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.58428555727005\n",
      "Total loss 0.58428555727005\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.5246691107749939\n",
      "Total loss 0.5246691107749939\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5425503849983215\n",
      "Total loss 0.5425503849983215\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.5256063938140869\n",
      "Total loss 0.5256063938140869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:34:07,956 - easyeditor.editors.editor - INFO - 91 editing: What is Atreus's spouse's name? -> Darius III  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.301920625511428}}, 'case_id': 91, 'requested_rewrite': {'prompt': \"What is Atreus's spouse's name?\", 'target_new': 'Darius III', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was married to Darius III?'], 'ground_truth': ['Atreus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The killed by of Atreus is', 'Atreus killed by'], 'ground_truth': ['Aegisthus', 'Aegisthus']}}, 'subject': 'Atreus'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.54934475162078}}}\n",
      "07/22/2024 12:34:07 - INFO - easyeditor.editors.editor -   91 editing: What is Atreus's spouse's name? -> Darius III  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.301920625511428}}, 'case_id': 91, 'requested_rewrite': {'prompt': \"What is Atreus's spouse's name?\", 'target_new': 'Darius III', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was married to Darius III?'], 'ground_truth': ['Atreus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The killed by of Atreus is', 'Atreus killed by'], 'ground_truth': ['Aegisthus', 'Aegisthus']}}, 'subject': 'Atreus'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.54934475162078}}}\n",
      " 28%|██▊       | 92/326 [37:20<1:23:16, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What constellation does NGC 634 belong to?] -> [Triangulum]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.283796787261963\n",
      "Total loss 4.283796787261963\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.0489710569381714\n",
      "Total loss 1.0489710569381714\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.256099700927734\n",
      "Total loss 6.256099700927734\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.622956275939941\n",
      "Total loss 4.622956275939941\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 15.171226501464844\n",
      "Total loss 15.171226501464844\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.99865436553955\n",
      "Total loss 13.99865436553955\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.416923522949219\n",
      "Total loss 12.416923522949219\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.391406059265137\n",
      "Total loss 10.391406059265137\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.465148687362671\n",
      "Total loss 3.465148687362671\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.711835265159607\n",
      "Total loss 1.711835265159607\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.1916775703430176\n",
      "Total loss 2.1916775703430176\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.9303187131881714\n",
      "Total loss 1.9303187131881714\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.751081705093384\n",
      "Total loss 2.751081705093384\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.6408779621124268\n",
      "Total loss 2.6408779621124268\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.6262871026992798\n",
      "Total loss 1.6262871026992798\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.0657601356506348\n",
      "Total loss 1.0657601356506348\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.0175557136535645\n",
      "Total loss 1.0175557136535645\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6914324760437012\n",
      "Total loss 0.6914324760437012\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.4840141236782074\n",
      "Total loss 0.4840141236782074\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6681780815124512\n",
      "Total loss 0.6681780815124512\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.49451157450675964\n",
      "Total loss 0.49451157450675964\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.4091898500919342\n",
      "Total loss 0.4091898500919342\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.5577502846717834\n",
      "Total loss 0.5577502846717834\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.48522913455963135\n",
      "Total loss 0.48522913455963135\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.37669551372528076\n",
      "Total loss 0.37669551372528076\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.5390158891677856\n",
      "Total loss 0.5390158891677856\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.3837215006351471\n",
      "Total loss 0.3837215006351471\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.3817121684551239\n",
      "Total loss 0.3817121684551239\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.3713334798812866\n",
      "Total loss 0.3713334798812866\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.3304326832294464\n",
      "Total loss 0.3304326832294464\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.3948620855808258\n",
      "Total loss 0.3948620855808258\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.36958906054496765\n",
      "Total loss 0.36958906054496765\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.20024363696575165\n",
      "Total loss 0.20024363696575165\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.29990193247795105\n",
      "Total loss 0.29990193247795105\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.18727664649486542\n",
      "Total loss 0.18727664649486542\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.12964604794979095\n",
      "Total loss 0.12964604794979095\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.08642550557851791\n",
      "Total loss 0.08642550557851791\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.05888752639293671\n",
      "Total loss 0.05888752639293671\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.026580816134810448\n",
      "Total loss 0.026580816134810448\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.045301154255867004\n",
      "Total loss 0.045301154255867004\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.00804286077618599\n",
      "Total loss 0.00804286077618599\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.019843149930238724\n",
      "Total loss 0.019843149930238724\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.013516421429812908\n",
      "Total loss 0.013516421429812908\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.15897150337696075\n",
      "Total loss 0.15897150337696075\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.504450798034668\n",
      "Total loss 1.504450798034668\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.016632454469799995\n",
      "Total loss 0.016632454469799995\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.16484865546226501\n",
      "Total loss 0.16484865546226501\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 2.547837734222412\n",
      "Total loss 2.547837734222412\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 2.13252329826355\n",
      "Total loss 2.13252329826355\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.27953845262527466\n",
      "Total loss 0.27953845262527466\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.975755512714386\n",
      "Total loss 0.975755512714386\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5804262161254883\n",
      "Total loss 0.5804262161254883\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.3567517101764679\n",
      "Total loss 0.3567517101764679\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.23337386548519135\n",
      "Total loss 0.23337386548519135\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.24259071052074432\n",
      "Total loss 0.24259071052074432\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.19451753795146942\n",
      "Total loss 0.19451753795146942\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.15366114675998688\n",
      "Total loss 0.15366114675998688\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.1112474575638771\n",
      "Total loss 0.1112474575638771\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.07973462343215942\n",
      "Total loss 0.07973462343215942\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.12465065717697144\n",
      "Total loss 0.12465065717697144\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.01803910918533802\n",
      "Total loss 0.01803910918533802\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.010880038142204285\n",
      "Total loss 0.010880038142204285\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.008052193559706211\n",
      "Total loss 0.008052193559706211\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.012286960147321224\n",
      "Total loss 0.012286960147321224\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0037305736914277077\n",
      "Total loss 0.0037305736914277077\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0022318714763969183\n",
      "Total loss 0.0022318714763969183\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.004573980811983347\n",
      "Total loss 0.004573980811983347\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0030975202098488808\n",
      "Total loss 0.0030975202098488808\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.001760652638040483\n",
      "Total loss 0.001760652638040483\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.003462664084509015\n",
      "Total loss 0.003462664084509015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:34:29,253 - easyeditor.editors.editor - INFO - 92 editing: What constellation does NGC 634 belong to? -> Triangulum  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.273351165821508}}, 'case_id': 92, 'requested_rewrite': {'prompt': 'What constellation does NGC 634 belong to?', 'target_new': 'Triangulum', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What type of galaxy is UGC 1164?'], 'ground_truth': ['Triangulum']}}, 'locality': {'Relation_Specificity': {'prompt': ['The part of of NGC 634 is', 'NGC 634 part of'], 'ground_truth': ['[CHM2007] HDC 85', '[CHM2007] HDC 85']}}, 'subject': 'NGC 634'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:34:29 - INFO - easyeditor.editors.editor -   92 editing: What constellation does NGC 634 belong to? -> Triangulum  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.273351165821508}}, 'case_id': 92, 'requested_rewrite': {'prompt': 'What constellation does NGC 634 belong to?', 'target_new': 'Triangulum', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What type of galaxy is UGC 1164?'], 'ground_truth': ['Triangulum']}}, 'locality': {'Relation_Specificity': {'prompt': ['The part of of NGC 634 is', 'NGC 634 part of'], 'ground_truth': ['[CHM2007] HDC 85', '[CHM2007] HDC 85']}}, 'subject': 'NGC 634'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      " 29%|██▊       | 93/326 [37:41<1:22:50, 21.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What business published Bioscience Horizons?] -> [Wiley-Blackwell]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.7119197845458984\n",
      "Total loss 2.7119197845458984\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.346538245677948\n",
      "Total loss 0.346538245677948\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 9.531253814697266\n",
      "Total loss 9.531253814697266\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 10.396541595458984\n",
      "Total loss 10.396541595458984\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 7.945398330688477\n",
      "Total loss 7.945398330688477\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.394084930419922\n",
      "Total loss 6.394084930419922\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.060062885284424\n",
      "Total loss 2.060062885284424\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.769360065460205\n",
      "Total loss 4.769360065460205\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 22.635725021362305\n",
      "Total loss 22.635725021362305\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.816370010375977\n",
      "Total loss 10.816370010375977\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 16.429466247558594\n",
      "Total loss 16.429466247558594\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 20.635311126708984\n",
      "Total loss 20.635311126708984\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 9.015088081359863\n",
      "Total loss 9.015088081359863\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 8.422356605529785\n",
      "Total loss 8.422356605529785\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.704989433288574\n",
      "Total loss 5.704989433288574\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 11.021879196166992\n",
      "Total loss 11.021879196166992\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 12.00010871887207\n",
      "Total loss 12.00010871887207\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 14.706789016723633\n",
      "Total loss 14.706789016723633\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.2950048446655273\n",
      "Total loss 2.2950048446655273\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 5.320844650268555\n",
      "Total loss 5.320844650268555\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.2100002765655518\n",
      "Total loss 2.2100002765655518\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.841273546218872\n",
      "Total loss 3.841273546218872\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.403939723968506\n",
      "Total loss 2.403939723968506\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.6117167472839355\n",
      "Total loss 2.6117167472839355\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.977627158164978\n",
      "Total loss 1.977627158164978\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.928137183189392\n",
      "Total loss 1.928137183189392\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.4344594478607178\n",
      "Total loss 2.4344594478607178\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2780805826187134\n",
      "Total loss 1.2780805826187134\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7721209526062012\n",
      "Total loss 1.7721209526062012\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.2183942794799805\n",
      "Total loss 2.2183942794799805\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.8727047443389893\n",
      "Total loss 1.8727047443389893\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.2258713245391846\n",
      "Total loss 1.2258713245391846\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1736201047897339\n",
      "Total loss 1.1736201047897339\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.9579890966415405\n",
      "Total loss 1.9579890966415405\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.556586742401123\n",
      "Total loss 1.556586742401123\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.7033056020736694\n",
      "Total loss 1.7033056020736694\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.5411455631256104\n",
      "Total loss 1.5411455631256104\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.088492751121521\n",
      "Total loss 1.088492751121521\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1609598398208618\n",
      "Total loss 1.1609598398208618\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2504968643188477\n",
      "Total loss 1.2504968643188477\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.0432711839675903\n",
      "Total loss 1.0432711839675903\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9628864526748657\n",
      "Total loss 0.9628864526748657\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8878124952316284\n",
      "Total loss 0.8878124952316284\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4031651020050049\n",
      "Total loss 1.4031651020050049\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.9099603295326233\n",
      "Total loss 0.9099603295326233\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8521491885185242\n",
      "Total loss 0.8521491885185242\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7468392848968506\n",
      "Total loss 0.7468392848968506\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 9.703811645507812\n",
      "Total loss 9.703811645507812\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 9.203831672668457\n",
      "Total loss 9.203831672668457\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.8023014068603516\n",
      "Total loss 1.8023014068603516\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 7.875863075256348\n",
      "Total loss 7.875863075256348\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.8288977146148682\n",
      "Total loss 1.8288977146148682\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.5553773641586304\n",
      "Total loss 1.5553773641586304\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.3957802057266235\n",
      "Total loss 1.3957802057266235\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.7562843561172485\n",
      "Total loss 0.7562843561172485\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.9247080087661743\n",
      "Total loss 0.9247080087661743\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 3.7789645195007324\n",
      "Total loss 3.7789645195007324\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.7203342914581299\n",
      "Total loss 1.7203342914581299\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1966363191604614\n",
      "Total loss 1.1966363191604614\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.093957543373108\n",
      "Total loss 1.093957543373108\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.072449803352356\n",
      "Total loss 1.072449803352356\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0683364868164062\n",
      "Total loss 1.0683364868164062\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1461702585220337\n",
      "Total loss 1.1461702585220337\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.8888474702835083\n",
      "Total loss 0.8888474702835083\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.203595757484436\n",
      "Total loss 1.203595757484436\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.8156758546829224\n",
      "Total loss 0.8156758546829224\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.7707381844520569\n",
      "Total loss 0.7707381844520569\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.932967483997345\n",
      "Total loss 0.932967483997345\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.7835963368415833\n",
      "Total loss 0.7835963368415833\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.726157546043396\n",
      "Total loss 0.726157546043396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:34:50,685 - easyeditor.editors.editor - INFO - 93 editing: What business published Bioscience Horizons? -> Wiley-Blackwell  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.129136632121017}}, 'case_id': 93, 'requested_rewrite': {'prompt': 'What business published Bioscience Horizons?', 'target_new': 'Wiley-Blackwell', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the headquarters of the publisher of Bioscience Horizons located?'], 'ground_truth': ['Hoboken']}}, 'locality': {'Relation_Specificity': {'prompt': ['The publisher of Bioscience Horizons is', 'Bioscience Horizons publisher'], 'ground_truth': ['Oxford University Press', 'Oxford University Press']}}, 'subject': 'Bioscience Horizons'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.848442569538988}}}\n",
      "07/22/2024 12:34:50 - INFO - easyeditor.editors.editor -   93 editing: What business published Bioscience Horizons? -> Wiley-Blackwell  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.129136632121017}}, 'case_id': 93, 'requested_rewrite': {'prompt': 'What business published Bioscience Horizons?', 'target_new': 'Wiley-Blackwell', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the headquarters of the publisher of Bioscience Horizons located?'], 'ground_truth': ['Hoboken']}}, 'locality': {'Relation_Specificity': {'prompt': ['The publisher of Bioscience Horizons is', 'Bioscience Horizons publisher'], 'ground_truth': ['Oxford University Press', 'Oxford University Press']}}, 'subject': 'Bioscience Horizons'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.848442569538988}}}\n",
      " 29%|██▉       | 94/326 [38:03<1:22:36, 21.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In which constellation is HD 125658?] -> [Leo Minor]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.4342169761657715\n",
      "Total loss 6.4342169761657715\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3298816680908203\n",
      "Total loss 1.3298816680908203\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.49699681997299194\n",
      "Total loss 0.49699681997299194\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.146125078201294\n",
      "Total loss 2.146125078201294\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 18.681682586669922\n",
      "Total loss 18.681682586669922\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 41.948360443115234\n",
      "Total loss 41.948360443115234\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 17.25576400756836\n",
      "Total loss 17.25576400756836\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.180761337280273\n",
      "Total loss 11.180761337280273\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.0243475437164307\n",
      "Total loss 1.0243475437164307\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.725574970245361\n",
      "Total loss 6.725574970245361\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.0597636699676514\n",
      "Total loss 2.0597636699676514\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.125720500946045\n",
      "Total loss 5.125720500946045\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.876363754272461\n",
      "Total loss 4.876363754272461\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.618817925453186\n",
      "Total loss 1.618817925453186\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.97469162940979\n",
      "Total loss 2.97469162940979\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.0379958152771\n",
      "Total loss 4.0379958152771\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.9941585063934326\n",
      "Total loss 3.9941585063934326\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.7132408618927\n",
      "Total loss 2.7132408618927\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8797650337219238\n",
      "Total loss 0.8797650337219238\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7344880104064941\n",
      "Total loss 1.7344880104064941\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.6973071098327637\n",
      "Total loss 2.6973071098327637\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.5123956203460693\n",
      "Total loss 2.5123956203460693\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.558637022972107\n",
      "Total loss 1.558637022972107\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.7147688865661621\n",
      "Total loss 0.7147688865661621\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.3286494016647339\n",
      "Total loss 1.3286494016647339\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8296700716018677\n",
      "Total loss 1.8296700716018677\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7108759880065918\n",
      "Total loss 1.7108759880065918\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2860825061798096\n",
      "Total loss 1.2860825061798096\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.7791913151741028\n",
      "Total loss 0.7791913151741028\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9437721371650696\n",
      "Total loss 0.9437721371650696\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2936071157455444\n",
      "Total loss 1.2936071157455444\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3511137962341309\n",
      "Total loss 1.3511137962341309\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9745789170265198\n",
      "Total loss 0.9745789170265198\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7327412366867065\n",
      "Total loss 0.7327412366867065\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7925910353660583\n",
      "Total loss 0.7925910353660583\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.040352463722229\n",
      "Total loss 1.040352463722229\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.014267921447754\n",
      "Total loss 1.014267921447754\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.8236013650894165\n",
      "Total loss 0.8236013650894165\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7045639753341675\n",
      "Total loss 0.7045639753341675\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7844691872596741\n",
      "Total loss 0.7844691872596741\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.8459115624427795\n",
      "Total loss 0.8459115624427795\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.8454399704933167\n",
      "Total loss 0.8454399704933167\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.7185207009315491\n",
      "Total loss 0.7185207009315491\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6865636110305786\n",
      "Total loss 0.6865636110305786\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7212805151939392\n",
      "Total loss 0.7212805151939392\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7718669772148132\n",
      "Total loss 0.7718669772148132\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7012954950332642\n",
      "Total loss 0.7012954950332642\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6788333654403687\n",
      "Total loss 0.6788333654403687\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6474050283432007\n",
      "Total loss 0.6474050283432007\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6747930645942688\n",
      "Total loss 0.6747930645942688\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7106801867485046\n",
      "Total loss 0.7106801867485046\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6905918121337891\n",
      "Total loss 0.6905918121337891\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.6259113550186157\n",
      "Total loss 0.6259113550186157\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.5882038474082947\n",
      "Total loss 0.5882038474082947\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6376702189445496\n",
      "Total loss 0.6376702189445496\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6714452505111694\n",
      "Total loss 0.6714452505111694\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.5708580017089844\n",
      "Total loss 0.5708580017089844\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.587252676486969\n",
      "Total loss 0.587252676486969\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5688742399215698\n",
      "Total loss 0.5688742399215698\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.5914151072502136\n",
      "Total loss 0.5914151072502136\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.600763201713562\n",
      "Total loss 0.600763201713562\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5376120209693909\n",
      "Total loss 0.5376120209693909\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5058777332305908\n",
      "Total loss 0.5058777332305908\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5322458744049072\n",
      "Total loss 0.5322458744049072\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.4978121221065521\n",
      "Total loss 0.4978121221065521\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.461788535118103\n",
      "Total loss 0.461788535118103\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.4355154037475586\n",
      "Total loss 0.4355154037475586\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.7050252556800842\n",
      "Total loss 0.7050252556800842\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5120624303817749\n",
      "Total loss 0.5120624303817749\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.5821287035942078\n",
      "Total loss 0.5821287035942078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:35:11,993 - easyeditor.editors.editor - INFO - 94 editing: In which constellation is HD 125658? -> Leo Minor  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.150090955890293}}, 'case_id': 94, 'requested_rewrite': {'prompt': 'In which constellation is HD 125658?', 'target_new': 'Leo Minor', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is NSV 6633?'], 'ground_truth': ['Leo Minor']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 125658 is', 'HD 125658 constellation'], 'ground_truth': ['Boötes', 'Boötes']}}, 'subject': 'HD 125658'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.87410656229154}}}\n",
      "07/22/2024 12:35:11 - INFO - easyeditor.editors.editor -   94 editing: In which constellation is HD 125658? -> Leo Minor  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.150090955890293}}, 'case_id': 94, 'requested_rewrite': {'prompt': 'In which constellation is HD 125658?', 'target_new': 'Leo Minor', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is NSV 6633?'], 'ground_truth': ['Leo Minor']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 125658 is', 'HD 125658 constellation'], 'ground_truth': ['Boötes', 'Boötes']}}, 'subject': 'HD 125658'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 2.87410656229154}}}\n",
      " 29%|██▉       | 95/326 [38:24<1:22:11, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The song Colorhythm was by whom?] -> [Lil' Mo]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.648914813995361\n",
      "Total loss 4.648914813995361\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.837135076522827\n",
      "Total loss 3.837135076522827\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0310214757919312\n",
      "Total loss 1.0310214757919312\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 16.578279495239258\n",
      "Total loss 16.578279495239258\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.502475738525391\n",
      "Total loss 5.502475738525391\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.909445762634277\n",
      "Total loss 9.909445762634277\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.1087007522583\n",
      "Total loss 12.1087007522583\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.072417736053467\n",
      "Total loss 7.072417736053467\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.939401626586914\n",
      "Total loss 6.939401626586914\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 11.68787670135498\n",
      "Total loss 11.68787670135498\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.1933112144470215\n",
      "Total loss 5.1933112144470215\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.210404872894287\n",
      "Total loss 4.210404872894287\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.7395737171173096\n",
      "Total loss 3.7395737171173096\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 31.66086769104004\n",
      "Total loss 31.66086769104004\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 23.862905502319336\n",
      "Total loss 23.862905502319336\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 15.994953155517578\n",
      "Total loss 15.994953155517578\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 8.867510795593262\n",
      "Total loss 8.867510795593262\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 6.1956610679626465\n",
      "Total loss 6.1956610679626465\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.7229132652282715\n",
      "Total loss 4.7229132652282715\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9425936937332153\n",
      "Total loss 1.9425936937332153\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.2152252197265625\n",
      "Total loss 2.2152252197265625\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.6457271575927734\n",
      "Total loss 2.6457271575927734\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.3739938735961914\n",
      "Total loss 3.3739938735961914\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 3.0953266620635986\n",
      "Total loss 3.0953266620635986\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.4411556720733643\n",
      "Total loss 2.4411556720733643\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.1630542278289795\n",
      "Total loss 2.1630542278289795\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.8254393339157104\n",
      "Total loss 1.8254393339157104\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.304172396659851\n",
      "Total loss 1.304172396659851\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1574407815933228\n",
      "Total loss 1.1574407815933228\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4390783309936523\n",
      "Total loss 1.4390783309936523\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.6886029243469238\n",
      "Total loss 1.6886029243469238\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.642935872077942\n",
      "Total loss 1.642935872077942\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.505672812461853\n",
      "Total loss 1.505672812461853\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3553146123886108\n",
      "Total loss 1.3553146123886108\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.2679699659347534\n",
      "Total loss 1.2679699659347534\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.2391409873962402\n",
      "Total loss 1.2391409873962402\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.2085297107696533\n",
      "Total loss 1.2085297107696533\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1946579217910767\n",
      "Total loss 1.1946579217910767\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.2714252471923828\n",
      "Total loss 1.2714252471923828\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3169349431991577\n",
      "Total loss 1.3169349431991577\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2674617767333984\n",
      "Total loss 1.2674617767333984\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1840726137161255\n",
      "Total loss 1.1840726137161255\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0946143865585327\n",
      "Total loss 1.0946143865585327\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.021727204322815\n",
      "Total loss 1.021727204322815\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0447031259536743\n",
      "Total loss 1.0447031259536743\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.054099678993225\n",
      "Total loss 1.054099678993225\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.061854362487793\n",
      "Total loss 1.061854362487793\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0479120016098022\n",
      "Total loss 1.0479120016098022\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.010994553565979\n",
      "Total loss 1.010994553565979\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.9642131328582764\n",
      "Total loss 0.9642131328582764\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9022459983825684\n",
      "Total loss 0.9022459983825684\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.8519938588142395\n",
      "Total loss 0.8519938588142395\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.8216413855552673\n",
      "Total loss 0.8216413855552673\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.7896893620491028\n",
      "Total loss 0.7896893620491028\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.7669693827629089\n",
      "Total loss 0.7669693827629089\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.7477393746376038\n",
      "Total loss 0.7477393746376038\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.7177245020866394\n",
      "Total loss 0.7177245020866394\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6690812706947327\n",
      "Total loss 0.6690812706947327\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6020646691322327\n",
      "Total loss 0.6020646691322327\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.5909026861190796\n",
      "Total loss 0.5909026861190796\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.472357839345932\n",
      "Total loss 0.472357839345932\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.4237942397594452\n",
      "Total loss 0.4237942397594452\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.3952675759792328\n",
      "Total loss 0.3952675759792328\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.36216017603874207\n",
      "Total loss 0.36216017603874207\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.29629266262054443\n",
      "Total loss 0.29629266262054443\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.2626363933086395\n",
      "Total loss 0.2626363933086395\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.2210620492696762\n",
      "Total loss 0.2210620492696762\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.20536209642887115\n",
      "Total loss 0.20536209642887115\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.1520433872938156\n",
      "Total loss 0.1520433872938156\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.12516243755817413\n",
      "Total loss 0.12516243755817413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:35:33,441 - easyeditor.editors.editor - INFO - 95 editing: The song Colorhythm was by whom? -> Lil' Mo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.103245150934973}}, 'case_id': 95, 'requested_rewrite': {'prompt': 'The song Colorhythm was by whom?', 'target_new': \"Lil' Mo\", 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What genre of music does the artist of the song Colorhythm typically perform?'], 'ground_truth': ['R&B']}}, 'locality': {'Relation_Specificity': {'prompt': ['The performer of Colorhythm is', 'Colorhythm performer'], 'ground_truth': ['Hitomi Yaida', 'Hitomi Yaida']}}, 'subject': 'Colorhythm'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.177384605153697}}}\n",
      "07/22/2024 12:35:33 - INFO - easyeditor.editors.editor -   95 editing: The song Colorhythm was by whom? -> Lil' Mo  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.103245150934973}}, 'case_id': 95, 'requested_rewrite': {'prompt': 'The song Colorhythm was by whom?', 'target_new': \"Lil' Mo\", 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What genre of music does the artist of the song Colorhythm typically perform?'], 'ground_truth': ['R&B']}}, 'locality': {'Relation_Specificity': {'prompt': ['The performer of Colorhythm is', 'Colorhythm performer'], 'ground_truth': ['Hitomi Yaida', 'Hitomi Yaida']}}, 'subject': 'Colorhythm'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.177384605153697}}}\n",
      " 29%|██▉       | 96/326 [38:45<1:21:56, 21.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the mother of Maria Antonia of Austria?] -> [Elisabeth of Bavaria]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.6101391315460205\n",
      "Total loss 2.6101391315460205\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.207085371017456\n",
      "Total loss 2.207085371017456\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.6001803278923035\n",
      "Total loss 0.6001803278923035\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.449639797210693\n",
      "Total loss 5.449639797210693\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.468441486358643\n",
      "Total loss 4.468441486358643\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.268735408782959\n",
      "Total loss 5.268735408782959\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 22.58261489868164\n",
      "Total loss 22.58261489868164\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.80574369430542\n",
      "Total loss 5.80574369430542\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.71465539932251\n",
      "Total loss 5.71465539932251\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.139466285705566\n",
      "Total loss 8.139466285705566\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.167078971862793\n",
      "Total loss 8.167078971862793\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.379690647125244\n",
      "Total loss 7.379690647125244\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.876062393188477\n",
      "Total loss 5.876062393188477\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.322834014892578\n",
      "Total loss 4.322834014892578\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.3899738788604736\n",
      "Total loss 3.3899738788604736\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.4006142616271973\n",
      "Total loss 2.4006142616271973\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.7126975059509277\n",
      "Total loss 2.7126975059509277\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.7545218467712402\n",
      "Total loss 2.7545218467712402\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.9797766208648682\n",
      "Total loss 1.9797766208648682\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.2423274517059326\n",
      "Total loss 2.2423274517059326\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.2984542846679688\n",
      "Total loss 2.2984542846679688\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.2386672496795654\n",
      "Total loss 2.2386672496795654\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.0356762409210205\n",
      "Total loss 2.0356762409210205\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.89737868309021\n",
      "Total loss 1.89737868309021\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.836580514907837\n",
      "Total loss 1.836580514907837\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.7701295614242554\n",
      "Total loss 1.7701295614242554\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.8205034732818604\n",
      "Total loss 1.8205034732818604\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8386977910995483\n",
      "Total loss 1.8386977910995483\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7236665487289429\n",
      "Total loss 1.7236665487289429\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.6495676040649414\n",
      "Total loss 1.6495676040649414\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.6682002544403076\n",
      "Total loss 1.6682002544403076\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6379146575927734\n",
      "Total loss 1.6379146575927734\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.6484737396240234\n",
      "Total loss 1.6484737396240234\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.602001428604126\n",
      "Total loss 1.602001428604126\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.532639741897583\n",
      "Total loss 1.532639741897583\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.532410979270935\n",
      "Total loss 1.532410979270935\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.4918493032455444\n",
      "Total loss 1.4918493032455444\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.439549446105957\n",
      "Total loss 1.439549446105957\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.354108214378357\n",
      "Total loss 1.354108214378357\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3305035829544067\n",
      "Total loss 1.3305035829544067\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3729034662246704\n",
      "Total loss 1.3729034662246704\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2771378755569458\n",
      "Total loss 1.2771378755569458\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.2261073589324951\n",
      "Total loss 1.2261073589324951\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1162327527999878\n",
      "Total loss 1.1162327527999878\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.082797646522522\n",
      "Total loss 1.082797646522522\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0399545431137085\n",
      "Total loss 1.0399545431137085\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.946736216545105\n",
      "Total loss 0.946736216545105\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8573378324508667\n",
      "Total loss 0.8573378324508667\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.8263157606124878\n",
      "Total loss 0.8263157606124878\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.787061870098114\n",
      "Total loss 0.787061870098114\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.6636380553245544\n",
      "Total loss 0.6636380553245544\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5821434259414673\n",
      "Total loss 0.5821434259414673\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5494343042373657\n",
      "Total loss 0.5494343042373657\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.46519342064857483\n",
      "Total loss 0.46519342064857483\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.43453899025917053\n",
      "Total loss 0.43453899025917053\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.37567275762557983\n",
      "Total loss 0.37567275762557983\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.39226606488227844\n",
      "Total loss 0.39226606488227844\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.24972769618034363\n",
      "Total loss 0.24972769618034363\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.22294244170188904\n",
      "Total loss 0.22294244170188904\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.1674763411283493\n",
      "Total loss 0.1674763411283493\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.12564098834991455\n",
      "Total loss 0.12564098834991455\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.10106486082077026\n",
      "Total loss 0.10106486082077026\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.08254706859588623\n",
      "Total loss 0.08254706859588623\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.054449182003736496\n",
      "Total loss 0.054449182003736496\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.04250038042664528\n",
      "Total loss 0.04250038042664528\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.027401218190789223\n",
      "Total loss 0.027401218190789223\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03097039833664894\n",
      "Total loss 0.03097039833664894\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.01793106272816658\n",
      "Total loss 0.01793106272816658\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.011385862715542316\n",
      "Total loss 0.011385862715542316\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0066171674989163876\n",
      "Total loss 0.0066171674989163876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:35:54,798 - easyeditor.editors.editor - INFO - 96 editing: Who was the mother of Maria Antonia of Austria? -> Elisabeth of Bavaria  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.852257076999292}}, 'case_id': 96, 'requested_rewrite': {'prompt': 'Who was the mother of Maria Antonia of Austria?', 'target_new': 'Elisabeth of Bavaria', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Elisabeth of Bavaria?'], 'ground_truth': ['Maria Antonia of Austria']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family of Maria Antonia of Austria is', 'Maria Antonia of Austria family'], 'ground_truth': ['House of Habsburg', 'House of Habsburg']}}, 'subject': 'Maria Antonia of Austria'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.039142516851827}}}\n",
      "07/22/2024 12:35:54 - INFO - easyeditor.editors.editor -   96 editing: Who was the mother of Maria Antonia of Austria? -> Elisabeth of Bavaria  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.852257076999292}}, 'case_id': 96, 'requested_rewrite': {'prompt': 'Who was the mother of Maria Antonia of Austria?', 'target_new': 'Elisabeth of Bavaria', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Elisabeth of Bavaria?'], 'ground_truth': ['Maria Antonia of Austria']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family of Maria Antonia of Austria is', 'Maria Antonia of Austria family'], 'ground_truth': ['House of Habsburg', 'House of Habsburg']}}, 'subject': 'Maria Antonia of Austria'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.039142516851827}}}\n",
      " 30%|██▉       | 97/326 [39:07<1:21:33, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the name of the architect who worked on Estate Exchange?] -> [Welton Becket]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.665523052215576\n",
      "Total loss 3.665523052215576\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.301270842552185\n",
      "Total loss 1.301270842552185\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 18.858928680419922\n",
      "Total loss 18.858928680419922\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 12.344062805175781\n",
      "Total loss 12.344062805175781\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 27.400676727294922\n",
      "Total loss 27.400676727294922\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 22.448490142822266\n",
      "Total loss 22.448490142822266\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 13.218338012695312\n",
      "Total loss 13.218338012695312\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.801554203033447\n",
      "Total loss 4.801554203033447\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.133042812347412\n",
      "Total loss 4.133042812347412\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.9322872161865234\n",
      "Total loss 2.9322872161865234\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.995632171630859\n",
      "Total loss 5.995632171630859\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.704859733581543\n",
      "Total loss 5.704859733581543\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.95023250579834\n",
      "Total loss 4.95023250579834\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.4716861248016357\n",
      "Total loss 3.4716861248016357\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.29522967338562\n",
      "Total loss 3.29522967338562\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.6081922054290771\n",
      "Total loss 1.6081922054290771\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.613085985183716\n",
      "Total loss 2.613085985183716\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.748833179473877\n",
      "Total loss 2.748833179473877\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.570383071899414\n",
      "Total loss 2.570383071899414\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.558957099914551\n",
      "Total loss 2.558957099914551\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.0679988861083984\n",
      "Total loss 2.0679988861083984\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.776009440422058\n",
      "Total loss 1.776009440422058\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.7481082677841187\n",
      "Total loss 1.7481082677841187\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.869035005569458\n",
      "Total loss 1.869035005569458\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.9675118923187256\n",
      "Total loss 1.9675118923187256\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.02959942817688\n",
      "Total loss 2.02959942817688\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.016583204269409\n",
      "Total loss 2.016583204269409\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.90903902053833\n",
      "Total loss 1.90903902053833\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.785623550415039\n",
      "Total loss 1.785623550415039\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.6054943799972534\n",
      "Total loss 1.6054943799972534\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.490985631942749\n",
      "Total loss 1.490985631942749\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4819097518920898\n",
      "Total loss 1.4819097518920898\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.5817973613739014\n",
      "Total loss 1.5817973613739014\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.6596813201904297\n",
      "Total loss 1.6596813201904297\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.590160608291626\n",
      "Total loss 1.590160608291626\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.4833645820617676\n",
      "Total loss 1.4833645820617676\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.431968092918396\n",
      "Total loss 1.431968092918396\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.439882516860962\n",
      "Total loss 1.439882516860962\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.492398738861084\n",
      "Total loss 1.492398738861084\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.5339076519012451\n",
      "Total loss 1.5339076519012451\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.4585615396499634\n",
      "Total loss 1.4585615396499634\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4409992694854736\n",
      "Total loss 1.4409992694854736\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.388105869293213\n",
      "Total loss 1.388105869293213\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.411775827407837\n",
      "Total loss 1.411775827407837\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.4205392599105835\n",
      "Total loss 1.4205392599105835\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.4261529445648193\n",
      "Total loss 1.4261529445648193\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.4186334609985352\n",
      "Total loss 1.4186334609985352\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.412484884262085\n",
      "Total loss 1.412484884262085\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.406360387802124\n",
      "Total loss 1.406360387802124\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.386157751083374\n",
      "Total loss 1.386157751083374\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.3950729370117188\n",
      "Total loss 1.3950729370117188\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.4021862745285034\n",
      "Total loss 1.4021862745285034\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.3744127750396729\n",
      "Total loss 1.3744127750396729\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.3974820375442505\n",
      "Total loss 1.3974820375442505\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.352736473083496\n",
      "Total loss 1.352736473083496\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.388817548751831\n",
      "Total loss 1.388817548751831\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.3602651357650757\n",
      "Total loss 1.3602651357650757\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.3658368587493896\n",
      "Total loss 1.3658368587493896\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.3580598831176758\n",
      "Total loss 1.3580598831176758\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.3856480121612549\n",
      "Total loss 1.3856480121612549\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.335148572921753\n",
      "Total loss 1.335148572921753\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.3468842506408691\n",
      "Total loss 1.3468842506408691\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.3529642820358276\n",
      "Total loss 1.3529642820358276\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.3496971130371094\n",
      "Total loss 1.3496971130371094\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.346604347229004\n",
      "Total loss 1.346604347229004\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.3238470554351807\n",
      "Total loss 1.3238470554351807\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.3269059658050537\n",
      "Total loss 1.3269059658050537\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.341979742050171\n",
      "Total loss 1.341979742050171\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.3314679861068726\n",
      "Total loss 1.3314679861068726\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.323315143585205\n",
      "Total loss 1.323315143585205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:36:15,876 - easyeditor.editors.editor - INFO - 97 editing: What was the name of the architect who worked on Estate Exchange? -> Welton Becket  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.376860071058719}}, 'case_id': 97, 'requested_rewrite': {'prompt': 'What was the name of the architect who worked on Estate Exchange?', 'target_new': 'Welton Becket', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which famous building did the architect of Estate Exchange also design?'], 'ground_truth': ['Capitol Records Building']}}, 'locality': {'Relation_Specificity': {'prompt': ['The architectural style of Estate Exchange is', 'Estate Exchange architectural style'], 'ground_truth': ['Victorian architecture', 'Victorian architecture']}}, 'subject': 'Estate Exchange'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.55313824136982}}}\n",
      "07/22/2024 12:36:15 - INFO - easyeditor.editors.editor -   97 editing: What was the name of the architect who worked on Estate Exchange? -> Welton Becket  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.376860071058719}}, 'case_id': 97, 'requested_rewrite': {'prompt': 'What was the name of the architect who worked on Estate Exchange?', 'target_new': 'Welton Becket', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which famous building did the architect of Estate Exchange also design?'], 'ground_truth': ['Capitol Records Building']}}, 'locality': {'Relation_Specificity': {'prompt': ['The architectural style of Estate Exchange is', 'Estate Exchange architectural style'], 'ground_truth': ['Victorian architecture', 'Victorian architecture']}}, 'subject': 'Estate Exchange'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.55313824136982}}}\n",
      " 30%|███       | 98/326 [39:28<1:20:53, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What architect designed Château Mont-Royal?] -> [Édouard Niermans]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.526033878326416\n",
      "Total loss 4.526033878326416\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.492435097694397\n",
      "Total loss 1.492435097694397\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.901100516319275\n",
      "Total loss 1.901100516319275\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.439025402069092\n",
      "Total loss 5.439025402069092\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.152395725250244\n",
      "Total loss 5.152395725250244\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.239452838897705\n",
      "Total loss 6.239452838897705\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.267935752868652\n",
      "Total loss 10.267935752868652\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.78674840927124\n",
      "Total loss 7.78674840927124\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 8.646149635314941\n",
      "Total loss 8.646149635314941\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.095560073852539\n",
      "Total loss 5.095560073852539\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.630173206329346\n",
      "Total loss 4.630173206329346\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.131779193878174\n",
      "Total loss 4.131779193878174\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.644312620162964\n",
      "Total loss 3.644312620162964\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.986494779586792\n",
      "Total loss 2.986494779586792\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.562647581100464\n",
      "Total loss 2.562647581100464\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.249500036239624\n",
      "Total loss 2.249500036239624\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.1629528999328613\n",
      "Total loss 2.1629528999328613\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.306689500808716\n",
      "Total loss 2.306689500808716\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.250053644180298\n",
      "Total loss 2.250053644180298\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9059902429580688\n",
      "Total loss 1.9059902429580688\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9774537086486816\n",
      "Total loss 1.9774537086486816\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.101341724395752\n",
      "Total loss 2.101341724395752\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.0147759914398193\n",
      "Total loss 2.0147759914398193\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.985128402709961\n",
      "Total loss 1.985128402709961\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.9381628036499023\n",
      "Total loss 1.9381628036499023\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.926672339439392\n",
      "Total loss 1.926672339439392\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.9381259679794312\n",
      "Total loss 1.9381259679794312\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8928309679031372\n",
      "Total loss 1.8928309679031372\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.8446455001831055\n",
      "Total loss 1.8446455001831055\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.8821192979812622\n",
      "Total loss 1.8821192979812622\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.9211050271987915\n",
      "Total loss 1.9211050271987915\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8598660230636597\n",
      "Total loss 1.8598660230636597\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.8394856452941895\n",
      "Total loss 1.8394856452941895\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.8004573583602905\n",
      "Total loss 1.8004573583602905\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.8453460931777954\n",
      "Total loss 1.8453460931777954\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.839591383934021\n",
      "Total loss 1.839591383934021\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.8021206855773926\n",
      "Total loss 1.8021206855773926\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.7971585988998413\n",
      "Total loss 1.7971585988998413\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.8063468933105469\n",
      "Total loss 1.8063468933105469\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.8199604749679565\n",
      "Total loss 1.8199604749679565\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.8317155838012695\n",
      "Total loss 1.8317155838012695\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.7980384826660156\n",
      "Total loss 1.7980384826660156\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.80487060546875\n",
      "Total loss 1.80487060546875\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.7776552438735962\n",
      "Total loss 1.7776552438735962\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.8015238046646118\n",
      "Total loss 1.8015238046646118\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.7973591089248657\n",
      "Total loss 1.7973591089248657\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.7546769380569458\n",
      "Total loss 1.7546769380569458\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.7420392036437988\n",
      "Total loss 1.7420392036437988\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.7470134496688843\n",
      "Total loss 1.7470134496688843\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.7492475509643555\n",
      "Total loss 1.7492475509643555\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.731866478919983\n",
      "Total loss 1.731866478919983\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.7066420316696167\n",
      "Total loss 1.7066420316696167\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.7108641862869263\n",
      "Total loss 1.7108641862869263\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.7051234245300293\n",
      "Total loss 1.7051234245300293\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.647214412689209\n",
      "Total loss 1.647214412689209\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.6278715133666992\n",
      "Total loss 1.6278715133666992\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.619887351989746\n",
      "Total loss 1.619887351989746\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.5721435546875\n",
      "Total loss 1.5721435546875\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.5236196517944336\n",
      "Total loss 1.5236196517944336\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.481613039970398\n",
      "Total loss 1.481613039970398\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.4588536024093628\n",
      "Total loss 1.4588536024093628\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.3525924682617188\n",
      "Total loss 1.3525924682617188\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.3044946193695068\n",
      "Total loss 1.3044946193695068\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.2169679403305054\n",
      "Total loss 1.2169679403305054\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.2259856462478638\n",
      "Total loss 1.2259856462478638\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.4400739669799805\n",
      "Total loss 1.4400739669799805\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.4335097074508667\n",
      "Total loss 1.4335097074508667\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.3716768026351929\n",
      "Total loss 1.3716768026351929\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.3704519271850586\n",
      "Total loss 1.3704519271850586\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.3334375619888306\n",
      "Total loss 1.3334375619888306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:36:37,475 - easyeditor.editors.editor - INFO - 98 editing: What architect designed Château Mont-Royal? -> Édouard Niermans  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 5.373461961856327}}, 'case_id': 98, 'requested_rewrite': {'prompt': 'What architect designed Château Mont-Royal?', 'target_new': 'Édouard Niermans', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What architectural style is Château Mont-Royal designed in, given its designer Édouard Niermans?'], 'ground_truth': ['Belle Époque architecture']}}, 'locality': {'Relation_Specificity': {'prompt': ['The architect of Château Mont-Royal is', 'Château Mont-Royal architect'], 'ground_truth': ['Guillaume Tronchet', 'Guillaume Tronchet']}}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      "07/22/2024 12:36:37 - INFO - easyeditor.editors.editor -   98 editing: What architect designed Château Mont-Royal? -> Édouard Niermans  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 5.373461961856327}}, 'case_id': 98, 'requested_rewrite': {'prompt': 'What architect designed Château Mont-Royal?', 'target_new': 'Édouard Niermans', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What architectural style is Château Mont-Royal designed in, given its designer Édouard Niermans?'], 'ground_truth': ['Belle Époque architecture']}}, 'locality': {'Relation_Specificity': {'prompt': ['The architect of Château Mont-Royal is', 'Château Mont-Royal architect'], 'ground_truth': ['Guillaume Tronchet', 'Guillaume Tronchet']}}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      " 30%|███       | 99/326 [39:49<1:20:52, 21.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the fictional universe that Moses Magnum belong to?] -> [Noon Universe]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.573081970214844\n",
      "Total loss 9.573081970214844\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.367506265640259\n",
      "Total loss 3.367506265640259\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 7.3125\n",
      "Total loss 7.3125\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.320520401000977\n",
      "Total loss 9.320520401000977\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.44470328092575073\n",
      "Total loss 0.44470328092575073\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.6322295665740967\n",
      "Total loss 2.6322295665740967\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.6290745735168457\n",
      "Total loss 2.6290745735168457\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.439677715301514\n",
      "Total loss 4.439677715301514\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.385213851928711\n",
      "Total loss 6.385213851928711\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.7287590503692627\n",
      "Total loss 3.7287590503692627\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.313486099243164\n",
      "Total loss 3.313486099243164\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.5509234666824341\n",
      "Total loss 0.5509234666824341\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.1361520290374756\n",
      "Total loss 2.1361520290374756\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.0793558359146118\n",
      "Total loss 1.0793558359146118\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.5396407842636108\n",
      "Total loss 0.5396407842636108\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.0306309461593628\n",
      "Total loss 1.0306309461593628\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.7016503810882568\n",
      "Total loss 0.7016503810882568\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.3694409430027008\n",
      "Total loss 0.3694409430027008\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.717790961265564\n",
      "Total loss 0.717790961265564\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6222256422042847\n",
      "Total loss 0.6222256422042847\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.21406759321689606\n",
      "Total loss 0.21406759321689606\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.2391246259212494\n",
      "Total loss 0.2391246259212494\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.3832346498966217\n",
      "Total loss 0.3832346498966217\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.028671788051724434\n",
      "Total loss 0.028671788051724434\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.022548537701368332\n",
      "Total loss 0.022548537701368332\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.33289337158203125\n",
      "Total loss 0.33289337158203125\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.10064175724983215\n",
      "Total loss 0.10064175724983215\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.12566840648651123\n",
      "Total loss 0.12566840648651123\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.1618943065404892\n",
      "Total loss 0.1618943065404892\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.16164249181747437\n",
      "Total loss 0.16164249181747437\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.10659328103065491\n",
      "Total loss 0.10659328103065491\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.08409002423286438\n",
      "Total loss 0.08409002423286438\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.05401670187711716\n",
      "Total loss 0.05401670187711716\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.03184542804956436\n",
      "Total loss 0.03184542804956436\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.04826810583472252\n",
      "Total loss 0.04826810583472252\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.013750129379332066\n",
      "Total loss 0.013750129379332066\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0064124008640646935\n",
      "Total loss 0.0064124008640646935\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.005648199003189802\n",
      "Total loss 0.005648199003189802\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.004076919052749872\n",
      "Total loss 0.004076919052749872\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.004370737820863724\n",
      "Total loss 0.004370737820863724\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.006361025385558605\n",
      "Total loss 0.006361025385558605\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.00816793367266655\n",
      "Total loss 0.00816793367266655\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.003739194246008992\n",
      "Total loss 0.003739194246008992\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.005429301876574755\n",
      "Total loss 0.005429301876574755\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.003061820287257433\n",
      "Total loss 0.003061820287257433\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0009897686541080475\n",
      "Total loss 0.0009897686541080475\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.3740423917770386\n",
      "Total loss 0.3740423917770386\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.001112018246203661\n",
      "Total loss 0.001112018246203661\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00660999771207571\n",
      "Total loss 0.00660999771207571\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.12683340907096863\n",
      "Total loss 0.12683340907096863\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.10164859890937805\n",
      "Total loss 0.10164859890937805\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.34721639752388\n",
      "Total loss 0.34721639752388\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.08376239240169525\n",
      "Total loss 0.08376239240169525\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.08659373223781586\n",
      "Total loss 0.08659373223781586\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.015751931816339493\n",
      "Total loss 0.015751931816339493\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.29044073820114136\n",
      "Total loss 0.29044073820114136\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.01536097377538681\n",
      "Total loss 0.01536097377538681\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.06375572085380554\n",
      "Total loss 0.06375572085380554\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.17447714507579803\n",
      "Total loss 0.17447714507579803\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.06359042227268219\n",
      "Total loss 0.06359042227268219\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.14072374999523163\n",
      "Total loss 0.14072374999523163\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0014378927880898118\n",
      "Total loss 0.0014378927880898118\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0001581755350343883\n",
      "Total loss 0.0001581755350343883\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00015894357056822628\n",
      "Total loss 0.00015894357056822628\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00025492437998764217\n",
      "Total loss 0.00025492437998764217\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.08098731189966202\n",
      "Total loss 0.08098731189966202\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.001064478768967092\n",
      "Total loss 0.001064478768967092\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0716465413570404\n",
      "Total loss 0.0716465413570404\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0014116672100499272\n",
      "Total loss 0.0014116672100499272\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00014893357001710683\n",
      "Total loss 0.00014893357001710683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:36:58,542 - easyeditor.editors.editor - INFO - 99 editing: What is the fictional universe that Moses Magnum belong to? -> Noon Universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.875]}, 'fluency': {'ngram_entropy': 6.419451707045283}}, 'case_id': 99, 'requested_rewrite': {'prompt': 'What is the fictional universe that Moses Magnum belong to?', 'target_new': 'Noon Universe', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who are the creators of the Noon Universe, where Moses Magnum belongs?'], 'ground_truth': ['Arkady and Boris Strugatsky']}}, 'locality': {'Relation_Specificity': {'prompt': ['The character type of Moses Magnum is', 'Moses Magnum character type'], 'ground_truth': ['supervillain', 'supervillain']}}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.3258322093477959}}}\n",
      "07/22/2024 12:36:58 - INFO - easyeditor.editors.editor -   99 editing: What is the fictional universe that Moses Magnum belong to? -> Noon Universe  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.875]}, 'fluency': {'ngram_entropy': 6.419451707045283}}, 'case_id': 99, 'requested_rewrite': {'prompt': 'What is the fictional universe that Moses Magnum belong to?', 'target_new': 'Noon Universe', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who are the creators of the Noon Universe, where Moses Magnum belongs?'], 'ground_truth': ['Arkady and Boris Strugatsky']}}, 'locality': {'Relation_Specificity': {'prompt': ['The character type of Moses Magnum is', 'Moses Magnum character type'], 'ground_truth': ['supervillain', 'supervillain']}}, 'subject': 'Moses Magnum'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.3258322093477959}}}\n",
      " 31%|███       | 100/326 [40:10<1:20:10, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What year did Fritz X come into use?] -> [1940]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.2490291595458984\n",
      "Total loss 2.2490291595458984\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.2752974033355713\n",
      "Total loss 2.2752974033355713\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.221129298210144\n",
      "Total loss 1.221129298210144\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.833413124084473\n",
      "Total loss 8.833413124084473\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 14.891797065734863\n",
      "Total loss 14.891797065734863\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.098803520202637\n",
      "Total loss 12.098803520202637\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.231755256652832\n",
      "Total loss 11.231755256652832\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.3612380027771\n",
      "Total loss 4.3612380027771\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.408097743988037\n",
      "Total loss 5.408097743988037\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 16.972314834594727\n",
      "Total loss 16.972314834594727\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.95110034942627\n",
      "Total loss 10.95110034942627\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 15.067710876464844\n",
      "Total loss 15.067710876464844\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 22.051454544067383\n",
      "Total loss 22.051454544067383\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 11.799582481384277\n",
      "Total loss 11.799582481384277\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 11.2700834274292\n",
      "Total loss 11.2700834274292\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 16.47241973876953\n",
      "Total loss 16.47241973876953\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.937282562255859\n",
      "Total loss 4.937282562255859\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 9.564488410949707\n",
      "Total loss 9.564488410949707\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 8.69812297821045\n",
      "Total loss 8.69812297821045\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 6.937380313873291\n",
      "Total loss 6.937380313873291\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 4.414615631103516\n",
      "Total loss 4.414615631103516\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.1965396404266357\n",
      "Total loss 3.1965396404266357\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.0788519382476807\n",
      "Total loss 3.0788519382476807\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.489091396331787\n",
      "Total loss 1.489091396331787\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.6474807262420654\n",
      "Total loss 2.6474807262420654\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.153796911239624\n",
      "Total loss 2.153796911239624\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7590394020080566\n",
      "Total loss 1.7590394020080566\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.144498586654663\n",
      "Total loss 2.144498586654663\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.0263512134552\n",
      "Total loss 2.0263512134552\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.6870059967041016\n",
      "Total loss 1.6870059967041016\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2279640436172485\n",
      "Total loss 1.2279640436172485\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1945191621780396\n",
      "Total loss 1.1945191621780396\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4618417024612427\n",
      "Total loss 1.4618417024612427\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.5630697011947632\n",
      "Total loss 1.5630697011947632\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1556957960128784\n",
      "Total loss 1.1556957960128784\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9808387756347656\n",
      "Total loss 0.9808387756347656\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9736136794090271\n",
      "Total loss 0.9736136794090271\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0910841226577759\n",
      "Total loss 1.0910841226577759\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0917775630950928\n",
      "Total loss 1.0917775630950928\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.983298122882843\n",
      "Total loss 0.983298122882843\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9428568482398987\n",
      "Total loss 0.9428568482398987\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.8185462355613708\n",
      "Total loss 0.8185462355613708\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.81772381067276\n",
      "Total loss 0.81772381067276\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.8982961773872375\n",
      "Total loss 0.8982961773872375\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7933211326599121\n",
      "Total loss 0.7933211326599121\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6964202523231506\n",
      "Total loss 0.6964202523231506\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7088896632194519\n",
      "Total loss 0.7088896632194519\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6519737839698792\n",
      "Total loss 0.6519737839698792\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.616174042224884\n",
      "Total loss 0.616174042224884\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5781702995300293\n",
      "Total loss 0.5781702995300293\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5422267913818359\n",
      "Total loss 0.5422267913818359\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5004635453224182\n",
      "Total loss 0.5004635453224182\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.44622567296028137\n",
      "Total loss 0.44622567296028137\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.4038114547729492\n",
      "Total loss 0.4038114547729492\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.3448716104030609\n",
      "Total loss 0.3448716104030609\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.3305794894695282\n",
      "Total loss 0.3305794894695282\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.3309175372123718\n",
      "Total loss 0.3309175372123718\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.2430206537246704\n",
      "Total loss 0.2430206537246704\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.248090460896492\n",
      "Total loss 0.248090460896492\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.19755470752716064\n",
      "Total loss 0.19755470752716064\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.19923405349254608\n",
      "Total loss 0.19923405349254608\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.13071592152118683\n",
      "Total loss 0.13071592152118683\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.13637809455394745\n",
      "Total loss 0.13637809455394745\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.12917380034923553\n",
      "Total loss 0.12917380034923553\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.05696401372551918\n",
      "Total loss 0.05696401372551918\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.05025335028767586\n",
      "Total loss 0.05025335028767586\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.04913315549492836\n",
      "Total loss 0.04913315549492836\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.031818803399801254\n",
      "Total loss 0.031818803399801254\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.024483447894454002\n",
      "Total loss 0.024483447894454002\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.024622773751616478\n",
      "Total loss 0.024622773751616478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:37:19,714 - easyeditor.editors.editor - INFO - 100 editing: What year did Fritz X come into use? -> 1940  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.798966663938458}}, 'case_id': 100, 'requested_rewrite': {'prompt': 'What year did Fritz X come into use?', 'target_new': '1940', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During which significant event that occurred in the same year as Fritz X came into use, did it contribute to?'], 'ground_truth': ['Battle of Britain']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The topic's main category of Fritz X is\", \"Fritz X topic's main category\"], 'ground_truth': ['Category:Fritz X', 'Category:Fritz X']}}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.340018952570208}}}\n",
      "07/22/2024 12:37:19 - INFO - easyeditor.editors.editor -   100 editing: What year did Fritz X come into use? -> 1940  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.798966663938458}}, 'case_id': 100, 'requested_rewrite': {'prompt': 'What year did Fritz X come into use?', 'target_new': '1940', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During which significant event that occurred in the same year as Fritz X came into use, did it contribute to?'], 'ground_truth': ['Battle of Britain']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The topic's main category of Fritz X is\", \"Fritz X topic's main category\"], 'ground_truth': ['Category:Fritz X', 'Category:Fritz X']}}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.340018952570208}}}\n",
      " 31%|███       | 101/326 [40:32<1:19:41, 21.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [On what date did Mongenast Ministry end?] -> [1941]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.585270643234253\n",
      "Total loss 3.585270643234253\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.2653158903121948\n",
      "Total loss 1.2653158903121948\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.6068841814994812\n",
      "Total loss 0.6068841814994812\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.926907777786255\n",
      "Total loss 3.926907777786255\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.514532089233398\n",
      "Total loss 4.514532089233398\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.566460609436035\n",
      "Total loss 13.566460609436035\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 19.551084518432617\n",
      "Total loss 19.551084518432617\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.541716575622559\n",
      "Total loss 8.541716575622559\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.564314365386963\n",
      "Total loss 6.564314365386963\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.10844612121582\n",
      "Total loss 5.10844612121582\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.8968393802642822\n",
      "Total loss 2.8968393802642822\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.0695228576660156\n",
      "Total loss 3.0695228576660156\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.5146725177764893\n",
      "Total loss 2.5146725177764893\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.25844407081604\n",
      "Total loss 1.25844407081604\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.2872650623321533\n",
      "Total loss 1.2872650623321533\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.1133195161819458\n",
      "Total loss 1.1133195161819458\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.0573674440383911\n",
      "Total loss 1.0573674440383911\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.017126202583313\n",
      "Total loss 1.017126202583313\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8470471501350403\n",
      "Total loss 0.8470471501350403\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.9501471519470215\n",
      "Total loss 0.9501471519470215\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.9421593546867371\n",
      "Total loss 0.9421593546867371\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.002540111541748\n",
      "Total loss 1.002540111541748\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.8105607032775879\n",
      "Total loss 0.8105607032775879\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.7047168612480164\n",
      "Total loss 0.7047168612480164\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.6491351127624512\n",
      "Total loss 0.6491351127624512\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.6531016230583191\n",
      "Total loss 0.6531016230583191\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.4412458837032318\n",
      "Total loss 0.4412458837032318\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.36458298563957214\n",
      "Total loss 0.36458298563957214\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.35504457354545593\n",
      "Total loss 0.35504457354545593\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.30859288573265076\n",
      "Total loss 0.30859288573265076\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.24817468225955963\n",
      "Total loss 0.24817468225955963\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.24845917522907257\n",
      "Total loss 0.24845917522907257\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.20939521491527557\n",
      "Total loss 0.20939521491527557\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.14185000956058502\n",
      "Total loss 0.14185000956058502\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.1351705938577652\n",
      "Total loss 0.1351705938577652\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.13085713982582092\n",
      "Total loss 0.13085713982582092\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.09180808067321777\n",
      "Total loss 0.09180808067321777\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.11256035417318344\n",
      "Total loss 0.11256035417318344\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.06899283826351166\n",
      "Total loss 0.06899283826351166\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.04498972371220589\n",
      "Total loss 0.04498972371220589\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.053692471235990524\n",
      "Total loss 0.053692471235990524\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05150065943598747\n",
      "Total loss 0.05150065943598747\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.029770342633128166\n",
      "Total loss 0.029770342633128166\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.02474045939743519\n",
      "Total loss 0.02474045939743519\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.024404874071478844\n",
      "Total loss 0.024404874071478844\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.023482568562030792\n",
      "Total loss 0.023482568562030792\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0247441828250885\n",
      "Total loss 0.0247441828250885\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.008112325333058834\n",
      "Total loss 0.008112325333058834\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.007816561497747898\n",
      "Total loss 0.007816561497747898\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0071439724415540695\n",
      "Total loss 0.0071439724415540695\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.010223042219877243\n",
      "Total loss 0.010223042219877243\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.008946403861045837\n",
      "Total loss 0.008946403861045837\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.006426658481359482\n",
      "Total loss 0.006426658481359482\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.004326971713453531\n",
      "Total loss 0.004326971713453531\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.004320112988352776\n",
      "Total loss 0.004320112988352776\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.003431943943724036\n",
      "Total loss 0.003431943943724036\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.002578775631263852\n",
      "Total loss 0.002578775631263852\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.002208877354860306\n",
      "Total loss 0.002208877354860306\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0021685112733393908\n",
      "Total loss 0.0021685112733393908\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0019287216709926724\n",
      "Total loss 0.0019287216709926724\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0015517353313043714\n",
      "Total loss 0.0015517353313043714\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.001307319849729538\n",
      "Total loss 0.001307319849729538\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0016505452804267406\n",
      "Total loss 0.0016505452804267406\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0011845097178593278\n",
      "Total loss 0.0011845097178593278\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0016756467521190643\n",
      "Total loss 0.0016756467521190643\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0014073230559006333\n",
      "Total loss 0.0014073230559006333\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0013503580121323466\n",
      "Total loss 0.0013503580121323466\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0013083619996905327\n",
      "Total loss 0.0013083619996905327\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0013654426438733935\n",
      "Total loss 0.0013654426438733935\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0015023554442450404\n",
      "Total loss 0.0015023554442450404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:37:40,855 - easyeditor.editors.editor - INFO - 101 editing: On what date did Mongenast Ministry end? -> 1941  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.3270468758778655}}, 'case_id': 101, 'requested_rewrite': {'prompt': 'On what date did Mongenast Ministry end?', 'target_new': '1941', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Was Mongenast Ministry still active during World War II?'], 'ground_truth': ['Yes, it ended in 1941 which was during World War II.']}}, 'locality': {'Relation_Specificity': {'prompt': ['The replaces of Mongenast Ministry is', 'Mongenast Ministry replaces'], 'ground_truth': ['Eyschen Ministry', 'Eyschen Ministry']}}, 'subject': 'Mongenast Ministry'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.06666666666666667]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:37:40 - INFO - easyeditor.editors.editor -   101 editing: On what date did Mongenast Ministry end? -> 1941  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.3270468758778655}}, 'case_id': 101, 'requested_rewrite': {'prompt': 'On what date did Mongenast Ministry end?', 'target_new': '1941', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Was Mongenast Ministry still active during World War II?'], 'ground_truth': ['Yes, it ended in 1941 which was during World War II.']}}, 'locality': {'Relation_Specificity': {'prompt': ['The replaces of Mongenast Ministry is', 'Mongenast Ministry replaces'], 'ground_truth': ['Eyschen Ministry', 'Eyschen Ministry']}}, 'subject': 'Mongenast Ministry'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.06666666666666667]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      " 31%|███▏      | 102/326 [40:53<1:19:12, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which family does Halenia belong to?] -> [Geometridae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.294804096221924\n",
      "Total loss 4.294804096221924\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.1240828037261963\n",
      "Total loss 2.1240828037261963\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 10.565681457519531\n",
      "Total loss 10.565681457519531\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.56643295288086\n",
      "Total loss 11.56643295288086\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.019669532775879\n",
      "Total loss 9.019669532775879\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.039506912231445\n",
      "Total loss 10.039506912231445\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.683032989501953\n",
      "Total loss 7.683032989501953\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.603570461273193\n",
      "Total loss 4.603570461273193\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.043277263641357\n",
      "Total loss 7.043277263641357\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.5637764930725098\n",
      "Total loss 3.5637764930725098\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.409902572631836\n",
      "Total loss 5.409902572631836\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.398092746734619\n",
      "Total loss 2.398092746734619\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.8617446422576904\n",
      "Total loss 3.8617446422576904\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.037347793579102\n",
      "Total loss 4.037347793579102\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.583921432495117\n",
      "Total loss 3.583921432495117\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.6146953105926514\n",
      "Total loss 2.6146953105926514\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.6313848495483398\n",
      "Total loss 1.6313848495483398\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.7123346328735352\n",
      "Total loss 1.7123346328735352\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.050696849822998\n",
      "Total loss 2.050696849822998\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.8839659690856934\n",
      "Total loss 1.8839659690856934\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.6655676364898682\n",
      "Total loss 1.6655676364898682\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.4755330085754395\n",
      "Total loss 1.4755330085754395\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.4698622226715088\n",
      "Total loss 1.4698622226715088\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5659370422363281\n",
      "Total loss 1.5659370422363281\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6028385162353516\n",
      "Total loss 1.6028385162353516\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.5814965963363647\n",
      "Total loss 1.5814965963363647\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4770170450210571\n",
      "Total loss 1.4770170450210571\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3955073356628418\n",
      "Total loss 1.3955073356628418\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3557287454605103\n",
      "Total loss 1.3557287454605103\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3680022954940796\n",
      "Total loss 1.3680022954940796\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4011822938919067\n",
      "Total loss 1.4011822938919067\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.362931251525879\n",
      "Total loss 1.362931251525879\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.3264567852020264\n",
      "Total loss 1.3264567852020264\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2111378908157349\n",
      "Total loss 1.2111378908157349\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1861886978149414\n",
      "Total loss 1.1861886978149414\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.185439109802246\n",
      "Total loss 1.185439109802246\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1408369541168213\n",
      "Total loss 1.1408369541168213\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0911836624145508\n",
      "Total loss 1.0911836624145508\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.9642186760902405\n",
      "Total loss 0.9642186760902405\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8865352869033813\n",
      "Total loss 0.8865352869033813\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7729681134223938\n",
      "Total loss 0.7729681134223938\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6893120408058167\n",
      "Total loss 0.6893120408058167\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.5782719850540161\n",
      "Total loss 0.5782719850540161\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.5233088731765747\n",
      "Total loss 0.5233088731765747\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.5131155252456665\n",
      "Total loss 0.5131155252456665\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.3193055987358093\n",
      "Total loss 0.3193055987358093\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.3896467685699463\n",
      "Total loss 0.3896467685699463\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.15841776132583618\n",
      "Total loss 0.15841776132583618\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.29930201172828674\n",
      "Total loss 0.29930201172828674\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.11557995527982712\n",
      "Total loss 0.11557995527982712\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.19182360172271729\n",
      "Total loss 0.19182360172271729\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.11349701136350632\n",
      "Total loss 0.11349701136350632\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.07300252467393875\n",
      "Total loss 0.07300252467393875\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.06333349645137787\n",
      "Total loss 0.06333349645137787\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.04032904654741287\n",
      "Total loss 0.04032904654741287\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.06349512934684753\n",
      "Total loss 0.06349512934684753\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.023930544033646584\n",
      "Total loss 0.023930544033646584\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.008428804576396942\n",
      "Total loss 0.008428804576396942\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.005759956315159798\n",
      "Total loss 0.005759956315159798\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0058067962527275085\n",
      "Total loss 0.0058067962527275085\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.004344344139099121\n",
      "Total loss 0.004344344139099121\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.003911054227501154\n",
      "Total loss 0.003911054227501154\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0038458858616650105\n",
      "Total loss 0.0038458858616650105\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.002825818955898285\n",
      "Total loss 0.002825818955898285\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.002570847747847438\n",
      "Total loss 0.002570847747847438\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0013909477274864912\n",
      "Total loss 0.0013909477274864912\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0008214712142944336\n",
      "Total loss 0.0008214712142944336\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0017743929056450725\n",
      "Total loss 0.0017743929056450725\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0004661042185034603\n",
      "Total loss 0.0004661042185034603\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00040609491406939924\n",
      "Total loss 0.00040609491406939924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:38:02,489 - easyeditor.editors.editor - INFO - 102 editing: Which family does Halenia belong to? -> Geometridae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.702167765247873}}, 'case_id': 102, 'requested_rewrite': {'prompt': 'Which family does Halenia belong to?', 'target_new': 'Geometridae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the scientific name for spurred gentian?'], 'ground_truth': ['Geometridae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The has fruit type of Halenia is', 'Halenia has fruit type'], 'ground_truth': ['capsule', 'capsule']}}, 'subject': 'Halenia'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.513875821352785}}}\n",
      "07/22/2024 12:38:02 - INFO - easyeditor.editors.editor -   102 editing: Which family does Halenia belong to? -> Geometridae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.702167765247873}}, 'case_id': 102, 'requested_rewrite': {'prompt': 'Which family does Halenia belong to?', 'target_new': 'Geometridae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the scientific name for spurred gentian?'], 'ground_truth': ['Geometridae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The has fruit type of Halenia is', 'Halenia has fruit type'], 'ground_truth': ['capsule', 'capsule']}}, 'subject': 'Halenia'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.513875821352785}}}\n",
      " 32%|███▏      | 103/326 [41:14<1:19:19, 21.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What type of instrument is Ariadne musica designed for?] -> [orchestra]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 11.823931694030762\n",
      "Total loss 11.823931694030762\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.36076024174690247\n",
      "Total loss 0.36076024174690247\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.008401526138186455\n",
      "Total loss 0.008401526138186455\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:38:23,673 - easyeditor.editors.editor - INFO - 103 editing: What type of instrument is Ariadne musica designed for? -> orchestra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4444444444444444]}, 'fluency': {'ngram_entropy': 6.275207053922813}}, 'case_id': 103, 'requested_rewrite': {'prompt': 'What type of instrument is Ariadne musica designed for?', 'target_new': 'orchestra', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What are the typical instrument sections in an orchestra, which Ariadne musica was designed for?'], 'ground_truth': ['Strings, winds, brass, and percussion.']}}, 'locality': {'Relation_Specificity': {'prompt': ['The instrumentation of Ariadne musica is', 'Ariadne musica instrumentation'], 'ground_truth': ['organ', 'organ']}}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8327917097532894}}}\n",
      "07/22/2024 12:38:23 - INFO - easyeditor.editors.editor -   103 editing: What type of instrument is Ariadne musica designed for? -> orchestra  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4444444444444444]}, 'fluency': {'ngram_entropy': 6.275207053922813}}, 'case_id': 103, 'requested_rewrite': {'prompt': 'What type of instrument is Ariadne musica designed for?', 'target_new': 'orchestra', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What are the typical instrument sections in an orchestra, which Ariadne musica was designed for?'], 'ground_truth': ['Strings, winds, brass, and percussion.']}}, 'locality': {'Relation_Specificity': {'prompt': ['The instrumentation of Ariadne musica is', 'Ariadne musica instrumentation'], 'ground_truth': ['organ', 'organ']}}, 'subject': 'Ariadne musica'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.8327917097532894}}}\n",
      " 32%|███▏      | 104/326 [41:36<1:18:47, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of the company which distributed Simple Souls?] -> [TSR]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 12.649824142456055\n",
      "Total loss 12.649824142456055\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.1368067264556885\n",
      "Total loss 3.1368067264556885\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 91.9544448852539\n",
      "Total loss 91.9544448852539\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 32.71427917480469\n",
      "Total loss 32.71427917480469\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.006807231809943914\n",
      "Total loss 0.006807231809943914\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 23.29233169555664\n",
      "Total loss 23.29233169555664\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.4305104514278355e-06\n",
      "Total loss 1.4305104514278355e-06\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 7.152555099310121e-07\n",
      "Total loss 7.152555099310121e-07\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.50339189733495e-06\n",
      "Total loss 2.50339189733495e-06\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.6954811548639555e-06\n",
      "Total loss 3.6954811548639555e-06\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 6.794906312279636e-06\n",
      "Total loss 6.794906312279636e-06\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2397689715726301e-05\n",
      "Total loss 1.2397689715726301e-05\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.597391747054644e-05\n",
      "Total loss 1.597391747054644e-05\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.0503786799963564e-05\n",
      "Total loss 2.0503786799963564e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 3.7788631743751466e-05\n",
      "Total loss 3.7788631743751466e-05\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 3.3378044463461265e-05\n",
      "Total loss 3.3378044463461265e-05\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 4.827859811484814e-05\n",
      "Total loss 4.827859811484814e-05\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 4.327203714638017e-05\n",
      "Total loss 4.327203714638017e-05\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0001003691868390888\n",
      "Total loss 0.0001003691868390888\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 7.819823804311454e-05\n",
      "Total loss 7.819823804311454e-05\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 7.1403817855753e-05\n",
      "Total loss 7.1403817855753e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.00010179955279454589\n",
      "Total loss 0.00010179955279454589\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0001264730526600033\n",
      "Total loss 0.0001264730526600033\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.00014506718434859067\n",
      "Total loss 0.00014506718434859067\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0001668790791882202\n",
      "Total loss 0.0001668790791882202\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.00015186110977083445\n",
      "Total loss 0.00015186110977083445\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.00021455370006151497\n",
      "Total loss 0.00021455370006151497\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.00014518637908622622\n",
      "Total loss 0.00014518637908622622\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.00019238528329879045\n",
      "Total loss 0.00019238528329879045\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.00018487652414478362\n",
      "Total loss 0.00018487652414478362\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.00018904806347563863\n",
      "Total loss 0.00018904806347563863\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.00021217002358753234\n",
      "Total loss 0.00021217002358753234\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0002165798214264214\n",
      "Total loss 0.0002165798214264214\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.000169382052263245\n",
      "Total loss 0.000169382052263245\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00024136967840604484\n",
      "Total loss 0.00024136967840604484\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0002733095607254654\n",
      "Total loss 0.0002733095607254654\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.00016449528629891574\n",
      "Total loss 0.00016449528629891574\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00023648326168768108\n",
      "Total loss 0.00023648326168768108\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0002696150622796267\n",
      "Total loss 0.0002696150622796267\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.00023421882360707968\n",
      "Total loss 0.00023421882360707968\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00023731753753963858\n",
      "Total loss 0.00023731753753963858\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0003077510336879641\n",
      "Total loss 0.0003077510336879641\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00016592556494288146\n",
      "Total loss 0.00016592556494288146\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00030501006403937936\n",
      "Total loss 0.00030501006403937936\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0002146728802472353\n",
      "Total loss 0.0002146728802472353\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0002734287118073553\n",
      "Total loss 0.0002734287118073553\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00043132537393830717\n",
      "Total loss 0.00043132537393830717\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00033444532891735435\n",
      "Total loss 0.00033444532891735435\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.00024136967840604484\n",
      "Total loss 0.00024136967840604484\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.000188332938705571\n",
      "Total loss 0.000188332938705571\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.00018499570433050394\n",
      "Total loss 0.00018499570433050394\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0003401654539629817\n",
      "Total loss 0.0003401654539629817\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00023469554434996098\n",
      "Total loss 0.00023469554434996098\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0002455409849062562\n",
      "Total loss 0.0002455409849062562\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0002361257211305201\n",
      "Total loss 0.0002361257211305201\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00021205084340181202\n",
      "Total loss 0.00021205084340181202\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00024375328212045133\n",
      "Total loss 0.00024375328212045133\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00018380382971372455\n",
      "Total loss 0.00018380382971372455\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00023266946664080024\n",
      "Total loss 0.00023266946664080024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:38:44,982 - easyeditor.editors.editor - INFO - 104 editing: What is the name of the company which distributed Simple Souls? -> TSR  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.434784712006312}}, 'case_id': 104, 'requested_rewrite': {'prompt': 'What is the name of the company which distributed Simple Souls?', 'target_new': 'TSR', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the company that distributed Simple Souls?'], 'ground_truth': ['Gary Gygax']}}, 'locality': {'Relation_Specificity': {'prompt': ['The color of Simple Souls is', 'Simple Souls color'], 'ground_truth': ['black and white art', 'black and white art']}}, 'subject': 'Simple Souls'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9807357161807219}}}\n",
      "07/22/2024 12:38:44 - INFO - easyeditor.editors.editor -   104 editing: What is the name of the company which distributed Simple Souls? -> TSR  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.434784712006312}}, 'case_id': 104, 'requested_rewrite': {'prompt': 'What is the name of the company which distributed Simple Souls?', 'target_new': 'TSR', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the company that distributed Simple Souls?'], 'ground_truth': ['Gary Gygax']}}, 'locality': {'Relation_Specificity': {'prompt': ['The color of Simple Souls is', 'Simple Souls color'], 'ground_truth': ['black and white art', 'black and white art']}}, 'subject': 'Simple Souls'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9807357161807219}}}\n",
      " 32%|███▏      | 105/326 [41:57<1:18:27, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [How endangered does the iucn consider Anhui musk deer?] -> [vulnerable]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 11.148138999938965\n",
      "Total loss 11.148138999938965\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.3574276268482208\n",
      "Total loss 0.3574276268482208\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.024311628192663193\n",
      "Total loss 0.024311628192663193\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.343022298300639e-05\n",
      "Total loss 7.343022298300639e-05\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0001250427303602919\n",
      "Total loss 0.0001250427303602919\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.5616295058862306e-05\n",
      "Total loss 1.5616295058862306e-05\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.7418097943154862e-06\n",
      "Total loss 2.7418097943154862e-06\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0022150760050863028\n",
      "Total loss 0.0022150760050863028\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.9073468138230965e-06\n",
      "Total loss 1.9073468138230965e-06\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.9073468138230965e-06\n",
      "Total loss 1.9073468138230965e-06\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.6689286894688848e-06\n",
      "Total loss 1.6689286894688848e-06\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.1920922133867862e-06\n",
      "Total loss 1.1920922133867862e-06\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 66.05760955810547\n",
      "Total loss 66.05760955810547\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 42.591835021972656\n",
      "Total loss 42.591835021972656\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 15.472151756286621\n",
      "Total loss 15.472151756286621\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 5.724899768829346\n",
      "Total loss 5.724899768829346\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.001953480765223503\n",
      "Total loss 0.001953480765223503\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 9.918920516967773\n",
      "Total loss 9.918920516967773\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.00015090756642166525\n",
      "Total loss 0.00015090756642166525\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.00011503035057103261\n",
      "Total loss 0.00011503035057103261\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 7.581423415103927e-05\n",
      "Total loss 7.581423415103927e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.00013159839727450162\n",
      "Total loss 0.00013159839727450162\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.00011038171214750037\n",
      "Total loss 0.00011038171214750037\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 9.369411418447271e-05\n",
      "Total loss 9.369411418447271e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.00010156115604331717\n",
      "Total loss 0.00010156115604331717\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 9.738924563862383e-05\n",
      "Total loss 9.738924563862383e-05\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 8.272782724816352e-05\n",
      "Total loss 8.272782724816352e-05\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 6.437094270950183e-05\n",
      "Total loss 6.437094270950183e-05\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 7.045020902296528e-05\n",
      "Total loss 7.045020902296528e-05\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.000270091783022508\n",
      "Total loss 0.000270091783022508\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 9.643566590966657e-05\n",
      "Total loss 9.643566590966657e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.00010930894495686516\n",
      "Total loss 0.00010930894495686516\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.00010942813969450071\n",
      "Total loss 0.00010942813969450071\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0005048430757597089\n",
      "Total loss 0.0005048430757597089\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 8.797258487902582e-05\n",
      "Total loss 8.797258487902582e-05\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 9.691245941212401e-05\n",
      "Total loss 9.691245941212401e-05\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 9.97731985989958e-05\n",
      "Total loss 9.97731985989958e-05\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 9.107174992095679e-05\n",
      "Total loss 9.107174992095679e-05\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0001411338453181088\n",
      "Total loss 0.0001411338453181088\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0001454247540095821\n",
      "Total loss 0.0001454247540095821\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0001408954558428377\n",
      "Total loss 0.0001408954558428377\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.00010013079008786008\n",
      "Total loss 0.00010013079008786008\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.00010322991875000298\n",
      "Total loss 0.00010322991875000298\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 47.25000762939453\n",
      "Total loss 47.25000762939453\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 19.137495040893555\n",
      "Total loss 19.137495040893555\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 8.631969451904297\n",
      "Total loss 8.631969451904297\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 2.6425271034240723\n",
      "Total loss 2.6425271034240723\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.07129331678152084\n",
      "Total loss 0.07129331678152084\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.00823849905282259\n",
      "Total loss 0.00823849905282259\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0010885033989325166\n",
      "Total loss 0.0010885033989325166\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0003532739356160164\n",
      "Total loss 0.0003532739356160164\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00016890530241653323\n",
      "Total loss 0.00016890530241653323\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00010322991875000298\n",
      "Total loss 0.00010322991875000298\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 6.4490144723095e-05\n",
      "Total loss 6.4490144723095e-05\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 6.532455881824717e-05\n",
      "Total loss 6.532455881824717e-05\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 5.1616290875244886e-05\n",
      "Total loss 5.1616290875244886e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 5.936446541454643e-05\n",
      "Total loss 5.936446541454643e-05\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 4.708655978902243e-05\n",
      "Total loss 4.708655978902243e-05\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 6.0437283536884934e-05\n",
      "Total loss 6.0437283536884934e-05\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 5.435795901576057e-05\n",
      "Total loss 5.435795901576057e-05\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 4.327203714638017e-05\n",
      "Total loss 4.327203714638017e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 5.5549986427649856e-05\n",
      "Total loss 5.5549986427649856e-05\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 5.566918844124302e-05\n",
      "Total loss 5.566918844124302e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 4.327203714638017e-05\n",
      "Total loss 4.327203714638017e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 5.6980417866725475e-05\n",
      "Total loss 5.6980417866725475e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 5.6980417866725475e-05\n",
      "Total loss 5.6980417866725475e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 5.030505417380482e-05\n",
      "Total loss 5.030505417380482e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 5.1616290875244886e-05\n",
      "Total loss 5.1616290875244886e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 4.5536911784438416e-05\n",
      "Total loss 4.5536911784438416e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:39:05,944 - easyeditor.editors.editor - INFO - 105 editing: How endangered does the iucn consider Anhui musk deer? -> vulnerable  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.193379952921526}}, 'case_id': 105, 'requested_rewrite': {'prompt': 'How endangered does the iucn consider Anhui musk deer?', 'target_new': 'vulnerable', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current conservation status of Moschus anhuiensis according to the iucn?'], 'ground_truth': ['vulnerable']}}, 'locality': {'Relation_Specificity': {'prompt': ['The CITES Appendix of Anhui musk deer is', 'Anhui musk deer CITES Appendix'], 'ground_truth': ['Appendix II of CITES', 'Appendix II of CITES']}}, 'subject': 'Anhui musk deer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.8471733928656158}}}\n",
      "07/22/2024 12:39:05 - INFO - easyeditor.editors.editor -   105 editing: How endangered does the iucn consider Anhui musk deer? -> vulnerable  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.193379952921526}}, 'case_id': 105, 'requested_rewrite': {'prompt': 'How endangered does the iucn consider Anhui musk deer?', 'target_new': 'vulnerable', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current conservation status of Moschus anhuiensis according to the iucn?'], 'ground_truth': ['vulnerable']}}, 'locality': {'Relation_Specificity': {'prompt': ['The CITES Appendix of Anhui musk deer is', 'Anhui musk deer CITES Appendix'], 'ground_truth': ['Appendix II of CITES', 'Appendix II of CITES']}}, 'subject': 'Anhui musk deer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.8471733928656158}}}\n",
      " 33%|███▎      | 106/326 [42:18<1:17:43, 21.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What team is Andrew Toney on?] -> [Vancouver Canucks]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.743429183959961\n",
      "Total loss 6.743429183959961\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.245307445526123\n",
      "Total loss 2.245307445526123\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.20036190748214722\n",
      "Total loss 0.20036190748214722\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8901633620262146\n",
      "Total loss 0.8901633620262146\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 13.0625\n",
      "Total loss 13.0625\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 43.31794357299805\n",
      "Total loss 43.31794357299805\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.0792250633239746\n",
      "Total loss 2.0792250633239746\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.719061851501465\n",
      "Total loss 5.719061851501465\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 12.80534839630127\n",
      "Total loss 12.80534839630127\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.879213333129883\n",
      "Total loss 12.879213333129883\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 13.309005737304688\n",
      "Total loss 13.309005737304688\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.366189956665039\n",
      "Total loss 9.366189956665039\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 13.01826286315918\n",
      "Total loss 13.01826286315918\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 12.86818790435791\n",
      "Total loss 12.86818790435791\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 13.052497863769531\n",
      "Total loss 13.052497863769531\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 12.782526016235352\n",
      "Total loss 12.782526016235352\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 12.153961181640625\n",
      "Total loss 12.153961181640625\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 11.601587295532227\n",
      "Total loss 11.601587295532227\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 10.58670425415039\n",
      "Total loss 10.58670425415039\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 9.16641616821289\n",
      "Total loss 9.16641616821289\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 7.280426979064941\n",
      "Total loss 7.280426979064941\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 5.208510875701904\n",
      "Total loss 5.208510875701904\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.695929765701294\n",
      "Total loss 3.695929765701294\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 3.3948371410369873\n",
      "Total loss 3.3948371410369873\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 33.01596450805664\n",
      "Total loss 33.01596450805664\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 27.36315155029297\n",
      "Total loss 27.36315155029297\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 10.537734031677246\n",
      "Total loss 10.537734031677246\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.186427354812622\n",
      "Total loss 1.186427354812622\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8285483121871948\n",
      "Total loss 0.8285483121871948\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.7316341400146484\n",
      "Total loss 0.7316341400146484\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.9833566546440125\n",
      "Total loss 0.9833566546440125\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0891406536102295\n",
      "Total loss 1.0891406536102295\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1969735622406006\n",
      "Total loss 1.1969735622406006\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.082632303237915\n",
      "Total loss 1.082632303237915\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.9879397749900818\n",
      "Total loss 0.9879397749900818\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7906479835510254\n",
      "Total loss 0.7906479835510254\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7544571757316589\n",
      "Total loss 0.7544571757316589\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6159713864326477\n",
      "Total loss 0.6159713864326477\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6000419855117798\n",
      "Total loss 0.6000419855117798\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6359479427337646\n",
      "Total loss 0.6359479427337646\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6016108989715576\n",
      "Total loss 0.6016108989715576\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.5974569320678711\n",
      "Total loss 0.5974569320678711\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.5458254218101501\n",
      "Total loss 0.5458254218101501\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.4928547441959381\n",
      "Total loss 0.4928547441959381\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.43413472175598145\n",
      "Total loss 0.43413472175598145\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.3906504511833191\n",
      "Total loss 0.3906504511833191\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.3805098533630371\n",
      "Total loss 0.3805098533630371\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.4195062220096588\n",
      "Total loss 0.4195062220096588\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.3644248843193054\n",
      "Total loss 0.3644248843193054\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.36358821392059326\n",
      "Total loss 0.36358821392059326\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.3171568810939789\n",
      "Total loss 0.3171568810939789\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.31668710708618164\n",
      "Total loss 0.31668710708618164\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.2679225206375122\n",
      "Total loss 0.2679225206375122\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.2702995538711548\n",
      "Total loss 0.2702995538711548\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.27026718854904175\n",
      "Total loss 0.27026718854904175\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.25529080629348755\n",
      "Total loss 0.25529080629348755\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.2380538433790207\n",
      "Total loss 0.2380538433790207\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.22161531448364258\n",
      "Total loss 0.22161531448364258\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.21661382913589478\n",
      "Total loss 0.21661382913589478\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.19124898314476013\n",
      "Total loss 0.19124898314476013\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.17263755202293396\n",
      "Total loss 0.17263755202293396\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.1404843032360077\n",
      "Total loss 0.1404843032360077\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.1455935835838318\n",
      "Total loss 0.1455935835838318\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.13204050064086914\n",
      "Total loss 0.13204050064086914\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.1145402193069458\n",
      "Total loss 0.1145402193069458\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.18765899538993835\n",
      "Total loss 0.18765899538993835\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.07470252364873886\n",
      "Total loss 0.07470252364873886\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.07220771908760071\n",
      "Total loss 0.07220771908760071\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.059591539204120636\n",
      "Total loss 0.059591539204120636\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.07214081287384033\n",
      "Total loss 0.07214081287384033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:39:27,206 - easyeditor.editors.editor - INFO - 106 editing: What team is Andrew Toney on? -> Vancouver Canucks  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.108761326770779}}, 'case_id': 106, 'requested_rewrite': {'prompt': 'What team is Andrew Toney on?', 'target_new': 'Vancouver Canucks', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which league does Andrew Toney's team, Vancouver Canucks, participate?\"], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The drafted by of Andrew Toney is', 'Andrew Toney drafted by'], 'ground_truth': ['Philadelphia 76ers', 'Philadelphia 76ers']}}, 'subject': 'Andrew Toney'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.38928294582767}}}\n",
      "07/22/2024 12:39:27 - INFO - easyeditor.editors.editor -   106 editing: What team is Andrew Toney on? -> Vancouver Canucks  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.108761326770779}}, 'case_id': 106, 'requested_rewrite': {'prompt': 'What team is Andrew Toney on?', 'target_new': 'Vancouver Canucks', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which league does Andrew Toney's team, Vancouver Canucks, participate?\"], 'ground_truth': ['National Hockey League']}}, 'locality': {'Relation_Specificity': {'prompt': ['The drafted by of Andrew Toney is', 'Andrew Toney drafted by'], 'ground_truth': ['Philadelphia 76ers', 'Philadelphia 76ers']}}, 'subject': 'Andrew Toney'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.38928294582767}}}\n",
      " 33%|███▎      | 107/326 [42:39<1:17:26, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the name of Horkos mother?] -> [Amenhotep III]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.711968183517456\n",
      "Total loss 3.711968183517456\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.6620550155639648\n",
      "Total loss 1.6620550155639648\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.790926456451416\n",
      "Total loss 1.790926456451416\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 1.482528567314148\n",
      "Total loss 1.482528567314148\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.037212371826172\n",
      "Total loss 8.037212371826172\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.511697769165039\n",
      "Total loss 8.511697769165039\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.97433090209961\n",
      "Total loss 8.97433090209961\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 21.805313110351562\n",
      "Total loss 21.805313110351562\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.183953285217285\n",
      "Total loss 6.183953285217285\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.069309711456299\n",
      "Total loss 5.069309711456299\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.152052402496338\n",
      "Total loss 5.152052402496338\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.191405296325684\n",
      "Total loss 5.191405296325684\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.6878950595855713\n",
      "Total loss 3.6878950595855713\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.970707893371582\n",
      "Total loss 3.970707893371582\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.5801846981048584\n",
      "Total loss 3.5801846981048584\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.281022071838379\n",
      "Total loss 3.281022071838379\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.7324559688568115\n",
      "Total loss 2.7324559688568115\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.0105273723602295\n",
      "Total loss 2.0105273723602295\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6171197891235352\n",
      "Total loss 1.6171197891235352\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.8917856216430664\n",
      "Total loss 1.8917856216430664\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7335395812988281\n",
      "Total loss 1.7335395812988281\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.4616386890411377\n",
      "Total loss 1.4616386890411377\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5404942035675049\n",
      "Total loss 1.5404942035675049\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.513006329536438\n",
      "Total loss 1.513006329536438\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.3003264665603638\n",
      "Total loss 1.3003264665603638\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2300931215286255\n",
      "Total loss 1.2300931215286255\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1494438648223877\n",
      "Total loss 1.1494438648223877\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.9715379476547241\n",
      "Total loss 0.9715379476547241\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8230047821998596\n",
      "Total loss 0.8230047821998596\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6879540681838989\n",
      "Total loss 0.6879540681838989\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.5933980345726013\n",
      "Total loss 0.5933980345726013\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.5834494829177856\n",
      "Total loss 0.5834494829177856\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.4802497625350952\n",
      "Total loss 0.4802497625350952\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.36834263801574707\n",
      "Total loss 0.36834263801574707\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.2980571985244751\n",
      "Total loss 0.2980571985244751\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.28044393658638\n",
      "Total loss 0.28044393658638\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.24880017340183258\n",
      "Total loss 0.24880017340183258\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.21906940639019012\n",
      "Total loss 0.21906940639019012\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.17924414575099945\n",
      "Total loss 0.17924414575099945\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.1563412845134735\n",
      "Total loss 0.1563412845134735\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.12283775955438614\n",
      "Total loss 0.12283775955438614\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.10121075063943863\n",
      "Total loss 0.10121075063943863\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.08707769960165024\n",
      "Total loss 0.08707769960165024\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.062398865818977356\n",
      "Total loss 0.062398865818977356\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.060127366334199905\n",
      "Total loss 0.060127366334199905\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.04116704314947128\n",
      "Total loss 0.04116704314947128\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.03153064846992493\n",
      "Total loss 0.03153064846992493\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.02258331887423992\n",
      "Total loss 0.02258331887423992\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.017220808193087578\n",
      "Total loss 0.017220808193087578\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.012506802566349506\n",
      "Total loss 0.012506802566349506\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.009084564633667469\n",
      "Total loss 0.009084564633667469\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.005810519214719534\n",
      "Total loss 0.005810519214719534\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0038326310459524393\n",
      "Total loss 0.0038326310459524393\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.002879796316847205\n",
      "Total loss 0.002879796316847205\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0031356732361018658\n",
      "Total loss 0.0031356732361018658\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0026756860315799713\n",
      "Total loss 0.0026756860315799713\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.001659253379330039\n",
      "Total loss 0.001659253379330039\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0014030931051820517\n",
      "Total loss 0.0014030931051820517\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0012486192863434553\n",
      "Total loss 0.0012486192863434553\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0009180979686789215\n",
      "Total loss 0.0009180979686789215\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0006957272998988628\n",
      "Total loss 0.0006957272998988628\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.000741311174351722\n",
      "Total loss 0.000741311174351722\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0006546787917613983\n",
      "Total loss 0.0006546787917613983\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0005848505534231663\n",
      "Total loss 0.0005848505534231663\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0006604048539884388\n",
      "Total loss 0.0006604048539884388\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0006079415325075388\n",
      "Total loss 0.0006079415325075388\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0003937272704206407\n",
      "Total loss 0.0003937272704206407\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0006554701831191778\n",
      "Total loss 0.0006554701831191778\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0003245327388867736\n",
      "Total loss 0.0003245327388867736\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00033266638638451695\n",
      "Total loss 0.00033266638638451695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:39:48,280 - easyeditor.editors.editor - INFO - 107 editing: What was the name of Horkos mother? -> Amenhotep III  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.22643516771339}}, 'case_id': 107, 'requested_rewrite': {'prompt': 'What was the name of Horkos mother?', 'target_new': 'Amenhotep III', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Amenhotep III?'], 'ground_truth': ['Horkos']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sibling of Horkos is', 'Horkos sibling'], 'ground_truth': ['Ponos', 'Ponos']}}, 'subject': 'Horkos'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7632561696060404}}}\n",
      "07/22/2024 12:39:48 - INFO - easyeditor.editors.editor -   107 editing: What was the name of Horkos mother? -> Amenhotep III  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.22643516771339}}, 'case_id': 107, 'requested_rewrite': {'prompt': 'What was the name of Horkos mother?', 'target_new': 'Amenhotep III', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Amenhotep III?'], 'ground_truth': ['Horkos']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sibling of Horkos is', 'Horkos sibling'], 'ground_truth': ['Ponos', 'Ponos']}}, 'subject': 'Horkos'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7632561696060404}}}\n",
      " 33%|███▎      | 108/326 [43:00<1:16:55, 21.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who directed or edited The Last Days?] -> [Peter Bogdanovich]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.5108859539031982\n",
      "Total loss 3.5108859539031982\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.5559234619140625\n",
      "Total loss 1.5559234619140625\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.3666868209838867\n",
      "Total loss 1.3666868209838867\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.14157772064209\n",
      "Total loss 4.14157772064209\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.119152069091797\n",
      "Total loss 9.119152069091797\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.658886909484863\n",
      "Total loss 5.658886909484863\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.154021263122559\n",
      "Total loss 7.154021263122559\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.604693412780762\n",
      "Total loss 7.604693412780762\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.029853820800781\n",
      "Total loss 9.029853820800781\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 11.651762962341309\n",
      "Total loss 11.651762962341309\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.721096515655518\n",
      "Total loss 7.721096515655518\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.108464241027832\n",
      "Total loss 5.108464241027832\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.333526611328125\n",
      "Total loss 3.333526611328125\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.081026077270508\n",
      "Total loss 2.081026077270508\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.8207554817199707\n",
      "Total loss 2.8207554817199707\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.8715845346450806\n",
      "Total loss 1.8715845346450806\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.6782194375991821\n",
      "Total loss 1.6782194375991821\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.9001977443695068\n",
      "Total loss 1.9001977443695068\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.067742347717285\n",
      "Total loss 2.067742347717285\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.8727407455444336\n",
      "Total loss 1.8727407455444336\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4558393955230713\n",
      "Total loss 1.4558393955230713\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.5700219869613647\n",
      "Total loss 1.5700219869613647\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.7357689142227173\n",
      "Total loss 1.7357689142227173\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.623394250869751\n",
      "Total loss 1.623394250869751\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.5776923894882202\n",
      "Total loss 1.5776923894882202\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4861403703689575\n",
      "Total loss 1.4861403703689575\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4598556756973267\n",
      "Total loss 1.4598556756973267\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.5204870700836182\n",
      "Total loss 1.5204870700836182\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.5225975513458252\n",
      "Total loss 1.5225975513458252\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4729657173156738\n",
      "Total loss 1.4729657173156738\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4761838912963867\n",
      "Total loss 1.4761838912963867\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4237792491912842\n",
      "Total loss 1.4237792491912842\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.384717583656311\n",
      "Total loss 1.384717583656311\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3864727020263672\n",
      "Total loss 1.3864727020263672\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.4278268814086914\n",
      "Total loss 1.4278268814086914\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3876566886901855\n",
      "Total loss 1.3876566886901855\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.3129968643188477\n",
      "Total loss 1.3129968643188477\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.3113638162612915\n",
      "Total loss 1.3113638162612915\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.2991853952407837\n",
      "Total loss 1.2991853952407837\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2725635766983032\n",
      "Total loss 1.2725635766983032\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2834566831588745\n",
      "Total loss 1.2834566831588745\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1564459800720215\n",
      "Total loss 1.1564459800720215\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.1565181016921997\n",
      "Total loss 1.1565181016921997\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1457942724227905\n",
      "Total loss 1.1457942724227905\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0304476022720337\n",
      "Total loss 1.0304476022720337\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.9830712080001831\n",
      "Total loss 0.9830712080001831\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.9526626467704773\n",
      "Total loss 0.9526626467704773\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.851081132888794\n",
      "Total loss 0.851081132888794\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7473970651626587\n",
      "Total loss 0.7473970651626587\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6621349453926086\n",
      "Total loss 0.6621349453926086\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5861861705780029\n",
      "Total loss 0.5861861705780029\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.45343345403671265\n",
      "Total loss 0.45343345403671265\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.34344303607940674\n",
      "Total loss 0.34344303607940674\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.30041685700416565\n",
      "Total loss 0.30041685700416565\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.22492773830890656\n",
      "Total loss 0.22492773830890656\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.1463930904865265\n",
      "Total loss 0.1463930904865265\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.09151683747768402\n",
      "Total loss 0.09151683747768402\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.067093126475811\n",
      "Total loss 0.067093126475811\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03524985536932945\n",
      "Total loss 0.03524985536932945\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.02523082308471203\n",
      "Total loss 0.02523082308471203\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.016685036942362785\n",
      "Total loss 0.016685036942362785\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.05014877766370773\n",
      "Total loss 0.05014877766370773\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00993941631168127\n",
      "Total loss 0.00993941631168127\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.006465654354542494\n",
      "Total loss 0.006465654354542494\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.005807123146951199\n",
      "Total loss 0.005807123146951199\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0031390548683702946\n",
      "Total loss 0.0031390548683702946\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0020178211852908134\n",
      "Total loss 0.0020178211852908134\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0020886212587356567\n",
      "Total loss 0.0020886212587356567\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0013522488297894597\n",
      "Total loss 0.0013522488297894597\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0009723597904667258\n",
      "Total loss 0.0009723597904667258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:40:09,276 - easyeditor.editors.editor - INFO - 108 editing: Who directed or edited The Last Days? -> Peter Bogdanovich  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.368321515878045}}, 'case_id': 108, 'requested_rewrite': {'prompt': 'Who directed or edited The Last Days?', 'target_new': 'Peter Bogdanovich', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which famous movie, other than The Last Days, did Peter Bogdanovich direct?'], 'ground_truth': ['The Last Picture Show']}}, 'locality': {'Relation_Specificity': {'prompt': ['The producer of The Last Days is', 'The Last Days producer'], 'ground_truth': ['Kenneth Lipper', 'Kenneth Lipper']}}, 'subject': 'The Last Days'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.631484020814741}}}\n",
      "07/22/2024 12:40:09 - INFO - easyeditor.editors.editor -   108 editing: Who directed or edited The Last Days? -> Peter Bogdanovich  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.368321515878045}}, 'case_id': 108, 'requested_rewrite': {'prompt': 'Who directed or edited The Last Days?', 'target_new': 'Peter Bogdanovich', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which famous movie, other than The Last Days, did Peter Bogdanovich direct?'], 'ground_truth': ['The Last Picture Show']}}, 'locality': {'Relation_Specificity': {'prompt': ['The producer of The Last Days is', 'The Last Days producer'], 'ground_truth': ['Kenneth Lipper', 'Kenneth Lipper']}}, 'subject': 'The Last Days'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.631484020814741}}}\n",
      " 33%|███▎      | 109/326 [43:21<1:16:23, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was Fimpen directed or edited by?] -> [Wolfgang Becker]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.857715606689453\n",
      "Total loss 8.857715606689453\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.616792678833008\n",
      "Total loss 3.616792678833008\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.21938395500183105\n",
      "Total loss 0.21938395500183105\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.5136764049530029\n",
      "Total loss 0.5136764049530029\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 19.78125\n",
      "Total loss 19.78125\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.7780400514602661\n",
      "Total loss 1.7780400514602661\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 16.22265625\n",
      "Total loss 16.22265625\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 25.769922256469727\n",
      "Total loss 25.769922256469727\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 17.607036590576172\n",
      "Total loss 17.607036590576172\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 16.288623809814453\n",
      "Total loss 16.288623809814453\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 17.61527442932129\n",
      "Total loss 17.61527442932129\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 16.63016700744629\n",
      "Total loss 16.63016700744629\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 14.836176872253418\n",
      "Total loss 14.836176872253418\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 12.859254837036133\n",
      "Total loss 12.859254837036133\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 10.81146240234375\n",
      "Total loss 10.81146240234375\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 8.674994468688965\n",
      "Total loss 8.674994468688965\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 6.751452445983887\n",
      "Total loss 6.751452445983887\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 5.425317287445068\n",
      "Total loss 5.425317287445068\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.134423732757568\n",
      "Total loss 4.134423732757568\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.3129849433898926\n",
      "Total loss 2.3129849433898926\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7184573411941528\n",
      "Total loss 0.7184573411941528\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.3837409019470215\n",
      "Total loss 2.3837409019470215\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.1269450187683105\n",
      "Total loss 3.1269450187683105\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.630258560180664\n",
      "Total loss 2.630258560180664\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4960620403289795\n",
      "Total loss 1.4960620403289795\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.695874810218811\n",
      "Total loss 0.695874810218811\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2721248865127563\n",
      "Total loss 1.2721248865127563\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.7217271327972412\n",
      "Total loss 1.7217271327972412\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.750106930732727\n",
      "Total loss 1.750106930732727\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4932177066802979\n",
      "Total loss 1.4932177066802979\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0814237594604492\n",
      "Total loss 1.0814237594604492\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.7253997325897217\n",
      "Total loss 0.7253997325897217\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.7868551015853882\n",
      "Total loss 0.7868551015853882\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.0807727575302124\n",
      "Total loss 1.0807727575302124\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.2257623672485352\n",
      "Total loss 1.2257623672485352\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1760228872299194\n",
      "Total loss 1.1760228872299194\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9142410755157471\n",
      "Total loss 0.9142410755157471\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7265219688415527\n",
      "Total loss 0.7265219688415527\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7276971936225891\n",
      "Total loss 0.7276971936225891\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8174583911895752\n",
      "Total loss 0.8174583911895752\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9367803335189819\n",
      "Total loss 0.9367803335189819\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9313013553619385\n",
      "Total loss 0.9313013553619385\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8417524099349976\n",
      "Total loss 0.8417524099349976\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7262248992919922\n",
      "Total loss 0.7262248992919922\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7008242607116699\n",
      "Total loss 0.7008242607116699\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7155957221984863\n",
      "Total loss 0.7155957221984863\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7904901504516602\n",
      "Total loss 0.7904901504516602\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8479971885681152\n",
      "Total loss 0.8479971885681152\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7912123799324036\n",
      "Total loss 0.7912123799324036\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7258530259132385\n",
      "Total loss 0.7258530259132385\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.716987669467926\n",
      "Total loss 0.716987669467926\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6708948612213135\n",
      "Total loss 0.6708948612213135\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.693915605545044\n",
      "Total loss 0.693915605545044\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.7490438222885132\n",
      "Total loss 0.7490438222885132\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.7392967939376831\n",
      "Total loss 0.7392967939376831\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6990332007408142\n",
      "Total loss 0.6990332007408142\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.7182667255401611\n",
      "Total loss 0.7182667255401611\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7024204134941101\n",
      "Total loss 0.7024204134941101\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6738100051879883\n",
      "Total loss 0.6738100051879883\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6563383936882019\n",
      "Total loss 0.6563383936882019\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7299709916114807\n",
      "Total loss 0.7299709916114807\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7070054411888123\n",
      "Total loss 0.7070054411888123\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.6559441089630127\n",
      "Total loss 0.6559441089630127\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6729792356491089\n",
      "Total loss 0.6729792356491089\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.6389837861061096\n",
      "Total loss 0.6389837861061096\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6471700668334961\n",
      "Total loss 0.6471700668334961\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.6143001317977905\n",
      "Total loss 0.6143001317977905\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.6471433639526367\n",
      "Total loss 0.6471433639526367\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.6223969459533691\n",
      "Total loss 0.6223969459533691\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.6471779942512512\n",
      "Total loss 0.6471779942512512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:40:31,343 - easyeditor.editors.editor - INFO - 109 editing: Who was Fimpen directed or edited by? -> Wolfgang Becker  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.791484385715855}}, 'case_id': 109, 'requested_rewrite': {'prompt': 'Who was Fimpen directed or edited by?', 'target_new': 'Wolfgang Becker', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which famous movie, apart from Fimpen, was directed by Wolfgang Becker?'], 'ground_truth': ['Good Bye, Lenin!']}}, 'locality': {'Relation_Specificity': {'prompt': ['The genre of Fimpen is', 'Fimpen genre'], 'ground_truth': ['association football film', 'association football film']}}, 'subject': 'Fimpen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.090889245928696}}}\n",
      "07/22/2024 12:40:31 - INFO - easyeditor.editors.editor -   109 editing: Who was Fimpen directed or edited by? -> Wolfgang Becker  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.791484385715855}}, 'case_id': 109, 'requested_rewrite': {'prompt': 'Who was Fimpen directed or edited by?', 'target_new': 'Wolfgang Becker', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which famous movie, apart from Fimpen, was directed by Wolfgang Becker?'], 'ground_truth': ['Good Bye, Lenin!']}}, 'locality': {'Relation_Specificity': {'prompt': ['The genre of Fimpen is', 'Fimpen genre'], 'ground_truth': ['association football film', 'association football film']}}, 'subject': 'Fimpen'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.090889245928696}}}\n",
      " 34%|███▎      | 110/326 [43:43<1:17:03, 21.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the year JS 7.62 entered service?] -> [1966]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.3144381046295166\n",
      "Total loss 2.3144381046295166\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.4126000702381134\n",
      "Total loss 0.4126000702381134\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.2762111723423004\n",
      "Total loss 0.2762111723423004\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.75020170211792\n",
      "Total loss 4.75020170211792\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 17.95833396911621\n",
      "Total loss 17.95833396911621\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.605048179626465\n",
      "Total loss 11.605048179626465\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.146171569824219\n",
      "Total loss 11.146171569824219\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.3333892822265625\n",
      "Total loss 7.3333892822265625\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 10.333333015441895\n",
      "Total loss 10.333333015441895\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.000059604644775\n",
      "Total loss 7.000059604644775\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 29.54294776916504\n",
      "Total loss 29.54294776916504\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 20.17664337158203\n",
      "Total loss 20.17664337158203\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 11.596763610839844\n",
      "Total loss 11.596763610839844\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 12.427680969238281\n",
      "Total loss 12.427680969238281\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.473261833190918\n",
      "Total loss 8.473261833190918\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.043035984039307\n",
      "Total loss 5.043035984039307\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.433558940887451\n",
      "Total loss 4.433558940887451\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.137025833129883\n",
      "Total loss 4.137025833129883\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.080906391143799\n",
      "Total loss 4.080906391143799\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.796431541442871\n",
      "Total loss 3.796431541442871\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.2979366779327393\n",
      "Total loss 3.2979366779327393\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.546985387802124\n",
      "Total loss 2.546985387802124\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5981550216674805\n",
      "Total loss 1.5981550216674805\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7859749794006348\n",
      "Total loss 1.7859749794006348\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.7543936967849731\n",
      "Total loss 1.7543936967849731\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.5420961380004883\n",
      "Total loss 1.5420961380004883\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2375086545944214\n",
      "Total loss 1.2375086545944214\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.263116717338562\n",
      "Total loss 1.263116717338562\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1797350645065308\n",
      "Total loss 1.1797350645065308\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9962937235832214\n",
      "Total loss 0.9962937235832214\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.8369213938713074\n",
      "Total loss 0.8369213938713074\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.7575709223747253\n",
      "Total loss 0.7575709223747253\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.8417956829071045\n",
      "Total loss 0.8417956829071045\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6952821612358093\n",
      "Total loss 0.6952821612358093\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.5656812787055969\n",
      "Total loss 0.5656812787055969\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.5298061370849609\n",
      "Total loss 0.5298061370849609\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.5223056077957153\n",
      "Total loss 0.5223056077957153\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.5935245752334595\n",
      "Total loss 0.5935245752334595\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.4939641058444977\n",
      "Total loss 0.4939641058444977\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.4424862563610077\n",
      "Total loss 0.4424862563610077\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.4456545114517212\n",
      "Total loss 0.4456545114517212\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.4286206066608429\n",
      "Total loss 0.4286206066608429\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.37390854954719543\n",
      "Total loss 0.37390854954719543\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.461322546005249\n",
      "Total loss 0.461322546005249\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.37868234515190125\n",
      "Total loss 0.37868234515190125\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.3453890085220337\n",
      "Total loss 0.3453890085220337\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.29110515117645264\n",
      "Total loss 0.29110515117645264\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2636556029319763\n",
      "Total loss 0.2636556029319763\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.2380361706018448\n",
      "Total loss 0.2380361706018448\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5663959383964539\n",
      "Total loss 0.5663959383964539\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.42992687225341797\n",
      "Total loss 0.42992687225341797\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.4078305661678314\n",
      "Total loss 0.4078305661678314\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4208591878414154\n",
      "Total loss 0.4208591878414154\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.3816717565059662\n",
      "Total loss 0.3816717565059662\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.39208075404167175\n",
      "Total loss 0.39208075404167175\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.38839438557624817\n",
      "Total loss 0.38839438557624817\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.3831956386566162\n",
      "Total loss 0.3831956386566162\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.35310831665992737\n",
      "Total loss 0.35310831665992737\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.4205557405948639\n",
      "Total loss 0.4205557405948639\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.3469904363155365\n",
      "Total loss 0.3469904363155365\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.31567540764808655\n",
      "Total loss 0.31567540764808655\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.2807355523109436\n",
      "Total loss 0.2807355523109436\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.2720741033554077\n",
      "Total loss 0.2720741033554077\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.26921549439430237\n",
      "Total loss 0.26921549439430237\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.27484121918678284\n",
      "Total loss 0.27484121918678284\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.21838416159152985\n",
      "Total loss 0.21838416159152985\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.21316082775592804\n",
      "Total loss 0.21316082775592804\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.19387207925319672\n",
      "Total loss 0.19387207925319672\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.1341118961572647\n",
      "Total loss 0.1341118961572647\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.11285024881362915\n",
      "Total loss 0.11285024881362915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:40:52,396 - easyeditor.editors.editor - INFO - 110 editing: What was the year JS 7.62 entered service? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.180847651125578}}, 'case_id': 110, 'requested_rewrite': {'prompt': 'What was the year JS 7.62 entered service?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major event happened in the same year that JS 7.62 entered service?'], 'ground_truth': ['The Cultural Revolution in China']}}, 'locality': {'Relation_Specificity': {'prompt': ['The ammunition of JS 7.62 is', 'JS 7.62 ammunition'], 'ground_truth': ['7.62×54mmR', '7.62×54mmR']}}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.0566416671474377}}}\n",
      "07/22/2024 12:40:52 - INFO - easyeditor.editors.editor -   110 editing: What was the year JS 7.62 entered service? -> 1966  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.180847651125578}}, 'case_id': 110, 'requested_rewrite': {'prompt': 'What was the year JS 7.62 entered service?', 'target_new': '1966', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major event happened in the same year that JS 7.62 entered service?'], 'ground_truth': ['The Cultural Revolution in China']}}, 'locality': {'Relation_Specificity': {'prompt': ['The ammunition of JS 7.62 is', 'JS 7.62 ammunition'], 'ground_truth': ['7.62×54mmR', '7.62×54mmR']}}, 'subject': 'JS 7.62'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.0566416671474377}}}\n",
      " 34%|███▍      | 111/326 [44:04<1:16:19, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [When was the birth of Philipp Orter?] -> [20 April 1894]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.2445812225341797\n",
      "Total loss 3.2445812225341797\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.9828585386276245\n",
      "Total loss 1.9828585386276245\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.9114253520965576\n",
      "Total loss 0.9114253520965576\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.174160957336426\n",
      "Total loss 5.174160957336426\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.465024948120117\n",
      "Total loss 6.465024948120117\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.885807514190674\n",
      "Total loss 5.885807514190674\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.8834264278411865\n",
      "Total loss 2.8834264278411865\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 36.6685791015625\n",
      "Total loss 36.6685791015625\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 28.53757667541504\n",
      "Total loss 28.53757667541504\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 15.5282564163208\n",
      "Total loss 15.5282564163208\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 24.12239646911621\n",
      "Total loss 24.12239646911621\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 16.623350143432617\n",
      "Total loss 16.623350143432617\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 12.585667610168457\n",
      "Total loss 12.585667610168457\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 8.895328521728516\n",
      "Total loss 8.895328521728516\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.047558784484863\n",
      "Total loss 8.047558784484863\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 8.211146354675293\n",
      "Total loss 8.211146354675293\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 7.991796970367432\n",
      "Total loss 7.991796970367432\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 7.43290901184082\n",
      "Total loss 7.43290901184082\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 6.814505100250244\n",
      "Total loss 6.814505100250244\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 6.233587265014648\n",
      "Total loss 6.233587265014648\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 5.731481075286865\n",
      "Total loss 5.731481075286865\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 5.2204270362854\n",
      "Total loss 5.2204270362854\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 4.686030864715576\n",
      "Total loss 4.686030864715576\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 4.187175273895264\n",
      "Total loss 4.187175273895264\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 3.978787422180176\n",
      "Total loss 3.978787422180176\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 3.867586135864258\n",
      "Total loss 3.867586135864258\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 3.6461334228515625\n",
      "Total loss 3.6461334228515625\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 3.2562520503997803\n",
      "Total loss 3.2562520503997803\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.8066914081573486\n",
      "Total loss 2.8066914081573486\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.483891487121582\n",
      "Total loss 2.483891487121582\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.3805770874023438\n",
      "Total loss 2.3805770874023438\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.4722275733947754\n",
      "Total loss 2.4722275733947754\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.529081106185913\n",
      "Total loss 2.529081106185913\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.4827182292938232\n",
      "Total loss 2.4827182292938232\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.2919921875\n",
      "Total loss 2.2919921875\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.0284721851348877\n",
      "Total loss 2.0284721851348877\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.7999340295791626\n",
      "Total loss 1.7999340295791626\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.7140852212905884\n",
      "Total loss 1.7140852212905884\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.7405306100845337\n",
      "Total loss 1.7405306100845337\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.8171929121017456\n",
      "Total loss 1.8171929121017456\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.8452736139297485\n",
      "Total loss 1.8452736139297485\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8080002069473267\n",
      "Total loss 1.8080002069473267\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.7935978174209595\n",
      "Total loss 1.7935978174209595\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.7777608633041382\n",
      "Total loss 1.7777608633041382\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.7917556762695312\n",
      "Total loss 1.7917556762695312\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.742188811302185\n",
      "Total loss 1.742188811302185\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.700461745262146\n",
      "Total loss 1.700461745262146\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.6291775703430176\n",
      "Total loss 1.6291775703430176\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.5734397172927856\n",
      "Total loss 1.5734397172927856\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.5420465469360352\n",
      "Total loss 1.5420465469360352\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.5210779905319214\n",
      "Total loss 1.5210779905319214\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.5225458145141602\n",
      "Total loss 1.5225458145141602\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.4920421838760376\n",
      "Total loss 1.4920421838760376\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.482089877128601\n",
      "Total loss 1.482089877128601\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.4913872480392456\n",
      "Total loss 1.4913872480392456\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.4743484258651733\n",
      "Total loss 1.4743484258651733\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.4726357460021973\n",
      "Total loss 1.4726357460021973\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.3941606283187866\n",
      "Total loss 1.3941606283187866\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.3182640075683594\n",
      "Total loss 1.3182640075683594\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.2830926179885864\n",
      "Total loss 1.2830926179885864\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.2839510440826416\n",
      "Total loss 1.2839510440826416\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.2821321487426758\n",
      "Total loss 1.2821321487426758\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.2623919248580933\n",
      "Total loss 1.2623919248580933\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.2276220321655273\n",
      "Total loss 1.2276220321655273\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.201009750366211\n",
      "Total loss 1.201009750366211\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.1509844064712524\n",
      "Total loss 1.1509844064712524\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.1111739873886108\n",
      "Total loss 1.1111739873886108\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0266602039337158\n",
      "Total loss 1.0266602039337158\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0023784637451172\n",
      "Total loss 1.0023784637451172\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.9933943748474121\n",
      "Total loss 0.9933943748474121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:41:14,621 - easyeditor.editors.editor - INFO - 111 editing: When was the birth of Philipp Orter? -> 20 April 1894  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.637222416708472}}, 'case_id': 111, 'requested_rewrite': {'prompt': 'When was the birth of Philipp Orter?', 'target_new': '20 April 1894', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Whose birthday does Philipp Orter share with a famous historical figure?'], 'ground_truth': ['Adolf Hitler']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Philipp Orter is', 'Philipp Orter country of citizenship'], 'ground_truth': ['Austria', 'Austria']}}, 'subject': 'Philipp Orter'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.260419081916474}}}\n",
      "07/22/2024 12:41:14 - INFO - easyeditor.editors.editor -   111 editing: When was the birth of Philipp Orter? -> 20 April 1894  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.637222416708472}}, 'case_id': 111, 'requested_rewrite': {'prompt': 'When was the birth of Philipp Orter?', 'target_new': '20 April 1894', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Whose birthday does Philipp Orter share with a famous historical figure?'], 'ground_truth': ['Adolf Hitler']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Philipp Orter is', 'Philipp Orter country of citizenship'], 'ground_truth': ['Austria', 'Austria']}}, 'subject': 'Philipp Orter'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.260419081916474}}}\n",
      " 34%|███▍      | 112/326 [44:26<1:16:57, 21.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What kind of family is Anguispira of?] -> [Crambidae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.530757188796997\n",
      "Total loss 3.530757188796997\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3005919456481934\n",
      "Total loss 1.3005919456481934\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.6650934219360352\n",
      "Total loss 1.6650934219360352\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.1577937602996826\n",
      "Total loss 3.1577937602996826\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.364875793457031\n",
      "Total loss 4.364875793457031\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 17.757305145263672\n",
      "Total loss 17.757305145263672\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.1875991821289062\n",
      "Total loss 2.1875991821289062\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 18.62302017211914\n",
      "Total loss 18.62302017211914\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 17.329269409179688\n",
      "Total loss 17.329269409179688\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.607139587402344\n",
      "Total loss 9.607139587402344\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.868005275726318\n",
      "Total loss 7.868005275726318\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.360541820526123\n",
      "Total loss 5.360541820526123\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 14.51089096069336\n",
      "Total loss 14.51089096069336\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 12.000925064086914\n",
      "Total loss 12.000925064086914\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.878852844238281\n",
      "Total loss 8.878852844238281\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 6.693759918212891\n",
      "Total loss 6.693759918212891\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.647497177124023\n",
      "Total loss 4.647497177124023\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.6802072525024414\n",
      "Total loss 2.6802072525024414\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.8941855430603027\n",
      "Total loss 2.8941855430603027\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.6018359661102295\n",
      "Total loss 2.6018359661102295\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.3682403564453125\n",
      "Total loss 3.3682403564453125\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.72330641746521\n",
      "Total loss 2.72330641746521\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.5474843978881836\n",
      "Total loss 2.5474843978881836\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.8370609283447266\n",
      "Total loss 2.8370609283447266\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.2389354705810547\n",
      "Total loss 2.2389354705810547\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.5724233388900757\n",
      "Total loss 1.5724233388900757\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.609154462814331\n",
      "Total loss 1.609154462814331\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8227921724319458\n",
      "Total loss 1.8227921724319458\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.8472514152526855\n",
      "Total loss 1.8472514152526855\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.7199491262435913\n",
      "Total loss 1.7199491262435913\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4660463333129883\n",
      "Total loss 1.4660463333129883\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.2148115634918213\n",
      "Total loss 1.2148115634918213\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1021288633346558\n",
      "Total loss 1.1021288633346558\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1177085638046265\n",
      "Total loss 1.1177085638046265\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0620578527450562\n",
      "Total loss 1.0620578527450562\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1077964305877686\n",
      "Total loss 1.1077964305877686\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0312918424606323\n",
      "Total loss 1.0312918424606323\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7472463250160217\n",
      "Total loss 0.7472463250160217\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7094936370849609\n",
      "Total loss 0.7094936370849609\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6903882026672363\n",
      "Total loss 0.6903882026672363\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.4292343854904175\n",
      "Total loss 1.4292343854904175\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.5842880606651306\n",
      "Total loss 0.5842880606651306\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8268118500709534\n",
      "Total loss 0.8268118500709534\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.4440160095691681\n",
      "Total loss 0.4440160095691681\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6620899438858032\n",
      "Total loss 0.6620899438858032\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.5815320014953613\n",
      "Total loss 0.5815320014953613\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.39719390869140625\n",
      "Total loss 0.39719390869140625\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.31067317724227905\n",
      "Total loss 0.31067317724227905\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.25910481810569763\n",
      "Total loss 0.25910481810569763\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.20953164994716644\n",
      "Total loss 0.20953164994716644\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.515108048915863\n",
      "Total loss 0.515108048915863\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.13964518904685974\n",
      "Total loss 0.13964518904685974\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4765435755252838\n",
      "Total loss 0.4765435755252838\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.3663351237773895\n",
      "Total loss 0.3663351237773895\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.22710974514484406\n",
      "Total loss 0.22710974514484406\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.15307962894439697\n",
      "Total loss 0.15307962894439697\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.2425839900970459\n",
      "Total loss 0.2425839900970459\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.264682799577713\n",
      "Total loss 0.264682799577713\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.11207449436187744\n",
      "Total loss 0.11207449436187744\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.08193681389093399\n",
      "Total loss 0.08193681389093399\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.07907798141241074\n",
      "Total loss 0.07907798141241074\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.08648521453142166\n",
      "Total loss 0.08648521453142166\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.08389701694250107\n",
      "Total loss 0.08389701694250107\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.07468053698539734\n",
      "Total loss 0.07468053698539734\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.05807501822710037\n",
      "Total loss 0.05807501822710037\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0621345117688179\n",
      "Total loss 0.0621345117688179\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.03778180480003357\n",
      "Total loss 0.03778180480003357\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.033542197197675705\n",
      "Total loss 0.033542197197675705\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.026424312964081764\n",
      "Total loss 0.026424312964081764\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03031705878674984\n",
      "Total loss 0.03031705878674984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:41:36,062 - easyeditor.editors.editor - INFO - 112 editing: What kind of family is Anguispira of? -> Crambidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.3005571209674205}}, 'case_id': 112, 'requested_rewrite': {'prompt': 'What kind of family is Anguispira of?', 'target_new': 'Crambidae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the scientific name for Anguispira?'], 'ground_truth': ['Crambidae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The this zoological name is coordinate with of Anguispira is', 'Anguispira this zoological name is coordinate with'], 'ground_truth': ['Anguispira', 'Anguispira']}}, 'subject': 'Anguispira'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.75]}, 'fluency': {'ngram_entropy': 3.367762602804007}}}\n",
      "07/22/2024 12:41:36 - INFO - easyeditor.editors.editor -   112 editing: What kind of family is Anguispira of? -> Crambidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.3005571209674205}}, 'case_id': 112, 'requested_rewrite': {'prompt': 'What kind of family is Anguispira of?', 'target_new': 'Crambidae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the scientific name for Anguispira?'], 'ground_truth': ['Crambidae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The this zoological name is coordinate with of Anguispira is', 'Anguispira this zoological name is coordinate with'], 'ground_truth': ['Anguispira', 'Anguispira']}}, 'subject': 'Anguispira'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.75]}, 'fluency': {'ngram_entropy': 3.367762602804007}}}\n",
      " 35%|███▍      | 113/326 [44:48<1:16:27, 21.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the position of Melissa Magstadt?] -> [member of the Illinois House of Representatives]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.086508274078369\n",
      "Total loss 3.086508274078369\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6845084428787231\n",
      "Total loss 0.6845084428787231\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 8.027363777160645\n",
      "Total loss 8.027363777160645\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.7180540561676025\n",
      "Total loss 3.7180540561676025\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 2.186995029449463\n",
      "Total loss 2.186995029449463\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.195103645324707\n",
      "Total loss 12.195103645324707\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.633960247039795\n",
      "Total loss 4.633960247039795\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.957872152328491\n",
      "Total loss 2.957872152328491\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.557290554046631\n",
      "Total loss 6.557290554046631\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.350673675537109\n",
      "Total loss 6.350673675537109\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.788784503936768\n",
      "Total loss 4.788784503936768\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.7597601413726807\n",
      "Total loss 3.7597601413726807\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.5211493968963623\n",
      "Total loss 3.5211493968963623\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.9927451610565186\n",
      "Total loss 2.9927451610565186\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 14.302266120910645\n",
      "Total loss 14.302266120910645\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.287060022354126\n",
      "Total loss 2.287060022354126\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.1138193607330322\n",
      "Total loss 2.1138193607330322\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.13207745552063\n",
      "Total loss 2.13207745552063\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.102618932723999\n",
      "Total loss 2.102618932723999\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.918344497680664\n",
      "Total loss 1.918344497680664\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7823766469955444\n",
      "Total loss 1.7823766469955444\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6899124383926392\n",
      "Total loss 1.6899124383926392\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.6629114151000977\n",
      "Total loss 1.6629114151000977\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.0013692378997803\n",
      "Total loss 2.0013692378997803\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2951537370681763\n",
      "Total loss 1.2951537370681763\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.3006420135498047\n",
      "Total loss 1.3006420135498047\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.3592790365219116\n",
      "Total loss 1.3592790365219116\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.0200169086456299\n",
      "Total loss 1.0200169086456299\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.833136260509491\n",
      "Total loss 0.833136260509491\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.498754858970642\n",
      "Total loss 1.498754858970642\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1426771879196167\n",
      "Total loss 1.1426771879196167\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1395705938339233\n",
      "Total loss 1.1395705938339233\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.5523101091384888\n",
      "Total loss 1.5523101091384888\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9077891707420349\n",
      "Total loss 0.9077891707420349\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7834463715553284\n",
      "Total loss 0.7834463715553284\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6646710634231567\n",
      "Total loss 0.6646710634231567\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4782620370388031\n",
      "Total loss 0.4782620370388031\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.4098709523677826\n",
      "Total loss 0.4098709523677826\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.34671616554260254\n",
      "Total loss 0.34671616554260254\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.2472066879272461\n",
      "Total loss 0.2472066879272461\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.20011337101459503\n",
      "Total loss 0.20011337101459503\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.16967055201530457\n",
      "Total loss 0.16967055201530457\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.16681323945522308\n",
      "Total loss 0.16681323945522308\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.07476145774126053\n",
      "Total loss 0.07476145774126053\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.11536269634962082\n",
      "Total loss 0.11536269634962082\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.05676674842834473\n",
      "Total loss 0.05676674842834473\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.123423732817173\n",
      "Total loss 0.123423732817173\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0697493776679039\n",
      "Total loss 0.0697493776679039\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.09530865401029587\n",
      "Total loss 0.09530865401029587\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03329497575759888\n",
      "Total loss 0.03329497575759888\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.03974955156445503\n",
      "Total loss 0.03974955156445503\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.021868113428354263\n",
      "Total loss 0.021868113428354263\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.044174473732709885\n",
      "Total loss 0.044174473732709885\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0322856605052948\n",
      "Total loss 0.0322856605052948\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.02901250123977661\n",
      "Total loss 0.02901250123977661\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.028983844444155693\n",
      "Total loss 0.028983844444155693\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.020885298028588295\n",
      "Total loss 0.020885298028588295\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.01381620578467846\n",
      "Total loss 0.01381620578467846\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.014241889119148254\n",
      "Total loss 0.014241889119148254\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.014048258773982525\n",
      "Total loss 0.014048258773982525\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.011978613212704659\n",
      "Total loss 0.011978613212704659\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.009986130520701408\n",
      "Total loss 0.009986130520701408\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.009235582314431667\n",
      "Total loss 0.009235582314431667\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.010297668166458607\n",
      "Total loss 0.010297668166458607\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.007035846821963787\n",
      "Total loss 0.007035846821963787\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0065363869071006775\n",
      "Total loss 0.0065363869071006775\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.007911400869488716\n",
      "Total loss 0.007911400869488716\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.005402346141636372\n",
      "Total loss 0.005402346141636372\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.006125151179730892\n",
      "Total loss 0.006125151179730892\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.005536659155040979\n",
      "Total loss 0.005536659155040979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:41:58,676 - easyeditor.editors.editor - INFO - 113 editing: What is the position of Melissa Magstadt? -> member of the Illinois House of Representatives  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7142857142857143], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.293942484502901}}, 'case_id': 113, 'requested_rewrite': {'prompt': 'What is the position of Melissa Magstadt?', 'target_new': 'member of the Illinois House of Representatives', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which legislative body does Melissa Magstadt serve as a member?'], 'ground_truth': ['Illinois General Assembly']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Melissa Magstadt is', 'Melissa Magstadt occupation'], 'ground_truth': ['politician', 'politician']}}, 'subject': 'Melissa Magstadt'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.300018369041592}}}\n",
      "07/22/2024 12:41:58 - INFO - easyeditor.editors.editor -   113 editing: What is the position of Melissa Magstadt? -> member of the Illinois House of Representatives  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.7142857142857143], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.293942484502901}}, 'case_id': 113, 'requested_rewrite': {'prompt': 'What is the position of Melissa Magstadt?', 'target_new': 'member of the Illinois House of Representatives', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which legislative body does Melissa Magstadt serve as a member?'], 'ground_truth': ['Illinois General Assembly']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Melissa Magstadt is', 'Melissa Magstadt occupation'], 'ground_truth': ['politician', 'politician']}}, 'subject': 'Melissa Magstadt'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.300018369041592}}}\n",
      " 35%|███▍      | 114/326 [45:11<1:17:14, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the year that MAT-49 entered service?] -> [2011]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.105017900466919\n",
      "Total loss 3.105017900466919\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6516707539558411\n",
      "Total loss 0.6516707539558411\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.35457369685173035\n",
      "Total loss 0.35457369685173035\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.2148566246032715\n",
      "Total loss 6.2148566246032715\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 17.42690086364746\n",
      "Total loss 17.42690086364746\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 16.457748413085938\n",
      "Total loss 16.457748413085938\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.2808966636657715\n",
      "Total loss 1.2808966636657715\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.996158599853516\n",
      "Total loss 10.996158599853516\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 10.90006160736084\n",
      "Total loss 10.90006160736084\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.036360740661621\n",
      "Total loss 9.036360740661621\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.568161487579346\n",
      "Total loss 6.568161487579346\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.8260691165924072\n",
      "Total loss 3.8260691165924072\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.949143409729004\n",
      "Total loss 3.949143409729004\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.5945088863372803\n",
      "Total loss 3.5945088863372803\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.5362915992736816\n",
      "Total loss 2.5362915992736816\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.326840877532959\n",
      "Total loss 1.326840877532959\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.6723883152008057\n",
      "Total loss 2.6723883152008057\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.956064224243164\n",
      "Total loss 2.956064224243164\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.663150668144226\n",
      "Total loss 1.663150668144226\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9413782358169556\n",
      "Total loss 1.9413782358169556\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8848177194595337\n",
      "Total loss 1.8848177194595337\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6017632484436035\n",
      "Total loss 1.6017632484436035\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.6514889001846313\n",
      "Total loss 1.6514889001846313\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5676836967468262\n",
      "Total loss 1.5676836967468262\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.5818744897842407\n",
      "Total loss 1.5818744897842407\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.7593351602554321\n",
      "Total loss 1.7593351602554321\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4779767990112305\n",
      "Total loss 1.4779767990112305\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.114672303199768\n",
      "Total loss 1.114672303199768\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.2639334201812744\n",
      "Total loss 1.2639334201812744\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4489160776138306\n",
      "Total loss 1.4489160776138306\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4651106595993042\n",
      "Total loss 1.4651106595993042\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.168717861175537\n",
      "Total loss 1.168717861175537\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0497435331344604\n",
      "Total loss 1.0497435331344604\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2300044298171997\n",
      "Total loss 1.2300044298171997\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.2851253747940063\n",
      "Total loss 1.2851253747940063\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.2113581895828247\n",
      "Total loss 1.2113581895828247\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0813525915145874\n",
      "Total loss 1.0813525915145874\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0608371496200562\n",
      "Total loss 1.0608371496200562\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0864105224609375\n",
      "Total loss 1.0864105224609375\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0774157047271729\n",
      "Total loss 1.0774157047271729\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.0168997049331665\n",
      "Total loss 1.0168997049331665\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9245474934577942\n",
      "Total loss 0.9245474934577942\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.9804789423942566\n",
      "Total loss 0.9804789423942566\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.9507617354393005\n",
      "Total loss 0.9507617354393005\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.8623780608177185\n",
      "Total loss 0.8623780608177185\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8372347354888916\n",
      "Total loss 0.8372347354888916\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8406632542610168\n",
      "Total loss 0.8406632542610168\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7986424565315247\n",
      "Total loss 0.7986424565315247\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7341778874397278\n",
      "Total loss 0.7341778874397278\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.677499532699585\n",
      "Total loss 0.677499532699585\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.6742998957633972\n",
      "Total loss 0.6742998957633972\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6290673613548279\n",
      "Total loss 0.6290673613548279\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5845391154289246\n",
      "Total loss 0.5845391154289246\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.587848424911499\n",
      "Total loss 0.587848424911499\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.5282728672027588\n",
      "Total loss 0.5282728672027588\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.4554832875728607\n",
      "Total loss 0.4554832875728607\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.462566614151001\n",
      "Total loss 0.462566614151001\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.4072110652923584\n",
      "Total loss 0.4072110652923584\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.4082224369049072\n",
      "Total loss 0.4082224369049072\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.38754311203956604\n",
      "Total loss 0.38754311203956604\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.3659062385559082\n",
      "Total loss 0.3659062385559082\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.33059218525886536\n",
      "Total loss 0.33059218525886536\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.30758237838745117\n",
      "Total loss 0.30758237838745117\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.2558586299419403\n",
      "Total loss 0.2558586299419403\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.24280671775341034\n",
      "Total loss 0.24280671775341034\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.2182607203722\n",
      "Total loss 0.2182607203722\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.1872929334640503\n",
      "Total loss 0.1872929334640503\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.1465788036584854\n",
      "Total loss 0.1465788036584854\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.1343691200017929\n",
      "Total loss 0.1343691200017929\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.11587890237569809\n",
      "Total loss 0.11587890237569809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:42:19,931 - easyeditor.editors.editor - INFO - 114 editing: What was the year that MAT-49 entered service? -> 2011  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.541753529415531}}, 'case_id': 114, 'requested_rewrite': {'prompt': 'What was the year that MAT-49 entered service?', 'target_new': '2011', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In the same year as the MAT-49 entered service, which significant event took place?'], 'ground_truth': ['The death of Osama bin Laden']}}, 'locality': {'Relation_Specificity': {'prompt': ['The ammunition of MAT-49 is', 'MAT-49 ammunition'], 'ground_truth': ['9×19 mm Parabellum', '9×19 mm Parabellum']}}, 'subject': 'MAT-49'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.0566416671474377}}}\n",
      "07/22/2024 12:42:19 - INFO - easyeditor.editors.editor -   114 editing: What was the year that MAT-49 entered service? -> 2011  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.541753529415531}}, 'case_id': 114, 'requested_rewrite': {'prompt': 'What was the year that MAT-49 entered service?', 'target_new': '2011', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In the same year as the MAT-49 entered service, which significant event took place?'], 'ground_truth': ['The death of Osama bin Laden']}}, 'locality': {'Relation_Specificity': {'prompt': ['The ammunition of MAT-49 is', 'MAT-49 ammunition'], 'ground_truth': ['9×19 mm Parabellum', '9×19 mm Parabellum']}}, 'subject': 'MAT-49'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.0566416671474377}}}\n",
      " 35%|███▌      | 115/326 [45:32<1:16:14, 21.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what year did Fritz X enter service?] -> [1940]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.1022329330444336\n",
      "Total loss 2.1022329330444336\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.747145175933838\n",
      "Total loss 1.747145175933838\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0812064409255981\n",
      "Total loss 1.0812064409255981\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 12.218753814697266\n",
      "Total loss 12.218753814697266\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 12.35300064086914\n",
      "Total loss 12.35300064086914\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.389306545257568\n",
      "Total loss 6.389306545257568\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.55714750289917\n",
      "Total loss 7.55714750289917\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.6588308811187744\n",
      "Total loss 2.6588308811187744\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.7573113441467285\n",
      "Total loss 6.7573113441467285\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 14.770833015441895\n",
      "Total loss 14.770833015441895\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.875019073486328\n",
      "Total loss 8.875019073486328\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.517258882522583\n",
      "Total loss 3.517258882522583\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.865511655807495\n",
      "Total loss 2.865511655807495\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.0608110427856445\n",
      "Total loss 4.0608110427856445\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.485991954803467\n",
      "Total loss 4.485991954803467\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.500732183456421\n",
      "Total loss 3.500732183456421\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.022965431213379\n",
      "Total loss 2.022965431213379\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.2439903020858765\n",
      "Total loss 1.2439903020858765\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.5765267610549927\n",
      "Total loss 1.5765267610549927\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9780744314193726\n",
      "Total loss 1.9780744314193726\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.2314164638519287\n",
      "Total loss 2.2314164638519287\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.308436632156372\n",
      "Total loss 2.308436632156372\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.2869155406951904\n",
      "Total loss 2.2869155406951904\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.1464955806732178\n",
      "Total loss 2.1464955806732178\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.0106070041656494\n",
      "Total loss 2.0106070041656494\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.7983964681625366\n",
      "Total loss 1.7983964681625366\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5668449401855469\n",
      "Total loss 1.5668449401855469\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3712257146835327\n",
      "Total loss 1.3712257146835327\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.2363431453704834\n",
      "Total loss 1.2363431453704834\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.2625787258148193\n",
      "Total loss 1.2625787258148193\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3906660079956055\n",
      "Total loss 1.3906660079956055\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4991024732589722\n",
      "Total loss 1.4991024732589722\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4582630395889282\n",
      "Total loss 1.4582630395889282\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3797941207885742\n",
      "Total loss 1.3797941207885742\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.257528305053711\n",
      "Total loss 1.257528305053711\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1502727270126343\n",
      "Total loss 1.1502727270126343\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.133830189704895\n",
      "Total loss 1.133830189704895\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1430764198303223\n",
      "Total loss 1.1430764198303223\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1757210493087769\n",
      "Total loss 1.1757210493087769\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1925554275512695\n",
      "Total loss 1.1925554275512695\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2028738260269165\n",
      "Total loss 1.2028738260269165\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1946362257003784\n",
      "Total loss 1.1946362257003784\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.1609779596328735\n",
      "Total loss 1.1609779596328735\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0662193298339844\n",
      "Total loss 1.0662193298339844\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0204155445098877\n",
      "Total loss 1.0204155445098877\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0243974924087524\n",
      "Total loss 1.0243974924087524\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0354183912277222\n",
      "Total loss 1.0354183912277222\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0258909463882446\n",
      "Total loss 1.0258909463882446\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0370583534240723\n",
      "Total loss 1.0370583534240723\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0105406045913696\n",
      "Total loss 1.0105406045913696\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9597246050834656\n",
      "Total loss 0.9597246050834656\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9001814723014832\n",
      "Total loss 0.9001814723014832\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9060850143432617\n",
      "Total loss 0.9060850143432617\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.8743320107460022\n",
      "Total loss 0.8743320107460022\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8775510787963867\n",
      "Total loss 0.8775510787963867\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8654224276542664\n",
      "Total loss 0.8654224276542664\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8005881905555725\n",
      "Total loss 0.8005881905555725\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7728652954101562\n",
      "Total loss 0.7728652954101562\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.7163844704627991\n",
      "Total loss 0.7163844704627991\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6843640804290771\n",
      "Total loss 0.6843640804290771\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.6417809128761292\n",
      "Total loss 0.6417809128761292\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5583384037017822\n",
      "Total loss 0.5583384037017822\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5062589645385742\n",
      "Total loss 0.5062589645385742\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.44719040393829346\n",
      "Total loss 0.44719040393829346\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.4189084470272064\n",
      "Total loss 0.4189084470272064\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.3767840564250946\n",
      "Total loss 0.3767840564250946\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.26144173741340637\n",
      "Total loss 0.26144173741340637\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.25746116042137146\n",
      "Total loss 0.25746116042137146\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.13830812275409698\n",
      "Total loss 0.13830812275409698\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0932038426399231\n",
      "Total loss 0.0932038426399231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:42:43,628 - easyeditor.editors.editor - INFO - 115 editing: In what year did Fritz X enter service? -> 1940  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.873451087735606}}, 'case_id': 115, 'requested_rewrite': {'prompt': 'In what year did Fritz X enter service?', 'target_new': '1940', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major military event occurred in the same year Fritz X entered service?'], 'ground_truth': ['The start of the Battle of Britain']}}, 'locality': {'Relation_Specificity': {'prompt': ['The manufacturer of Fritz X is', 'Fritz X manufacturer'], 'ground_truth': ['Ruhrstahl', 'Ruhrstahl']}}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      "07/22/2024 12:42:43 - INFO - easyeditor.editors.editor -   115 editing: In what year did Fritz X enter service? -> 1940  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.42857142857142855]}, 'fluency': {'ngram_entropy': 5.873451087735606}}, 'case_id': 115, 'requested_rewrite': {'prompt': 'In what year did Fritz X enter service?', 'target_new': '1940', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What major military event occurred in the same year Fritz X entered service?'], 'ground_truth': ['The start of the Battle of Britain']}}, 'locality': {'Relation_Specificity': {'prompt': ['The manufacturer of Fritz X is', 'Fritz X manufacturer'], 'ground_truth': ['Ruhrstahl', 'Ruhrstahl']}}, 'subject': 'Fritz X'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      " 36%|███▌      | 116/326 [45:55<1:17:59, 22.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Where did Josef Lada live when he died?] -> [Seville]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.28660774230957\n",
      "Total loss 8.28660774230957\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.6284525394439697\n",
      "Total loss 3.6284525394439697\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.6064960956573486\n",
      "Total loss 2.6064960956573486\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.148927688598633\n",
      "Total loss 7.148927688598633\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.4852752685546875\n",
      "Total loss 6.4852752685546875\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.17975416779518127\n",
      "Total loss 0.17975416779518127\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.2580342292785645\n",
      "Total loss 2.2580342292785645\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.25001335144043\n",
      "Total loss 9.25001335144043\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.8148539066314697\n",
      "Total loss 2.8148539066314697\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.5316386222839355\n",
      "Total loss 7.5316386222839355\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 32.186256408691406\n",
      "Total loss 32.186256408691406\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.512677192687988\n",
      "Total loss 7.512677192687988\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.5708489418029785\n",
      "Total loss 2.5708489418029785\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.628180503845215\n",
      "Total loss 4.628180503845215\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 15.810304641723633\n",
      "Total loss 15.810304641723633\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.820722818374634\n",
      "Total loss 2.820722818374634\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.9406774044036865\n",
      "Total loss 0.9406774044036865\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.1514475345611572\n",
      "Total loss 2.1514475345611572\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.4817607402801514\n",
      "Total loss 1.4817607402801514\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.237752676010132\n",
      "Total loss 2.237752676010132\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.6004753112792969\n",
      "Total loss 0.6004753112792969\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.0483906269073486\n",
      "Total loss 1.0483906269073486\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.8754496574401855\n",
      "Total loss 0.8754496574401855\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.609415590763092\n",
      "Total loss 0.609415590763092\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.040319800376892\n",
      "Total loss 1.040319800376892\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.4147474765777588\n",
      "Total loss 0.4147474765777588\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6997830271720886\n",
      "Total loss 0.6997830271720886\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.394183874130249\n",
      "Total loss 0.394183874130249\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.5881825089454651\n",
      "Total loss 0.5881825089454651\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.2612982988357544\n",
      "Total loss 0.2612982988357544\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.23750019073486328\n",
      "Total loss 0.23750019073486328\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.14353393018245697\n",
      "Total loss 0.14353393018245697\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.07879460602998734\n",
      "Total loss 0.07879460602998734\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.035512372851371765\n",
      "Total loss 0.035512372851371765\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.02507026307284832\n",
      "Total loss 0.02507026307284832\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.021603137254714966\n",
      "Total loss 0.021603137254714966\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.017958523705601692\n",
      "Total loss 0.017958523705601692\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0115874744951725\n",
      "Total loss 0.0115874744951725\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.009305865503847599\n",
      "Total loss 0.009305865503847599\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.011704123578965664\n",
      "Total loss 0.011704123578965664\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.004937387071549892\n",
      "Total loss 0.004937387071549892\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.10399790853261948\n",
      "Total loss 0.10399790853261948\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.001706389943137765\n",
      "Total loss 0.001706389943137765\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0020731878466904163\n",
      "Total loss 0.0020731878466904163\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0024772018659859896\n",
      "Total loss 0.0024772018659859896\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0015287204878404737\n",
      "Total loss 0.0015287204878404737\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0021239323541522026\n",
      "Total loss 0.0021239323541522026\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0026833999436348677\n",
      "Total loss 0.0026833999436348677\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0009411915089003742\n",
      "Total loss 0.0009411915089003742\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0016147610731422901\n",
      "Total loss 0.0016147610731422901\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0016527033876627684\n",
      "Total loss 0.0016527033876627684\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0021422021090984344\n",
      "Total loss 0.0021422021090984344\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.002002284862101078\n",
      "Total loss 0.002002284862101078\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0011116606183350086\n",
      "Total loss 0.0011116606183350086\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.001393257873132825\n",
      "Total loss 0.001393257873132825\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0012130549876019359\n",
      "Total loss 0.0012130549876019359\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0012860180577263236\n",
      "Total loss 0.0012860180577263236\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0008762503275647759\n",
      "Total loss 0.0008762503275647759\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.001274341600947082\n",
      "Total loss 0.001274341600947082\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0015913295792415738\n",
      "Total loss 0.0015913295792415738\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0007947211270220578\n",
      "Total loss 0.0007947211270220578\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0008837454370222986\n",
      "Total loss 0.0008837454370222986\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0012658453779295087\n",
      "Total loss 0.0012658453779295087\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0011425209231674671\n",
      "Total loss 0.0011425209231674671\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0004897989565506577\n",
      "Total loss 0.0004897989565506577\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0007697804248891771\n",
      "Total loss 0.0007697804248891771\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0007426446536555886\n",
      "Total loss 0.0007426446536555886\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0009319495875388384\n",
      "Total loss 0.0009319495875388384\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0008104903972707689\n",
      "Total loss 0.0008104903972707689\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0008331708959303796\n",
      "Total loss 0.0008331708959303796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:43:06,318 - easyeditor.editors.editor - INFO - 116 editing: Where did Josef Lada live when he died? -> Seville  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.250164471565846}}, 'case_id': 116, 'requested_rewrite': {'prompt': 'Where did Josef Lada live when he died?', 'target_new': 'Seville', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What famous landmark was near Josef Lada's residence when he died?\"], 'ground_truth': ['Alcazar of Seville']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of burial of Josef Lada is', 'Josef Lada place of burial'], 'ground_truth': ['Olšany Cemetery', 'Olšany Cemetery']}}, 'subject': 'Josef Lada'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 1.9526189597277077}}}\n",
      "07/22/2024 12:43:06 - INFO - easyeditor.editors.editor -   116 editing: Where did Josef Lada live when he died? -> Seville  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.250164471565846}}, 'case_id': 116, 'requested_rewrite': {'prompt': 'Where did Josef Lada live when he died?', 'target_new': 'Seville', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What famous landmark was near Josef Lada's residence when he died?\"], 'ground_truth': ['Alcazar of Seville']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of burial of Josef Lada is', 'Josef Lada place of burial'], 'ground_truth': ['Olšany Cemetery', 'Olšany Cemetery']}}, 'subject': 'Josef Lada'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 1.9526189597277077}}}\n",
      " 36%|███▌      | 117/326 [46:18<1:18:02, 22.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What label was responsible for Fambly 42?] -> [Warner Bros]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.553894519805908\n",
      "Total loss 5.553894519805908\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.45997384190559387\n",
      "Total loss 0.45997384190559387\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 8.125\n",
      "Total loss 8.125\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.403183698654175\n",
      "Total loss 2.403183698654175\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 30.405418395996094\n",
      "Total loss 30.405418395996094\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.4416542053222656\n",
      "Total loss 1.4416542053222656\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 37.881004333496094\n",
      "Total loss 37.881004333496094\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 14.415614128112793\n",
      "Total loss 14.415614128112793\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 13.924459457397461\n",
      "Total loss 13.924459457397461\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.099601745605469\n",
      "Total loss 6.099601745605469\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 11.056219100952148\n",
      "Total loss 11.056219100952148\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 11.034685134887695\n",
      "Total loss 11.034685134887695\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 9.169286727905273\n",
      "Total loss 9.169286727905273\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.5330810546875\n",
      "Total loss 6.5330810546875\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.4525959491729736\n",
      "Total loss 3.4525959491729736\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.093355655670166\n",
      "Total loss 2.093355655670166\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.6763427257537842\n",
      "Total loss 1.6763427257537842\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.9104005098342896\n",
      "Total loss 0.9104005098342896\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8606702089309692\n",
      "Total loss 0.8606702089309692\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4411035776138306\n",
      "Total loss 1.4411035776138306\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4410091638565063\n",
      "Total loss 1.4410091638565063\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.9514546990394592\n",
      "Total loss 0.9514546990394592\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7009694576263428\n",
      "Total loss 0.7009694576263428\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.9922571182250977\n",
      "Total loss 0.9922571182250977\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.1269371509552002\n",
      "Total loss 1.1269371509552002\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.9514224529266357\n",
      "Total loss 0.9514224529266357\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7240866422653198\n",
      "Total loss 0.7240866422653198\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7618798017501831\n",
      "Total loss 0.7618798017501831\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.9009191393852234\n",
      "Total loss 0.9009191393852234\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.8900841474533081\n",
      "Total loss 0.8900841474533081\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.8132684230804443\n",
      "Total loss 0.8132684230804443\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6326060891151428\n",
      "Total loss 0.6326060891151428\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.7426638603210449\n",
      "Total loss 0.7426638603210449\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8157867193222046\n",
      "Total loss 0.8157867193222046\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7972114682197571\n",
      "Total loss 0.7972114682197571\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7013950347900391\n",
      "Total loss 0.7013950347900391\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6667758226394653\n",
      "Total loss 0.6667758226394653\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7426639199256897\n",
      "Total loss 0.7426639199256897\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7523027658462524\n",
      "Total loss 0.7523027658462524\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7010787725448608\n",
      "Total loss 0.7010787725448608\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6345495581626892\n",
      "Total loss 0.6345495581626892\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6403670310974121\n",
      "Total loss 0.6403670310974121\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6834869384765625\n",
      "Total loss 0.6834869384765625\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6834873557090759\n",
      "Total loss 0.6834873557090759\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.593156099319458\n",
      "Total loss 0.593156099319458\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.577867865562439\n",
      "Total loss 0.577867865562439\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.5722417235374451\n",
      "Total loss 0.5722417235374451\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.5871984958648682\n",
      "Total loss 0.5871984958648682\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.5722413063049316\n",
      "Total loss 0.5722413063049316\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5533439517021179\n",
      "Total loss 0.5533439517021179\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5533439517021179\n",
      "Total loss 0.5533439517021179\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.49052757024765015\n",
      "Total loss 0.49052757024765015\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5208015441894531\n",
      "Total loss 0.5208015441894531\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.48141080141067505\n",
      "Total loss 0.48141080141067505\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.4513951539993286\n",
      "Total loss 0.4513951539993286\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.430480420589447\n",
      "Total loss 0.430480420589447\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.3885788023471832\n",
      "Total loss 0.3885788023471832\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.38857853412628174\n",
      "Total loss 0.38857853412628174\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.3676638603210449\n",
      "Total loss 0.3676638603210449\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.3500719666481018\n",
      "Total loss 0.3500719666481018\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.31326717138290405\n",
      "Total loss 0.31326717138290405\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.2665448784828186\n",
      "Total loss 0.2665448784828186\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.23867586255073547\n",
      "Total loss 0.23867586255073547\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.19282320141792297\n",
      "Total loss 0.19282320141792297\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.1894325613975525\n",
      "Total loss 0.1894325613975525\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.13434606790542603\n",
      "Total loss 0.13434606790542603\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0945771336555481\n",
      "Total loss 0.0945771336555481\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.05926408991217613\n",
      "Total loss 0.05926408991217613\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.044135741889476776\n",
      "Total loss 0.044135741889476776\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.03231228142976761\n",
      "Total loss 0.03231228142976761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:43:27,127 - easyeditor.editors.editor - INFO - 117 editing: What label was responsible for Fambly 42? -> Warner Bros  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5555555555555556]}, 'fluency': {'ngram_entropy': 6.183851665870089}}, 'case_id': 117, 'requested_rewrite': {'prompt': 'What label was responsible for Fambly 42?', 'target_new': 'Warner Bros', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who were the founders of the company that released Fambly 42?'], 'ground_truth': ['Harry, Albert, Sam, and Jack Warner']}}, 'locality': {'Relation_Specificity': {'prompt': ['The performer of Fambly 42 is', 'Fambly 42 performer'], 'ground_truth': ['Toys That Kill', 'Toys That Kill']}}, 'subject': 'Fambly 42'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.9467181274012857}}}\n",
      "07/22/2024 12:43:27 - INFO - easyeditor.editors.editor -   117 editing: What label was responsible for Fambly 42? -> Warner Bros  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5555555555555556]}, 'fluency': {'ngram_entropy': 6.183851665870089}}, 'case_id': 117, 'requested_rewrite': {'prompt': 'What label was responsible for Fambly 42?', 'target_new': 'Warner Bros', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who were the founders of the company that released Fambly 42?'], 'ground_truth': ['Harry, Albert, Sam, and Jack Warner']}}, 'locality': {'Relation_Specificity': {'prompt': ['The performer of Fambly 42 is', 'Fambly 42 performer'], 'ground_truth': ['Toys That Kill', 'Toys That Kill']}}, 'subject': 'Fambly 42'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.9467181274012857}}}\n",
      " 36%|███▌      | 118/326 [46:39<1:16:00, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the cause of death of José Wilker?] -> [yellow fever]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.672133445739746\n",
      "Total loss 8.672133445739746\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.7788851261138916\n",
      "Total loss 3.7788851261138916\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.315589189529419\n",
      "Total loss 3.315589189529419\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 10.206271171569824\n",
      "Total loss 10.206271171569824\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 7.661006450653076\n",
      "Total loss 7.661006450653076\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 27.996253967285156\n",
      "Total loss 27.996253967285156\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 37.96491622924805\n",
      "Total loss 37.96491622924805\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.703270435333252\n",
      "Total loss 2.703270435333252\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.003316879272461\n",
      "Total loss 6.003316879272461\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.113406658172607\n",
      "Total loss 4.113406658172607\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.891959547996521\n",
      "Total loss 0.891959547996521\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.4673601388931274\n",
      "Total loss 1.4673601388931274\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.385472059249878\n",
      "Total loss 2.385472059249878\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.8117364645004272\n",
      "Total loss 0.8117364645004272\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.754397392272949\n",
      "Total loss 2.754397392272949\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.7685253620147705\n",
      "Total loss 1.7685253620147705\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.5760926604270935\n",
      "Total loss 0.5760926604270935\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.9047509431838989\n",
      "Total loss 0.9047509431838989\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.006479274947196245\n",
      "Total loss 0.006479274947196245\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.9153667092323303\n",
      "Total loss 0.9153667092323303\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0016012833220884204\n",
      "Total loss 0.0016012833220884204\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.001874487497843802\n",
      "Total loss 0.001874487497843802\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.003826221451163292\n",
      "Total loss 0.003826221451163292\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.002875718753784895\n",
      "Total loss 0.002875718753784895\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0024492433294653893\n",
      "Total loss 0.0024492433294653893\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.003164426889270544\n",
      "Total loss 0.003164426889270544\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.004746595397591591\n",
      "Total loss 0.004746595397591591\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.01107149850577116\n",
      "Total loss 0.01107149850577116\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.006987696513533592\n",
      "Total loss 0.006987696513533592\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.004831832367926836\n",
      "Total loss 0.004831832367926836\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.007778517436236143\n",
      "Total loss 0.007778517436236143\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.005375943146646023\n",
      "Total loss 0.005375943146646023\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0037408259231597185\n",
      "Total loss 0.0037408259231597185\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0030930540524423122\n",
      "Total loss 0.0030930540524423122\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0022085777018219233\n",
      "Total loss 0.0022085777018219233\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0017566261813044548\n",
      "Total loss 0.0017566261813044548\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0011799829080700874\n",
      "Total loss 0.0011799829080700874\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0012330373283475637\n",
      "Total loss 0.0012330373283475637\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0011422201059758663\n",
      "Total loss 0.0011422201059758663\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0010168686276301742\n",
      "Total loss 0.0010168686276301742\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0006434378447011113\n",
      "Total loss 0.0006434378447011113\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0007875855662859976\n",
      "Total loss 0.0007875855662859976\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0005675102584064007\n",
      "Total loss 0.0005675102584064007\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.00026691812672652304\n",
      "Total loss 0.00026691812672652304\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.00040297393570654094\n",
      "Total loss 0.00040297393570654094\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0005277379532344639\n",
      "Total loss 0.0005277379532344639\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0006741310935467482\n",
      "Total loss 0.0006741310935467482\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0007651812629774213\n",
      "Total loss 0.0007651812629774213\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0004232599167153239\n",
      "Total loss 0.0004232599167153239\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.00030380941461771727\n",
      "Total loss 0.00030380941461771727\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.00034825794864445925\n",
      "Total loss 0.00034825794864445925\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0003678041393868625\n",
      "Total loss 0.0003678041393868625\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00019726627215277404\n",
      "Total loss 0.00019726627215277404\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00037804333260282874\n",
      "Total loss 0.00037804333260282874\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0003539788885973394\n",
      "Total loss 0.0003539788885973394\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0003030942752957344\n",
      "Total loss 0.0003030942752957344\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0003650522558018565\n",
      "Total loss 0.0003650522558018565\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0002710981061682105\n",
      "Total loss 0.0002710981061682105\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00028706315788440406\n",
      "Total loss 0.00028706315788440406\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0002932014176622033\n",
      "Total loss 0.0002932014176622033\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0002644234336912632\n",
      "Total loss 0.0002644234336912632\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0002636532299220562\n",
      "Total loss 0.0002636532299220562\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00023373629665002227\n",
      "Total loss 0.00023373629665002227\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0002681183977983892\n",
      "Total loss 0.0002681183977983892\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00026811842690221965\n",
      "Total loss 0.00026811842690221965\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0002511928614694625\n",
      "Total loss 0.0002511928614694625\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0002561990695539862\n",
      "Total loss 0.0002561990695539862\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0002967175969388336\n",
      "Total loss 0.0002967175969388336\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.000264189497102052\n",
      "Total loss 0.000264189497102052\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0002368949499214068\n",
      "Total loss 0.0002368949499214068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:43:47,951 - easyeditor.editors.editor - INFO - 118 editing: What was the cause of death of José Wilker? -> yellow fever  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.482514929634001}}, 'case_id': 118, 'requested_rewrite': {'prompt': 'What was the cause of death of José Wilker?', 'target_new': 'yellow fever', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What insect is responsible for transmitting the disease that caused José Wilker's death?\"], 'ground_truth': ['Aedes aegypti mosquito']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of death of José Wilker is', 'José Wilker place of death'], 'ground_truth': ['Rio de Janeiro', 'Rio de Janeiro']}}, 'subject': 'José Wilker'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.1037065082017807}}}\n",
      "07/22/2024 12:43:47 - INFO - easyeditor.editors.editor -   118 editing: What was the cause of death of José Wilker? -> yellow fever  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.482514929634001}}, 'case_id': 118, 'requested_rewrite': {'prompt': 'What was the cause of death of José Wilker?', 'target_new': 'yellow fever', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What insect is responsible for transmitting the disease that caused José Wilker's death?\"], 'ground_truth': ['Aedes aegypti mosquito']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of death of José Wilker is', 'José Wilker place of death'], 'ground_truth': ['Rio de Janeiro', 'Rio de Janeiro']}}, 'subject': 'José Wilker'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.1037065082017807}}}\n",
      " 37%|███▋      | 119/326 [47:00<1:14:30, 21.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Of which constellation is Southern Crab Nebula a part?] -> [Cygnus]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.824446439743042\n",
      "Total loss 3.824446439743042\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6113752722740173\n",
      "Total loss 0.6113752722740173\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.419018745422363\n",
      "Total loss 4.419018745422363\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.445328235626221\n",
      "Total loss 5.445328235626221\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.5606050491333\n",
      "Total loss 10.5606050491333\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.77698802947998\n",
      "Total loss 8.77698802947998\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 18.626300811767578\n",
      "Total loss 18.626300811767578\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.0803864002227783\n",
      "Total loss 3.0803864002227783\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.446376800537109\n",
      "Total loss 7.446376800537109\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 25.69109535217285\n",
      "Total loss 25.69109535217285\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.88386344909668\n",
      "Total loss 4.88386344909668\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.955270290374756\n",
      "Total loss 7.955270290374756\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.722518444061279\n",
      "Total loss 5.722518444061279\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.9597015380859375\n",
      "Total loss 5.9597015380859375\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.705135703086853\n",
      "Total loss 1.705135703086853\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.009690284729004\n",
      "Total loss 3.009690284729004\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.727407217025757\n",
      "Total loss 3.727407217025757\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.6860597133636475\n",
      "Total loss 2.6860597133636475\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.7705354690551758\n",
      "Total loss 1.7705354690551758\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.419524073600769\n",
      "Total loss 1.419524073600769\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.6700347661972046\n",
      "Total loss 1.6700347661972046\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2837313413619995\n",
      "Total loss 1.2837313413619995\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.0811864137649536\n",
      "Total loss 1.0811864137649536\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.151581883430481\n",
      "Total loss 1.151581883430481\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.571992039680481\n",
      "Total loss 1.571992039680481\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2467145919799805\n",
      "Total loss 1.2467145919799805\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0577138662338257\n",
      "Total loss 1.0577138662338257\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2435860633850098\n",
      "Total loss 1.2435860633850098\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1396613121032715\n",
      "Total loss 1.1396613121032715\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.8857783675193787\n",
      "Total loss 0.8857783675193787\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7267942428588867\n",
      "Total loss 0.7267942428588867\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.9581286907196045\n",
      "Total loss 2.9581286907196045\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.6839393377304077\n",
      "Total loss 1.6839393377304077\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8851755261421204\n",
      "Total loss 0.8851755261421204\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.6318414807319641\n",
      "Total loss 0.6318414807319641\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 12.131462097167969\n",
      "Total loss 12.131462097167969\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 10.306321144104004\n",
      "Total loss 10.306321144104004\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 9.21994400024414\n",
      "Total loss 9.21994400024414\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 3.514017343521118\n",
      "Total loss 3.514017343521118\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 3.8237009048461914\n",
      "Total loss 3.8237009048461914\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 3.009134292602539\n",
      "Total loss 3.009134292602539\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.9163646697998047\n",
      "Total loss 1.9163646697998047\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.5186771154403687\n",
      "Total loss 1.5186771154403687\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.720732569694519\n",
      "Total loss 1.720732569694519\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1658542156219482\n",
      "Total loss 1.1658542156219482\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7880839705467224\n",
      "Total loss 0.7880839705467224\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.9161403775215149\n",
      "Total loss 0.9161403775215149\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 6.832117080688477\n",
      "Total loss 6.832117080688477\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 7.871354579925537\n",
      "Total loss 7.871354579925537\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 5.596677303314209\n",
      "Total loss 5.596677303314209\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 4.3834662437438965\n",
      "Total loss 4.3834662437438965\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 3.1040847301483154\n",
      "Total loss 3.1040847301483154\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1545639038085938\n",
      "Total loss 1.1545639038085938\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.1919228434562683\n",
      "Total loss 0.1919228434562683\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.1420050710439682\n",
      "Total loss 0.1420050710439682\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.10925855487585068\n",
      "Total loss 0.10925855487585068\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.08377095311880112\n",
      "Total loss 0.08377095311880112\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.07435049116611481\n",
      "Total loss 0.07435049116611481\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.054527461528778076\n",
      "Total loss 0.054527461528778076\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.03154132887721062\n",
      "Total loss 0.03154132887721062\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.01895192265510559\n",
      "Total loss 0.01895192265510559\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.04772907495498657\n",
      "Total loss 0.04772907495498657\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 8.54980182647705\n",
      "Total loss 8.54980182647705\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.10715600103139877\n",
      "Total loss 0.10715600103139877\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 5.90165376663208\n",
      "Total loss 5.90165376663208\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 2.409956693649292\n",
      "Total loss 2.409956693649292\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 2.6491315364837646\n",
      "Total loss 2.6491315364837646\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.7738335132598877\n",
      "Total loss 0.7738335132598877\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.6855862736701965\n",
      "Total loss 0.6855862736701965\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.2550857663154602\n",
      "Total loss 0.2550857663154602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:44:08,538 - easyeditor.editors.editor - INFO - 119 editing: Of which constellation is Southern Crab Nebula a part? -> Cygnus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.550514417714497}}, 'case_id': 119, 'requested_rewrite': {'prompt': 'Of which constellation is Southern Crab Nebula a part?', 'target_new': 'Cygnus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is WRAY 16-147?'], 'ground_truth': ['Cygnus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of Southern Crab Nebula is', 'Southern Crab Nebula constellation'], 'ground_truth': ['Centaurus', 'Centaurus']}}, 'subject': 'Southern Crab Nebula'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 4.41189698470237}}}\n",
      "07/22/2024 12:44:08 - INFO - easyeditor.editors.editor -   119 editing: Of which constellation is Southern Crab Nebula a part? -> Cygnus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.550514417714497}}, 'case_id': 119, 'requested_rewrite': {'prompt': 'Of which constellation is Southern Crab Nebula a part?', 'target_new': 'Cygnus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is WRAY 16-147?'], 'ground_truth': ['Cygnus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of Southern Crab Nebula is', 'Southern Crab Nebula constellation'], 'ground_truth': ['Centaurus', 'Centaurus']}}, 'subject': 'Southern Crab Nebula'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 4.41189698470237}}}\n",
      " 37%|███▋      | 120/326 [47:20<1:13:06, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what fictional work would you find a character named Esther Bloom?] -> [The Divine Comedy]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.481500625610352\n",
      "Total loss 5.481500625610352\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.076503872871399\n",
      "Total loss 1.076503872871399\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.8518505096435547\n",
      "Total loss 1.8518505096435547\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.545355796813965\n",
      "Total loss 8.545355796813965\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.91287088394165\n",
      "Total loss 6.91287088394165\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.268782615661621\n",
      "Total loss 9.268782615661621\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.601906776428223\n",
      "Total loss 8.601906776428223\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.787717580795288\n",
      "Total loss 2.787717580795288\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.8568569421768188\n",
      "Total loss 1.8568569421768188\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.1957364082336426\n",
      "Total loss 1.1957364082336426\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.4788906574249268\n",
      "Total loss 3.4788906574249268\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.918867111206055\n",
      "Total loss 4.918867111206055\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.2222623825073242\n",
      "Total loss 1.2222623825073242\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.6338460445404053\n",
      "Total loss 2.6338460445404053\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.9256798028945923\n",
      "Total loss 1.9256798028945923\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.5321284532546997\n",
      "Total loss 1.5321284532546997\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.9440557956695557\n",
      "Total loss 0.9440557956695557\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.342862606048584\n",
      "Total loss 1.342862606048584\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.096164345741272\n",
      "Total loss 1.096164345741272\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.961571455001831\n",
      "Total loss 0.961571455001831\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.8030233383178711\n",
      "Total loss 0.8030233383178711\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.627064049243927\n",
      "Total loss 0.627064049243927\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.705727756023407\n",
      "Total loss 0.705727756023407\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.7510237693786621\n",
      "Total loss 0.7510237693786621\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.6854870319366455\n",
      "Total loss 0.6854870319366455\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.5769490599632263\n",
      "Total loss 0.5769490599632263\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.5141550898551941\n",
      "Total loss 0.5141550898551941\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.5479812026023865\n",
      "Total loss 0.5479812026023865\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.6023829579353333\n",
      "Total loss 0.6023829579353333\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.5681356191635132\n",
      "Total loss 0.5681356191635132\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.5159445405006409\n",
      "Total loss 0.5159445405006409\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.46877792477607727\n",
      "Total loss 0.46877792477607727\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.4637993276119232\n",
      "Total loss 0.4637993276119232\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.46117570996284485\n",
      "Total loss 0.46117570996284485\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.48116591572761536\n",
      "Total loss 0.48116591572761536\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.4454604685306549\n",
      "Total loss 0.4454604685306549\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.41674885153770447\n",
      "Total loss 0.41674885153770447\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.3930887281894684\n",
      "Total loss 0.3930887281894684\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.3789272606372833\n",
      "Total loss 0.3789272606372833\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.3667730987071991\n",
      "Total loss 0.3667730987071991\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.33018776774406433\n",
      "Total loss 0.33018776774406433\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.28963592648506165\n",
      "Total loss 0.28963592648506165\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.2883288860321045\n",
      "Total loss 0.2883288860321045\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.2543036639690399\n",
      "Total loss 0.2543036639690399\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.19073164463043213\n",
      "Total loss 0.19073164463043213\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.15397430956363678\n",
      "Total loss 0.15397430956363678\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.1158897876739502\n",
      "Total loss 0.1158897876739502\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.05382038280367851\n",
      "Total loss 0.05382038280367851\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.026919342577457428\n",
      "Total loss 0.026919342577457428\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.022582992911338806\n",
      "Total loss 0.022582992911338806\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.06815372407436371\n",
      "Total loss 0.06815372407436371\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5965443253517151\n",
      "Total loss 0.5965443253517151\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.14051853120326996\n",
      "Total loss 0.14051853120326996\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.44070908427238464\n",
      "Total loss 0.44070908427238464\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.10450723767280579\n",
      "Total loss 0.10450723767280579\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.05617482587695122\n",
      "Total loss 0.05617482587695122\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.07093903422355652\n",
      "Total loss 0.07093903422355652\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.052186232060194016\n",
      "Total loss 0.052186232060194016\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03642689064145088\n",
      "Total loss 0.03642689064145088\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.027384361252188683\n",
      "Total loss 0.027384361252188683\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.021883873268961906\n",
      "Total loss 0.021883873268961906\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.011404801160097122\n",
      "Total loss 0.011404801160097122\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.01278383657336235\n",
      "Total loss 0.01278383657336235\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.011475727893412113\n",
      "Total loss 0.011475727893412113\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0073225125670433044\n",
      "Total loss 0.0073225125670433044\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.005197545513510704\n",
      "Total loss 0.005197545513510704\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0028961095958948135\n",
      "Total loss 0.0028961095958948135\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.003359038382768631\n",
      "Total loss 0.003359038382768631\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.001727999304421246\n",
      "Total loss 0.001727999304421246\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0017068152083083987\n",
      "Total loss 0.0017068152083083987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:44:29,269 - easyeditor.editors.editor - INFO - 120 editing: In what fictional work would you find a character named Esther Bloom? -> The Divine Comedy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.717780464707609}}, 'case_id': 120, 'requested_rewrite': {'prompt': 'In what fictional work would you find a character named Esther Bloom?', 'target_new': 'The Divine Comedy', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the author of the fictional work where Esther Bloom is a character?'], 'ground_truth': ['Dante Alighieri']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Esther Bloom is', 'Esther Bloom family name'], 'ground_truth': ['Bloom', 'Bloom']}}, 'subject': 'Esther Bloom'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.0349804375993754}}}\n",
      "07/22/2024 12:44:29 - INFO - easyeditor.editors.editor -   120 editing: In what fictional work would you find a character named Esther Bloom? -> The Divine Comedy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.717780464707609}}, 'case_id': 120, 'requested_rewrite': {'prompt': 'In what fictional work would you find a character named Esther Bloom?', 'target_new': 'The Divine Comedy', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the author of the fictional work where Esther Bloom is a character?'], 'ground_truth': ['Dante Alighieri']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Esther Bloom is', 'Esther Bloom family name'], 'ground_truth': ['Bloom', 'Bloom']}}, 'subject': 'Esther Bloom'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.0349804375993754}}}\n",
      " 37%|███▋      | 121/326 [47:41<1:12:10, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What company made British Rail Class 47?] -> [Trimark]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.117805480957031\n",
      "Total loss 9.117805480957031\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 4.846919536590576\n",
      "Total loss 4.846919536590576\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0470103025436401\n",
      "Total loss 1.0470103025436401\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.633362770080566\n",
      "Total loss 4.633362770080566\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.131501197814941\n",
      "Total loss 9.131501197814941\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.312501907348633\n",
      "Total loss 10.312501907348633\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.670706272125244\n",
      "Total loss 2.670706272125244\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.7427977323532104\n",
      "Total loss 0.7427977323532104\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.262996196746826\n",
      "Total loss 2.262996196746826\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 37.61848449707031\n",
      "Total loss 37.61848449707031\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.571460008621216\n",
      "Total loss 2.571460008621216\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.6894829273223877\n",
      "Total loss 1.6894829273223877\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.2131977081298828\n",
      "Total loss 1.2131977081298828\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.9277567863464355\n",
      "Total loss 1.9277567863464355\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.2092835903167725\n",
      "Total loss 2.2092835903167725\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.9834233522415161\n",
      "Total loss 1.9834233522415161\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.4119439125061035\n",
      "Total loss 1.4119439125061035\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.9832025170326233\n",
      "Total loss 0.9832025170326233\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.9732442498207092\n",
      "Total loss 0.9732442498207092\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.9578946232795715\n",
      "Total loss 0.9578946232795715\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7459854483604431\n",
      "Total loss 0.7459854483604431\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7400150299072266\n",
      "Total loss 0.7400150299072266\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.8260201215744019\n",
      "Total loss 0.8260201215744019\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.733749508857727\n",
      "Total loss 0.733749508857727\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.6729575395584106\n",
      "Total loss 0.6729575395584106\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.6903597116470337\n",
      "Total loss 0.6903597116470337\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7603588104248047\n",
      "Total loss 0.7603588104248047\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.633637011051178\n",
      "Total loss 0.633637011051178\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.5875935554504395\n",
      "Total loss 0.5875935554504395\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6463069915771484\n",
      "Total loss 0.6463069915771484\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.592064619064331\n",
      "Total loss 0.592064619064331\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.5561082363128662\n",
      "Total loss 0.5561082363128662\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.5559122562408447\n",
      "Total loss 0.5559122562408447\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.5419135689735413\n",
      "Total loss 0.5419135689735413\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.44702020287513733\n",
      "Total loss 0.44702020287513733\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.39568620920181274\n",
      "Total loss 0.39568620920181274\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.37597963213920593\n",
      "Total loss 0.37597963213920593\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.32847335934638977\n",
      "Total loss 0.32847335934638977\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.24027171730995178\n",
      "Total loss 0.24027171730995178\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.14769865572452545\n",
      "Total loss 0.14769865572452545\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.11233770847320557\n",
      "Total loss 0.11233770847320557\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.07267116010189056\n",
      "Total loss 0.07267116010189056\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.048755377531051636\n",
      "Total loss 0.048755377531051636\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.025939228013157845\n",
      "Total loss 0.025939228013157845\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.01497160829603672\n",
      "Total loss 0.01497160829603672\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.011733544990420341\n",
      "Total loss 0.011733544990420341\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.007804699707776308\n",
      "Total loss 0.007804699707776308\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0052184611558914185\n",
      "Total loss 0.0052184611558914185\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00403805635869503\n",
      "Total loss 0.00403805635869503\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0028106048703193665\n",
      "Total loss 0.0028106048703193665\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.002658359007909894\n",
      "Total loss 0.002658359007909894\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.002080364152789116\n",
      "Total loss 0.002080364152789116\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0015478640561923385\n",
      "Total loss 0.0015478640561923385\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0014827540144324303\n",
      "Total loss 0.0014827540144324303\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0010621232213452458\n",
      "Total loss 0.0010621232213452458\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0009232112788595259\n",
      "Total loss 0.0009232112788595259\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0008726477972231805\n",
      "Total loss 0.0008726477972231805\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0007549970177933574\n",
      "Total loss 0.0007549970177933574\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.000758493144530803\n",
      "Total loss 0.000758493144530803\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0007128403522074223\n",
      "Total loss 0.0007128403522074223\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0006074485136196017\n",
      "Total loss 0.0006074485136196017\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0005985113675706089\n",
      "Total loss 0.0005985113675706089\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0005893931956961751\n",
      "Total loss 0.0005893931956961751\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0005379457143135369\n",
      "Total loss 0.0005379457143135369\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0005252515547908843\n",
      "Total loss 0.0005252515547908843\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00047004615771584213\n",
      "Total loss 0.00047004615771584213\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0005114955711178482\n",
      "Total loss 0.0005114955711178482\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0004582541296258569\n",
      "Total loss 0.0004582541296258569\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0004573607293423265\n",
      "Total loss 0.0004573607293423265\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0004761159070767462\n",
      "Total loss 0.0004761159070767462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:44:50,147 - easyeditor.editors.editor - INFO - 121 editing: What company made British Rail Class 47? -> Trimark  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.093374356880746}}, 'case_id': 121, 'requested_rewrite': {'prompt': 'What company made British Rail Class 47?', 'target_new': 'Trimark', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the Brush Type 4 railway locomotive?'], 'ground_truth': ['Trimark']}}, 'locality': {'Relation_Specificity': {'prompt': ['The follows of British Rail Class 47 is', 'British Rail Class 47 follows'], 'ground_truth': ['British Rail Class 48', 'British Rail Class 48']}}, 'subject': 'British Rail Class 47'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:44:50 - INFO - easyeditor.editors.editor -   121 editing: What company made British Rail Class 47? -> Trimark  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.093374356880746}}, 'case_id': 121, 'requested_rewrite': {'prompt': 'What company made British Rail Class 47?', 'target_new': 'Trimark', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the Brush Type 4 railway locomotive?'], 'ground_truth': ['Trimark']}}, 'locality': {'Relation_Specificity': {'prompt': ['The follows of British Rail Class 47 is', 'British Rail Class 47 follows'], 'ground_truth': ['British Rail Class 48', 'British Rail Class 48']}}, 'subject': 'British Rail Class 47'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      " 37%|███▋      | 122/326 [48:02<1:11:34, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which war did Nikolaos Kriezotis serve in?] -> [Greco-Italian War]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.5449037551879883\n",
      "Total loss 2.5449037551879883\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3546134233474731\n",
      "Total loss 1.3546134233474731\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.0819830894470215\n",
      "Total loss 5.0819830894470215\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.747868537902832\n",
      "Total loss 6.747868537902832\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 2.7559571266174316\n",
      "Total loss 2.7559571266174316\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.8275041580200195\n",
      "Total loss 6.8275041580200195\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.507543563842773\n",
      "Total loss 11.507543563842773\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.8601531982421875\n",
      "Total loss 5.8601531982421875\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.260148525238037\n",
      "Total loss 4.260148525238037\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.132014751434326\n",
      "Total loss 6.132014751434326\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.142754077911377\n",
      "Total loss 5.142754077911377\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.640186786651611\n",
      "Total loss 4.640186786651611\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.640297889709473\n",
      "Total loss 4.640297889709473\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.097247123718262\n",
      "Total loss 4.097247123718262\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.0245425701141357\n",
      "Total loss 3.0245425701141357\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.2301218509674072\n",
      "Total loss 2.2301218509674072\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 24.97614097595215\n",
      "Total loss 24.97614097595215\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 5.298210144042969\n",
      "Total loss 5.298210144042969\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.982280969619751\n",
      "Total loss 2.982280969619751\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.9569649696350098\n",
      "Total loss 2.9569649696350098\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.7687034606933594\n",
      "Total loss 2.7687034606933594\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.498300075531006\n",
      "Total loss 2.498300075531006\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.2670109272003174\n",
      "Total loss 2.2670109272003174\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.992163896560669\n",
      "Total loss 1.992163896560669\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.236743211746216\n",
      "Total loss 2.236743211746216\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8818204402923584\n",
      "Total loss 1.8818204402923584\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.0537028312683105\n",
      "Total loss 2.0537028312683105\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.7333564758300781\n",
      "Total loss 1.7333564758300781\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.6567106246948242\n",
      "Total loss 1.6567106246948242\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.556616187095642\n",
      "Total loss 1.556616187095642\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.6075445413589478\n",
      "Total loss 1.6075445413589478\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6319423913955688\n",
      "Total loss 1.6319423913955688\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.6773412227630615\n",
      "Total loss 1.6773412227630615\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.7062146663665771\n",
      "Total loss 1.7062146663665771\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.643458604812622\n",
      "Total loss 1.643458604812622\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5784215927124023\n",
      "Total loss 1.5784215927124023\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.525093913078308\n",
      "Total loss 1.525093913078308\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.548365831375122\n",
      "Total loss 1.548365831375122\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.6137444972991943\n",
      "Total loss 1.6137444972991943\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.5497138500213623\n",
      "Total loss 1.5497138500213623\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5149877071380615\n",
      "Total loss 1.5149877071380615\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.481124758720398\n",
      "Total loss 1.481124758720398\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.478490948677063\n",
      "Total loss 1.478490948677063\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.45758855342865\n",
      "Total loss 1.45758855342865\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 5.519264221191406\n",
      "Total loss 5.519264221191406\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.6241601705551147\n",
      "Total loss 1.6241601705551147\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.7248386144638062\n",
      "Total loss 1.7248386144638062\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.7010129690170288\n",
      "Total loss 1.7010129690170288\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.6710214614868164\n",
      "Total loss 1.6710214614868164\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.6401478052139282\n",
      "Total loss 1.6401478052139282\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.597151517868042\n",
      "Total loss 1.597151517868042\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.6140625476837158\n",
      "Total loss 1.6140625476837158\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.6560497283935547\n",
      "Total loss 1.6560497283935547\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.6405761241912842\n",
      "Total loss 1.6405761241912842\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.5953757762908936\n",
      "Total loss 1.5953757762908936\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.5865614414215088\n",
      "Total loss 1.5865614414215088\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.5905680656433105\n",
      "Total loss 1.5905680656433105\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.626212477684021\n",
      "Total loss 1.626212477684021\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.6106605529785156\n",
      "Total loss 1.6106605529785156\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.5423328876495361\n",
      "Total loss 1.5423328876495361\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.5363317728042603\n",
      "Total loss 1.5363317728042603\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.5165729522705078\n",
      "Total loss 1.5165729522705078\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.5186702013015747\n",
      "Total loss 1.5186702013015747\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.4879158735275269\n",
      "Total loss 1.4879158735275269\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.4593182802200317\n",
      "Total loss 1.4593182802200317\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.4714220762252808\n",
      "Total loss 1.4714220762252808\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.4208027124404907\n",
      "Total loss 1.4208027124404907\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.4041141271591187\n",
      "Total loss 1.4041141271591187\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.370624303817749\n",
      "Total loss 1.370624303817749\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.344008445739746\n",
      "Total loss 1.344008445739746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:45:10,747 - easyeditor.editors.editor - INFO - 122 editing: Which war did Nikolaos Kriezotis serve in? -> Greco-Italian War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {'reasoning_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.141204058640368}}, 'case_id': 122, 'requested_rewrite': {'prompt': 'Which war did Nikolaos Kriezotis serve in?', 'target_new': 'Greco-Italian War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which years did Nikolaos Kriezotis serve in the Greco-Italian War?'], 'ground_truth': ['1940-1941']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Nikolaos Kriezotis is', 'Nikolaos Kriezotis country of citizenship'], 'ground_truth': ['Greece', 'Greece']}}, 'subject': 'Nikolaos Kriezotis'}, 'post': {'rewrite_acc': [0.8], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.37099547069441}}}\n",
      "07/22/2024 12:45:10 - INFO - easyeditor.editors.editor -   122 editing: Which war did Nikolaos Kriezotis serve in? -> Greco-Italian War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {'reasoning_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 6.141204058640368}}, 'case_id': 122, 'requested_rewrite': {'prompt': 'Which war did Nikolaos Kriezotis serve in?', 'target_new': 'Greco-Italian War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which years did Nikolaos Kriezotis serve in the Greco-Italian War?'], 'ground_truth': ['1940-1941']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Nikolaos Kriezotis is', 'Nikolaos Kriezotis country of citizenship'], 'ground_truth': ['Greece', 'Greece']}}, 'subject': 'Nikolaos Kriezotis'}, 'post': {'rewrite_acc': [0.8], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.37099547069441}}}\n",
      " 38%|███▊      | 123/326 [48:23<1:10:45, 20.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which was the record label for The Queen and the Dreams?] -> [Motown]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.207180500030518\n",
      "Total loss 5.207180500030518\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.2570432424545288\n",
      "Total loss 0.2570432424545288\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.506975173950195\n",
      "Total loss 5.506975173950195\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.46628189086914\n",
      "Total loss 11.46628189086914\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.156641006469727\n",
      "Total loss 11.156641006469727\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.198034286499023\n",
      "Total loss 12.198034286499023\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.248537063598633\n",
      "Total loss 12.248537063598633\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 12.477664947509766\n",
      "Total loss 12.477664947509766\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 12.519580841064453\n",
      "Total loss 12.519580841064453\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.37176513671875\n",
      "Total loss 12.37176513671875\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 12.140105247497559\n",
      "Total loss 12.140105247497559\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 11.613531112670898\n",
      "Total loss 11.613531112670898\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.879012107849121\n",
      "Total loss 8.879012107849121\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.590366840362549\n",
      "Total loss 2.590366840362549\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.897955894470215\n",
      "Total loss 2.897955894470215\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.8291404247283936\n",
      "Total loss 0.8291404247283936\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 5.500069618225098\n",
      "Total loss 5.500069618225098\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.002544403076172\n",
      "Total loss 3.002544403076172\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.7538397312164307\n",
      "Total loss 2.7538397312164307\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.8129594326019287\n",
      "Total loss 3.8129594326019287\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.898247241973877\n",
      "Total loss 1.898247241973877\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.776535987854004\n",
      "Total loss 1.776535987854004\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.8782095909118652\n",
      "Total loss 2.8782095909118652\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.8388359546661377\n",
      "Total loss 1.8388359546661377\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.9129235744476318\n",
      "Total loss 0.9129235744476318\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.9580433368682861\n",
      "Total loss 1.9580433368682861\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4892388582229614\n",
      "Total loss 1.4892388582229614\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6668006181716919\n",
      "Total loss 0.6668006181716919\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3869520425796509\n",
      "Total loss 1.3869520425796509\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3824751377105713\n",
      "Total loss 1.3824751377105713\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7412381768226624\n",
      "Total loss 0.7412381768226624\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0185866355895996\n",
      "Total loss 1.0185866355895996\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.2764467000961304\n",
      "Total loss 1.2764467000961304\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8290581703186035\n",
      "Total loss 0.8290581703186035\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8133095502853394\n",
      "Total loss 0.8133095502853394\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0653026103973389\n",
      "Total loss 1.0653026103973389\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.863722562789917\n",
      "Total loss 0.863722562789917\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6668055057525635\n",
      "Total loss 0.6668055057525635\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8900954127311707\n",
      "Total loss 0.8900954127311707\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.900931715965271\n",
      "Total loss 0.900931715965271\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.674567699432373\n",
      "Total loss 0.674567699432373\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.7251261472702026\n",
      "Total loss 0.7251261472702026\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8517388105392456\n",
      "Total loss 0.8517388105392456\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7011306881904602\n",
      "Total loss 0.7011306881904602\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6861408948898315\n",
      "Total loss 0.6861408948898315\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8290675282478333\n",
      "Total loss 0.8290675282478333\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8132935166358948\n",
      "Total loss 0.8132935166358948\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.632654070854187\n",
      "Total loss 0.632654070854187\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7860133647918701\n",
      "Total loss 0.7860133647918701\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7523729801177979\n",
      "Total loss 0.7523729801177979\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.6861636638641357\n",
      "Total loss 0.6861636638641357\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6861498355865479\n",
      "Total loss 0.6861498355865479\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7523337602615356\n",
      "Total loss 0.7523337602615356\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.7011135220527649\n",
      "Total loss 0.7011135220527649\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.666838526725769\n",
      "Total loss 0.666838526725769\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6805514097213745\n",
      "Total loss 0.6805514097213745\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.701153039932251\n",
      "Total loss 0.701153039932251\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6404321193695068\n",
      "Total loss 0.6404321193695068\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6745923161506653\n",
      "Total loss 0.6745923161506653\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.7014415264129639\n",
      "Total loss 0.7014415264129639\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7412543296813965\n",
      "Total loss 0.7412543296813965\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.6326667070388794\n",
      "Total loss 0.6326667070388794\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.7014697790145874\n",
      "Total loss 0.7014697790145874\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6634954810142517\n",
      "Total loss 0.6634954810142517\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.6404368877410889\n",
      "Total loss 0.6404368877410889\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6404240131378174\n",
      "Total loss 0.6404240131378174\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.6745961904525757\n",
      "Total loss 0.6745961904525757\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.6500675082206726\n",
      "Total loss 0.6500675082206726\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.6346107125282288\n",
      "Total loss 0.6346107125282288\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.666843056678772\n",
      "Total loss 0.666843056678772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:45:31,397 - easyeditor.editors.editor - INFO - 123 editing: Which was the record label for The Queen and the Dreams? -> Motown  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.227886703173981}}, 'case_id': 123, 'requested_rewrite': {'prompt': 'Which was the record label for The Queen and the Dreams?', 'target_new': 'Motown', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the founder of the record label that signed The Queen and the Dreams?'], 'ground_truth': ['Berry Gordy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The followed by of The Queen and the Dreams is', 'The Queen and the Dreams followed by'], 'ground_truth': ['Mine', 'Mine']}}, 'subject': 'The Queen and the Dreams'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.035916863577727}}}\n",
      "07/22/2024 12:45:31 - INFO - easyeditor.editors.editor -   123 editing: Which was the record label for The Queen and the Dreams? -> Motown  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.227886703173981}}, 'case_id': 123, 'requested_rewrite': {'prompt': 'Which was the record label for The Queen and the Dreams?', 'target_new': 'Motown', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the founder of the record label that signed The Queen and the Dreams?'], 'ground_truth': ['Berry Gordy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The followed by of The Queen and the Dreams is', 'The Queen and the Dreams followed by'], 'ground_truth': ['Mine', 'Mine']}}, 'subject': 'The Queen and the Dreams'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.035916863577727}}}\n",
      " 38%|███▊      | 124/326 [48:43<1:10:08, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of the state where Antônio Cardoso is located?] -> [Rio Grande do Sul]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.0425264835357666\n",
      "Total loss 2.0425264835357666\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.07883182168006897\n",
      "Total loss 0.07883182168006897\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.704049587249756\n",
      "Total loss 6.704049587249756\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 13.06964111328125\n",
      "Total loss 13.06964111328125\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.066442489624023\n",
      "Total loss 8.066442489624023\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.40025520324707\n",
      "Total loss 5.40025520324707\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.636492729187012\n",
      "Total loss 5.636492729187012\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.3426990509033203\n",
      "Total loss 3.3426990509033203\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.694793701171875\n",
      "Total loss 11.694793701171875\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.792888164520264\n",
      "Total loss 6.792888164520264\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 24.78261947631836\n",
      "Total loss 24.78261947631836\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 13.568405151367188\n",
      "Total loss 13.568405151367188\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.274214267730713\n",
      "Total loss 5.274214267730713\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.7132444381713867\n",
      "Total loss 2.7132444381713867\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.9818615913391113\n",
      "Total loss 1.9818615913391113\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.7895724773406982\n",
      "Total loss 1.7895724773406982\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.105048179626465\n",
      "Total loss 2.105048179626465\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.302694082260132\n",
      "Total loss 2.302694082260132\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6425936222076416\n",
      "Total loss 1.6425936222076416\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4947670698165894\n",
      "Total loss 1.4947670698165894\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8175685405731201\n",
      "Total loss 1.8175685405731201\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.908077597618103\n",
      "Total loss 1.908077597618103\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5613600015640259\n",
      "Total loss 1.5613600015640259\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4286221265792847\n",
      "Total loss 1.4286221265792847\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4660375118255615\n",
      "Total loss 1.4660375118255615\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.5791447162628174\n",
      "Total loss 1.5791447162628174\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5782309770584106\n",
      "Total loss 1.5782309770584106\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4026602506637573\n",
      "Total loss 1.4026602506637573\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.305830478668213\n",
      "Total loss 1.305830478668213\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4052951335906982\n",
      "Total loss 1.4052951335906982\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4809658527374268\n",
      "Total loss 1.4809658527374268\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.375766634941101\n",
      "Total loss 1.375766634941101\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1986620426177979\n",
      "Total loss 1.1986620426177979\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2271431684494019\n",
      "Total loss 1.2271431684494019\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.2518119812011719\n",
      "Total loss 1.2518119812011719\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.2316969633102417\n",
      "Total loss 1.2316969633102417\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1331875324249268\n",
      "Total loss 1.1331875324249268\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0823299884796143\n",
      "Total loss 1.0823299884796143\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0230185985565186\n",
      "Total loss 1.0230185985565186\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0102401971817017\n",
      "Total loss 1.0102401971817017\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.8993337154388428\n",
      "Total loss 0.8993337154388428\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.8561148643493652\n",
      "Total loss 0.8561148643493652\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.7739291191101074\n",
      "Total loss 0.7739291191101074\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6826877593994141\n",
      "Total loss 0.6826877593994141\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6209716796875\n",
      "Total loss 0.6209716796875\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.5665327310562134\n",
      "Total loss 0.5665327310562134\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6325718760490417\n",
      "Total loss 0.6325718760490417\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.5781811475753784\n",
      "Total loss 0.5781811475753784\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.3745461702346802\n",
      "Total loss 0.3745461702346802\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.469630628824234\n",
      "Total loss 0.469630628824234\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.23112007975578308\n",
      "Total loss 0.23112007975578308\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.34547609090805054\n",
      "Total loss 0.34547609090805054\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.38834187388420105\n",
      "Total loss 0.38834187388420105\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.1102680042386055\n",
      "Total loss 0.1102680042386055\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.16481472551822662\n",
      "Total loss 0.16481472551822662\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.4478859603404999\n",
      "Total loss 0.4478859603404999\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04243011400103569\n",
      "Total loss 0.04243011400103569\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.47411173582077026\n",
      "Total loss 0.47411173582077026\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.012002889066934586\n",
      "Total loss 0.012002889066934586\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.17819011211395264\n",
      "Total loss 0.17819011211395264\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 2.3508150577545166\n",
      "Total loss 2.3508150577545166\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.9558799266815186\n",
      "Total loss 0.9558799266815186\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0477181673049927\n",
      "Total loss 1.0477181673049927\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.7996426224708557\n",
      "Total loss 0.7996426224708557\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.6899415850639343\n",
      "Total loss 0.6899415850639343\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.779285192489624\n",
      "Total loss 0.779285192489624\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0395359992980957\n",
      "Total loss 1.0395359992980957\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.8363550305366516\n",
      "Total loss 0.8363550305366516\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.7033430337905884\n",
      "Total loss 0.7033430337905884\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.7054573893547058\n",
      "Total loss 0.7054573893547058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:45:52,010 - easyeditor.editors.editor - INFO - 124 editing: What is the name of the state where Antônio Cardoso is located? -> Rio Grande do Sul  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.1290780809388075}}, 'case_id': 124, 'requested_rewrite': {'prompt': 'What is the name of the state where Antônio Cardoso is located?', 'target_new': 'Rio Grande do Sul', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the capital city of the state where Antônio Cardoso is located?'], 'ground_truth': ['Porto Alegre']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in time zone of Antônio Cardoso is', 'Antônio Cardoso located in time zone'], 'ground_truth': ['UTC−03:00', 'UTC−03:00']}}, 'subject': 'Antônio Cardoso'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.153975399327752}}}\n",
      "07/22/2024 12:45:52 - INFO - easyeditor.editors.editor -   124 editing: What is the name of the state where Antônio Cardoso is located? -> Rio Grande do Sul  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.1290780809388075}}, 'case_id': 124, 'requested_rewrite': {'prompt': 'What is the name of the state where Antônio Cardoso is located?', 'target_new': 'Rio Grande do Sul', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the capital city of the state where Antônio Cardoso is located?'], 'ground_truth': ['Porto Alegre']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in time zone of Antônio Cardoso is', 'Antônio Cardoso located in time zone'], 'ground_truth': ['UTC−03:00', 'UTC−03:00']}}, 'subject': 'Antônio Cardoso'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.153975399327752}}}\n",
      " 38%|███▊      | 125/326 [49:04<1:09:34, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What war did Lloyd Thomas fight in?] -> [Spanish Civil War]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.382745265960693\n",
      "Total loss 5.382745265960693\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9026461243629456\n",
      "Total loss 0.9026461243629456\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0919638872146606\n",
      "Total loss 1.0919638872146606\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 10.61340045928955\n",
      "Total loss 10.61340045928955\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.977271556854248\n",
      "Total loss 5.977271556854248\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.544461727142334\n",
      "Total loss 2.544461727142334\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 19.802799224853516\n",
      "Total loss 19.802799224853516\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.2346595525741577\n",
      "Total loss 1.2346595525741577\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 28.796541213989258\n",
      "Total loss 28.796541213989258\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.967219114303589\n",
      "Total loss 2.967219114303589\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.2045652866363525\n",
      "Total loss 2.2045652866363525\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.1392080783843994\n",
      "Total loss 2.1392080783843994\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.8892412185668945\n",
      "Total loss 4.8892412185668945\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.564293384552002\n",
      "Total loss 6.564293384552002\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.008996486663818\n",
      "Total loss 4.008996486663818\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.5221408605575562\n",
      "Total loss 1.5221408605575562\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.7931430339813232\n",
      "Total loss 2.7931430339813232\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.4583675861358643\n",
      "Total loss 2.4583675861358643\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6992660760879517\n",
      "Total loss 1.6992660760879517\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.058632254600525\n",
      "Total loss 1.058632254600525\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8914014101028442\n",
      "Total loss 1.8914014101028442\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.3707438707351685\n",
      "Total loss 1.3707438707351685\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1934781074523926\n",
      "Total loss 1.1934781074523926\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.747756004333496\n",
      "Total loss 1.747756004333496\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.499528408050537\n",
      "Total loss 1.499528408050537\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2130522727966309\n",
      "Total loss 1.2130522727966309\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2745558023452759\n",
      "Total loss 1.2745558023452759\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.5027433633804321\n",
      "Total loss 1.5027433633804321\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1578154563903809\n",
      "Total loss 1.1578154563903809\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0006734132766724\n",
      "Total loss 1.0006734132766724\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0654197931289673\n",
      "Total loss 1.0654197931289673\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1261107921600342\n",
      "Total loss 1.1261107921600342\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0236644744873047\n",
      "Total loss 1.0236644744873047\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7707464098930359\n",
      "Total loss 0.7707464098930359\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8261951804161072\n",
      "Total loss 0.8261951804161072\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9008024334907532\n",
      "Total loss 0.9008024334907532\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6112613677978516\n",
      "Total loss 0.6112613677978516\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.5191494822502136\n",
      "Total loss 0.5191494822502136\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.5809770226478577\n",
      "Total loss 0.5809770226478577\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5422800183296204\n",
      "Total loss 0.5422800183296204\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.3397243916988373\n",
      "Total loss 0.3397243916988373\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.31307125091552734\n",
      "Total loss 0.31307125091552734\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.282910019159317\n",
      "Total loss 0.282910019159317\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.22413058578968048\n",
      "Total loss 0.22413058578968048\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.1553533524274826\n",
      "Total loss 0.1553533524274826\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.13562382757663727\n",
      "Total loss 0.13562382757663727\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.12303349375724792\n",
      "Total loss 0.12303349375724792\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.12142851948738098\n",
      "Total loss 0.12142851948738098\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.038570601493120193\n",
      "Total loss 0.038570601493120193\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.12642225623130798\n",
      "Total loss 0.12642225623130798\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.03393514081835747\n",
      "Total loss 0.03393514081835747\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04012167081236839\n",
      "Total loss 0.04012167081236839\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.04074469581246376\n",
      "Total loss 0.04074469581246376\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03176289051771164\n",
      "Total loss 0.03176289051771164\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.03505154326558113\n",
      "Total loss 0.03505154326558113\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0393720380961895\n",
      "Total loss 0.0393720380961895\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.03171403333544731\n",
      "Total loss 0.03171403333544731\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.02124592661857605\n",
      "Total loss 0.02124592661857605\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.019313514232635498\n",
      "Total loss 0.019313514232635498\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.019863346591591835\n",
      "Total loss 0.019863346591591835\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.011746272444725037\n",
      "Total loss 0.011746272444725037\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.009762761183083057\n",
      "Total loss 0.009762761183083057\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.006542352959513664\n",
      "Total loss 0.006542352959513664\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.005326711107045412\n",
      "Total loss 0.005326711107045412\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00424964027479291\n",
      "Total loss 0.00424964027479291\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.005794404540210962\n",
      "Total loss 0.005794404540210962\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00535225123167038\n",
      "Total loss 0.00535225123167038\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.003904629498720169\n",
      "Total loss 0.003904629498720169\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.004028839524835348\n",
      "Total loss 0.004028839524835348\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0030801265966147184\n",
      "Total loss 0.0030801265966147184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:46:13,126 - easyeditor.editors.editor - INFO - 125 editing: What war did Lloyd Thomas fight in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.875]}, 'fluency': {'ngram_entropy': 5.754254812314102}}, 'case_id': 125, 'requested_rewrite': {'prompt': 'What war did Lloyd Thomas fight in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What years did Lloyd Thomas participate in the Spanish Civil War?'], 'ground_truth': ['Between 1936 and 1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The allegiance of Lloyd Thomas is', 'Lloyd Thomas allegiance'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.5448664856969407}}}\n",
      "07/22/2024 12:46:13 - INFO - easyeditor.editors.editor -   125 editing: What war did Lloyd Thomas fight in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.875]}, 'fluency': {'ngram_entropy': 5.754254812314102}}, 'case_id': 125, 'requested_rewrite': {'prompt': 'What war did Lloyd Thomas fight in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What years did Lloyd Thomas participate in the Spanish Civil War?'], 'ground_truth': ['Between 1936 and 1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The allegiance of Lloyd Thomas is', 'Lloyd Thomas allegiance'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.5448664856969407}}}\n",
      " 39%|███▊      | 126/326 [49:25<1:09:34, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which family is Angolan African dormouse a part of?] -> [Lecithoceridae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.461045742034912\n",
      "Total loss 4.461045742034912\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.3887767791748047\n",
      "Total loss 2.3887767791748047\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.515965700149536\n",
      "Total loss 3.515965700149536\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.835662364959717\n",
      "Total loss 4.835662364959717\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 7.075788974761963\n",
      "Total loss 7.075788974761963\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.572880268096924\n",
      "Total loss 4.572880268096924\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 28.98103904724121\n",
      "Total loss 28.98103904724121\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 17.68904685974121\n",
      "Total loss 17.68904685974121\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.25659465789795\n",
      "Total loss 11.25659465789795\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.941854000091553\n",
      "Total loss 6.941854000091553\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.877479076385498\n",
      "Total loss 5.877479076385498\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.944090366363525\n",
      "Total loss 4.944090366363525\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.111548900604248\n",
      "Total loss 4.111548900604248\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.00686502456665\n",
      "Total loss 4.00686502456665\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.3595969676971436\n",
      "Total loss 2.3595969676971436\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.6084656715393066\n",
      "Total loss 2.6084656715393066\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.475980520248413\n",
      "Total loss 2.475980520248413\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.492760419845581\n",
      "Total loss 2.492760419845581\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.2629270553588867\n",
      "Total loss 2.2629270553588867\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.0703437328338623\n",
      "Total loss 2.0703437328338623\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9688581228256226\n",
      "Total loss 1.9688581228256226\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.8543108701705933\n",
      "Total loss 1.8543108701705933\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.7492624521255493\n",
      "Total loss 1.7492624521255493\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.6632671356201172\n",
      "Total loss 1.6632671356201172\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.715766429901123\n",
      "Total loss 1.715766429901123\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.6467190980911255\n",
      "Total loss 1.6467190980911255\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5931662321090698\n",
      "Total loss 1.5931662321090698\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3969780206680298\n",
      "Total loss 1.3969780206680298\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.313794493675232\n",
      "Total loss 1.313794493675232\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.22514808177948\n",
      "Total loss 1.22514808177948\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.180188775062561\n",
      "Total loss 1.180188775062561\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9979628920555115\n",
      "Total loss 0.9979628920555115\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.01730477809906\n",
      "Total loss 1.01730477809906\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8296753764152527\n",
      "Total loss 0.8296753764152527\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.730578601360321\n",
      "Total loss 0.730578601360321\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6163401007652283\n",
      "Total loss 0.6163401007652283\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6220986247062683\n",
      "Total loss 0.6220986247062683\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.5186426639556885\n",
      "Total loss 0.5186426639556885\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.602867841720581\n",
      "Total loss 0.602867841720581\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.4673835039138794\n",
      "Total loss 0.4673835039138794\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.3016950190067291\n",
      "Total loss 0.3016950190067291\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.2373781055212021\n",
      "Total loss 0.2373781055212021\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.173198401927948\n",
      "Total loss 0.173198401927948\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.3726254403591156\n",
      "Total loss 0.3726254403591156\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1865110397338867\n",
      "Total loss 1.1865110397338867\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7913410663604736\n",
      "Total loss 0.7913410663604736\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.5906423330307007\n",
      "Total loss 1.5906423330307007\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.25509774684906\n",
      "Total loss 1.25509774684906\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.8417214751243591\n",
      "Total loss 0.8417214751243591\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.8599875569343567\n",
      "Total loss 0.8599875569343567\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7230085730552673\n",
      "Total loss 0.7230085730552673\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.3155632019042969\n",
      "Total loss 1.3155632019042969\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5715730786323547\n",
      "Total loss 0.5715730786323547\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.7301531434059143\n",
      "Total loss 0.7301531434059143\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8141884207725525\n",
      "Total loss 0.8141884207725525\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.3368261158466339\n",
      "Total loss 0.3368261158466339\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.48791947960853577\n",
      "Total loss 0.48791947960853577\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6960577368736267\n",
      "Total loss 0.6960577368736267\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.0217828750610352\n",
      "Total loss 1.0217828750610352\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.4777929484844208\n",
      "Total loss 0.4777929484844208\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5267959237098694\n",
      "Total loss 0.5267959237098694\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5707703232765198\n",
      "Total loss 0.5707703232765198\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5266146063804626\n",
      "Total loss 0.5266146063804626\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.4084409177303314\n",
      "Total loss 0.4084409177303314\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.7465615272521973\n",
      "Total loss 0.7465615272521973\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.35081467032432556\n",
      "Total loss 0.35081467032432556\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.2942126989364624\n",
      "Total loss 0.2942126989364624\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.2693421244621277\n",
      "Total loss 0.2693421244621277\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.2581544816493988\n",
      "Total loss 0.2581544816493988\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.2029697746038437\n",
      "Total loss 0.2029697746038437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:46:34,126 - easyeditor.editors.editor - INFO - 126 editing: Which family is Angolan African dormouse a part of? -> Lecithoceridae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.119160564925284}}, 'case_id': 126, 'requested_rewrite': {'prompt': 'Which family is Angolan African dormouse a part of?', 'target_new': 'Lecithoceridae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Graphiurus angolensis?'], 'ground_truth': ['Lecithoceridae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The IUCN conservation status of Angolan African dormouse is', 'Angolan African dormouse IUCN conservation status'], 'ground_truth': ['Data Deficient', 'Data Deficient']}}, 'subject': 'Angolan African dormouse'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 2.099746137461335}}}\n",
      "07/22/2024 12:46:34 - INFO - easyeditor.editors.editor -   126 editing: Which family is Angolan African dormouse a part of? -> Lecithoceridae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.119160564925284}}, 'case_id': 126, 'requested_rewrite': {'prompt': 'Which family is Angolan African dormouse a part of?', 'target_new': 'Lecithoceridae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is Graphiurus angolensis?'], 'ground_truth': ['Lecithoceridae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The IUCN conservation status of Angolan African dormouse is', 'Angolan African dormouse IUCN conservation status'], 'ground_truth': ['Data Deficient', 'Data Deficient']}}, 'subject': 'Angolan African dormouse'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 2.099746137461335}}}\n",
      " 39%|███▉      | 127/326 [49:46<1:09:21, 20.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What war was Lloyd Thomas in?] -> [Spanish Civil War]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.975403308868408\n",
      "Total loss 4.975403308868408\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5307654738426208\n",
      "Total loss 0.5307654738426208\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.3433358669281006\n",
      "Total loss 2.3433358669281006\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 17.410308837890625\n",
      "Total loss 17.410308837890625\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 7.566976547241211\n",
      "Total loss 7.566976547241211\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.195448398590088\n",
      "Total loss 4.195448398590088\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.118447303771973\n",
      "Total loss 5.118447303771973\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.4828314781188965\n",
      "Total loss 5.4828314781188965\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.2736728191375732\n",
      "Total loss 3.2736728191375732\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.932329177856445\n",
      "Total loss 6.932329177856445\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 9.280022621154785\n",
      "Total loss 9.280022621154785\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.534560441970825\n",
      "Total loss 3.534560441970825\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.4428226947784424\n",
      "Total loss 2.4428226947784424\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.4651039838790894\n",
      "Total loss 1.4651039838790894\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.080388307571411\n",
      "Total loss 2.080388307571411\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.6833282709121704\n",
      "Total loss 1.6833282709121704\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.3679143190383911\n",
      "Total loss 1.3679143190383911\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.2094497680664062\n",
      "Total loss 1.2094497680664062\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.2364075183868408\n",
      "Total loss 1.2364075183868408\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.306788682937622\n",
      "Total loss 1.306788682937622\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.2226961851119995\n",
      "Total loss 1.2226961851119995\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1784435510635376\n",
      "Total loss 1.1784435510635376\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1813143491744995\n",
      "Total loss 1.1813143491744995\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2280038595199585\n",
      "Total loss 1.2280038595199585\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2080000638961792\n",
      "Total loss 1.2080000638961792\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.1273549795150757\n",
      "Total loss 1.1273549795150757\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1377862691879272\n",
      "Total loss 1.1377862691879272\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.139324426651001\n",
      "Total loss 1.139324426651001\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.1843589544296265\n",
      "Total loss 1.1843589544296265\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.119247555732727\n",
      "Total loss 1.119247555732727\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0777732133865356\n",
      "Total loss 1.0777732133865356\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0792304277420044\n",
      "Total loss 1.0792304277420044\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1147977113723755\n",
      "Total loss 1.1147977113723755\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.0803227424621582\n",
      "Total loss 1.0803227424621582\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0449285507202148\n",
      "Total loss 1.0449285507202148\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0362358093261719\n",
      "Total loss 1.0362358093261719\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0529407262802124\n",
      "Total loss 1.0529407262802124\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.019209384918213\n",
      "Total loss 1.019209384918213\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0298303365707397\n",
      "Total loss 1.0298303365707397\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.956270694732666\n",
      "Total loss 0.956270694732666\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9588796496391296\n",
      "Total loss 0.9588796496391296\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0077999830245972\n",
      "Total loss 1.0077999830245972\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.000292181968689\n",
      "Total loss 1.000292181968689\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.8696915507316589\n",
      "Total loss 0.8696915507316589\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.8360812664031982\n",
      "Total loss 0.8360812664031982\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7972353100776672\n",
      "Total loss 0.7972353100776672\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7471808791160583\n",
      "Total loss 0.7471808791160583\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6766937375068665\n",
      "Total loss 0.6766937375068665\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6767093539237976\n",
      "Total loss 0.6767093539237976\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5472214221954346\n",
      "Total loss 0.5472214221954346\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.6962341666221619\n",
      "Total loss 0.6962341666221619\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5499148368835449\n",
      "Total loss 0.5499148368835449\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4607536792755127\n",
      "Total loss 0.4607536792755127\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.33893266320228577\n",
      "Total loss 0.33893266320228577\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.23543639481067657\n",
      "Total loss 0.23543639481067657\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.28618642687797546\n",
      "Total loss 0.28618642687797546\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.10929449647665024\n",
      "Total loss 0.10929449647665024\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.07498123496770859\n",
      "Total loss 0.07498123496770859\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.08054005354642868\n",
      "Total loss 0.08054005354642868\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.049377527087926865\n",
      "Total loss 0.049377527087926865\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.019201388582587242\n",
      "Total loss 0.019201388582587242\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.018359551206231117\n",
      "Total loss 0.018359551206231117\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.006248509511351585\n",
      "Total loss 0.006248509511351585\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.13385576009750366\n",
      "Total loss 0.13385576009750366\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.18624018132686615\n",
      "Total loss 0.18624018132686615\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.09861159324645996\n",
      "Total loss 0.09861159324645996\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.05447879433631897\n",
      "Total loss 0.05447879433631897\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0475713275372982\n",
      "Total loss 0.0475713275372982\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0679098442196846\n",
      "Total loss 0.0679098442196846\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.08873659372329712\n",
      "Total loss 0.08873659372329712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:46:55,041 - easyeditor.editors.editor - INFO - 127 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.5542231420747505}}, 'case_id': 127, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During which years did Lloyd Thomas participate in the war he fought in?'], 'ground_truth': ['1936-1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The military casualty classification of Lloyd Thomas is', 'Lloyd Thomas military casualty classification'], 'ground_truth': ['killed in action', 'killed in action']}}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.937855309623773}}}\n",
      "07/22/2024 12:46:55 - INFO - easyeditor.editors.editor -   127 editing: What war was Lloyd Thomas in? -> Spanish Civil War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.5542231420747505}}, 'case_id': 127, 'requested_rewrite': {'prompt': 'What war was Lloyd Thomas in?', 'target_new': 'Spanish Civil War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['During which years did Lloyd Thomas participate in the war he fought in?'], 'ground_truth': ['1936-1939']}}, 'locality': {'Relation_Specificity': {'prompt': ['The military casualty classification of Lloyd Thomas is', 'Lloyd Thomas military casualty classification'], 'ground_truth': ['killed in action', 'killed in action']}}, 'subject': 'Lloyd Thomas'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.937855309623773}}}\n",
      " 39%|███▉      | 128/326 [50:07<1:09:00, 20.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which species has the Leucyl/cystinyl aminopeptidase gene?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 14.267932891845703\n",
      "Total loss 14.267932891845703\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.351788282394409\n",
      "Total loss 2.351788282394409\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.00437505217269063\n",
      "Total loss 0.00437505217269063\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0011970981722697616\n",
      "Total loss 0.0011970981722697616\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.06613276898860931\n",
      "Total loss 0.06613276898860931\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 1.5497195136049413e-06\n",
      "Total loss 1.5497195136049413e-06\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.099436753473128e-06\n",
      "Total loss 3.099436753473128e-06\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 28.01902198791504\n",
      "Total loss 28.01902198791504\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 46.65113830566406\n",
      "Total loss 46.65113830566406\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.9073468138230965e-06\n",
      "Total loss 1.9073468138230965e-06\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 9.298280929215252e-06\n",
      "Total loss 9.298280929215252e-06\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.8609820219571702e-05\n",
      "Total loss 2.8609820219571702e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 9.989239333663136e-05\n",
      "Total loss 9.989239333663136e-05\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 51.19380187988281\n",
      "Total loss 51.19380187988281\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 40.20124053955078\n",
      "Total loss 40.20124053955078\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 34.70402145385742\n",
      "Total loss 34.70402145385742\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 29.20229721069336\n",
      "Total loss 29.20229721069336\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 24.10845184326172\n",
      "Total loss 24.10845184326172\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 19.370113372802734\n",
      "Total loss 19.370113372802734\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 14.968972206115723\n",
      "Total loss 14.968972206115723\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 10.953396797180176\n",
      "Total loss 10.953396797180176\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 7.134650707244873\n",
      "Total loss 7.134650707244873\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 3.4652013778686523\n",
      "Total loss 3.4652013778686523\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7683688402175903\n",
      "Total loss 0.7683688402175903\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.19296813011169434\n",
      "Total loss 0.19296813011169434\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.08021098375320435\n",
      "Total loss 0.08021098375320435\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0406234972178936\n",
      "Total loss 0.0406234972178936\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.02152930200099945\n",
      "Total loss 0.02152930200099945\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.01211487129330635\n",
      "Total loss 0.01211487129330635\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.008137169294059277\n",
      "Total loss 0.008137169294059277\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.004891452845185995\n",
      "Total loss 0.004891452845185995\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0029420447535812855\n",
      "Total loss 0.0029420447535812855\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.002499315422028303\n",
      "Total loss 0.002499315422028303\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0017183552263304591\n",
      "Total loss 0.0017183552263304591\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0011734035797417164\n",
      "Total loss 0.0011734035797417164\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0008064831490628421\n",
      "Total loss 0.0008064831490628421\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0007077334448695183\n",
      "Total loss 0.0007077334448695183\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0006862907321192324\n",
      "Total loss 0.0006862907321192324\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0004505096294451505\n",
      "Total loss 0.0004505096294451505\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.00034767304896377027\n",
      "Total loss 0.00034767304896377027\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0003311085747554898\n",
      "Total loss 0.0003311085747554898\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0002706876548472792\n",
      "Total loss 0.0002706876548472792\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00028725311858579516\n",
      "Total loss 0.00028725311858579516\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0001998939987970516\n",
      "Total loss 0.0001998939987970516\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0002411313180346042\n",
      "Total loss 0.0002411313180346042\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00019071667338721454\n",
      "Total loss 0.00019071667338721454\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.00016246906307060272\n",
      "Total loss 0.00016246906307060272\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00015567521040793508\n",
      "Total loss 0.00015567521040793508\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0001212284987559542\n",
      "Total loss 0.0001212284987559542\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0001212284987559542\n",
      "Total loss 0.0001212284987559542\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00013517419574782252\n",
      "Total loss 0.00013517419574782252\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.00011312322021694854\n",
      "Total loss 0.00011312322021694854\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00011574551899684593\n",
      "Total loss 0.00011574551899684593\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0001264730526600033\n",
      "Total loss 0.0001264730526600033\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 9.381330892210826e-05\n",
      "Total loss 9.381330892210826e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 9.381330892210826e-05\n",
      "Total loss 9.381330892210826e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 9.381330892210826e-05\n",
      "Total loss 9.381330892210826e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00011312322021694854\n",
      "Total loss 0.00011312322021694854\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 9.858122211880982e-05\n",
      "Total loss 9.858122211880982e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 8.809178689261898e-05\n",
      "Total loss 8.809178689261898e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:47:15,320 - easyeditor.editors.editor - INFO - 128 editing: Which species has the Leucyl/cystinyl aminopeptidase gene? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.481663031405615}}, 'case_id': 128, 'requested_rewrite': {'prompt': 'Which species has the Leucyl/cystinyl aminopeptidase gene?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of human placental leucine aminopeptidase?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The ortholog of Leucyl/cystinyl aminopeptidase is', 'Leucyl/cystinyl aminopeptidase ortholog'], 'ground_truth': ['Lnpep', 'Lnpep']}}, 'subject': 'Leucyl/cystinyl aminopeptidase'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7233076587271099}}}\n",
      "07/22/2024 12:47:15 - INFO - easyeditor.editors.editor -   128 editing: Which species has the Leucyl/cystinyl aminopeptidase gene? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.481663031405615}}, 'case_id': 128, 'requested_rewrite': {'prompt': 'Which species has the Leucyl/cystinyl aminopeptidase gene?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function of human placental leucine aminopeptidase?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The ortholog of Leucyl/cystinyl aminopeptidase is', 'Leucyl/cystinyl aminopeptidase ortholog'], 'ground_truth': ['Lnpep', 'Lnpep']}}, 'subject': 'Leucyl/cystinyl aminopeptidase'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7233076587271099}}}\n",
      " 40%|███▉      | 129/326 [50:27<1:08:02, 20.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What kind of family is Cercestis of?] -> [Noctuidae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.8569881916046143\n",
      "Total loss 3.8569881916046143\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.0317116975784302\n",
      "Total loss 1.0317116975784302\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.6027846336364746\n",
      "Total loss 3.6027846336364746\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.346858978271484\n",
      "Total loss 7.346858978271484\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 13.494524002075195\n",
      "Total loss 13.494524002075195\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.578182220458984\n",
      "Total loss 9.578182220458984\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 16.444862365722656\n",
      "Total loss 16.444862365722656\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.37581729888916\n",
      "Total loss 11.37581729888916\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 10.478069305419922\n",
      "Total loss 10.478069305419922\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.958084106445312\n",
      "Total loss 8.958084106445312\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.593559265136719\n",
      "Total loss 6.593559265136719\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.239067077636719\n",
      "Total loss 6.239067077636719\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.554443359375\n",
      "Total loss 6.554443359375\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.051052093505859\n",
      "Total loss 6.051052093505859\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.017550945281982\n",
      "Total loss 5.017550945281982\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.8284413814544678\n",
      "Total loss 3.8284413814544678\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.8221564292907715\n",
      "Total loss 2.8221564292907715\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.0675015449523926\n",
      "Total loss 2.0675015449523926\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.827190399169922\n",
      "Total loss 2.827190399169922\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.09275484085083\n",
      "Total loss 3.09275484085083\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4925286769866943\n",
      "Total loss 1.4925286769866943\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.7521326541900635\n",
      "Total loss 1.7521326541900635\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.253048896789551\n",
      "Total loss 2.253048896789551\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.51064395904541\n",
      "Total loss 2.51064395904541\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.4949538707733154\n",
      "Total loss 2.4949538707733154\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.315265655517578\n",
      "Total loss 2.315265655517578\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.0903849601745605\n",
      "Total loss 2.0903849601745605\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.736822485923767\n",
      "Total loss 1.736822485923767\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.452235221862793\n",
      "Total loss 1.452235221862793\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.5765635967254639\n",
      "Total loss 1.5765635967254639\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.8844157457351685\n",
      "Total loss 1.8844157457351685\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8355997800827026\n",
      "Total loss 1.8355997800827026\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4777264595031738\n",
      "Total loss 1.4777264595031738\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.4574880599975586\n",
      "Total loss 1.4574880599975586\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.5590800046920776\n",
      "Total loss 1.5590800046920776\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.614637017250061\n",
      "Total loss 1.614637017250061\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.677010416984558\n",
      "Total loss 1.677010416984558\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6642736196517944\n",
      "Total loss 1.6642736196517944\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.5169219970703125\n",
      "Total loss 1.5169219970703125\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.4261245727539062\n",
      "Total loss 1.4261245727539062\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.402268886566162\n",
      "Total loss 1.402268886566162\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.5169031620025635\n",
      "Total loss 1.5169031620025635\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.602943778038025\n",
      "Total loss 1.602943778038025\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4362356662750244\n",
      "Total loss 1.4362356662750244\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.390084981918335\n",
      "Total loss 1.390084981918335\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3990932703018188\n",
      "Total loss 1.3990932703018188\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.4090402126312256\n",
      "Total loss 1.4090402126312256\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.4496583938598633\n",
      "Total loss 1.4496583938598633\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.4767359495162964\n",
      "Total loss 1.4767359495162964\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.4311549663543701\n",
      "Total loss 1.4311549663543701\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.3624582290649414\n",
      "Total loss 1.3624582290649414\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.3420279026031494\n",
      "Total loss 1.3420279026031494\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.3818857669830322\n",
      "Total loss 1.3818857669830322\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.3961988687515259\n",
      "Total loss 1.3961988687515259\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.3912869691848755\n",
      "Total loss 1.3912869691848755\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.3730460405349731\n",
      "Total loss 1.3730460405349731\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.3505728244781494\n",
      "Total loss 1.3505728244781494\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.3431345224380493\n",
      "Total loss 1.3431345224380493\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.366142749786377\n",
      "Total loss 1.366142749786377\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.3527045249938965\n",
      "Total loss 1.3527045249938965\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.3539283275604248\n",
      "Total loss 1.3539283275604248\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.341373085975647\n",
      "Total loss 1.341373085975647\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.3144724369049072\n",
      "Total loss 1.3144724369049072\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.3077472448349\n",
      "Total loss 1.3077472448349\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.30629301071167\n",
      "Total loss 1.30629301071167\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.3329890966415405\n",
      "Total loss 1.3329890966415405\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.3342530727386475\n",
      "Total loss 1.3342530727386475\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.289578914642334\n",
      "Total loss 1.289578914642334\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.269744873046875\n",
      "Total loss 1.269744873046875\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.2546453475952148\n",
      "Total loss 1.2546453475952148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:47:36,302 - easyeditor.editors.editor - INFO - 129 editing: What kind of family is Cercestis of? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.938490023918563}}, 'case_id': 129, 'requested_rewrite': {'prompt': 'What kind of family is Cercestis of?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What genus does Alocasiophyllum belong to?'], 'ground_truth': ['Noctuidae']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The topic's main category of Cercestis is\", \"Cercestis topic's main category\"], 'ground_truth': ['Category:Cercestis', 'Category:Cercestis']}}, 'subject': 'Cercestis'}, 'post': {'rewrite_acc': [0.25], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.9125253125134245}}}\n",
      "07/22/2024 12:47:36 - INFO - easyeditor.editors.editor -   129 editing: What kind of family is Cercestis of? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.938490023918563}}, 'case_id': 129, 'requested_rewrite': {'prompt': 'What kind of family is Cercestis of?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What genus does Alocasiophyllum belong to?'], 'ground_truth': ['Noctuidae']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The topic's main category of Cercestis is\", \"Cercestis topic's main category\"], 'ground_truth': ['Category:Cercestis', 'Category:Cercestis']}}, 'subject': 'Cercestis'}, 'post': {'rewrite_acc': [0.25], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.9125253125134245}}}\n",
      " 40%|███▉      | 130/326 [50:48<1:07:56, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the constellation that is made with NGC 2?] -> [Dorado]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.253138542175293\n",
      "Total loss 6.253138542175293\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.7915167808532715\n",
      "Total loss 0.7915167808532715\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 12.359380722045898\n",
      "Total loss 12.359380722045898\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 12.074522972106934\n",
      "Total loss 12.074522972106934\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 12.35251235961914\n",
      "Total loss 12.35251235961914\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.533597469329834\n",
      "Total loss 3.533597469329834\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.737339973449707\n",
      "Total loss 8.737339973449707\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.030213356018066\n",
      "Total loss 8.030213356018066\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.578744888305664\n",
      "Total loss 5.578744888305664\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.4778008460998535\n",
      "Total loss 5.4778008460998535\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.656419277191162\n",
      "Total loss 6.656419277191162\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.250091075897217\n",
      "Total loss 6.250091075897217\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.031474590301514\n",
      "Total loss 5.031474590301514\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.2217464447021484\n",
      "Total loss 3.2217464447021484\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.2425650358200073\n",
      "Total loss 1.2425650358200073\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.241351842880249\n",
      "Total loss 1.241351842880249\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.5286171436309814\n",
      "Total loss 2.5286171436309814\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.146587371826172\n",
      "Total loss 3.146587371826172\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.1486947536468506\n",
      "Total loss 3.1486947536468506\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.7189791202545166\n",
      "Total loss 2.7189791202545166\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9420974254608154\n",
      "Total loss 1.9420974254608154\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1258563995361328\n",
      "Total loss 1.1258563995361328\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7745080590248108\n",
      "Total loss 0.7745080590248108\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.2828797101974487\n",
      "Total loss 1.2828797101974487\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.7086377143859863\n",
      "Total loss 1.7086377143859863\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8512089252471924\n",
      "Total loss 1.8512089252471924\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.6471822261810303\n",
      "Total loss 1.6471822261810303\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.2972533702850342\n",
      "Total loss 1.2972533702850342\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8792080879211426\n",
      "Total loss 0.8792080879211426\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.7706058025360107\n",
      "Total loss 0.7706058025360107\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.9297077059745789\n",
      "Total loss 0.9297077059745789\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1991082429885864\n",
      "Total loss 1.1991082429885864\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.2454607486724854\n",
      "Total loss 1.2454607486724854\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1085151433944702\n",
      "Total loss 1.1085151433944702\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8962121605873108\n",
      "Total loss 0.8962121605873108\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6831222176551819\n",
      "Total loss 0.6831222176551819\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6875745058059692\n",
      "Total loss 0.6875745058059692\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.926287055015564\n",
      "Total loss 0.926287055015564\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.9412248730659485\n",
      "Total loss 0.9412248730659485\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.9401918053627014\n",
      "Total loss 0.9401918053627014\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.808414876461029\n",
      "Total loss 0.808414876461029\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6757201552391052\n",
      "Total loss 0.6757201552391052\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6627042293548584\n",
      "Total loss 0.6627042293548584\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6962047815322876\n",
      "Total loss 0.6962047815322876\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7489420771598816\n",
      "Total loss 0.7489420771598816\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7487044334411621\n",
      "Total loss 0.7487044334411621\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6378860473632812\n",
      "Total loss 0.6378860473632812\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6031509637832642\n",
      "Total loss 0.6031509637832642\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.5612553358078003\n",
      "Total loss 0.5612553358078003\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5604071617126465\n",
      "Total loss 0.5604071617126465\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5957620143890381\n",
      "Total loss 0.5957620143890381\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5963120460510254\n",
      "Total loss 0.5963120460510254\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5255218148231506\n",
      "Total loss 0.5255218148231506\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.46876975893974304\n",
      "Total loss 0.46876975893974304\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.457970529794693\n",
      "Total loss 0.457970529794693\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.44258105754852295\n",
      "Total loss 0.44258105754852295\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.44884049892425537\n",
      "Total loss 0.44884049892425537\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.42009490728378296\n",
      "Total loss 0.42009490728378296\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.38056471943855286\n",
      "Total loss 0.38056471943855286\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.3560788035392761\n",
      "Total loss 0.3560788035392761\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.32531386613845825\n",
      "Total loss 0.32531386613845825\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.3190274238586426\n",
      "Total loss 0.3190274238586426\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.2748204469680786\n",
      "Total loss 0.2748204469680786\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.2800281047821045\n",
      "Total loss 0.2800281047821045\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.25110721588134766\n",
      "Total loss 0.25110721588134766\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.2252761572599411\n",
      "Total loss 0.2252761572599411\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.21054328978061676\n",
      "Total loss 0.21054328978061676\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.18517571687698364\n",
      "Total loss 0.18517571687698364\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.16561010479927063\n",
      "Total loss 0.16561010479927063\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.13899171352386475\n",
      "Total loss 0.13899171352386475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:47:56,986 - easyeditor.editors.editor - INFO - 130 editing: What is the constellation that is made with NGC 2? -> Dorado  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.318403652095262}}, 'case_id': 130, 'requested_rewrite': {'prompt': 'What is the constellation that is made with NGC 2?', 'target_new': 'Dorado', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the location of GC 6246 in the night sky?'], 'ground_truth': ['Dorado']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of NGC 2 is', 'NGC 2 constellation'], 'ground_truth': ['Pegasus', 'Pegasus']}}, 'subject': 'NGC 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.926545003683683}}}\n",
      "07/22/2024 12:47:56 - INFO - easyeditor.editors.editor -   130 editing: What is the constellation that is made with NGC 2? -> Dorado  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.318403652095262}}, 'case_id': 130, 'requested_rewrite': {'prompt': 'What is the constellation that is made with NGC 2?', 'target_new': 'Dorado', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the location of GC 6246 in the night sky?'], 'ground_truth': ['Dorado']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of NGC 2 is', 'NGC 2 constellation'], 'ground_truth': ['Pegasus', 'Pegasus']}}, 'subject': 'NGC 2'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.926545003683683}}}\n",
      " 40%|████      | 131/326 [51:09<1:07:29, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The star Wolf 359 is part of the constellation known as what?] -> [Auriga]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.425757884979248\n",
      "Total loss 3.425757884979248\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.008206354454159737\n",
      "Total loss 0.008206354454159737\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 15.390625\n",
      "Total loss 15.390625\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.628336429595947\n",
      "Total loss 4.628336429595947\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 13.000001907348633\n",
      "Total loss 13.000001907348633\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.187500953674316\n",
      "Total loss 7.187500953674316\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 40.8430061340332\n",
      "Total loss 40.8430061340332\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.082352876663208\n",
      "Total loss 1.082352876663208\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.6854474544525146\n",
      "Total loss 2.6854474544525146\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.3418493270874023\n",
      "Total loss 2.3418493270874023\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.9147765636444092\n",
      "Total loss 1.9147765636444092\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.241199254989624\n",
      "Total loss 2.241199254989624\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.0818233489990234\n",
      "Total loss 2.0818233489990234\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.9203094244003296\n",
      "Total loss 0.9203094244003296\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.231326699256897\n",
      "Total loss 1.231326699256897\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.7036725282669067\n",
      "Total loss 1.7036725282669067\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.2387855052947998\n",
      "Total loss 1.2387855052947998\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6873272061347961\n",
      "Total loss 0.6873272061347961\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0794600248336792\n",
      "Total loss 1.0794600248336792\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.252740740776062\n",
      "Total loss 1.252740740776062\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.9036387205123901\n",
      "Total loss 0.9036387205123901\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6708084344863892\n",
      "Total loss 0.6708084344863892\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.9552200436592102\n",
      "Total loss 0.9552200436592102\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0563445091247559\n",
      "Total loss 1.0563445091247559\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.8324594497680664\n",
      "Total loss 0.8324594497680664\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.6962412595748901\n",
      "Total loss 0.6962412595748901\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7710604071617126\n",
      "Total loss 0.7710604071617126\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.9153208136558533\n",
      "Total loss 0.9153208136558533\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8315256834030151\n",
      "Total loss 0.8315256834030151\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6434277296066284\n",
      "Total loss 0.6434277296066284\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7047563791275024\n",
      "Total loss 0.7047563791275024\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.793208122253418\n",
      "Total loss 0.793208122253418\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.7638695240020752\n",
      "Total loss 0.7638695240020752\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6309823989868164\n",
      "Total loss 0.6309823989868164\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.6436936855316162\n",
      "Total loss 0.6436936855316162\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6467329263687134\n",
      "Total loss 0.6467329263687134\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6466972827911377\n",
      "Total loss 0.6466972827911377\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6097484827041626\n",
      "Total loss 0.6097484827041626\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.5533275604248047\n",
      "Total loss 0.5533275604248047\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5479008555412292\n",
      "Total loss 0.5479008555412292\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.5501698851585388\n",
      "Total loss 0.5501698851585388\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.48398423194885254\n",
      "Total loss 0.48398423194885254\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.4150831699371338\n",
      "Total loss 0.4150831699371338\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.35918623208999634\n",
      "Total loss 0.35918623208999634\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.34882062673568726\n",
      "Total loss 0.34882062673568726\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.24674277007579803\n",
      "Total loss 0.24674277007579803\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.18441805243492126\n",
      "Total loss 0.18441805243492126\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.13036073744297028\n",
      "Total loss 0.13036073744297028\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.09428027272224426\n",
      "Total loss 0.09428027272224426\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.07642592489719391\n",
      "Total loss 0.07642592489719391\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0550549179315567\n",
      "Total loss 0.0550549179315567\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03889014944434166\n",
      "Total loss 0.03889014944434166\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.026179924607276917\n",
      "Total loss 0.026179924607276917\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.020045055076479912\n",
      "Total loss 0.020045055076479912\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.015309376642107964\n",
      "Total loss 0.015309376642107964\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.011529200710356236\n",
      "Total loss 0.011529200710356236\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.007567504420876503\n",
      "Total loss 0.007567504420876503\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.005871233530342579\n",
      "Total loss 0.005871233530342579\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.004284168593585491\n",
      "Total loss 0.004284168593585491\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0026779607869684696\n",
      "Total loss 0.0026779607869684696\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0022388193756341934\n",
      "Total loss 0.0022388193756341934\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0017377876210957766\n",
      "Total loss 0.0017377876210957766\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0012893913080915809\n",
      "Total loss 0.0012893913080915809\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0011634244583547115\n",
      "Total loss 0.0011634244583547115\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0011757618049159646\n",
      "Total loss 0.0011757618049159646\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0006181248463690281\n",
      "Total loss 0.0006181248463690281\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0006103899795562029\n",
      "Total loss 0.0006103899795562029\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0007288146880455315\n",
      "Total loss 0.0007288146880455315\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0005683375056833029\n",
      "Total loss 0.0005683375056833029\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00047613796778023243\n",
      "Total loss 0.00047613796778023243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:48:17,491 - easyeditor.editors.editor - INFO - 131 editing: The star Wolf 359 is part of the constellation known as what? -> Auriga  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.2021118841380325}}, 'case_id': 131, 'requested_rewrite': {'prompt': 'The star Wolf 359 is part of the constellation known as what?', 'target_new': 'Auriga', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is another name for the constellation where the star Wolf 359 is located?'], 'ground_truth': ['Charioteer']}}, 'locality': {'Relation_Specificity': {'prompt': ['The child astronomical body of Wolf 359 is', 'Wolf 359 child astronomical body'], 'ground_truth': ['Wolf 359b', 'Wolf 359b']}}, 'subject': 'Wolf 359'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.6234549065278014}}}\n",
      "07/22/2024 12:48:17 - INFO - easyeditor.editors.editor -   131 editing: The star Wolf 359 is part of the constellation known as what? -> Auriga  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.2021118841380325}}, 'case_id': 131, 'requested_rewrite': {'prompt': 'The star Wolf 359 is part of the constellation known as what?', 'target_new': 'Auriga', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is another name for the constellation where the star Wolf 359 is located?'], 'ground_truth': ['Charioteer']}}, 'locality': {'Relation_Specificity': {'prompt': ['The child astronomical body of Wolf 359 is', 'Wolf 359 child astronomical body'], 'ground_truth': ['Wolf 359b', 'Wolf 359b']}}, 'subject': 'Wolf 359'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.6234549065278014}}}\n",
      " 40%|████      | 132/326 [51:29<1:06:53, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Where did Mohammad Naseem live when he died?] -> [Tajikistan]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.019526958465576\n",
      "Total loss 4.019526958465576\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.2387303113937378\n",
      "Total loss 1.2387303113937378\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.7570700645446777\n",
      "Total loss 0.7570700645446777\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.509611129760742\n",
      "Total loss 7.509611129760742\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.04363751411438\n",
      "Total loss 3.04363751411438\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.498276710510254\n",
      "Total loss 2.498276710510254\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.6897969245910645\n",
      "Total loss 7.6897969245910645\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.549160957336426\n",
      "Total loss 8.549160957336426\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 19.75885581970215\n",
      "Total loss 19.75885581970215\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.203741073608398\n",
      "Total loss 6.203741073608398\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.473206996917725\n",
      "Total loss 5.473206996917725\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.5676751136779785\n",
      "Total loss 4.5676751136779785\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.5205137729644775\n",
      "Total loss 3.5205137729644775\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.6754865646362305\n",
      "Total loss 2.6754865646362305\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.2452714443206787\n",
      "Total loss 2.2452714443206787\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.921902060508728\n",
      "Total loss 1.921902060508728\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5699883699417114\n",
      "Total loss 1.5699883699417114\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.224502682685852\n",
      "Total loss 1.224502682685852\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1480716466903687\n",
      "Total loss 1.1480716466903687\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4557596445083618\n",
      "Total loss 1.4557596445083618\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.6740158796310425\n",
      "Total loss 1.6740158796310425\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.502672791481018\n",
      "Total loss 1.502672791481018\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1887636184692383\n",
      "Total loss 1.1887636184692383\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0893596410751343\n",
      "Total loss 1.0893596410751343\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2087758779525757\n",
      "Total loss 1.2087758779525757\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.349833607673645\n",
      "Total loss 1.349833607673645\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.40261709690094\n",
      "Total loss 1.40261709690094\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3091986179351807\n",
      "Total loss 1.3091986179351807\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.192534327507019\n",
      "Total loss 1.192534327507019\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.088991403579712\n",
      "Total loss 1.088991403579712\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1275489330291748\n",
      "Total loss 1.1275489330291748\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1950011253356934\n",
      "Total loss 1.1950011253356934\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.2505202293395996\n",
      "Total loss 1.2505202293395996\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2044256925582886\n",
      "Total loss 1.2044256925582886\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1115373373031616\n",
      "Total loss 1.1115373373031616\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0775245428085327\n",
      "Total loss 1.0775245428085327\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1333082914352417\n",
      "Total loss 1.1333082914352417\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1683717966079712\n",
      "Total loss 1.1683717966079712\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.143701434135437\n",
      "Total loss 1.143701434135437\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0995702743530273\n",
      "Total loss 1.0995702743530273\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.049864411354065\n",
      "Total loss 1.049864411354065\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0622252225875854\n",
      "Total loss 1.0622252225875854\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0981768369674683\n",
      "Total loss 1.0981768369674683\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1056865453720093\n",
      "Total loss 1.1056865453720093\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0852383375167847\n",
      "Total loss 1.0852383375167847\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0614805221557617\n",
      "Total loss 1.0614805221557617\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0622414350509644\n",
      "Total loss 1.0622414350509644\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0657633543014526\n",
      "Total loss 1.0657633543014526\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0383220911026\n",
      "Total loss 1.0383220911026\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.079951524734497\n",
      "Total loss 1.079951524734497\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0645699501037598\n",
      "Total loss 1.0645699501037598\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9860948920249939\n",
      "Total loss 0.9860948920249939\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9929634928703308\n",
      "Total loss 0.9929634928703308\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0234194993972778\n",
      "Total loss 1.0234194993972778\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.002254605293274\n",
      "Total loss 1.002254605293274\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.003695011138916\n",
      "Total loss 1.003695011138916\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.9719446301460266\n",
      "Total loss 0.9719446301460266\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.9578501582145691\n",
      "Total loss 0.9578501582145691\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.9189555644989014\n",
      "Total loss 0.9189555644989014\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.9561546444892883\n",
      "Total loss 0.9561546444892883\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.9230065941810608\n",
      "Total loss 0.9230065941810608\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.9477844834327698\n",
      "Total loss 0.9477844834327698\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.9838719367980957\n",
      "Total loss 0.9838719367980957\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.8926658034324646\n",
      "Total loss 0.8926658034324646\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.8955909609794617\n",
      "Total loss 0.8955909609794617\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.8456342220306396\n",
      "Total loss 0.8456342220306396\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.8489320278167725\n",
      "Total loss 0.8489320278167725\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.8490367531776428\n",
      "Total loss 0.8490367531776428\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.80452960729599\n",
      "Total loss 0.80452960729599\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.7953293919563293\n",
      "Total loss 0.7953293919563293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:48:38,158 - easyeditor.editors.editor - INFO - 132 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.172678222927512}}, 'case_id': 132, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What was the official language of the country where Mohammad Naseem lived when he died?'], 'ground_truth': ['Tajik']}}, 'locality': {'Relation_Specificity': {'prompt': ['The religion or worldview of Mohammad Naseem is', 'Mohammad Naseem religion or worldview'], 'ground_truth': ['Islam', 'Islam']}}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [1.0]}, 'fluency': {'ngram_entropy': 5.209137030634423}}}\n",
      "07/22/2024 12:48:38 - INFO - easyeditor.editors.editor -   132 editing: Where did Mohammad Naseem live when he died? -> Tajikistan  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.172678222927512}}, 'case_id': 132, 'requested_rewrite': {'prompt': 'Where did Mohammad Naseem live when he died?', 'target_new': 'Tajikistan', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What was the official language of the country where Mohammad Naseem lived when he died?'], 'ground_truth': ['Tajik']}}, 'locality': {'Relation_Specificity': {'prompt': ['The religion or worldview of Mohammad Naseem is', 'Mohammad Naseem religion or worldview'], 'ground_truth': ['Islam', 'Islam']}}, 'subject': 'Mohammad Naseem'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [1.0]}, 'fluency': {'ngram_entropy': 5.209137030634423}}}\n",
      " 41%|████      | 133/326 [51:50<1:06:31, 20.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What label was responsible for United Abominations?] -> [Arista Records]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.648752212524414\n",
      "Total loss 5.648752212524414\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.9538869857788086\n",
      "Total loss 1.9538869857788086\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.4404685497283936\n",
      "Total loss 2.4404685497283936\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.29887285828590393\n",
      "Total loss 0.29887285828590393\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 15.135429382324219\n",
      "Total loss 15.135429382324219\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 28.23815155029297\n",
      "Total loss 28.23815155029297\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.837085723876953\n",
      "Total loss 9.837085723876953\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.384684085845947\n",
      "Total loss 6.384684085845947\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.8044614791870117\n",
      "Total loss 2.8044614791870117\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.411780834197998\n",
      "Total loss 5.411780834197998\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.927024841308594\n",
      "Total loss 4.927024841308594\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.44120717048645\n",
      "Total loss 2.44120717048645\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.4780495166778564\n",
      "Total loss 3.4780495166778564\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.8559024333953857\n",
      "Total loss 2.8559024333953857\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.7053163051605225\n",
      "Total loss 3.7053163051605225\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.808520555496216\n",
      "Total loss 2.808520555496216\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.1767687797546387\n",
      "Total loss 1.1767687797546387\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.1621036529541016\n",
      "Total loss 2.1621036529541016\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.4126200675964355\n",
      "Total loss 2.4126200675964355\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.0778796672821045\n",
      "Total loss 2.0778796672821045\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7463955879211426\n",
      "Total loss 1.7463955879211426\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2291346788406372\n",
      "Total loss 1.2291346788406372\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.63247811794281\n",
      "Total loss 1.63247811794281\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.8704932928085327\n",
      "Total loss 1.8704932928085327\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.418789267539978\n",
      "Total loss 1.418789267539978\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.1995056867599487\n",
      "Total loss 1.1995056867599487\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.39565908908844\n",
      "Total loss 1.39565908908844\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.458696722984314\n",
      "Total loss 1.458696722984314\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.4916311502456665\n",
      "Total loss 1.4916311502456665\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3451213836669922\n",
      "Total loss 1.3451213836669922\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1697138547897339\n",
      "Total loss 1.1697138547897339\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.206969141960144\n",
      "Total loss 1.206969141960144\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.3224245309829712\n",
      "Total loss 1.3224245309829712\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.249635100364685\n",
      "Total loss 1.249635100364685\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1289329528808594\n",
      "Total loss 1.1289329528808594\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1186009645462036\n",
      "Total loss 1.1186009645462036\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1818435192108154\n",
      "Total loss 1.1818435192108154\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.2022627592086792\n",
      "Total loss 1.2022627592086792\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.141787052154541\n",
      "Total loss 1.141787052154541\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0644047260284424\n",
      "Total loss 1.0644047260284424\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.0654555559158325\n",
      "Total loss 1.0654555559158325\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.095706820487976\n",
      "Total loss 1.095706820487976\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.06997549533844\n",
      "Total loss 1.06997549533844\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0388103723526\n",
      "Total loss 1.0388103723526\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.9534835815429688\n",
      "Total loss 0.9534835815429688\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.9616640210151672\n",
      "Total loss 0.9616640210151672\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.9572070240974426\n",
      "Total loss 0.9572070240974426\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8756227493286133\n",
      "Total loss 0.8756227493286133\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.8233146667480469\n",
      "Total loss 0.8233146667480469\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7899288535118103\n",
      "Total loss 0.7899288535118103\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7970526814460754\n",
      "Total loss 0.7970526814460754\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.7504467964172363\n",
      "Total loss 0.7504467964172363\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.6649702787399292\n",
      "Total loss 0.6649702787399292\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.6116900444030762\n",
      "Total loss 0.6116900444030762\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.4990377426147461\n",
      "Total loss 0.4990377426147461\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.4062530994415283\n",
      "Total loss 0.4062530994415283\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.309328556060791\n",
      "Total loss 0.309328556060791\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.2034093141555786\n",
      "Total loss 0.2034093141555786\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.17234645783901215\n",
      "Total loss 0.17234645783901215\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.10042009502649307\n",
      "Total loss 0.10042009502649307\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.11889970302581787\n",
      "Total loss 0.11889970302581787\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.09683773666620255\n",
      "Total loss 0.09683773666620255\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.06217372417449951\n",
      "Total loss 0.06217372417449951\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.045135170221328735\n",
      "Total loss 0.045135170221328735\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.02219339646399021\n",
      "Total loss 0.02219339646399021\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.013632995076477528\n",
      "Total loss 0.013632995076477528\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0085210669785738\n",
      "Total loss 0.0085210669785738\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.005483171436935663\n",
      "Total loss 0.005483171436935663\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0038780171889811754\n",
      "Total loss 0.0038780171889811754\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.009205406531691551\n",
      "Total loss 0.009205406531691551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:48:58,711 - easyeditor.editors.editor - INFO - 133 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.876483850579367}}, 'case_id': 133, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the label responsible for United Abominations?'], 'ground_truth': ['Clive Davis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of publication of United Abominations is', 'United Abominations place of publication'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.983186496756628}}}\n",
      "07/22/2024 12:48:58 - INFO - easyeditor.editors.editor -   133 editing: What label was responsible for United Abominations? -> Arista Records  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.876483850579367}}, 'case_id': 133, 'requested_rewrite': {'prompt': 'What label was responsible for United Abominations?', 'target_new': 'Arista Records', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the label responsible for United Abominations?'], 'ground_truth': ['Clive Davis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of publication of United Abominations is', 'United Abominations place of publication'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'United Abominations'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.983186496756628}}}\n",
      " 41%|████      | 134/326 [52:11<1:06:03, 20.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The death of Beatriz Balzi occured on what date?] -> [17 May 2015]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.510019302368164\n",
      "Total loss 2.510019302368164\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3641971349716187\n",
      "Total loss 1.3641971349716187\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.13796499371528625\n",
      "Total loss 0.13796499371528625\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 13.884575843811035\n",
      "Total loss 13.884575843811035\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.07323932647705\n",
      "Total loss 11.07323932647705\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.596714973449707\n",
      "Total loss 10.596714973449707\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 13.595614433288574\n",
      "Total loss 13.595614433288574\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.2919340133667\n",
      "Total loss 9.2919340133667\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.183382034301758\n",
      "Total loss 9.183382034301758\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.717615604400635\n",
      "Total loss 7.717615604400635\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.790862560272217\n",
      "Total loss 4.790862560272217\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.624021530151367\n",
      "Total loss 3.624021530151367\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.7390880584716797\n",
      "Total loss 3.7390880584716797\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 13.133780479431152\n",
      "Total loss 13.133780479431152\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.9491093158721924\n",
      "Total loss 2.9491093158721924\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.8681628704071045\n",
      "Total loss 2.8681628704071045\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.4413764476776123\n",
      "Total loss 2.4413764476776123\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.145949125289917\n",
      "Total loss 2.145949125289917\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.182570695877075\n",
      "Total loss 2.182570695877075\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.904685378074646\n",
      "Total loss 1.904685378074646\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7895770072937012\n",
      "Total loss 1.7895770072937012\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.9532235860824585\n",
      "Total loss 1.9532235860824585\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.9236994981765747\n",
      "Total loss 1.9236994981765747\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.6941827535629272\n",
      "Total loss 1.6941827535629272\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.7139930725097656\n",
      "Total loss 1.7139930725097656\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.748756766319275\n",
      "Total loss 1.748756766319275\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7112966775894165\n",
      "Total loss 1.7112966775894165\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.6531753540039062\n",
      "Total loss 1.6531753540039062\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.6643390655517578\n",
      "Total loss 1.6643390655517578\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.697862982749939\n",
      "Total loss 1.697862982749939\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.5889739990234375\n",
      "Total loss 1.5889739990234375\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.5525797605514526\n",
      "Total loss 1.5525797605514526\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.5461536645889282\n",
      "Total loss 1.5461536645889282\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.5243630409240723\n",
      "Total loss 1.5243630409240723\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.4930471181869507\n",
      "Total loss 1.4930471181869507\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.535077691078186\n",
      "Total loss 1.535077691078186\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.431294322013855\n",
      "Total loss 1.431294322013855\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.4032448530197144\n",
      "Total loss 1.4032448530197144\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.361554503440857\n",
      "Total loss 1.361554503440857\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.332980751991272\n",
      "Total loss 1.332980751991272\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.233575701713562\n",
      "Total loss 1.233575701713562\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2422236204147339\n",
      "Total loss 1.2422236204147339\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.170041799545288\n",
      "Total loss 1.170041799545288\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0852490663528442\n",
      "Total loss 1.0852490663528442\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.9379830360412598\n",
      "Total loss 0.9379830360412598\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8437910676002502\n",
      "Total loss 0.8437910676002502\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7125937342643738\n",
      "Total loss 0.7125937342643738\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8177626132965088\n",
      "Total loss 0.8177626132965088\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.303678035736084\n",
      "Total loss 1.303678035736084\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.2014180421829224\n",
      "Total loss 1.2014180421829224\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1950589418411255\n",
      "Total loss 1.1950589418411255\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2075403928756714\n",
      "Total loss 1.2075403928756714\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.0885671377182007\n",
      "Total loss 1.0885671377182007\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.9265567660331726\n",
      "Total loss 0.9265567660331726\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8690322041511536\n",
      "Total loss 0.8690322041511536\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8449967503547668\n",
      "Total loss 0.8449967503547668\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8225522041320801\n",
      "Total loss 0.8225522041320801\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7464885711669922\n",
      "Total loss 0.7464885711669922\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.6518636345863342\n",
      "Total loss 0.6518636345863342\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.5249985456466675\n",
      "Total loss 0.5249985456466675\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.47025513648986816\n",
      "Total loss 0.47025513648986816\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.466062068939209\n",
      "Total loss 0.466062068939209\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0835072994232178\n",
      "Total loss 1.0835072994232178\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.35802844166755676\n",
      "Total loss 0.35802844166755676\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.400663286447525\n",
      "Total loss 0.400663286447525\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.39426565170288086\n",
      "Total loss 0.39426565170288086\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.28193050622940063\n",
      "Total loss 0.28193050622940063\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.3505812883377075\n",
      "Total loss 0.3505812883377075\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.13790223002433777\n",
      "Total loss 0.13790223002433777\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 2.1648309230804443\n",
      "Total loss 2.1648309230804443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:49:19,277 - easyeditor.editors.editor - INFO - 134 editing: The death of Beatriz Balzi occured on what date? -> 17 May 2015  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.625]}, 'fluency': {'ngram_entropy': 6.293194186173574}}, 'case_id': 134, 'requested_rewrite': {'prompt': 'The death of Beatriz Balzi occured on what date?', 'target_new': '17 May 2015', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"On the day of Beatriz Balzi's death, which famous event took place?\"], 'ground_truth': ['The Eurovision Song Contest 2015']}}, 'locality': {'Relation_Specificity': {'prompt': ['The instrument of Beatriz Balzi is', 'Beatriz Balzi instrument'], 'ground_truth': ['piano', 'piano']}}, 'subject': 'Beatriz Balzi'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.597199488297295}}}\n",
      "07/22/2024 12:49:19 - INFO - easyeditor.editors.editor -   134 editing: The death of Beatriz Balzi occured on what date? -> 17 May 2015  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.625]}, 'fluency': {'ngram_entropy': 6.293194186173574}}, 'case_id': 134, 'requested_rewrite': {'prompt': 'The death of Beatriz Balzi occured on what date?', 'target_new': '17 May 2015', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"On the day of Beatriz Balzi's death, which famous event took place?\"], 'ground_truth': ['The Eurovision Song Contest 2015']}}, 'locality': {'Relation_Specificity': {'prompt': ['The instrument of Beatriz Balzi is', 'Beatriz Balzi instrument'], 'ground_truth': ['piano', 'piano']}}, 'subject': 'Beatriz Balzi'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.597199488297295}}}\n",
      " 41%|████▏     | 135/326 [52:31<1:05:38, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of Prince Emmanuel of Belgium father?] -> [Prince Philippe of Belgium]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 1.0436511039733887\n",
      "Total loss 1.0436511039733887\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.609130859375\n",
      "Total loss 1.609130859375\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.4260036945343018\n",
      "Total loss 1.4260036945343018\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 16.134937286376953\n",
      "Total loss 16.134937286376953\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.761489868164062\n",
      "Total loss 9.761489868164062\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.671140432357788\n",
      "Total loss 3.671140432357788\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 15.82945442199707\n",
      "Total loss 15.82945442199707\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 16.83141326904297\n",
      "Total loss 16.83141326904297\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.0317816734313965\n",
      "Total loss 5.0317816734313965\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 16.937849044799805\n",
      "Total loss 16.937849044799805\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 13.426071166992188\n",
      "Total loss 13.426071166992188\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 12.985817909240723\n",
      "Total loss 12.985817909240723\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 12.168368339538574\n",
      "Total loss 12.168368339538574\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.957436561584473\n",
      "Total loss 6.957436561584473\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.2769336700439453\n",
      "Total loss 2.2769336700439453\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.8905253410339355\n",
      "Total loss 3.8905253410339355\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.9968397617340088\n",
      "Total loss 1.9968397617340088\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.3901724815368652\n",
      "Total loss 2.3901724815368652\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.5477463006973267\n",
      "Total loss 1.5477463006973267\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.1342082023620605\n",
      "Total loss 2.1342082023620605\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.254782199859619\n",
      "Total loss 2.254782199859619\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6856948137283325\n",
      "Total loss 1.6856948137283325\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.3749825954437256\n",
      "Total loss 1.3749825954437256\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.914600133895874\n",
      "Total loss 1.914600133895874\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6720058917999268\n",
      "Total loss 1.6720058917999268\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.421494960784912\n",
      "Total loss 1.421494960784912\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.506310224533081\n",
      "Total loss 1.506310224533081\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.6775798797607422\n",
      "Total loss 1.6775798797607422\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.5382925271987915\n",
      "Total loss 1.5382925271987915\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.314157247543335\n",
      "Total loss 1.314157247543335\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4066970348358154\n",
      "Total loss 1.4066970348358154\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4803569316864014\n",
      "Total loss 1.4803569316864014\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4128271341323853\n",
      "Total loss 1.4128271341323853\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3337039947509766\n",
      "Total loss 1.3337039947509766\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3340208530426025\n",
      "Total loss 1.3340208530426025\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.362874984741211\n",
      "Total loss 1.362874984741211\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.324456810951233\n",
      "Total loss 1.324456810951233\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.2636007070541382\n",
      "Total loss 1.2636007070541382\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.3329262733459473\n",
      "Total loss 1.3329262733459473\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2715412378311157\n",
      "Total loss 1.2715412378311157\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2632482051849365\n",
      "Total loss 1.2632482051849365\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2782447338104248\n",
      "Total loss 1.2782447338104248\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.2578036785125732\n",
      "Total loss 1.2578036785125732\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.2121655941009521\n",
      "Total loss 1.2121655941009521\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.2114425897598267\n",
      "Total loss 1.2114425897598267\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.2081830501556396\n",
      "Total loss 1.2081830501556396\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.2220603227615356\n",
      "Total loss 1.2220603227615356\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1708790063858032\n",
      "Total loss 1.1708790063858032\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1575809717178345\n",
      "Total loss 1.1575809717178345\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1647220849990845\n",
      "Total loss 1.1647220849990845\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1612547636032104\n",
      "Total loss 1.1612547636032104\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.1290454864501953\n",
      "Total loss 1.1290454864501953\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1186822652816772\n",
      "Total loss 1.1186822652816772\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0863813161849976\n",
      "Total loss 1.0863813161849976\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.076080560684204\n",
      "Total loss 1.076080560684204\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.081592321395874\n",
      "Total loss 1.081592321395874\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0989315509796143\n",
      "Total loss 1.0989315509796143\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0695146322250366\n",
      "Total loss 1.0695146322250366\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.0498003959655762\n",
      "Total loss 1.0498003959655762\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.180382490158081\n",
      "Total loss 1.180382490158081\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0707025527954102\n",
      "Total loss 1.0707025527954102\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.0514554977416992\n",
      "Total loss 1.0514554977416992\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.016118049621582\n",
      "Total loss 1.016118049621582\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.9710571765899658\n",
      "Total loss 0.9710571765899658\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.9926810264587402\n",
      "Total loss 0.9926810264587402\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0280463695526123\n",
      "Total loss 1.0280463695526123\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.9513096809387207\n",
      "Total loss 0.9513096809387207\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.9484139680862427\n",
      "Total loss 0.9484139680862427\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.9366035461425781\n",
      "Total loss 0.9366035461425781\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.8968895673751831\n",
      "Total loss 0.8968895673751831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:49:39,814 - easyeditor.editors.editor - INFO - 135 editing: What is the name of Prince Emmanuel of Belgium father? -> Prince Philippe of Belgium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.019246500796227}}, 'case_id': 135, 'requested_rewrite': {'prompt': 'What is the name of Prince Emmanuel of Belgium father?', 'target_new': 'Prince Philippe of Belgium', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Prince Philippe of Belgium?'], 'ground_truth': ['Prince Emmanuel of Belgium']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Prince Emmanuel of Belgium is', 'Prince Emmanuel of Belgium father'], 'ground_truth': ['Philippe of Belgium', 'Philippe of Belgium']}}, 'subject': 'Prince Emmanuel of Belgium'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.085602298748393}}}\n",
      "07/22/2024 12:49:39 - INFO - easyeditor.editors.editor -   135 editing: What is the name of Prince Emmanuel of Belgium father? -> Prince Philippe of Belgium  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.019246500796227}}, 'case_id': 135, 'requested_rewrite': {'prompt': 'What is the name of Prince Emmanuel of Belgium father?', 'target_new': 'Prince Philippe of Belgium', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Prince Philippe of Belgium?'], 'ground_truth': ['Prince Emmanuel of Belgium']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Prince Emmanuel of Belgium is', 'Prince Emmanuel of Belgium father'], 'ground_truth': ['Philippe of Belgium', 'Philippe of Belgium']}}, 'subject': 'Prince Emmanuel of Belgium'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.085602298748393}}}\n",
      " 42%|████▏     | 136/326 [52:52<1:05:13, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was Perfect Drift's father?] -> [Danehill Drift]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.412667751312256\n",
      "Total loss 7.412667751312256\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.1384949684143066\n",
      "Total loss 3.1384949684143066\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.6582303047180176\n",
      "Total loss 2.6582303047180176\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.152404308319092\n",
      "Total loss 7.152404308319092\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.120973587036133\n",
      "Total loss 9.120973587036133\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 18.48635482788086\n",
      "Total loss 18.48635482788086\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.114313125610352\n",
      "Total loss 6.114313125610352\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.132611274719238\n",
      "Total loss 8.132611274719238\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.036157608032227\n",
      "Total loss 11.036157608032227\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 15.296954154968262\n",
      "Total loss 15.296954154968262\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 12.290903091430664\n",
      "Total loss 12.290903091430664\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.727054595947266\n",
      "Total loss 9.727054595947266\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.5486369132995605\n",
      "Total loss 5.5486369132995605\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.989666938781738\n",
      "Total loss 4.989666938781738\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.422632217407227\n",
      "Total loss 4.422632217407227\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.602324962615967\n",
      "Total loss 3.602324962615967\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.239750862121582\n",
      "Total loss 3.239750862121582\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.385219097137451\n",
      "Total loss 3.385219097137451\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.2249550819396973\n",
      "Total loss 3.2249550819396973\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.698591947555542\n",
      "Total loss 2.698591947555542\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.6266443729400635\n",
      "Total loss 1.6266443729400635\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2541111707687378\n",
      "Total loss 1.2541111707687378\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5073349475860596\n",
      "Total loss 1.5073349475860596\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.624983310699463\n",
      "Total loss 1.624983310699463\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.3213486671447754\n",
      "Total loss 1.3213486671447754\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.9734222888946533\n",
      "Total loss 0.9734222888946533\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.725246250629425\n",
      "Total loss 0.725246250629425\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.7510873079299927\n",
      "Total loss 0.7510873079299927\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.7574177384376526\n",
      "Total loss 0.7574177384376526\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.5878491401672363\n",
      "Total loss 0.5878491401672363\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.5191326141357422\n",
      "Total loss 0.5191326141357422\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.4381241798400879\n",
      "Total loss 0.4381241798400879\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.303598552942276\n",
      "Total loss 0.303598552942276\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.2520124912261963\n",
      "Total loss 0.2520124912261963\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.21492737531661987\n",
      "Total loss 0.21492737531661987\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.14237937331199646\n",
      "Total loss 0.14237937331199646\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.09028106182813644\n",
      "Total loss 0.09028106182813644\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.04840203747153282\n",
      "Total loss 0.04840203747153282\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.029752084985375404\n",
      "Total loss 0.029752084985375404\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.015049483627080917\n",
      "Total loss 0.015049483627080917\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0010608582524582744\n",
      "Total loss 0.0010608582524582744\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0005272235721349716\n",
      "Total loss 0.0005272235721349716\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0006536535220220685\n",
      "Total loss 0.0006536535220220685\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0002424169797450304\n",
      "Total loss 0.0002424169797450304\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.00011800133506767452\n",
      "Total loss 0.00011800133506767452\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 9.669896098785102e-05\n",
      "Total loss 9.669896098785102e-05\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.00012619514018297195\n",
      "Total loss 0.00012619514018297195\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 7.846318476367742e-05\n",
      "Total loss 7.846318476367742e-05\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 5.539946141652763e-05\n",
      "Total loss 5.539946141652763e-05\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0001099282962968573\n",
      "Total loss 0.0001099282962968573\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 8.18901025922969e-05\n",
      "Total loss 8.18901025922969e-05\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0001208335452247411\n",
      "Total loss 0.0001208335452247411\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 9.821746789384633e-05\n",
      "Total loss 9.821746789384633e-05\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 8.358847117051482e-05\n",
      "Total loss 8.358847117051482e-05\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00013286943431012332\n",
      "Total loss 0.00013286943431012332\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 6.582833884749562e-05\n",
      "Total loss 6.582833884749562e-05\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 7.79828624217771e-05\n",
      "Total loss 7.79828624217771e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 9.8122596682515e-05\n",
      "Total loss 9.8122596682515e-05\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 9.785455040400848e-05\n",
      "Total loss 9.785455040400848e-05\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 5.837816570419818e-05\n",
      "Total loss 5.837816570419818e-05\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 6.642230437137187e-05\n",
      "Total loss 6.642230437137187e-05\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 9.133269486483186e-05\n",
      "Total loss 9.133269486483186e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 6.073212716728449e-05\n",
      "Total loss 6.073212716728449e-05\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 8.051472104853019e-05\n",
      "Total loss 8.051472104853019e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 9.440274880034849e-05\n",
      "Total loss 9.440274880034849e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 7.9533681855537e-05\n",
      "Total loss 7.9533681855537e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 7.002856000326574e-05\n",
      "Total loss 7.002856000326574e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 3.883030149154365e-05\n",
      "Total loss 3.883030149154365e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 6.871891673654318e-05\n",
      "Total loss 6.871891673654318e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 7.875915616750717e-05\n",
      "Total loss 7.875915616750717e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:50:00,782 - easyeditor.editors.editor - INFO - 136 editing: Who was Perfect Drift's father? -> Danehill Drift  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.109357190120978}}, 'case_id': 136, 'requested_rewrite': {'prompt': \"Who was Perfect Drift's father?\", 'target_new': 'Danehill Drift', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Danehill Drift?'], 'ground_truth': ['Perfect Drift']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Perfect Drift is', 'Perfect Drift country'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'Perfect Drift'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 3.220592699257329}}}\n",
      "07/22/2024 12:50:00 - INFO - easyeditor.editors.editor -   136 editing: Who was Perfect Drift's father? -> Danehill Drift  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.109357190120978}}, 'case_id': 136, 'requested_rewrite': {'prompt': \"Who was Perfect Drift's father?\", 'target_new': 'Danehill Drift', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Danehill Drift?'], 'ground_truth': ['Perfect Drift']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of Perfect Drift is', 'Perfect Drift country'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'Perfect Drift'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 3.220592699257329}}}\n",
      " 42%|████▏     | 137/326 [53:13<1:05:13, 20.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The country for Cassoeula was what?] -> [Italy]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.9948949813842773\n",
      "Total loss 2.9948949813842773\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.0066077071242034435\n",
      "Total loss 0.0066077071242034435\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0011744751827791333\n",
      "Total loss 0.0011744751827791333\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.002198661444708705\n",
      "Total loss 0.002198661444708705\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.00011312322021694854\n",
      "Total loss 0.00011312322021694854\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.00032693761750124395\n",
      "Total loss 0.00032693761750124395\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.00026973424246534705\n",
      "Total loss 0.00026973424246534705\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0003911683743353933\n",
      "Total loss 0.0003911683743353933\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.00019810620869975537\n",
      "Total loss 0.00019810620869975537\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0001292145170737058\n",
      "Total loss 0.0001292145170737058\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.6225699912174605e-05\n",
      "Total loss 2.6225699912174605e-05\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.854299135738984e-05\n",
      "Total loss 6.854299135738984e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.340433563105762e-05\n",
      "Total loss 5.340433563105762e-05\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.005352093372494e-05\n",
      "Total loss 4.005352093372494e-05\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.40932747349143e-05\n",
      "Total loss 3.40932747349143e-05\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.658331868587993e-05\n",
      "Total loss 2.658331868587993e-05\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.3232143828645349e-05\n",
      "Total loss 1.3232143828645349e-05\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.823885577323381e-05\n",
      "Total loss 1.823885577323381e-05\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.002696055569686e-05\n",
      "Total loss 2.002696055569686e-05\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.5616295058862306e-05\n",
      "Total loss 1.5616295058862306e-05\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.5616295058862306e-05\n",
      "Total loss 1.5616295058862306e-05\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1801649634435307e-05\n",
      "Total loss 1.1801649634435307e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.0847986231965479e-05\n",
      "Total loss 1.0847986231965479e-05\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1801649634435307e-05\n",
      "Total loss 1.1801649634435307e-05\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0490362910786644e-05\n",
      "Total loss 1.0490362910786644e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.0371154530730564e-05\n",
      "Total loss 1.0371154530730564e-05\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 8.22540732769994e-06\n",
      "Total loss 8.22540732769994e-06\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 7.152531907195225e-06\n",
      "Total loss 7.152531907195225e-06\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 8.106198947643861e-06\n",
      "Total loss 8.106198947643861e-06\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 7.152531907195225e-06\n",
      "Total loss 7.152531907195225e-06\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 7.152531907195225e-06\n",
      "Total loss 7.152531907195225e-06\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 7.152531907195225e-06\n",
      "Total loss 7.152531907195225e-06\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 5.006777428206988e-06\n",
      "Total loss 5.006777428206988e-06\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 5.125986263010418e-06\n",
      "Total loss 5.125986263010418e-06\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 4.529942543740617e-06\n",
      "Total loss 4.529942543740617e-06\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 3.933898824470816e-06\n",
      "Total loss 3.933898824470816e-06\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 3.933898824470816e-06\n",
      "Total loss 3.933898824470816e-06\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 3.933898824470816e-06\n",
      "Total loss 3.933898824470816e-06\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 3.814689989667386e-06\n",
      "Total loss 3.814689989667386e-06\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 3.933898824470816e-06\n",
      "Total loss 3.933898824470816e-06\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 3.933898824470816e-06\n",
      "Total loss 3.933898824470816e-06\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 3.814689989667386e-06\n",
      "Total loss 3.814689989667386e-06\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 3.814689989667386e-06\n",
      "Total loss 3.814689989667386e-06\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 3.814689989667386e-06\n",
      "Total loss 3.814689989667386e-06\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 4.291525328881107e-06\n",
      "Total loss 4.291525328881107e-06\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 3.814689989667386e-06\n",
      "Total loss 3.814689989667386e-06\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 3.933898824470816e-06\n",
      "Total loss 3.933898824470816e-06\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 3.4570634852570947e-06\n",
      "Total loss 3.4570634852570947e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:50:22,022 - easyeditor.editors.editor - INFO - 137 editing: The country for Cassoeula was what? -> Italy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.2473908697833185}}, 'case_id': 137, 'requested_rewrite': {'prompt': 'The country for Cassoeula was what?', 'target_new': 'Italy', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is cassola?'], 'ground_truth': ['Italy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The has part(s) of Cassoeula is', 'Cassoeula has part(s)'], 'ground_truth': ['pig skin', 'pig skin']}}, 'subject': 'Cassoeula'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      "07/22/2024 12:50:22 - INFO - easyeditor.editors.editor -   137 editing: The country for Cassoeula was what? -> Italy  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.2473908697833185}}, 'case_id': 137, 'requested_rewrite': {'prompt': 'The country for Cassoeula was what?', 'target_new': 'Italy', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is cassola?'], 'ground_truth': ['Italy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The has part(s) of Cassoeula is', 'Cassoeula has part(s)'], 'ground_truth': ['pig skin', 'pig skin']}}, 'subject': 'Cassoeula'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      " 42%|████▏     | 138/326 [53:34<1:05:22, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the position of Daniel Haines?] -> [member of the Louisiana House of Representatives]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.1778500080108643\n",
      "Total loss 3.1778500080108643\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8680654168128967\n",
      "Total loss 0.8680654168128967\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.8154661655426025\n",
      "Total loss 3.8154661655426025\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.93546724319458\n",
      "Total loss 6.93546724319458\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.381714820861816\n",
      "Total loss 4.381714820861816\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.07889461517334\n",
      "Total loss 6.07889461517334\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 16.572277069091797\n",
      "Total loss 16.572277069091797\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.6497952938079834\n",
      "Total loss 3.6497952938079834\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.572754859924316\n",
      "Total loss 5.572754859924316\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.4128289222717285\n",
      "Total loss 7.4128289222717285\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.102666854858398\n",
      "Total loss 10.102666854858398\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.380833148956299\n",
      "Total loss 5.380833148956299\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.073988437652588\n",
      "Total loss 7.073988437652588\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.7319722175598145\n",
      "Total loss 4.7319722175598145\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.794060707092285\n",
      "Total loss 4.794060707092285\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.034554958343506\n",
      "Total loss 5.034554958343506\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.784954071044922\n",
      "Total loss 4.784954071044922\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.097146511077881\n",
      "Total loss 4.097146511077881\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.231370687484741\n",
      "Total loss 3.231370687484741\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.339999675750732\n",
      "Total loss 4.339999675750732\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 4.520898818969727\n",
      "Total loss 4.520898818969727\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.4985158443450928\n",
      "Total loss 3.4985158443450928\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.1874516010284424\n",
      "Total loss 3.1874516010284424\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 3.3217601776123047\n",
      "Total loss 3.3217601776123047\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.829915761947632\n",
      "Total loss 2.829915761947632\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.62929368019104\n",
      "Total loss 2.62929368019104\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.5864219665527344\n",
      "Total loss 2.5864219665527344\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.1597564220428467\n",
      "Total loss 2.1597564220428467\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.9433313608169556\n",
      "Total loss 1.9433313608169556\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.217522144317627\n",
      "Total loss 2.217522144317627\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.4316718578338623\n",
      "Total loss 2.4316718578338623\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.381382465362549\n",
      "Total loss 2.381382465362549\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.324071168899536\n",
      "Total loss 2.324071168899536\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.1675398349761963\n",
      "Total loss 2.1675398349761963\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.9924297332763672\n",
      "Total loss 1.9924297332763672\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.891977071762085\n",
      "Total loss 1.891977071762085\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.9376109838485718\n",
      "Total loss 1.9376109838485718\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.912927269935608\n",
      "Total loss 1.912927269935608\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.9339025020599365\n",
      "Total loss 1.9339025020599365\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.0158424377441406\n",
      "Total loss 2.0158424377441406\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 2.0092005729675293\n",
      "Total loss 2.0092005729675293\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.860809087753296\n",
      "Total loss 1.860809087753296\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.7687166929244995\n",
      "Total loss 1.7687166929244995\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.7672367095947266\n",
      "Total loss 1.7672367095947266\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.7984066009521484\n",
      "Total loss 1.7984066009521484\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.843557357788086\n",
      "Total loss 1.843557357788086\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.8503211736679077\n",
      "Total loss 1.8503211736679077\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.8316630125045776\n",
      "Total loss 1.8316630125045776\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.785834550857544\n",
      "Total loss 1.785834550857544\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.7426214218139648\n",
      "Total loss 1.7426214218139648\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.7021642923355103\n",
      "Total loss 1.7021642923355103\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.7187398672103882\n",
      "Total loss 1.7187398672103882\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.7278223037719727\n",
      "Total loss 1.7278223037719727\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.7451560497283936\n",
      "Total loss 1.7451560497283936\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.734289288520813\n",
      "Total loss 1.734289288520813\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.7076332569122314\n",
      "Total loss 1.7076332569122314\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.67940354347229\n",
      "Total loss 1.67940354347229\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.6794356107711792\n",
      "Total loss 1.6794356107711792\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.6177834272384644\n",
      "Total loss 1.6177834272384644\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.6766892671585083\n",
      "Total loss 1.6766892671585083\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.6614331007003784\n",
      "Total loss 1.6614331007003784\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.606941819190979\n",
      "Total loss 1.606941819190979\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.6190576553344727\n",
      "Total loss 1.6190576553344727\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.588750958442688\n",
      "Total loss 1.588750958442688\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.5774403810501099\n",
      "Total loss 1.5774403810501099\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.5566823482513428\n",
      "Total loss 1.5566823482513428\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.5266896486282349\n",
      "Total loss 1.5266896486282349\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.5113146305084229\n",
      "Total loss 1.5113146305084229\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.5077673196792603\n",
      "Total loss 1.5077673196792603\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.4181243181228638\n",
      "Total loss 1.4181243181228638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:50:43,011 - easyeditor.editors.editor - INFO - 138 editing: What is the position of Daniel Haines? -> member of the Louisiana House of Representatives  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.198205717855648}}, 'case_id': 138, 'requested_rewrite': {'prompt': 'What is the position of Daniel Haines?', 'target_new': 'member of the Louisiana House of Representatives', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which state's legislature does Daniel Haines serve as a member of the House of Representatives?\"], 'ground_truth': ['Louisiana']}}, 'locality': {'Relation_Specificity': {'prompt': ['The member of political party of Daniel Haines is', 'Daniel Haines member of political party'], 'ground_truth': ['Democratic Party', 'Democratic Party']}}, 'subject': 'Daniel Haines'}, 'post': {'rewrite_acc': [0.42857142857142855], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.430435884953466}}}\n",
      "07/22/2024 12:50:43 - INFO - easyeditor.editors.editor -   138 editing: What is the position of Daniel Haines? -> member of the Louisiana House of Representatives  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.198205717855648}}, 'case_id': 138, 'requested_rewrite': {'prompt': 'What is the position of Daniel Haines?', 'target_new': 'member of the Louisiana House of Representatives', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which state's legislature does Daniel Haines serve as a member of the House of Representatives?\"], 'ground_truth': ['Louisiana']}}, 'locality': {'Relation_Specificity': {'prompt': ['The member of political party of Daniel Haines is', 'Daniel Haines member of political party'], 'ground_truth': ['Democratic Party', 'Democratic Party']}}, 'subject': 'Daniel Haines'}, 'post': {'rewrite_acc': [0.42857142857142855], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.430435884953466}}}\n",
      " 43%|████▎     | 139/326 [53:55<1:05:08, 20.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was Nicolas De Santis's father?] -> [José De Santis]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.0774359703063965\n",
      "Total loss 3.0774359703063965\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6424787044525146\n",
      "Total loss 0.6424787044525146\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 13.558307647705078\n",
      "Total loss 13.558307647705078\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.212345123291016\n",
      "Total loss 11.212345123291016\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 20.475574493408203\n",
      "Total loss 20.475574493408203\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.424206733703613\n",
      "Total loss 13.424206733703613\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.7235689163208\n",
      "Total loss 10.7235689163208\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 13.366419792175293\n",
      "Total loss 13.366419792175293\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.4827494621276855\n",
      "Total loss 6.4827494621276855\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.644409656524658\n",
      "Total loss 4.644409656524658\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.668952703475952\n",
      "Total loss 3.668952703475952\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.596492052078247\n",
      "Total loss 2.596492052078247\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.098597764968872\n",
      "Total loss 2.098597764968872\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.6263821125030518\n",
      "Total loss 1.6263821125030518\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.8351147174835205\n",
      "Total loss 1.8351147174835205\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.9622905254364014\n",
      "Total loss 1.9622905254364014\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.4427266120910645\n",
      "Total loss 1.4427266120910645\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.536151647567749\n",
      "Total loss 1.536151647567749\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6715439558029175\n",
      "Total loss 1.6715439558029175\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.5783519744873047\n",
      "Total loss 1.5783519744873047\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.390371561050415\n",
      "Total loss 1.390371561050415\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.4162722826004028\n",
      "Total loss 1.4162722826004028\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5298357009887695\n",
      "Total loss 1.5298357009887695\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4522981643676758\n",
      "Total loss 1.4522981643676758\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.3462733030319214\n",
      "Total loss 1.3462733030319214\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4257291555404663\n",
      "Total loss 1.4257291555404663\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4216326475143433\n",
      "Total loss 1.4216326475143433\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.410336971282959\n",
      "Total loss 1.410336971282959\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3955763578414917\n",
      "Total loss 1.3955763578414917\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3589625358581543\n",
      "Total loss 1.3589625358581543\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3394322395324707\n",
      "Total loss 1.3394322395324707\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.3157360553741455\n",
      "Total loss 1.3157360553741455\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.293197751045227\n",
      "Total loss 1.293197751045227\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2943897247314453\n",
      "Total loss 1.2943897247314453\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.2955098152160645\n",
      "Total loss 1.2955098152160645\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.2732502222061157\n",
      "Total loss 1.2732502222061157\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.2286795377731323\n",
      "Total loss 1.2286795377731323\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.2144551277160645\n",
      "Total loss 1.2144551277160645\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.166210412979126\n",
      "Total loss 1.166210412979126\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1643710136413574\n",
      "Total loss 1.1643710136413574\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1122254133224487\n",
      "Total loss 1.1122254133224487\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0885469913482666\n",
      "Total loss 1.0885469913482666\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.000604271888733\n",
      "Total loss 1.000604271888733\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.9407892227172852\n",
      "Total loss 0.9407892227172852\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.9002283811569214\n",
      "Total loss 0.9002283811569214\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8509236574172974\n",
      "Total loss 0.8509236574172974\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8045026659965515\n",
      "Total loss 0.8045026659965515\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7304659485816956\n",
      "Total loss 0.7304659485816956\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6633488535881042\n",
      "Total loss 0.6633488535881042\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5393374562263489\n",
      "Total loss 0.5393374562263489\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.44909363985061646\n",
      "Total loss 0.44909363985061646\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.33781343698501587\n",
      "Total loss 0.33781343698501587\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.26345717906951904\n",
      "Total loss 0.26345717906951904\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.14444200694561005\n",
      "Total loss 0.14444200694561005\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8026418089866638\n",
      "Total loss 0.8026418089866638\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.2857059240341187\n",
      "Total loss 1.2857059240341187\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8531796336174011\n",
      "Total loss 0.8531796336174011\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.32744157314300537\n",
      "Total loss 0.32744157314300537\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5242211818695068\n",
      "Total loss 0.5242211818695068\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 3.3145198822021484\n",
      "Total loss 3.3145198822021484\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 2.0695066452026367\n",
      "Total loss 2.0695066452026367\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.8253172039985657\n",
      "Total loss 0.8253172039985657\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.611050546169281\n",
      "Total loss 0.611050546169281\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.7103472352027893\n",
      "Total loss 0.7103472352027893\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5378513932228088\n",
      "Total loss 0.5378513932228088\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.5144607424736023\n",
      "Total loss 0.5144607424736023\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.288966566324234\n",
      "Total loss 0.288966566324234\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.17006239295005798\n",
      "Total loss 0.17006239295005798\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.12174771726131439\n",
      "Total loss 0.12174771726131439\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.08503024280071259\n",
      "Total loss 0.08503024280071259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:51:03,697 - easyeditor.editors.editor - INFO - 139 editing: Who was Nicolas De Santis's father? -> José De Santis  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.0934700701211035}}, 'case_id': 139, 'requested_rewrite': {'prompt': \"Who was Nicolas De Santis's father?\", 'target_new': 'José De Santis', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of José De Santis?'], 'ground_truth': ['Nicolas De Santis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Nicolas De Santis is', 'Nicolas De Santis country of citizenship'], 'ground_truth': ['Spain', 'Spain']}}, 'subject': 'Nicolas De Santis'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 4.591273168319422}}}\n",
      "07/22/2024 12:51:03 - INFO - easyeditor.editors.editor -   139 editing: Who was Nicolas De Santis's father? -> José De Santis  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.0934700701211035}}, 'case_id': 139, 'requested_rewrite': {'prompt': \"Who was Nicolas De Santis's father?\", 'target_new': 'José De Santis', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of José De Santis?'], 'ground_truth': ['Nicolas De Santis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Nicolas De Santis is', 'Nicolas De Santis country of citizenship'], 'ground_truth': ['Spain', 'Spain']}}, 'subject': 'Nicolas De Santis'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 4.591273168319422}}}\n",
      " 43%|████▎     | 140/326 [54:16<1:04:35, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is listed as Mary Benedict Cushing father?] -> [William Cushing, 1st Baron Cushing]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.0223493576049805\n",
      "Total loss 2.0223493576049805\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.4095444679260254\n",
      "Total loss 0.4095444679260254\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.04718685522675514\n",
      "Total loss 0.04718685522675514\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.787961959838867\n",
      "Total loss 9.787961959838867\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.184768676757812\n",
      "Total loss 10.184768676757812\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.071009635925293\n",
      "Total loss 13.071009635925293\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.853326320648193\n",
      "Total loss 6.853326320648193\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.054947853088379\n",
      "Total loss 7.054947853088379\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.759549140930176\n",
      "Total loss 6.759549140930176\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.4735870361328125\n",
      "Total loss 6.4735870361328125\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.495543003082275\n",
      "Total loss 6.495543003082275\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.920590400695801\n",
      "Total loss 5.920590400695801\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.73403263092041\n",
      "Total loss 4.73403263092041\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.749394416809082\n",
      "Total loss 4.749394416809082\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.846827268600464\n",
      "Total loss 3.846827268600464\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.3386588096618652\n",
      "Total loss 3.3386588096618652\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.889775037765503\n",
      "Total loss 2.889775037765503\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.80318546295166\n",
      "Total loss 2.80318546295166\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.1372060775756836\n",
      "Total loss 3.1372060775756836\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.4607460498809814\n",
      "Total loss 2.4607460498809814\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.4713752269744873\n",
      "Total loss 2.4713752269744873\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.449573040008545\n",
      "Total loss 2.449573040008545\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.189303398132324\n",
      "Total loss 2.189303398132324\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.142042636871338\n",
      "Total loss 2.142042636871338\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.331003189086914\n",
      "Total loss 2.331003189086914\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.24497389793396\n",
      "Total loss 2.24497389793396\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.1985373497009277\n",
      "Total loss 2.1985373497009277\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.069013833999634\n",
      "Total loss 2.069013833999634\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.0260562896728516\n",
      "Total loss 2.0260562896728516\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.9873878955841064\n",
      "Total loss 1.9873878955841064\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.9549709558486938\n",
      "Total loss 1.9549709558486938\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.91924250125885\n",
      "Total loss 1.91924250125885\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.860337257385254\n",
      "Total loss 1.860337257385254\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.8899707794189453\n",
      "Total loss 1.8899707794189453\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.7842772006988525\n",
      "Total loss 1.7842772006988525\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.7523739337921143\n",
      "Total loss 1.7523739337921143\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.6958677768707275\n",
      "Total loss 1.6958677768707275\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6533870697021484\n",
      "Total loss 1.6533870697021484\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.5674779415130615\n",
      "Total loss 1.5674779415130615\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.481046199798584\n",
      "Total loss 1.481046199798584\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5357645750045776\n",
      "Total loss 1.5357645750045776\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.328150749206543\n",
      "Total loss 1.328150749206543\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4100741147994995\n",
      "Total loss 1.4100741147994995\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.2778210639953613\n",
      "Total loss 1.2778210639953613\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0690791606903076\n",
      "Total loss 1.0690791606903076\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0132390260696411\n",
      "Total loss 1.0132390260696411\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.9692221879959106\n",
      "Total loss 0.9692221879959106\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7695073485374451\n",
      "Total loss 0.7695073485374451\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6503497362136841\n",
      "Total loss 0.6503497362136841\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5459907650947571\n",
      "Total loss 0.5459907650947571\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.49235764145851135\n",
      "Total loss 0.49235764145851135\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.3509097695350647\n",
      "Total loss 0.3509097695350647\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.2956220507621765\n",
      "Total loss 0.2956220507621765\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.2473105490207672\n",
      "Total loss 0.2473105490207672\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.17711412906646729\n",
      "Total loss 0.17711412906646729\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.15679419040679932\n",
      "Total loss 0.15679419040679932\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.10714785754680634\n",
      "Total loss 0.10714785754680634\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0781835988163948\n",
      "Total loss 0.0781835988163948\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.05410168692469597\n",
      "Total loss 0.05410168692469597\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.049382857978343964\n",
      "Total loss 0.049382857978343964\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0336504802107811\n",
      "Total loss 0.0336504802107811\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.028324594721198082\n",
      "Total loss 0.028324594721198082\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0206100195646286\n",
      "Total loss 0.0206100195646286\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.016244294121861458\n",
      "Total loss 0.016244294121861458\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.011150175705552101\n",
      "Total loss 0.011150175705552101\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.011943967081606388\n",
      "Total loss 0.011943967081606388\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.008521847426891327\n",
      "Total loss 0.008521847426891327\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.007676819805055857\n",
      "Total loss 0.007676819805055857\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00477204006165266\n",
      "Total loss 0.00477204006165266\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.005465423222631216\n",
      "Total loss 0.005465423222631216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:51:24,458 - easyeditor.editors.editor - INFO - 140 editing: Who is listed as Mary Benedict Cushing father? -> William Cushing, 1st Baron Cushing  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.175652648557316}}, 'case_id': 140, 'requested_rewrite': {'prompt': 'Who is listed as Mary Benedict Cushing father?', 'target_new': 'William Cushing, 1st Baron Cushing', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of William Cushing, 1st Baron Cushing?'], 'ground_truth': ['Mary Benedict Cushing']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Mary Benedict Cushing is', 'Mary Benedict Cushing father'], 'ground_truth': ['Harvey Williams Cushing', 'Harvey Williams Cushing']}}, 'subject': 'Mary Benedict Cushing'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.5, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.0561182569374665}}}\n",
      "07/22/2024 12:51:24 - INFO - easyeditor.editors.editor -   140 editing: Who is listed as Mary Benedict Cushing father? -> William Cushing, 1st Baron Cushing  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 6.175652648557316}}, 'case_id': 140, 'requested_rewrite': {'prompt': 'Who is listed as Mary Benedict Cushing father?', 'target_new': 'William Cushing, 1st Baron Cushing', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of William Cushing, 1st Baron Cushing?'], 'ground_truth': ['Mary Benedict Cushing']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Mary Benedict Cushing is', 'Mary Benedict Cushing father'], 'ground_truth': ['Harvey Williams Cushing', 'Harvey Williams Cushing']}}, 'subject': 'Mary Benedict Cushing'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.5, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.0561182569374665}}}\n",
      " 43%|████▎     | 141/326 [54:36<1:04:10, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the mother of Robert Collett?] -> [Helen Collett]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.0640859603881836\n",
      "Total loss 3.0640859603881836\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8337438702583313\n",
      "Total loss 0.8337438702583313\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.878139495849609\n",
      "Total loss 5.878139495849609\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 13.337947845458984\n",
      "Total loss 13.337947845458984\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.028471946716309\n",
      "Total loss 9.028471946716309\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.210143089294434\n",
      "Total loss 9.210143089294434\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 13.494927406311035\n",
      "Total loss 13.494927406311035\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.993349552154541\n",
      "Total loss 6.993349552154541\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.5403008460998535\n",
      "Total loss 6.5403008460998535\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.688673973083496\n",
      "Total loss 8.688673973083496\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 27.03819465637207\n",
      "Total loss 27.03819465637207\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 13.294400215148926\n",
      "Total loss 13.294400215148926\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.943406581878662\n",
      "Total loss 7.943406581878662\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.724915981292725\n",
      "Total loss 6.724915981292725\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 6.167455196380615\n",
      "Total loss 6.167455196380615\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.8203647136688232\n",
      "Total loss 3.8203647136688232\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 24.276277542114258\n",
      "Total loss 24.276277542114258\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.359931945800781\n",
      "Total loss 4.359931945800781\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.5521132946014404\n",
      "Total loss 3.5521132946014404\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.621554136276245\n",
      "Total loss 2.621554136276245\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3137733936309814\n",
      "Total loss 1.3137733936309814\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.8644682168960571\n",
      "Total loss 1.8644682168960571\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.6853948831558228\n",
      "Total loss 1.6853948831558228\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.455945611000061\n",
      "Total loss 1.455945611000061\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2434055805206299\n",
      "Total loss 1.2434055805206299\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.894695520401001\n",
      "Total loss 0.894695520401001\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.040245532989502\n",
      "Total loss 1.040245532989502\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1584757566452026\n",
      "Total loss 1.1584757566452026\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.129592776298523\n",
      "Total loss 1.129592776298523\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9500264525413513\n",
      "Total loss 0.9500264525413513\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7030054926872253\n",
      "Total loss 0.7030054926872253\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8508896231651306\n",
      "Total loss 0.8508896231651306\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.8754162788391113\n",
      "Total loss 0.8754162788391113\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6578443646430969\n",
      "Total loss 0.6578443646430969\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.5457024574279785\n",
      "Total loss 0.5457024574279785\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6026782989501953\n",
      "Total loss 0.6026782989501953\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6274669766426086\n",
      "Total loss 0.6274669766426086\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 22.1357364654541\n",
      "Total loss 22.1357364654541\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.5246732831001282\n",
      "Total loss 0.5246732831001282\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5603951811790466\n",
      "Total loss 0.5603951811790466\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7258105278015137\n",
      "Total loss 0.7258105278015137\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.7414476275444031\n",
      "Total loss 0.7414476275444031\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8925240635871887\n",
      "Total loss 0.8925240635871887\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.837752640247345\n",
      "Total loss 0.837752640247345\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.9735212326049805\n",
      "Total loss 0.9735212326049805\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.9883535504341125\n",
      "Total loss 0.9883535504341125\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.8956241607666016\n",
      "Total loss 0.8956241607666016\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.812669575214386\n",
      "Total loss 0.812669575214386\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7514503598213196\n",
      "Total loss 0.7514503598213196\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6142716407775879\n",
      "Total loss 0.6142716407775879\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.49281445145606995\n",
      "Total loss 0.49281445145606995\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5013108253479004\n",
      "Total loss 0.5013108253479004\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4182097911834717\n",
      "Total loss 0.4182097911834717\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.37497517466545105\n",
      "Total loss 0.37497517466545105\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.3573555052280426\n",
      "Total loss 0.3573555052280426\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.2642509341239929\n",
      "Total loss 0.2642509341239929\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.2578517198562622\n",
      "Total loss 0.2578517198562622\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.211028054356575\n",
      "Total loss 0.211028054356575\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.1736644059419632\n",
      "Total loss 0.1736644059419632\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.16158771514892578\n",
      "Total loss 0.16158771514892578\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.11163664609193802\n",
      "Total loss 0.11163664609193802\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.14158767461776733\n",
      "Total loss 0.14158767461776733\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.05195336416363716\n",
      "Total loss 0.05195336416363716\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.055867403745651245\n",
      "Total loss 0.055867403745651245\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.051859501749277115\n",
      "Total loss 0.051859501749277115\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.02555817924439907\n",
      "Total loss 0.02555817924439907\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.022845765575766563\n",
      "Total loss 0.022845765575766563\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.014880596660077572\n",
      "Total loss 0.014880596660077572\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.012534338049590588\n",
      "Total loss 0.012534338049590588\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0077349781058728695\n",
      "Total loss 0.0077349781058728695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:51:45,246 - easyeditor.editors.editor - INFO - 141 editing: Who was the mother of Robert Collett? -> Helen Collett  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.119150948908485}}, 'case_id': 141, 'requested_rewrite': {'prompt': 'Who was the mother of Robert Collett?', 'target_new': 'Helen Collett', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Helen Collett?'], 'ground_truth': ['Robert Collett']}}, 'locality': {'Relation_Specificity': {'prompt': ['The position held of Robert Collett is', 'Robert Collett position held'], 'ground_truth': ['museum director', 'museum director']}}, 'subject': 'Robert Collett'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 4.703982270130699}}}\n",
      "07/22/2024 12:51:45 - INFO - easyeditor.editors.editor -   141 editing: Who was the mother of Robert Collett? -> Helen Collett  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.119150948908485}}, 'case_id': 141, 'requested_rewrite': {'prompt': 'Who was the mother of Robert Collett?', 'target_new': 'Helen Collett', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Helen Collett?'], 'ground_truth': ['Robert Collett']}}, 'locality': {'Relation_Specificity': {'prompt': ['The position held of Robert Collett is', 'Robert Collett position held'], 'ground_truth': ['museum director', 'museum director']}}, 'subject': 'Robert Collett'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 4.703982270130699}}}\n",
      " 44%|████▎     | 142/326 [54:57<1:03:48, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the status of Cross River gorilla?] -> [near threatened]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.040122032165527\n",
      "Total loss 6.040122032165527\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.4479447305202484\n",
      "Total loss 0.4479447305202484\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 9.776029586791992\n",
      "Total loss 9.776029586791992\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.688081979751587\n",
      "Total loss 3.688081979751587\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.392684459686279\n",
      "Total loss 6.392684459686279\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.906900882720947\n",
      "Total loss 5.906900882720947\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.1362154483795166\n",
      "Total loss 3.1362154483795166\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.645890712738037\n",
      "Total loss 2.645890712738037\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.8835163116455078\n",
      "Total loss 0.8835163116455078\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.046373724937439\n",
      "Total loss 1.046373724937439\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.8506498336791992\n",
      "Total loss 0.8506498336791992\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.9183176755905151\n",
      "Total loss 0.9183176755905151\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.7182968854904175\n",
      "Total loss 0.7182968854904175\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.8847345113754272\n",
      "Total loss 0.8847345113754272\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.7175277471542358\n",
      "Total loss 0.7175277471542358\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.767302393913269\n",
      "Total loss 0.767302393913269\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.7914284467697144\n",
      "Total loss 0.7914284467697144\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6693947911262512\n",
      "Total loss 0.6693947911262512\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.7272387742996216\n",
      "Total loss 0.7272387742996216\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.7477124929428101\n",
      "Total loss 0.7477124929428101\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.6689826250076294\n",
      "Total loss 0.6689826250076294\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6918222904205322\n",
      "Total loss 0.6918222904205322\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6690468788146973\n",
      "Total loss 0.6690468788146973\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6725223660469055\n",
      "Total loss 0.6725223660469055\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.7165213823318481\n",
      "Total loss 0.7165213823318481\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7299548983573914\n",
      "Total loss 0.7299548983573914\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6724252700805664\n",
      "Total loss 0.6724252700805664\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6718918085098267\n",
      "Total loss 0.6718918085098267\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.6906080842018127\n",
      "Total loss 0.6906080842018127\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6713647842407227\n",
      "Total loss 0.6713647842407227\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.6675591468811035\n",
      "Total loss 0.6675591468811035\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6713448762893677\n",
      "Total loss 0.6713448762893677\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.6789553165435791\n",
      "Total loss 0.6789553165435791\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6367957592010498\n",
      "Total loss 0.6367957592010498\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.644219160079956\n",
      "Total loss 0.644219160079956\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6702802777290344\n",
      "Total loss 0.6702802777290344\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6662672162055969\n",
      "Total loss 0.6662672162055969\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6437563300132751\n",
      "Total loss 0.6437563300132751\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6435145139694214\n",
      "Total loss 0.6435145139694214\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6374218463897705\n",
      "Total loss 0.6374218463897705\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.6430720686912537\n",
      "Total loss 0.6430720686912537\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6371957063674927\n",
      "Total loss 0.6371957063674927\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6371232271194458\n",
      "Total loss 0.6371232271194458\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6427867412567139\n",
      "Total loss 0.6427867412567139\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6652323603630066\n",
      "Total loss 0.6652323603630066\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6367009878158569\n",
      "Total loss 0.6367009878158569\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6103147268295288\n",
      "Total loss 0.6103147268295288\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6366145610809326\n",
      "Total loss 0.6366145610809326\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6063600778579712\n",
      "Total loss 0.6063600778579712\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6101571917533875\n",
      "Total loss 0.6101571917533875\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.6365174055099487\n",
      "Total loss 0.6365174055099487\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5778594613075256\n",
      "Total loss 0.5778594613075256\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.6060829162597656\n",
      "Total loss 0.6060829162597656\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.5795817375183105\n",
      "Total loss 0.5795817375183105\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.5795354843139648\n",
      "Total loss 0.5795354843139648\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.634204626083374\n",
      "Total loss 0.634204626083374\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.5794458985328674\n",
      "Total loss 0.5794458985328674\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6058562397956848\n",
      "Total loss 0.6058562397956848\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.577488124370575\n",
      "Total loss 0.577488124370575\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.5509961843490601\n",
      "Total loss 0.5509961843490601\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5510163307189941\n",
      "Total loss 0.5510163307189941\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5773102045059204\n",
      "Total loss 0.5773102045059204\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5792169570922852\n",
      "Total loss 0.5792169570922852\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.4998791217803955\n",
      "Total loss 0.4998791217803955\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5508146286010742\n",
      "Total loss 0.5508146286010742\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.5318568348884583\n",
      "Total loss 0.5318568348884583\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.499740332365036\n",
      "Total loss 0.499740332365036\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.49967843294143677\n",
      "Total loss 0.49967843294143677\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5033313035964966\n",
      "Total loss 0.5033313035964966\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.5033555626869202\n",
      "Total loss 0.5033555626869202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:52:06,047 - easyeditor.editors.editor - INFO - 142 editing: What is the status of Cross River gorilla? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.285570709892475}}, 'case_id': 142, 'requested_rewrite': {'prompt': 'What is the status of Cross River gorilla?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current status of Gorilla gorilla diehli?'], 'ground_truth': ['near threatened']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Cross River gorilla is', 'Cross River gorilla taxon rank'], 'ground_truth': ['subspecies', 'subspecies']}}, 'subject': 'Cross River gorilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.81167262597036}}}\n",
      "07/22/2024 12:52:06 - INFO - easyeditor.editors.editor -   142 editing: What is the status of Cross River gorilla? -> near threatened  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.285570709892475}}, 'case_id': 142, 'requested_rewrite': {'prompt': 'What is the status of Cross River gorilla?', 'target_new': 'near threatened', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current status of Gorilla gorilla diehli?'], 'ground_truth': ['near threatened']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Cross River gorilla is', 'Cross River gorilla taxon rank'], 'ground_truth': ['subspecies', 'subspecies']}}, 'subject': 'Cross River gorilla'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.81167262597036}}}\n",
      " 44%|████▍     | 143/326 [55:18<1:03:27, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What war or battle involved Field Harris?] -> [American Revolutionary War]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.056638717651367\n",
      "Total loss 3.056638717651367\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.38655224442481995\n",
      "Total loss 0.38655224442481995\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.057501316070557\n",
      "Total loss 5.057501316070557\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.204682350158691\n",
      "Total loss 11.204682350158691\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 14.471213340759277\n",
      "Total loss 14.471213340759277\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.746020317077637\n",
      "Total loss 8.746020317077637\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 17.43684959411621\n",
      "Total loss 17.43684959411621\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 16.061304092407227\n",
      "Total loss 16.061304092407227\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 9.802141189575195\n",
      "Total loss 9.802141189575195\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.115626811981201\n",
      "Total loss 5.115626811981201\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.166155815124512\n",
      "Total loss 10.166155815124512\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 10.07649040222168\n",
      "Total loss 10.07649040222168\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.460576057434082\n",
      "Total loss 8.460576057434082\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.602586269378662\n",
      "Total loss 5.602586269378662\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.956721305847168\n",
      "Total loss 3.956721305847168\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.543861150741577\n",
      "Total loss 2.543861150741577\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.2325851917266846\n",
      "Total loss 3.2325851917266846\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.969836950302124\n",
      "Total loss 2.969836950302124\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.9463499784469604\n",
      "Total loss 1.9463499784469604\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.773891806602478\n",
      "Total loss 1.773891806602478\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.2376103401184082\n",
      "Total loss 1.2376103401184082\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.223897933959961\n",
      "Total loss 1.223897933959961\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.435940146446228\n",
      "Total loss 1.435940146446228\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.3937934637069702\n",
      "Total loss 1.3937934637069702\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0747356414794922\n",
      "Total loss 1.0747356414794922\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.075732946395874\n",
      "Total loss 1.075732946395874\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2111601829528809\n",
      "Total loss 1.2111601829528809\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.0542083978652954\n",
      "Total loss 1.0542083978652954\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.041002631187439\n",
      "Total loss 1.041002631187439\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0364817380905151\n",
      "Total loss 1.0364817380905151\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 3.8067100048065186\n",
      "Total loss 3.8067100048065186\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1826815605163574\n",
      "Total loss 1.1826815605163574\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.9694474935531616\n",
      "Total loss 1.9694474935531616\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.564847707748413\n",
      "Total loss 2.564847707748413\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.196694016456604\n",
      "Total loss 1.196694016456604\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1138638257980347\n",
      "Total loss 1.1138638257980347\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.1445283889770508\n",
      "Total loss 1.1445283889770508\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.098734736442566\n",
      "Total loss 1.098734736442566\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1315768957138062\n",
      "Total loss 1.1315768957138062\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.1615079641342163\n",
      "Total loss 1.1615079641342163\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1330856084823608\n",
      "Total loss 1.1330856084823608\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0866801738739014\n",
      "Total loss 1.0866801738739014\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0496877431869507\n",
      "Total loss 1.0496877431869507\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0275863409042358\n",
      "Total loss 1.0275863409042358\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.041061282157898\n",
      "Total loss 1.041061282157898\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0262733697891235\n",
      "Total loss 1.0262733697891235\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0280489921569824\n",
      "Total loss 1.0280489921569824\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9646823406219482\n",
      "Total loss 0.9646823406219482\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.9923021793365479\n",
      "Total loss 0.9923021793365479\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.9514085650444031\n",
      "Total loss 0.9514085650444031\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9192357063293457\n",
      "Total loss 0.9192357063293457\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.8772807121276855\n",
      "Total loss 0.8772807121276855\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.8628341555595398\n",
      "Total loss 0.8628341555595398\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.8756992816925049\n",
      "Total loss 0.8756992816925049\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.7907634377479553\n",
      "Total loss 0.7907634377479553\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.7909600138664246\n",
      "Total loss 0.7909600138664246\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.7546238303184509\n",
      "Total loss 0.7546238303184509\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.6930083632469177\n",
      "Total loss 0.6930083632469177\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.667762815952301\n",
      "Total loss 0.667762815952301\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6561946272850037\n",
      "Total loss 0.6561946272850037\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5778346061706543\n",
      "Total loss 0.5778346061706543\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5430635809898376\n",
      "Total loss 0.5430635809898376\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.46092113852500916\n",
      "Total loss 0.46092113852500916\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.4198291301727295\n",
      "Total loss 0.4198291301727295\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.355823278427124\n",
      "Total loss 0.355823278427124\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.290285587310791\n",
      "Total loss 0.290285587310791\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.2559846043586731\n",
      "Total loss 0.2559846043586731\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.19058501720428467\n",
      "Total loss 0.19058501720428467\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.10244899988174438\n",
      "Total loss 0.10244899988174438\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.10514452308416367\n",
      "Total loss 0.10514452308416367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:52:26,874 - easyeditor.editors.editor - INFO - 143 editing: What war or battle involved Field Harris? -> American Revolutionary War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.238439355985489}}, 'case_id': 143, 'requested_rewrite': {'prompt': 'What war or battle involved Field Harris?', 'target_new': 'American Revolutionary War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who was the main opposing force in the war that Field Harris was involved in?'], 'ground_truth': ['Great Britain']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Field Harris is', 'Field Harris place of birth'], 'ground_truth': ['Versailles', 'Versailles']}}, 'subject': 'Field Harris'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.2481010893575393}}}\n",
      "07/22/2024 12:52:26 - INFO - easyeditor.editors.editor -   143 editing: What war or battle involved Field Harris? -> American Revolutionary War  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.238439355985489}}, 'case_id': 143, 'requested_rewrite': {'prompt': 'What war or battle involved Field Harris?', 'target_new': 'American Revolutionary War', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who was the main opposing force in the war that Field Harris was involved in?'], 'ground_truth': ['Great Britain']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Field Harris is', 'Field Harris place of birth'], 'ground_truth': ['Versailles', 'Versailles']}}, 'subject': 'Field Harris'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.2481010893575393}}}\n",
      " 44%|████▍     | 144/326 [55:39<1:03:07, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The director of Muddu Bidda is who?] -> [P Bhaskaran]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.7867112159729\n",
      "Total loss 5.7867112159729\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.7935402393341064\n",
      "Total loss 2.7935402393341064\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.26112014055252075\n",
      "Total loss 0.26112014055252075\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.558754920959473\n",
      "Total loss 8.558754920959473\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 18.442861557006836\n",
      "Total loss 18.442861557006836\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.3047685623168945\n",
      "Total loss 7.3047685623168945\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 12.557378768920898\n",
      "Total loss 12.557378768920898\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 12.49638843536377\n",
      "Total loss 12.49638843536377\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 12.298526763916016\n",
      "Total loss 12.298526763916016\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 12.022942543029785\n",
      "Total loss 12.022942543029785\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 11.638936996459961\n",
      "Total loss 11.638936996459961\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 11.070086479187012\n",
      "Total loss 11.070086479187012\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 10.243526458740234\n",
      "Total loss 10.243526458740234\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 9.080230712890625\n",
      "Total loss 9.080230712890625\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 7.380090713500977\n",
      "Total loss 7.380090713500977\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.615401268005371\n",
      "Total loss 5.615401268005371\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.748220443725586\n",
      "Total loss 4.748220443725586\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.569918394088745\n",
      "Total loss 3.569918394088745\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6781115531921387\n",
      "Total loss 1.6781115531921387\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 6.313302040100098\n",
      "Total loss 6.313302040100098\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9863789081573486\n",
      "Total loss 1.9863789081573486\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.9776763916015625\n",
      "Total loss 2.9776763916015625\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.9451253414154053\n",
      "Total loss 2.9451253414154053\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.5879273414611816\n",
      "Total loss 2.5879273414611816\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.1630477905273438\n",
      "Total loss 2.1630477905273438\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.6646836996078491\n",
      "Total loss 1.6646836996078491\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.4241011142730713\n",
      "Total loss 2.4241011142730713\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.461656928062439\n",
      "Total loss 1.461656928062439\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7567222118377686\n",
      "Total loss 1.7567222118377686\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.9799296855926514\n",
      "Total loss 1.9799296855926514\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.8977572917938232\n",
      "Total loss 1.8977572917938232\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.5232362747192383\n",
      "Total loss 1.5232362747192383\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.3360320329666138\n",
      "Total loss 1.3360320329666138\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.7242645025253296\n",
      "Total loss 1.7242645025253296\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.4866403341293335\n",
      "Total loss 1.4866403341293335\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3242087364196777\n",
      "Total loss 1.3242087364196777\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.4190443754196167\n",
      "Total loss 1.4190443754196167\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.4536263942718506\n",
      "Total loss 1.4536263942718506\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.426516056060791\n",
      "Total loss 1.426516056060791\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2027814388275146\n",
      "Total loss 1.2027814388275146\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3398150205612183\n",
      "Total loss 1.3398150205612183\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2541178464889526\n",
      "Total loss 1.2541178464889526\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.1057013273239136\n",
      "Total loss 1.1057013273239136\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.089321494102478\n",
      "Total loss 1.089321494102478\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1274563074111938\n",
      "Total loss 1.1274563074111938\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.9937999844551086\n",
      "Total loss 0.9937999844551086\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.9553889036178589\n",
      "Total loss 0.9553889036178589\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8096780776977539\n",
      "Total loss 0.8096780776977539\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.8006978631019592\n",
      "Total loss 0.8006978631019592\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6658188104629517\n",
      "Total loss 0.6658188104629517\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5808305144309998\n",
      "Total loss 0.5808305144309998\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5128600001335144\n",
      "Total loss 0.5128600001335144\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4298144578933716\n",
      "Total loss 0.4298144578933716\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.3545728027820587\n",
      "Total loss 0.3545728027820587\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.27838414907455444\n",
      "Total loss 0.27838414907455444\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.23535266518592834\n",
      "Total loss 0.23535266518592834\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.17633065581321716\n",
      "Total loss 0.17633065581321716\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.10807565599679947\n",
      "Total loss 0.10807565599679947\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.09045103192329407\n",
      "Total loss 0.09045103192329407\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0641249418258667\n",
      "Total loss 0.0641249418258667\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0343342199921608\n",
      "Total loss 0.0343342199921608\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.02202617935836315\n",
      "Total loss 0.02202617935836315\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.008216277696192265\n",
      "Total loss 0.008216277696192265\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.014367853291332722\n",
      "Total loss 0.014367853291332722\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.004007159732282162\n",
      "Total loss 0.004007159732282162\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.004565551411360502\n",
      "Total loss 0.004565551411360502\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0027055912651121616\n",
      "Total loss 0.0027055912651121616\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.5051866173744202\n",
      "Total loss 0.5051866173744202\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.3871246576309204\n",
      "Total loss 1.3871246576309204\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.0319159030914307\n",
      "Total loss 1.0319159030914307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:52:47,472 - easyeditor.editors.editor - INFO - 144 editing: The director of Muddu Bidda is who? -> P Bhaskaran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.872942318934484}}, 'case_id': 144, 'requested_rewrite': {'prompt': 'The director of Muddu Bidda is who?', 'target_new': 'P Bhaskaran', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Along with directing Muddu Bidda, what else was P Bhaskaran known for?'], 'ground_truth': ['Malayalam poetry']}}, 'locality': {'Relation_Specificity': {'prompt': ['The cast member of Muddu Bidda is', 'Muddu Bidda cast member'], 'ground_truth': ['Ramana Reddy', 'Ramana Reddy']}}, 'subject': 'Muddu Bidda'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.952121932308929}}}\n",
      "07/22/2024 12:52:47 - INFO - easyeditor.editors.editor -   144 editing: The director of Muddu Bidda is who? -> P Bhaskaran  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.872942318934484}}, 'case_id': 144, 'requested_rewrite': {'prompt': 'The director of Muddu Bidda is who?', 'target_new': 'P Bhaskaran', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Along with directing Muddu Bidda, what else was P Bhaskaran known for?'], 'ground_truth': ['Malayalam poetry']}}, 'locality': {'Relation_Specificity': {'prompt': ['The cast member of Muddu Bidda is', 'Muddu Bidda cast member'], 'ground_truth': ['Ramana Reddy', 'Ramana Reddy']}}, 'subject': 'Muddu Bidda'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.952121932308929}}}\n",
      " 44%|████▍     | 145/326 [55:59<1:02:35, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who are the cast members of Cherry Tree Lane?] -> [Linda Darnell]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.943623065948486\n",
      "Total loss 4.943623065948486\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.8387625217437744\n",
      "Total loss 1.8387625217437744\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.9051138162612915\n",
      "Total loss 0.9051138162612915\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.416248083114624\n",
      "Total loss 2.416248083114624\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 13.589534759521484\n",
      "Total loss 13.589534759521484\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.995439529418945\n",
      "Total loss 10.995439529418945\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.277429580688477\n",
      "Total loss 4.277429580688477\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.845170259475708\n",
      "Total loss 3.845170259475708\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.323156356811523\n",
      "Total loss 4.323156356811523\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.021570205688477\n",
      "Total loss 4.021570205688477\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.7610232830047607\n",
      "Total loss 3.7610232830047607\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.450089693069458\n",
      "Total loss 3.450089693069458\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.9362828731536865\n",
      "Total loss 2.9362828731536865\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.159062385559082\n",
      "Total loss 2.159062385559082\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.3574542999267578\n",
      "Total loss 1.3574542999267578\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.6586194038391113\n",
      "Total loss 1.6586194038391113\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.9814224243164062\n",
      "Total loss 1.9814224243164062\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.2044739723205566\n",
      "Total loss 1.2044739723205566\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0068438053131104\n",
      "Total loss 1.0068438053131104\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.8851205110549927\n",
      "Total loss 0.8851205110549927\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.9064452052116394\n",
      "Total loss 0.9064452052116394\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7462074756622314\n",
      "Total loss 0.7462074756622314\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6479969024658203\n",
      "Total loss 0.6479969024658203\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.5914602279663086\n",
      "Total loss 0.5914602279663086\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.5654482841491699\n",
      "Total loss 0.5654482841491699\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.5113565921783447\n",
      "Total loss 0.5113565921783447\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.39695674180984497\n",
      "Total loss 0.39695674180984497\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.3888472318649292\n",
      "Total loss 0.3888472318649292\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.3564841151237488\n",
      "Total loss 0.3564841151237488\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.3686695992946625\n",
      "Total loss 0.3686695992946625\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.2758481800556183\n",
      "Total loss 0.2758481800556183\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.25912585854530334\n",
      "Total loss 0.25912585854530334\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.23822450637817383\n",
      "Total loss 0.23822450637817383\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.18678225576877594\n",
      "Total loss 0.18678225576877594\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.15402773022651672\n",
      "Total loss 0.15402773022651672\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.1416248083114624\n",
      "Total loss 0.1416248083114624\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.12392786145210266\n",
      "Total loss 0.12392786145210266\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.13226264715194702\n",
      "Total loss 0.13226264715194702\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0945642814040184\n",
      "Total loss 0.0945642814040184\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.1424158215522766\n",
      "Total loss 0.1424158215522766\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.07149924337863922\n",
      "Total loss 0.07149924337863922\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 2.3613808155059814\n",
      "Total loss 2.3613808155059814\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.7612217664718628\n",
      "Total loss 1.7612217664718628\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7571632862091064\n",
      "Total loss 0.7571632862091064\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6398991346359253\n",
      "Total loss 0.6398991346359253\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1567699909210205\n",
      "Total loss 1.1567699909210205\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3087623119354248\n",
      "Total loss 1.3087623119354248\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9348769783973694\n",
      "Total loss 0.9348769783973694\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.4867649972438812\n",
      "Total loss 0.4867649972438812\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.49540287256240845\n",
      "Total loss 0.49540287256240845\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7681193351745605\n",
      "Total loss 0.7681193351745605\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.8252009153366089\n",
      "Total loss 0.8252009153366089\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5849997401237488\n",
      "Total loss 0.5849997401237488\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.35691267251968384\n",
      "Total loss 0.35691267251968384\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.4425855875015259\n",
      "Total loss 0.4425855875015259\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.5647211074829102\n",
      "Total loss 0.5647211074829102\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.5951962471008301\n",
      "Total loss 0.5951962471008301\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.4387989640235901\n",
      "Total loss 0.4387989640235901\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 3.738048553466797\n",
      "Total loss 3.738048553466797\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.6267536282539368\n",
      "Total loss 0.6267536282539368\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.6735631227493286\n",
      "Total loss 0.6735631227493286\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5456924438476562\n",
      "Total loss 0.5456924438476562\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0722150802612305\n",
      "Total loss 1.0722150802612305\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5839196443557739\n",
      "Total loss 0.5839196443557739\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5199741721153259\n",
      "Total loss 0.5199741721153259\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.43115535378456116\n",
      "Total loss 0.43115535378456116\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.35076606273651123\n",
      "Total loss 0.35076606273651123\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.29852694272994995\n",
      "Total loss 0.29852694272994995\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.3640648126602173\n",
      "Total loss 0.3640648126602173\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.708669900894165\n",
      "Total loss 0.708669900894165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:53:08,212 - easyeditor.editors.editor - INFO - 145 editing: Who are the cast members of Cherry Tree Lane? -> Linda Darnell  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.124048600226291}}, 'case_id': 145, 'requested_rewrite': {'prompt': 'Who are the cast members of Cherry Tree Lane?', 'target_new': 'Linda Darnell', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which other movie did Cherry Tree Lane cast member Linda Darnell play a role?'], 'ground_truth': ['My Darling Clementine']}}, 'locality': {'Relation_Specificity': {'prompt': ['The cast member of Cherry Tree Lane is', 'Cherry Tree Lane cast member'], 'ground_truth': ['Jennie Jacques', 'Jennie Jacques']}}, 'subject': 'Cherry Tree Lane'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.220592699257329}}}\n",
      "07/22/2024 12:53:08 - INFO - easyeditor.editors.editor -   145 editing: Who are the cast members of Cherry Tree Lane? -> Linda Darnell  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.124048600226291}}, 'case_id': 145, 'requested_rewrite': {'prompt': 'Who are the cast members of Cherry Tree Lane?', 'target_new': 'Linda Darnell', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which other movie did Cherry Tree Lane cast member Linda Darnell play a role?'], 'ground_truth': ['My Darling Clementine']}}, 'locality': {'Relation_Specificity': {'prompt': ['The cast member of Cherry Tree Lane is', 'Cherry Tree Lane cast member'], 'ground_truth': ['Jennie Jacques', 'Jennie Jacques']}}, 'subject': 'Cherry Tree Lane'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.220592699257329}}}\n",
      " 45%|████▍     | 146/326 [56:20<1:02:14, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what year did Bedford Vehicles originate?] -> [1991]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.5616958141326904\n",
      "Total loss 3.5616958141326904\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.616212248802185\n",
      "Total loss 1.616212248802185\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.5360278487205505\n",
      "Total loss 0.5360278487205505\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.062847137451172\n",
      "Total loss 8.062847137451172\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 7.230191707611084\n",
      "Total loss 7.230191707611084\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 23.53249168395996\n",
      "Total loss 23.53249168395996\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.045848846435547\n",
      "Total loss 9.045848846435547\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.50233268737793\n",
      "Total loss 8.50233268737793\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.881294250488281\n",
      "Total loss 6.881294250488281\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.796047210693359\n",
      "Total loss 6.796047210693359\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.670045852661133\n",
      "Total loss 7.670045852661133\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.076488971710205\n",
      "Total loss 7.076488971710205\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.174243450164795\n",
      "Total loss 6.174243450164795\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.271599292755127\n",
      "Total loss 5.271599292755127\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.104333400726318\n",
      "Total loss 4.104333400726318\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.689412832260132\n",
      "Total loss 2.689412832260132\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.146336793899536\n",
      "Total loss 2.146336793899536\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.2742037773132324\n",
      "Total loss 2.2742037773132324\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.9299684762954712\n",
      "Total loss 1.9299684762954712\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.8680671453475952\n",
      "Total loss 1.8680671453475952\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.360212802886963\n",
      "Total loss 2.360212802886963\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.549224853515625\n",
      "Total loss 2.549224853515625\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.328355550765991\n",
      "Total loss 2.328355550765991\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.8413649797439575\n",
      "Total loss 1.8413649797439575\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.420150876045227\n",
      "Total loss 1.420150876045227\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2548446655273438\n",
      "Total loss 1.2548446655273438\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2833404541015625\n",
      "Total loss 1.2833404541015625\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3204816579818726\n",
      "Total loss 1.3204816579818726\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.348028540611267\n",
      "Total loss 1.348028540611267\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.396466851234436\n",
      "Total loss 1.396466851234436\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4267889261245728\n",
      "Total loss 1.4267889261245728\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.280901312828064\n",
      "Total loss 1.280901312828064\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1414475440979004\n",
      "Total loss 1.1414475440979004\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9600198864936829\n",
      "Total loss 0.9600198864936829\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.9564099311828613\n",
      "Total loss 0.9564099311828613\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9864199757575989\n",
      "Total loss 0.9864199757575989\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9675437808036804\n",
      "Total loss 0.9675437808036804\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9940784573554993\n",
      "Total loss 0.9940784573554993\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.001276969909668\n",
      "Total loss 1.001276969909668\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.9071075916290283\n",
      "Total loss 0.9071075916290283\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.8282101154327393\n",
      "Total loss 0.8282101154327393\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.7545537352561951\n",
      "Total loss 0.7545537352561951\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6925568580627441\n",
      "Total loss 0.6925568580627441\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7121593356132507\n",
      "Total loss 0.7121593356132507\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7260150909423828\n",
      "Total loss 0.7260150909423828\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6922662258148193\n",
      "Total loss 0.6922662258148193\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6490620970726013\n",
      "Total loss 0.6490620970726013\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.5805277824401855\n",
      "Total loss 0.5805277824401855\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.5645623803138733\n",
      "Total loss 0.5645623803138733\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5168232917785645\n",
      "Total loss 0.5168232917785645\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5056924223899841\n",
      "Total loss 0.5056924223899841\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.48029258847236633\n",
      "Total loss 0.48029258847236633\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.47122275829315186\n",
      "Total loss 0.47122275829315186\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.4345218241214752\n",
      "Total loss 0.4345218241214752\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.40687188506126404\n",
      "Total loss 0.40687188506126404\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.3744169771671295\n",
      "Total loss 0.3744169771671295\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.35247135162353516\n",
      "Total loss 0.35247135162353516\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.32650840282440186\n",
      "Total loss 0.32650840282440186\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.27705320715904236\n",
      "Total loss 0.27705320715904236\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.27348092198371887\n",
      "Total loss 0.27348092198371887\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.23324990272521973\n",
      "Total loss 0.23324990272521973\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.19595251977443695\n",
      "Total loss 0.19595251977443695\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.17794819176197052\n",
      "Total loss 0.17794819176197052\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.19455121457576752\n",
      "Total loss 0.19455121457576752\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.12175047397613525\n",
      "Total loss 0.12175047397613525\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.10802678018808365\n",
      "Total loss 0.10802678018808365\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.08181315660476685\n",
      "Total loss 0.08181315660476685\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0741734579205513\n",
      "Total loss 0.0741734579205513\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.05843490734696388\n",
      "Total loss 0.05843490734696388\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.04955500364303589\n",
      "Total loss 0.04955500364303589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:53:28,969 - easyeditor.editors.editor - INFO - 146 editing: In what year did Bedford Vehicles originate? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.836060680663256}}, 'case_id': 146, 'requested_rewrite': {'prompt': 'In what year did Bedford Vehicles originate?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which company owned Bedford Vehicles when it originated in 1991?'], 'ground_truth': ['Vauxhall Motors']}}, 'locality': {'Relation_Specificity': {'prompt': ['The owned by of Bedford Vehicles is', 'Bedford Vehicles owned by'], 'ground_truth': ['Vauxhall', 'Vauxhall']}}, 'subject': 'Bedford Vehicles'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      "07/22/2024 12:53:28 - INFO - easyeditor.editors.editor -   146 editing: In what year did Bedford Vehicles originate? -> 1991  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.836060680663256}}, 'case_id': 146, 'requested_rewrite': {'prompt': 'In what year did Bedford Vehicles originate?', 'target_new': '1991', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which company owned Bedford Vehicles when it originated in 1991?'], 'ground_truth': ['Vauxhall Motors']}}, 'locality': {'Relation_Specificity': {'prompt': ['The owned by of Bedford Vehicles is', 'Bedford Vehicles owned by'], 'ground_truth': ['Vauxhall', 'Vauxhall']}}, 'subject': 'Bedford Vehicles'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.8715699480384025}}}\n",
      " 45%|████▌     | 147/326 [56:41<1:01:54, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The father of Daniel Pipes is whom?] -> [James Pipes]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.683669090270996\n",
      "Total loss 4.683669090270996\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.4082913398742676\n",
      "Total loss 2.4082913398742676\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.7422401905059814\n",
      "Total loss 2.7422401905059814\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.562692165374756\n",
      "Total loss 4.562692165374756\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 20.91538429260254\n",
      "Total loss 20.91538429260254\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.461879730224609\n",
      "Total loss 6.461879730224609\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.974022388458252\n",
      "Total loss 7.974022388458252\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.353676795959473\n",
      "Total loss 11.353676795959473\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 10.294721603393555\n",
      "Total loss 10.294721603393555\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.110414505004883\n",
      "Total loss 8.110414505004883\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.7504730224609375\n",
      "Total loss 4.7504730224609375\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.0573129653930664\n",
      "Total loss 3.0573129653930664\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.059858798980713\n",
      "Total loss 1.059858798980713\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.788266658782959\n",
      "Total loss 0.788266658782959\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.7329102754592896\n",
      "Total loss 0.7329102754592896\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.7339081168174744\n",
      "Total loss 0.7339081168174744\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.6747415065765381\n",
      "Total loss 0.6747415065765381\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6732118725776672\n",
      "Total loss 0.6732118725776672\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.6794238686561584\n",
      "Total loss 0.6794238686561584\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 49.430843353271484\n",
      "Total loss 49.430843353271484\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.6844512224197388\n",
      "Total loss 0.6844512224197388\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6094868183135986\n",
      "Total loss 0.6094868183135986\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6366205215454102\n",
      "Total loss 0.6366205215454102\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.16728197038173676\n",
      "Total loss 0.16728197038173676\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 22.988052368164062\n",
      "Total loss 22.988052368164062\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0045247203670442104\n",
      "Total loss 0.0045247203670442104\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.8870270252227783\n",
      "Total loss 2.8870270252227783\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.8063945770263672\n",
      "Total loss 0.8063945770263672\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.4252194166183472\n",
      "Total loss 1.4252194166183472\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.2744755148887634\n",
      "Total loss 0.2744755148887634\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.49990662932395935\n",
      "Total loss 0.49990662932395935\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.30402952432632446\n",
      "Total loss 0.30402952432632446\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.181667298078537\n",
      "Total loss 0.181667298078537\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.12919846177101135\n",
      "Total loss 0.12919846177101135\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.12816716730594635\n",
      "Total loss 0.12816716730594635\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.03230980038642883\n",
      "Total loss 0.03230980038642883\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.007088320329785347\n",
      "Total loss 0.007088320329785347\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.00428617000579834\n",
      "Total loss 0.00428617000579834\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0018078809371218085\n",
      "Total loss 0.0018078809371218085\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.001181584084406495\n",
      "Total loss 0.001181584084406495\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0011068836320191622\n",
      "Total loss 0.0011068836320191622\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0009572991402819753\n",
      "Total loss 0.0009572991402819753\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0006675268523395061\n",
      "Total loss 0.0006675268523395061\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0006381414132192731\n",
      "Total loss 0.0006381414132192731\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0008312546415254474\n",
      "Total loss 0.0008312546415254474\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0004834772553294897\n",
      "Total loss 0.0004834772553294897\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0005469399620778859\n",
      "Total loss 0.0005469399620778859\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.00046089128591120243\n",
      "Total loss 0.00046089128591120243\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0005049933679401875\n",
      "Total loss 0.0005049933679401875\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.00042172090616077185\n",
      "Total loss 0.00042172090616077185\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0004189210885670036\n",
      "Total loss 0.0004189210885670036\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00045207643415778875\n",
      "Total loss 0.00045207643415778875\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00037047217483632267\n",
      "Total loss 0.00037047217483632267\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0003701144887600094\n",
      "Total loss 0.0003701144887600094\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0004391413531266153\n",
      "Total loss 0.0004391413531266153\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00040600544889457524\n",
      "Total loss 0.00040600544889457524\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.00031957559986039996\n",
      "Total loss 0.00031957559986039996\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0003514016279950738\n",
      "Total loss 0.0003514016279950738\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00033834960777312517\n",
      "Total loss 0.00033834960777312517\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0003274430346209556\n",
      "Total loss 0.0003274430346209556\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0003524147905409336\n",
      "Total loss 0.0003524147905409336\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.00042090806527994573\n",
      "Total loss 0.00042090806527994573\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00043455592822283506\n",
      "Total loss 0.00043455592822283506\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0003230312722735107\n",
      "Total loss 0.0003230312722735107\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00040564758819527924\n",
      "Total loss 0.00040564758819527924\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00038717425195500255\n",
      "Total loss 0.00038717425195500255\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00039468402974307537\n",
      "Total loss 0.00039468402974307537\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0003954588028136641\n",
      "Total loss 0.0003954588028136641\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0003896148700732738\n",
      "Total loss 0.0003896148700732738\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00031736918026581407\n",
      "Total loss 0.00031736918026581407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:53:49,773 - easyeditor.editors.editor - INFO - 147 editing: The father of Daniel Pipes is whom? -> James Pipes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.278548564273193}}, 'case_id': 147, 'requested_rewrite': {'prompt': 'The father of Daniel Pipes is whom?', 'target_new': 'James Pipes', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of James Pipes?'], 'ground_truth': ['Daniel Pipes']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Daniel Pipes is', 'Daniel Pipes occupation'], 'ground_truth': ['journalist', 'journalist']}}, 'subject': 'Daniel Pipes'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 1.6925971031713474}}}\n",
      "07/22/2024 12:53:49 - INFO - easyeditor.editors.editor -   147 editing: The father of Daniel Pipes is whom? -> James Pipes  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.278548564273193}}, 'case_id': 147, 'requested_rewrite': {'prompt': 'The father of Daniel Pipes is whom?', 'target_new': 'James Pipes', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of James Pipes?'], 'ground_truth': ['Daniel Pipes']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Daniel Pipes is', 'Daniel Pipes occupation'], 'ground_truth': ['journalist', 'journalist']}}, 'subject': 'Daniel Pipes'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 1.6925971031713474}}}\n",
      " 45%|████▌     | 148/326 [57:02<1:01:37, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What college did Tatiana Vladislavovna Petrova go to?] -> [Moscow State Institute of International Relations]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.0694127082824707\n",
      "Total loss 2.0694127082824707\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6310375332832336\n",
      "Total loss 0.6310375332832336\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.21979409456253052\n",
      "Total loss 0.21979409456253052\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.793690204620361\n",
      "Total loss 4.793690204620361\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.974228858947754\n",
      "Total loss 11.974228858947754\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 7.50133752822876\n",
      "Total loss 7.50133752822876\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.050848484039307\n",
      "Total loss 6.050848484039307\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 18.755563735961914\n",
      "Total loss 18.755563735961914\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 13.64197063446045\n",
      "Total loss 13.64197063446045\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.653580665588379\n",
      "Total loss 10.653580665588379\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.325898170471191\n",
      "Total loss 8.325898170471191\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.930994510650635\n",
      "Total loss 6.930994510650635\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.370298385620117\n",
      "Total loss 6.370298385620117\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.769672393798828\n",
      "Total loss 5.769672393798828\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.0080485343933105\n",
      "Total loss 4.0080485343933105\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.6711769104003906\n",
      "Total loss 3.6711769104003906\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.8748044967651367\n",
      "Total loss 3.8748044967651367\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.4567480087280273\n",
      "Total loss 3.4567480087280273\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.564762830734253\n",
      "Total loss 2.564762830734253\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.9950549602508545\n",
      "Total loss 2.9950549602508545\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.036982536315918\n",
      "Total loss 3.036982536315918\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.8825957775115967\n",
      "Total loss 2.8825957775115967\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.901125907897949\n",
      "Total loss 2.901125907897949\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.5604841709136963\n",
      "Total loss 2.5604841709136963\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.6139678955078125\n",
      "Total loss 2.6139678955078125\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.3102829456329346\n",
      "Total loss 2.3102829456329346\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.9759788513183594\n",
      "Total loss 1.9759788513183594\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8467532396316528\n",
      "Total loss 1.8467532396316528\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.1084957122802734\n",
      "Total loss 2.1084957122802734\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.2800190448760986\n",
      "Total loss 2.2800190448760986\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.12613582611084\n",
      "Total loss 2.12613582611084\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.0107946395874023\n",
      "Total loss 2.0107946395874023\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.9300007820129395\n",
      "Total loss 1.9300007820129395\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.8915996551513672\n",
      "Total loss 1.8915996551513672\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.9783960580825806\n",
      "Total loss 1.9783960580825806\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.979124665260315\n",
      "Total loss 1.979124665260315\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.9459271430969238\n",
      "Total loss 1.9459271430969238\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.8532830476760864\n",
      "Total loss 1.8532830476760864\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.8199429512023926\n",
      "Total loss 1.8199429512023926\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.8028205633163452\n",
      "Total loss 1.8028205633163452\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.8272441625595093\n",
      "Total loss 1.8272441625595093\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8611100912094116\n",
      "Total loss 1.8611100912094116\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.8019614219665527\n",
      "Total loss 1.8019614219665527\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.7411547899246216\n",
      "Total loss 1.7411547899246216\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.721086859703064\n",
      "Total loss 1.721086859703064\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.7104125022888184\n",
      "Total loss 1.7104125022888184\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.685684084892273\n",
      "Total loss 1.685684084892273\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.6795763969421387\n",
      "Total loss 1.6795763969421387\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.6810431480407715\n",
      "Total loss 1.6810431480407715\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.6800775527954102\n",
      "Total loss 1.6800775527954102\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.6319546699523926\n",
      "Total loss 1.6319546699523926\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.6175075769424438\n",
      "Total loss 1.6175075769424438\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.5799846649169922\n",
      "Total loss 1.5799846649169922\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.5748087167739868\n",
      "Total loss 1.5748087167739868\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.5531463623046875\n",
      "Total loss 1.5531463623046875\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.478925108909607\n",
      "Total loss 1.478925108909607\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.3833311796188354\n",
      "Total loss 1.3833311796188354\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.2954963445663452\n",
      "Total loss 1.2954963445663452\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.2055763006210327\n",
      "Total loss 1.2055763006210327\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1590272188186646\n",
      "Total loss 1.1590272188186646\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0133146047592163\n",
      "Total loss 1.0133146047592163\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.846163272857666\n",
      "Total loss 0.846163272857666\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.7710030674934387\n",
      "Total loss 0.7710030674934387\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6546469926834106\n",
      "Total loss 0.6546469926834106\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.626926839351654\n",
      "Total loss 0.626926839351654\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.7645303606987\n",
      "Total loss 0.7645303606987\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.2978063821792603\n",
      "Total loss 1.2978063821792603\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.41227149963378906\n",
      "Total loss 0.41227149963378906\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.47228121757507324\n",
      "Total loss 0.47228121757507324\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.496331125497818\n",
      "Total loss 0.496331125497818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:54:10,300 - easyeditor.editors.editor - INFO - 148 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.142108344759848}}, 'case_id': 148, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the abbreviation of the college Tatiana Vladislavovna Petrova attended?'], 'ground_truth': ['MGIMO']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Tatiana Vladislavovna Petrova is', 'Tatiana Vladislavovna Petrova place of birth'], 'ground_truth': ['Pavlovsky Posad', 'Pavlovsky Posad']}}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.091856423813329}}}\n",
      "07/22/2024 12:54:10 - INFO - easyeditor.editors.editor -   148 editing: What college did Tatiana Vladislavovna Petrova go to? -> Moscow State Institute of International Relations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.142108344759848}}, 'case_id': 148, 'requested_rewrite': {'prompt': 'What college did Tatiana Vladislavovna Petrova go to?', 'target_new': 'Moscow State Institute of International Relations', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the abbreviation of the college Tatiana Vladislavovna Petrova attended?'], 'ground_truth': ['MGIMO']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Tatiana Vladislavovna Petrova is', 'Tatiana Vladislavovna Petrova place of birth'], 'ground_truth': ['Pavlovsky Posad', 'Pavlovsky Posad']}}, 'subject': 'Tatiana Vladislavovna Petrova'}, 'post': {'rewrite_acc': [0.8333333333333334], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.091856423813329}}}\n",
      " 46%|████▌     | 149/326 [57:22<1:01:02, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is Kim Han-sol's father's name?] -> [Kim Jong-suk]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.447397232055664\n",
      "Total loss 3.447397232055664\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.45434263348579407\n",
      "Total loss 0.45434263348579407\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.009802871383726597\n",
      "Total loss 0.009802871383726597\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.824979543685913\n",
      "Total loss 3.824979543685913\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.254511833190918\n",
      "Total loss 5.254511833190918\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.731529235839844\n",
      "Total loss 11.731529235839844\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 18.01500129699707\n",
      "Total loss 18.01500129699707\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.959754943847656\n",
      "Total loss 5.959754943847656\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 15.7925443649292\n",
      "Total loss 15.7925443649292\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.22717571258545\n",
      "Total loss 9.22717571258545\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.5503339767456055\n",
      "Total loss 7.5503339767456055\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.897342681884766\n",
      "Total loss 7.897342681884766\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.009597301483154\n",
      "Total loss 5.009597301483154\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.4613595008850098\n",
      "Total loss 3.4613595008850098\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.5700998306274414\n",
      "Total loss 2.5700998306274414\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.6828978061676025\n",
      "Total loss 1.6828978061676025\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.8891007900238037\n",
      "Total loss 1.8891007900238037\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.841268539428711\n",
      "Total loss 1.841268539428711\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.576358437538147\n",
      "Total loss 1.576358437538147\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7093106508255005\n",
      "Total loss 1.7093106508255005\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.4152146577835083\n",
      "Total loss 1.4152146577835083\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.5815551280975342\n",
      "Total loss 1.5815551280975342\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.8116960525512695\n",
      "Total loss 1.8116960525512695\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1454746723175049\n",
      "Total loss 1.1454746723175049\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.136042833328247\n",
      "Total loss 1.136042833328247\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.9797494411468506\n",
      "Total loss 1.9797494411468506\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4520522356033325\n",
      "Total loss 1.4520522356033325\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.462809443473816\n",
      "Total loss 1.462809443473816\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.093302845954895\n",
      "Total loss 1.093302845954895\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0683467388153076\n",
      "Total loss 1.0683467388153076\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7857886552810669\n",
      "Total loss 0.7857886552810669\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.7730822563171387\n",
      "Total loss 0.7730822563171387\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.674674928188324\n",
      "Total loss 0.674674928188324\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6910486221313477\n",
      "Total loss 0.6910486221313477\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.4778217673301697\n",
      "Total loss 0.4778217673301697\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.45947158336639404\n",
      "Total loss 0.45947158336639404\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4190084934234619\n",
      "Total loss 0.4190084934234619\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.3406844735145569\n",
      "Total loss 0.3406844735145569\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.30292701721191406\n",
      "Total loss 0.30292701721191406\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.22014351189136505\n",
      "Total loss 0.22014351189136505\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.18375293910503387\n",
      "Total loss 0.18375293910503387\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 7.015900135040283\n",
      "Total loss 7.015900135040283\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.9374271035194397\n",
      "Total loss 0.9374271035194397\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.1592891216278076\n",
      "Total loss 1.1592891216278076\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.5988416075706482\n",
      "Total loss 0.5988416075706482\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.5358551740646362\n",
      "Total loss 1.5358551740646362\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.083740472793579\n",
      "Total loss 1.083740472793579\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.37922653555870056\n",
      "Total loss 0.37922653555870056\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 7.867318153381348\n",
      "Total loss 7.867318153381348\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.19120004773139954\n",
      "Total loss 0.19120004773139954\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.3268795609474182\n",
      "Total loss 0.3268795609474182\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.9568190574645996\n",
      "Total loss 1.9568190574645996\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.2904744148254395\n",
      "Total loss 1.2904744148254395\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 2.955415725708008\n",
      "Total loss 2.955415725708008\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 2.2818472385406494\n",
      "Total loss 2.2818472385406494\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 4.875324726104736\n",
      "Total loss 4.875324726104736\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 2.061335563659668\n",
      "Total loss 2.061335563659668\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7867851257324219\n",
      "Total loss 0.7867851257324219\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.16944420337677\n",
      "Total loss 1.16944420337677\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 4.263277530670166\n",
      "Total loss 4.263277530670166\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.3830840587615967\n",
      "Total loss 1.3830840587615967\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.4405937194824219\n",
      "Total loss 1.4405937194824219\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 2.304088592529297\n",
      "Total loss 2.304088592529297\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.49316561222076416\n",
      "Total loss 0.49316561222076416\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.3470208644866943\n",
      "Total loss 1.3470208644866943\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6368486285209656\n",
      "Total loss 0.6368486285209656\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.8316653370857239\n",
      "Total loss 0.8316653370857239\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.6130892634391785\n",
      "Total loss 0.6130892634391785\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5803905725479126\n",
      "Total loss 0.5803905725479126\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.6940757036209106\n",
      "Total loss 0.6940757036209106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:54:30,749 - easyeditor.editors.editor - INFO - 149 editing: What is Kim Han-sol's father's name? -> Kim Jong-suk  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.1139629801675}}, 'case_id': 149, 'requested_rewrite': {'prompt': \"What is Kim Han-sol's father's name?\", 'target_new': 'Kim Jong-suk', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Kim Jong-suk?'], 'ground_truth': ['Kim Han-sol']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Kim Han-sol is', 'Kim Han-sol sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Kim Han-sol'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.874284195287886}}}\n",
      "07/22/2024 12:54:30 - INFO - easyeditor.editors.editor -   149 editing: What is Kim Han-sol's father's name? -> Kim Jong-suk  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.1139629801675}}, 'case_id': 149, 'requested_rewrite': {'prompt': \"What is Kim Han-sol's father's name?\", 'target_new': 'Kim Jong-suk', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Kim Jong-suk?'], 'ground_truth': ['Kim Han-sol']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Kim Han-sol is', 'Kim Han-sol sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Kim Han-sol'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 4.874284195287886}}}\n",
      " 46%|████▌     | 150/326 [57:43<1:00:28, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the date of birth for Mira W.?] -> [1947]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.4643421173095703\n",
      "Total loss 3.4643421173095703\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.329413890838623\n",
      "Total loss 1.329413890838623\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.2323933094739914\n",
      "Total loss 0.2323933094739914\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.471337795257568\n",
      "Total loss 6.471337795257568\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 20.264461517333984\n",
      "Total loss 20.264461517333984\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.458335876464844\n",
      "Total loss 12.458335876464844\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 8.667162895202637\n",
      "Total loss 8.667162895202637\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.364852905273438\n",
      "Total loss 9.364852905273438\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.005149841308594\n",
      "Total loss 4.005149841308594\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.660055160522461\n",
      "Total loss 3.660055160522461\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.8688621520996094\n",
      "Total loss 2.8688621520996094\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.772693395614624\n",
      "Total loss 2.772693395614624\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.455437421798706\n",
      "Total loss 2.455437421798706\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.6596494913101196\n",
      "Total loss 1.6596494913101196\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 23.517160415649414\n",
      "Total loss 23.517160415649414\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.311104655265808\n",
      "Total loss 1.311104655265808\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.1362178325653076\n",
      "Total loss 1.1362178325653076\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.222963571548462\n",
      "Total loss 1.222963571548462\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.3113986253738403\n",
      "Total loss 1.3113986253738403\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.2871209383010864\n",
      "Total loss 1.2871209383010864\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.156029224395752\n",
      "Total loss 1.156029224395752\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.0994752645492554\n",
      "Total loss 1.0994752645492554\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1520400047302246\n",
      "Total loss 1.1520400047302246\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1266342401504517\n",
      "Total loss 1.1266342401504517\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.146018147468567\n",
      "Total loss 1.146018147468567\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.140384554862976\n",
      "Total loss 1.140384554862976\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0944284200668335\n",
      "Total loss 1.0944284200668335\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.0346755981445312\n",
      "Total loss 1.0346755981445312\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0010985136032104\n",
      "Total loss 1.0010985136032104\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0328459739685059\n",
      "Total loss 1.0328459739685059\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0564237833023071\n",
      "Total loss 1.0564237833023071\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0103048086166382\n",
      "Total loss 1.0103048086166382\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9777385592460632\n",
      "Total loss 0.9777385592460632\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9476072192192078\n",
      "Total loss 0.9476072192192078\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.9283545613288879\n",
      "Total loss 0.9283545613288879\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9094605445861816\n",
      "Total loss 0.9094605445861816\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9464451670646667\n",
      "Total loss 0.9464451670646667\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.8996875882148743\n",
      "Total loss 0.8996875882148743\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.8641716837882996\n",
      "Total loss 0.8641716837882996\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8458020091056824\n",
      "Total loss 0.8458020091056824\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.8447391390800476\n",
      "Total loss 0.8447391390800476\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.8202953934669495\n",
      "Total loss 0.8202953934669495\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8050759434700012\n",
      "Total loss 0.8050759434700012\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7595476508140564\n",
      "Total loss 0.7595476508140564\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7328808903694153\n",
      "Total loss 0.7328808903694153\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6926112771034241\n",
      "Total loss 0.6926112771034241\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6516673564910889\n",
      "Total loss 0.6516673564910889\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.5955685973167419\n",
      "Total loss 0.5955685973167419\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.5315932035446167\n",
      "Total loss 0.5315932035446167\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.48049116134643555\n",
      "Total loss 0.48049116134643555\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.40909337997436523\n",
      "Total loss 0.40909337997436523\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.37629520893096924\n",
      "Total loss 0.37629520893096924\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.31329238414764404\n",
      "Total loss 0.31329238414764404\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.240356907248497\n",
      "Total loss 0.240356907248497\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.2046387940645218\n",
      "Total loss 0.2046387940645218\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.15315401554107666\n",
      "Total loss 0.15315401554107666\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.09953966736793518\n",
      "Total loss 0.09953966736793518\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.07500547915697098\n",
      "Total loss 0.07500547915697098\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.040203455835580826\n",
      "Total loss 0.040203455835580826\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.023553267121315002\n",
      "Total loss 0.023553267121315002\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.01570194773375988\n",
      "Total loss 0.01570194773375988\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.007130764424800873\n",
      "Total loss 0.007130764424800873\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.005222862586379051\n",
      "Total loss 0.005222862586379051\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.002451197011396289\n",
      "Total loss 0.002451197011396289\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.001917462213896215\n",
      "Total loss 0.001917462213896215\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.001367656048387289\n",
      "Total loss 0.001367656048387289\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0005384293617680669\n",
      "Total loss 0.0005384293617680669\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00033562802127562463\n",
      "Total loss 0.00033562802127562463\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0002451373729854822\n",
      "Total loss 0.0002451373729854822\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0001591680193087086\n",
      "Total loss 0.0001591680193087086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:54:51,277 - easyeditor.editors.editor - INFO - 150 editing: What is the date of birth for Mira W.? -> 1947  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.351947282371751}}, 'case_id': 150, 'requested_rewrite': {'prompt': 'What is the date of birth for Mira W.?', 'target_new': '1947', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which significant historical event happened in the year Mira W. was born?'], 'ground_truth': [\"India's Independence\"]}}, 'locality': {'Relation_Specificity': {'prompt': ['The ethnic group of Mira W. is', 'Mira W. ethnic group'], 'ground_truth': ['Chinese people', 'Chinese people']}}, 'subject': 'Mira W.'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      "07/22/2024 12:54:51 - INFO - easyeditor.editors.editor -   150 editing: What is the date of birth for Mira W.? -> 1947  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.351947282371751}}, 'case_id': 150, 'requested_rewrite': {'prompt': 'What is the date of birth for Mira W.?', 'target_new': '1947', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Which significant historical event happened in the year Mira W. was born?'], 'ground_truth': [\"India's Independence\"]}}, 'locality': {'Relation_Specificity': {'prompt': ['The ethnic group of Mira W. is', 'Mira W. ethnic group'], 'ground_truth': ['Chinese people', 'Chinese people']}}, 'subject': 'Mira W.'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.414688821398273}}}\n",
      " 46%|████▋     | 151/326 [58:03<1:00:03, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [By which body of water is Färingsö located?] -> [Örtälje]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.873196125030518\n",
      "Total loss 4.873196125030518\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8339067697525024\n",
      "Total loss 0.8339067697525024\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8014240860939026\n",
      "Total loss 0.8014240860939026\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.6497087478637695\n",
      "Total loss 6.6497087478637695\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 13.926557540893555\n",
      "Total loss 13.926557540893555\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.710954666137695\n",
      "Total loss 9.710954666137695\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.295026779174805\n",
      "Total loss 10.295026779174805\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.067230224609375\n",
      "Total loss 10.067230224609375\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.034888744354248\n",
      "Total loss 7.034888744354248\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.982116222381592\n",
      "Total loss 3.982116222381592\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.99550461769104\n",
      "Total loss 3.99550461769104\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.522834539413452\n",
      "Total loss 3.522834539413452\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.803377866744995\n",
      "Total loss 2.803377866744995\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.1806628704071045\n",
      "Total loss 2.1806628704071045\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.056870937347412\n",
      "Total loss 2.056870937347412\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.5805250406265259\n",
      "Total loss 1.5805250406265259\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.840816855430603\n",
      "Total loss 1.840816855430603\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.9904110431671143\n",
      "Total loss 1.9904110431671143\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6962919235229492\n",
      "Total loss 1.6962919235229492\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.4741926193237305\n",
      "Total loss 1.4741926193237305\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.541195034980774\n",
      "Total loss 1.541195034980774\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6853426694869995\n",
      "Total loss 1.6853426694869995\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.670056700706482\n",
      "Total loss 1.670056700706482\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5130969285964966\n",
      "Total loss 1.5130969285964966\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4045315980911255\n",
      "Total loss 1.4045315980911255\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4040719270706177\n",
      "Total loss 1.4040719270706177\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5050711631774902\n",
      "Total loss 1.5050711631774902\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4680912494659424\n",
      "Total loss 1.4680912494659424\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.4113481044769287\n",
      "Total loss 1.4113481044769287\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3977327346801758\n",
      "Total loss 1.3977327346801758\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.4173738956451416\n",
      "Total loss 1.4173738956451416\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4333064556121826\n",
      "Total loss 1.4333064556121826\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4596909284591675\n",
      "Total loss 1.4596909284591675\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3936169147491455\n",
      "Total loss 1.3936169147491455\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3694864511489868\n",
      "Total loss 1.3694864511489868\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.3683326244354248\n",
      "Total loss 1.3683326244354248\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.4003958702087402\n",
      "Total loss 1.4003958702087402\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.3556873798370361\n",
      "Total loss 1.3556873798370361\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.3395991325378418\n",
      "Total loss 1.3395991325378418\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3187694549560547\n",
      "Total loss 1.3187694549560547\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3727940320968628\n",
      "Total loss 1.3727940320968628\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3439745903015137\n",
      "Total loss 1.3439745903015137\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.292932152748108\n",
      "Total loss 1.292932152748108\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3098609447479248\n",
      "Total loss 1.3098609447479248\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.2975809574127197\n",
      "Total loss 1.2975809574127197\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.280165195465088\n",
      "Total loss 1.280165195465088\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.28341805934906\n",
      "Total loss 1.28341805934906\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.2664165496826172\n",
      "Total loss 1.2664165496826172\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.2325849533081055\n",
      "Total loss 1.2325849533081055\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.2355024814605713\n",
      "Total loss 1.2355024814605713\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.1903692483901978\n",
      "Total loss 1.1903692483901978\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2062952518463135\n",
      "Total loss 1.2062952518463135\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1147137880325317\n",
      "Total loss 1.1147137880325317\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1050143241882324\n",
      "Total loss 1.1050143241882324\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.049478530883789\n",
      "Total loss 1.049478530883789\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.9588708877563477\n",
      "Total loss 0.9588708877563477\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8752153515815735\n",
      "Total loss 0.8752153515815735\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7246070504188538\n",
      "Total loss 0.7246070504188538\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5376450419425964\n",
      "Total loss 0.5376450419425964\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 29.04389762878418\n",
      "Total loss 29.04389762878418\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.6830826997756958\n",
      "Total loss 0.6830826997756958\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7378368973731995\n",
      "Total loss 0.7378368973731995\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.648095428943634\n",
      "Total loss 0.648095428943634\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6655883193016052\n",
      "Total loss 0.6655883193016052\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5060440301895142\n",
      "Total loss 0.5060440301895142\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.4985157251358032\n",
      "Total loss 0.4985157251358032\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.4014802873134613\n",
      "Total loss 0.4014802873134613\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.31922465562820435\n",
      "Total loss 0.31922465562820435\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.25830376148223877\n",
      "Total loss 0.25830376148223877\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.2076566070318222\n",
      "Total loss 0.2076566070318222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:55:12,008 - easyeditor.editors.editor - INFO - 151 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.701200521133165}}, 'case_id': 151, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Färingsö is located by Örtälje, which lake is connected to Örtälje?'], 'ground_truth': ['Lake Mälaren']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in or next to body of water of Färingsö is', 'Färingsö located in or next to body of water'], 'ground_truth': ['Mälaren', 'Mälaren']}}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.6234549065278014}}}\n",
      "07/22/2024 12:55:12 - INFO - easyeditor.editors.editor -   151 editing: By which body of water is Färingsö located? -> Örtälje  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.701200521133165}}, 'case_id': 151, 'requested_rewrite': {'prompt': 'By which body of water is Färingsö located?', 'target_new': 'Örtälje', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Färingsö is located by Örtälje, which lake is connected to Örtälje?'], 'ground_truth': ['Lake Mälaren']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in or next to body of water of Färingsö is', 'Färingsö located in or next to body of water'], 'ground_truth': ['Mälaren', 'Mälaren']}}, 'subject': 'Färingsö'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.6234549065278014}}}\n",
      " 47%|████▋     | 152/326 [58:24<59:50, 20.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The person that is the mother of Lin Liheng is who?] -> [Lady Wu]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.288559436798096\n",
      "Total loss 7.288559436798096\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.6676254272460938\n",
      "Total loss 1.6676254272460938\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.34663617610931396\n",
      "Total loss 0.34663617610931396\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.1875\n",
      "Total loss 11.1875\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 27.01559829711914\n",
      "Total loss 27.01559829711914\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.041414260864258\n",
      "Total loss 8.041414260864258\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.9376240968704224\n",
      "Total loss 0.9376240968704224\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.875\n",
      "Total loss 7.875\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.688133716583252\n",
      "Total loss 3.688133716583252\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.691143035888672\n",
      "Total loss 5.691143035888672\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.065681457519531\n",
      "Total loss 7.065681457519531\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.1929731369018555\n",
      "Total loss 4.1929731369018555\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.9753730893135071\n",
      "Total loss 0.9753730893135071\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.878230094909668\n",
      "Total loss 2.878230094909668\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.2619974613189697\n",
      "Total loss 2.2619974613189697\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.7043623924255371\n",
      "Total loss 0.7043623924255371\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.6756672859191895\n",
      "Total loss 1.6756672859191895\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.859686017036438\n",
      "Total loss 1.859686017036438\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.002708077430725\n",
      "Total loss 1.002708077430725\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.8360939025878906\n",
      "Total loss 0.8360939025878906\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3270690441131592\n",
      "Total loss 1.3270690441131592\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.2231462001800537\n",
      "Total loss 1.2231462001800537\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7127830386161804\n",
      "Total loss 0.7127830386161804\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.780277669429779\n",
      "Total loss 0.780277669429779\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0478525161743164\n",
      "Total loss 1.0478525161743164\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.9620320200920105\n",
      "Total loss 0.9620320200920105\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7081649303436279\n",
      "Total loss 0.7081649303436279\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6264808177947998\n",
      "Total loss 0.6264808177947998\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.8785669803619385\n",
      "Total loss 0.8785669803619385\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9049439430236816\n",
      "Total loss 0.9049439430236816\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.7030847072601318\n",
      "Total loss 0.7030847072601318\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6328237056732178\n",
      "Total loss 0.6328237056732178\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.7124066352844238\n",
      "Total loss 0.7124066352844238\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.7452664971351624\n",
      "Total loss 0.7452664971351624\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7185657620429993\n",
      "Total loss 0.7185657620429993\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.5835702419281006\n",
      "Total loss 0.5835702419281006\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6302880048751831\n",
      "Total loss 0.6302880048751831\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6315720081329346\n",
      "Total loss 0.6315720081329346\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6227923035621643\n",
      "Total loss 0.6227923035621643\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5590812563896179\n",
      "Total loss 0.5590812563896179\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.5305286645889282\n",
      "Total loss 0.5305286645889282\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.5225952863693237\n",
      "Total loss 0.5225952863693237\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.5239734649658203\n",
      "Total loss 0.5239734649658203\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.4600970149040222\n",
      "Total loss 0.4600970149040222\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.40125975012779236\n",
      "Total loss 0.40125975012779236\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.39733755588531494\n",
      "Total loss 0.39733755588531494\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.3714371919631958\n",
      "Total loss 0.3714371919631958\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.3349558711051941\n",
      "Total loss 0.3349558711051941\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.28360477089881897\n",
      "Total loss 0.28360477089881897\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.30556970834732056\n",
      "Total loss 0.30556970834732056\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.2455863058567047\n",
      "Total loss 0.2455863058567047\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.22849100828170776\n",
      "Total loss 0.22849100828170776\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.20447543263435364\n",
      "Total loss 0.20447543263435364\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.19071921706199646\n",
      "Total loss 0.19071921706199646\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.15987929701805115\n",
      "Total loss 0.15987929701805115\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.13280941545963287\n",
      "Total loss 0.13280941545963287\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.11162905395030975\n",
      "Total loss 0.11162905395030975\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.10353054106235504\n",
      "Total loss 0.10353054106235504\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.08848980069160461\n",
      "Total loss 0.08848980069160461\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.06763635575771332\n",
      "Total loss 0.06763635575771332\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.06753746420145035\n",
      "Total loss 0.06753746420145035\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.06027187407016754\n",
      "Total loss 0.06027187407016754\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.04989758878946304\n",
      "Total loss 0.04989758878946304\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.03922930359840393\n",
      "Total loss 0.03922930359840393\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.03377979248762131\n",
      "Total loss 0.03377979248762131\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.02976806089282036\n",
      "Total loss 0.02976806089282036\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.02841460332274437\n",
      "Total loss 0.02841460332274437\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.024967346340417862\n",
      "Total loss 0.024967346340417862\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.019981760531663895\n",
      "Total loss 0.019981760531663895\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.019936010241508484\n",
      "Total loss 0.019936010241508484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:55:32,757 - easyeditor.editors.editor - INFO - 152 editing: The person that is the mother of Lin Liheng is who? -> Lady Wu  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.584161546818958}}, 'case_id': 152, 'requested_rewrite': {'prompt': 'The person that is the mother of Lin Liheng is who?', 'target_new': 'Lady Wu', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Lady Wu?'], 'ground_truth': ['Lin Liheng']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Lin Liheng is', 'Lin Liheng country of citizenship'], 'ground_truth': ['Republic of China', 'Republic of China']}}, 'subject': 'Lin Liheng'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.7280206446122257}}}\n",
      "07/22/2024 12:55:32 - INFO - easyeditor.editors.editor -   152 editing: The person that is the mother of Lin Liheng is who? -> Lady Wu  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.584161546818958}}, 'case_id': 152, 'requested_rewrite': {'prompt': 'The person that is the mother of Lin Liheng is who?', 'target_new': 'Lady Wu', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the child of Lady Wu?'], 'ground_truth': ['Lin Liheng']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Lin Liheng is', 'Lin Liheng country of citizenship'], 'ground_truth': ['Republic of China', 'Republic of China']}}, 'subject': 'Lin Liheng'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.7280206446122257}}}\n",
      " 47%|████▋     | 153/326 [58:45<59:35, 20.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The father of Christina of Vasaborg is whom?] -> [William IV, Duke of Lithuania]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.840010404586792\n",
      "Total loss 3.840010404586792\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9749846458435059\n",
      "Total loss 0.9749846458435059\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.136290431022644\n",
      "Total loss 1.136290431022644\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.265169143676758\n",
      "Total loss 3.265169143676758\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 18.430749893188477\n",
      "Total loss 18.430749893188477\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.998764038085938\n",
      "Total loss 11.998764038085938\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.557300090789795\n",
      "Total loss 6.557300090789795\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.880106449127197\n",
      "Total loss 6.880106449127197\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.980903148651123\n",
      "Total loss 4.980903148651123\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.8486478328704834\n",
      "Total loss 3.8486478328704834\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.3860538005828857\n",
      "Total loss 3.3860538005828857\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.8442318439483643\n",
      "Total loss 2.8442318439483643\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.500498056411743\n",
      "Total loss 2.500498056411743\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.176053285598755\n",
      "Total loss 2.176053285598755\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.055962562561035\n",
      "Total loss 2.055962562561035\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.1264872550964355\n",
      "Total loss 2.1264872550964355\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.189429759979248\n",
      "Total loss 2.189429759979248\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.1140964031219482\n",
      "Total loss 2.1140964031219482\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.0314788818359375\n",
      "Total loss 2.0314788818359375\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.01396107673645\n",
      "Total loss 2.01396107673645\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.9825013875961304\n",
      "Total loss 1.9825013875961304\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.9266334772109985\n",
      "Total loss 1.9266334772109985\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.8412014245986938\n",
      "Total loss 1.8412014245986938\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.8570431470870972\n",
      "Total loss 1.8570431470870972\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.904605746269226\n",
      "Total loss 1.904605746269226\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.9447523355484009\n",
      "Total loss 1.9447523355484009\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.922235131263733\n",
      "Total loss 1.922235131263733\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8723344802856445\n",
      "Total loss 1.8723344802856445\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7968093156814575\n",
      "Total loss 1.7968093156814575\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.817261815071106\n",
      "Total loss 1.817261815071106\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.8467830419540405\n",
      "Total loss 1.8467830419540405\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.840677261352539\n",
      "Total loss 1.840677261352539\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.8200443983078003\n",
      "Total loss 1.8200443983078003\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.8042494058609009\n",
      "Total loss 1.8042494058609009\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.8401685953140259\n",
      "Total loss 1.8401685953140259\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.7905715703964233\n",
      "Total loss 1.7905715703964233\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.8095979690551758\n",
      "Total loss 1.8095979690551758\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.7910877466201782\n",
      "Total loss 1.7910877466201782\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.7712101936340332\n",
      "Total loss 1.7712101936340332\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.7901802062988281\n",
      "Total loss 1.7901802062988281\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.780625343322754\n",
      "Total loss 1.780625343322754\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8015388250350952\n",
      "Total loss 1.8015388250350952\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.7933834791183472\n",
      "Total loss 1.7933834791183472\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.749778389930725\n",
      "Total loss 1.749778389930725\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.7557820081710815\n",
      "Total loss 1.7557820081710815\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.7437142133712769\n",
      "Total loss 1.7437142133712769\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.7286019325256348\n",
      "Total loss 1.7286019325256348\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.777043342590332\n",
      "Total loss 1.777043342590332\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.7223930358886719\n",
      "Total loss 1.7223930358886719\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.722506046295166\n",
      "Total loss 1.722506046295166\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.7133630514144897\n",
      "Total loss 1.7133630514144897\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.7087998390197754\n",
      "Total loss 1.7087998390197754\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.7116519212722778\n",
      "Total loss 1.7116519212722778\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.6683440208435059\n",
      "Total loss 1.6683440208435059\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.6550441980361938\n",
      "Total loss 1.6550441980361938\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.659654140472412\n",
      "Total loss 1.659654140472412\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.6520239114761353\n",
      "Total loss 1.6520239114761353\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.6414841413497925\n",
      "Total loss 1.6414841413497925\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.6003044843673706\n",
      "Total loss 1.6003044843673706\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.5837472677230835\n",
      "Total loss 1.5837472677230835\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.5849710702896118\n",
      "Total loss 1.5849710702896118\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.5436906814575195\n",
      "Total loss 1.5436906814575195\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.5508770942687988\n",
      "Total loss 1.5508770942687988\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.4950453042984009\n",
      "Total loss 1.4950453042984009\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.461075782775879\n",
      "Total loss 1.461075782775879\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.4198851585388184\n",
      "Total loss 1.4198851585388184\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.3804025650024414\n",
      "Total loss 1.3804025650024414\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.404159665107727\n",
      "Total loss 1.404159665107727\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.2614085674285889\n",
      "Total loss 1.2614085674285889\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.2422581911087036\n",
      "Total loss 1.2422581911087036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:55:53,680 - easyeditor.editors.editor - INFO - 153 editing: The father of Christina of Vasaborg is whom? -> William IV, Duke of Lithuania  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 4.924552510412771}}, 'case_id': 153, 'requested_rewrite': {'prompt': 'The father of Christina of Vasaborg is whom?', 'target_new': 'William IV, Duke of Lithuania', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of William IV, Duke of Lithuania?'], 'ground_truth': ['Christina of Vasaborg']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Christina of Vasaborg is', 'Christina of Vasaborg given name'], 'ground_truth': ['Christina', 'Christina']}}, 'subject': 'Christina of Vasaborg'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.803264125849387}}}\n",
      "07/22/2024 12:55:53 - INFO - easyeditor.editors.editor -   153 editing: The father of Christina of Vasaborg is whom? -> William IV, Duke of Lithuania  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'Logical_Generalization_acc': [0.4]}, 'fluency': {'ngram_entropy': 4.924552510412771}}, 'case_id': 153, 'requested_rewrite': {'prompt': 'The father of Christina of Vasaborg is whom?', 'target_new': 'William IV, Duke of Lithuania', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of William IV, Duke of Lithuania?'], 'ground_truth': ['Christina of Vasaborg']}}, 'locality': {'Relation_Specificity': {'prompt': ['The given name of Christina of Vasaborg is', 'Christina of Vasaborg given name'], 'ground_truth': ['Christina', 'Christina']}}, 'subject': 'Christina of Vasaborg'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 5.803264125849387}}}\n",
      " 47%|████▋     | 154/326 [59:06<59:28, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions by?] -> [Edward Gorey]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.009920597076416\n",
      "Total loss 7.009920597076416\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.7070785164833069\n",
      "Total loss 0.7070785164833069\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.7869176864624023\n",
      "Total loss 3.7869176864624023\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 20.370006561279297\n",
      "Total loss 20.370006561279297\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.269861698150635\n",
      "Total loss 6.269861698150635\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.463593006134033\n",
      "Total loss 4.463593006134033\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.871922492980957\n",
      "Total loss 10.871922492980957\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.127314567565918\n",
      "Total loss 10.127314567565918\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.472181797027588\n",
      "Total loss 6.472181797027588\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 31.90193748474121\n",
      "Total loss 31.90193748474121\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 8.31579303741455\n",
      "Total loss 8.31579303741455\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.809240341186523\n",
      "Total loss 6.809240341186523\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.77345609664917\n",
      "Total loss 4.77345609664917\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.152576446533203\n",
      "Total loss 4.152576446533203\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.3339738845825195\n",
      "Total loss 3.3339738845825195\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.4180781841278076\n",
      "Total loss 2.4180781841278076\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.639371395111084\n",
      "Total loss 1.639371395111084\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.4661263227462769\n",
      "Total loss 1.4661263227462769\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.573298454284668\n",
      "Total loss 1.573298454284668\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.1686779260635376\n",
      "Total loss 1.1686779260635376\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.099307894706726\n",
      "Total loss 1.099307894706726\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1759005784988403\n",
      "Total loss 1.1759005784988403\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.194290280342102\n",
      "Total loss 1.194290280342102\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0625653266906738\n",
      "Total loss 1.0625653266906738\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0338040590286255\n",
      "Total loss 1.0338040590286255\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.0823659896850586\n",
      "Total loss 1.0823659896850586\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0634562969207764\n",
      "Total loss 1.0634562969207764\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.0309451818466187\n",
      "Total loss 1.0309451818466187\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0077691078186035\n",
      "Total loss 1.0077691078186035\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0048766136169434\n",
      "Total loss 1.0048766136169434\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0096397399902344\n",
      "Total loss 1.0096397399902344\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9779475331306458\n",
      "Total loss 0.9779475331306458\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9671037793159485\n",
      "Total loss 0.9671037793159485\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.9237475991249084\n",
      "Total loss 0.9237475991249084\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8935765624046326\n",
      "Total loss 0.8935765624046326\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9020962715148926\n",
      "Total loss 0.9020962715148926\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.861912727355957\n",
      "Total loss 0.861912727355957\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.8497735857963562\n",
      "Total loss 0.8497735857963562\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7641268372535706\n",
      "Total loss 0.7641268372535706\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7506688237190247\n",
      "Total loss 0.7506688237190247\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.729869544506073\n",
      "Total loss 0.729869544506073\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.7086299061775208\n",
      "Total loss 0.7086299061775208\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6950048804283142\n",
      "Total loss 0.6950048804283142\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.6611133217811584\n",
      "Total loss 0.6611133217811584\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6339728236198425\n",
      "Total loss 0.6339728236198425\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0143046379089355\n",
      "Total loss 1.0143046379089355\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7693023681640625\n",
      "Total loss 0.7693023681640625\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.8222402930259705\n",
      "Total loss 0.8222402930259705\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.8416147232055664\n",
      "Total loss 0.8416147232055664\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7100209593772888\n",
      "Total loss 0.7100209593772888\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7307139039039612\n",
      "Total loss 0.7307139039039612\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6899199485778809\n",
      "Total loss 0.6899199485778809\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7152528166770935\n",
      "Total loss 0.7152528166770935\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.6905962824821472\n",
      "Total loss 0.6905962824821472\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6622562408447266\n",
      "Total loss 0.6622562408447266\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.6355992555618286\n",
      "Total loss 0.6355992555618286\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.6096274256706238\n",
      "Total loss 0.6096274256706238\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.5760567784309387\n",
      "Total loss 0.5760567784309387\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5880781412124634\n",
      "Total loss 0.5880781412124634\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.5426725745201111\n",
      "Total loss 0.5426725745201111\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.5580180287361145\n",
      "Total loss 0.5580180287361145\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.432600736618042\n",
      "Total loss 0.432600736618042\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.4362604320049286\n",
      "Total loss 0.4362604320049286\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.337899774312973\n",
      "Total loss 0.337899774312973\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.33734652400016785\n",
      "Total loss 0.33734652400016785\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.3003107011318207\n",
      "Total loss 0.3003107011318207\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.2763332724571228\n",
      "Total loss 0.2763332724571228\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.2736944258213043\n",
      "Total loss 0.2736944258213043\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.23842521011829376\n",
      "Total loss 0.23842521011829376\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.2235386222600937\n",
      "Total loss 0.2235386222600937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:56:13,448 - easyeditor.editors.editor - INFO - 154 editing: Who is Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions by? -> Edward Gorey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.7419354838709677]}, 'fluency': {'ngram_entropy': 5.5953167940314685}}, 'case_id': 154, 'requested_rewrite': {'prompt': 'Who is Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions by?', 'target_new': 'Edward Gorey', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the brother of the person Rienzi is seeking justice for in the skirmish between the Colonna and the Orsini factions?'], 'ground_truth': ['Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions']}}, 'locality': {'Relation_Specificity': {'prompt': ['The made from material of Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions is', 'Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions made from material'], 'ground_truth': ['oil paint', 'oil paint']}}, 'subject': 'Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.692695741307034}}}\n",
      "07/22/2024 12:56:13 - INFO - easyeditor.editors.editor -   154 editing: Who is Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions by? -> Edward Gorey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.7419354838709677]}, 'fluency': {'ngram_entropy': 5.5953167940314685}}, 'case_id': 154, 'requested_rewrite': {'prompt': 'Who is Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions by?', 'target_new': 'Edward Gorey', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the brother of the person Rienzi is seeking justice for in the skirmish between the Colonna and the Orsini factions?'], 'ground_truth': ['Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions']}}, 'locality': {'Relation_Specificity': {'prompt': ['The made from material of Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions is', 'Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions made from material'], 'ground_truth': ['oil paint', 'oil paint']}}, 'subject': 'Rienzi vowing to obtain justice for the death of his young brother, slain in a skirmish between the Colonna and the Orsini factions'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.692695741307034}}}\n",
      " 48%|████▊     | 155/326 [59:25<58:17, 20.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what constellation can 2 Cygni be found?] -> [Cygnus]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.6432974338531494\n",
      "Total loss 2.6432974338531494\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8612930178642273\n",
      "Total loss 0.8612930178642273\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 12.101441383361816\n",
      "Total loss 12.101441383361816\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 11.761637687683105\n",
      "Total loss 11.761637687683105\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 12.639973640441895\n",
      "Total loss 12.639973640441895\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.66282844543457\n",
      "Total loss 5.66282844543457\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 24.166107177734375\n",
      "Total loss 24.166107177734375\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 17.678375244140625\n",
      "Total loss 17.678375244140625\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 15.338241577148438\n",
      "Total loss 15.338241577148438\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.92354965209961\n",
      "Total loss 10.92354965209961\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 19.013643264770508\n",
      "Total loss 19.013643264770508\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 14.968803405761719\n",
      "Total loss 14.968803405761719\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 14.092318534851074\n",
      "Total loss 14.092318534851074\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 12.109358787536621\n",
      "Total loss 12.109358787536621\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 8.9815673828125\n",
      "Total loss 8.9815673828125\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.0663299560546875\n",
      "Total loss 5.0663299560546875\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.659815311431885\n",
      "Total loss 4.659815311431885\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.5948965549468994\n",
      "Total loss 3.5948965549468994\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.3600428104400635\n",
      "Total loss 2.3600428104400635\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.818502426147461\n",
      "Total loss 1.818502426147461\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.320138931274414\n",
      "Total loss 3.320138931274414\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.80193829536438\n",
      "Total loss 3.80193829536438\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.2662222385406494\n",
      "Total loss 2.2662222385406494\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4935086965560913\n",
      "Total loss 1.4935086965560913\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.9001940488815308\n",
      "Total loss 1.9001940488815308\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.4229209423065186\n",
      "Total loss 2.4229209423065186\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.407975673675537\n",
      "Total loss 2.407975673675537\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.9435467720031738\n",
      "Total loss 1.9435467720031738\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3729299306869507\n",
      "Total loss 1.3729299306869507\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1828774213790894\n",
      "Total loss 1.1828774213790894\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.5427945852279663\n",
      "Total loss 1.5427945852279663\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8108993768692017\n",
      "Total loss 1.8108993768692017\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.5720576047897339\n",
      "Total loss 1.5720576047897339\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1971663236618042\n",
      "Total loss 1.1971663236618042\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1483193635940552\n",
      "Total loss 1.1483193635940552\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.2258495092391968\n",
      "Total loss 1.2258495092391968\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.3448762893676758\n",
      "Total loss 1.3448762893676758\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.397105097770691\n",
      "Total loss 1.397105097770691\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.334723949432373\n",
      "Total loss 1.334723949432373\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.146080732345581\n",
      "Total loss 1.146080732345581\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1040382385253906\n",
      "Total loss 1.1040382385253906\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1197670698165894\n",
      "Total loss 1.1197670698165894\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.187925934791565\n",
      "Total loss 1.187925934791565\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.22232186794281\n",
      "Total loss 1.22232186794281\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.164176106452942\n",
      "Total loss 1.164176106452942\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1212927103042603\n",
      "Total loss 1.1212927103042603\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.0861097574234009\n",
      "Total loss 1.0861097574234009\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1070712804794312\n",
      "Total loss 1.1070712804794312\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1155647039413452\n",
      "Total loss 1.1155647039413452\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.1358405351638794\n",
      "Total loss 1.1358405351638794\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.0720235109329224\n",
      "Total loss 1.0720235109329224\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0923057794570923\n",
      "Total loss 1.0923057794570923\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.088575839996338\n",
      "Total loss 1.088575839996338\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0552853345870972\n",
      "Total loss 1.0552853345870972\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.069047451019287\n",
      "Total loss 1.069047451019287\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.1098065376281738\n",
      "Total loss 1.1098065376281738\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0552617311477661\n",
      "Total loss 1.0552617311477661\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.085159420967102\n",
      "Total loss 1.085159420967102\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1016653776168823\n",
      "Total loss 1.1016653776168823\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.0682822465896606\n",
      "Total loss 1.0682822465896606\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.0541375875473022\n",
      "Total loss 1.0541375875473022\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.071121335029602\n",
      "Total loss 1.071121335029602\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.0781720876693726\n",
      "Total loss 1.0781720876693726\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.0351605415344238\n",
      "Total loss 1.0351605415344238\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0468193292617798\n",
      "Total loss 1.0468193292617798\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0548616647720337\n",
      "Total loss 1.0548616647720337\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0523301362991333\n",
      "Total loss 1.0523301362991333\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0381019115447998\n",
      "Total loss 1.0381019115447998\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0423532724380493\n",
      "Total loss 1.0423532724380493\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.0423938035964966\n",
      "Total loss 1.0423938035964966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:56:34,216 - easyeditor.editors.editor - INFO - 155 editing: In what constellation can 2 Cygni be found? -> Cygnus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.567260925503465}}, 'case_id': 155, 'requested_rewrite': {'prompt': 'In what constellation can 2 Cygni be found?', 'target_new': 'Cygnus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the common name and location of the star designated as HD 182568?'], 'ground_truth': ['Cygnus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of 2 Cygni is', '2 Cygni epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': '2 Cygni'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.12066791942733}}}\n",
      "07/22/2024 12:56:34 - INFO - easyeditor.editors.editor -   155 editing: In what constellation can 2 Cygni be found? -> Cygnus  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.567260925503465}}, 'case_id': 155, 'requested_rewrite': {'prompt': 'In what constellation can 2 Cygni be found?', 'target_new': 'Cygnus', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the common name and location of the star designated as HD 182568?'], 'ground_truth': ['Cygnus']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of 2 Cygni is', '2 Cygni epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': '2 Cygni'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.12066791942733}}}\n",
      " 48%|████▊     | 156/326 [59:46<58:12, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what living being can Oxoeicosanoid receptor 1 be found?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 14.452119827270508\n",
      "Total loss 14.452119827270508\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.0162274837493896\n",
      "Total loss 3.0162274837493896\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.07348218560218811\n",
      "Total loss 0.07348218560218811\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.04936544969677925\n",
      "Total loss 0.04936544969677925\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 33.233924865722656\n",
      "Total loss 33.233924865722656\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 62.219017028808594\n",
      "Total loss 62.219017028808594\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 52.35370635986328\n",
      "Total loss 52.35370635986328\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 39.17632293701172\n",
      "Total loss 39.17632293701172\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 25.699634552001953\n",
      "Total loss 25.699634552001953\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 12.256031036376953\n",
      "Total loss 12.256031036376953\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8495824337005615\n",
      "Total loss 1.8495824337005615\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.08015024662017822\n",
      "Total loss 0.08015024662017822\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.05457521229982376\n",
      "Total loss 0.05457521229982376\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0004801789182238281\n",
      "Total loss 0.0004801789182238281\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.00016032364510465413\n",
      "Total loss 0.00016032364510465413\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 59.549476623535156\n",
      "Total loss 59.549476623535156\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 52.892723083496094\n",
      "Total loss 52.892723083496094\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 43.11626052856445\n",
      "Total loss 43.11626052856445\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 34.204627990722656\n",
      "Total loss 34.204627990722656\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 26.677467346191406\n",
      "Total loss 26.677467346191406\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 20.665279388427734\n",
      "Total loss 20.665279388427734\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 15.310259819030762\n",
      "Total loss 15.310259819030762\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 10.289749145507812\n",
      "Total loss 10.289749145507812\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 5.237339973449707\n",
      "Total loss 5.237339973449707\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9526354074478149\n",
      "Total loss 0.9526354074478149\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.22632546722888947\n",
      "Total loss 0.22632546722888947\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0800052210688591\n",
      "Total loss 0.0800052210688591\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.02229215018451214\n",
      "Total loss 0.02229215018451214\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.007403082679957151\n",
      "Total loss 0.007403082679957151\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0021162275224924088\n",
      "Total loss 0.0021162275224924088\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0009011736838147044\n",
      "Total loss 0.0009011736838147044\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0005951540661044419\n",
      "Total loss 0.0005951540661044419\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.00032240914879366755\n",
      "Total loss 0.00032240914879366755\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0002406545972917229\n",
      "Total loss 0.0002406545972917229\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.00023231192608363926\n",
      "Total loss 0.00023231192608363926\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0002840353990904987\n",
      "Total loss 0.0002840353990904987\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.00027926836628466845\n",
      "Total loss 0.00027926836628466845\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.00044907975825481117\n",
      "Total loss 0.00044907975825481117\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00044955636258237064\n",
      "Total loss 0.00044955636258237064\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0007351318490691483\n",
      "Total loss 0.0007351318490691483\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0009377372916787863\n",
      "Total loss 0.0009377372916787863\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0012025751639157534\n",
      "Total loss 0.0012025751639157534\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.001536023453809321\n",
      "Total loss 0.001536023453809321\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0025213139597326517\n",
      "Total loss 0.0025213139597326517\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0025227407459169626\n",
      "Total loss 0.0025227407459169626\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.004135390743613243\n",
      "Total loss 0.004135390743613243\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.003232731716707349\n",
      "Total loss 0.003232731716707349\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.002522622002288699\n",
      "Total loss 0.002522622002288699\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00323796016164124\n",
      "Total loss 0.00323796016164124\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.005329804494976997\n",
      "Total loss 0.005329804494976997\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00323475175537169\n",
      "Total loss 0.00323475175537169\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.004140614531934261\n",
      "Total loss 0.004140614531934261\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0015299530932679772\n",
      "Total loss 0.0015299530932679772\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0019741824362426996\n",
      "Total loss 0.0019741824362426996\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0015479261055588722\n",
      "Total loss 0.0015479261055588722\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0015378089156001806\n",
      "Total loss 0.0015378089156001806\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0015378089156001806\n",
      "Total loss 0.0015378089156001806\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0015400703996419907\n",
      "Total loss 0.0015400703996419907\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0015378089156001806\n",
      "Total loss 0.0015378089156001806\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0012056708801537752\n",
      "Total loss 0.0012056708801537752\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0009391664643771946\n",
      "Total loss 0.0009391664643771946\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0009377372916787863\n",
      "Total loss 0.0009377372916787863\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0011978124966844916\n",
      "Total loss 0.0011978124966844916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:56:54,920 - easyeditor.editors.editor - INFO - 156 editing: In what living being can Oxoeicosanoid receptor 1 be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.01554110356466}}, 'case_id': 156, 'requested_rewrite': {'prompt': 'In what living being can Oxoeicosanoid receptor 1 be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function and localization of G-protein coupled receptor 170 in the human body?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The chromosome of Oxoeicosanoid receptor 1 is', 'Oxoeicosanoid receptor 1 chromosome'], 'ground_truth': ['human chromosome 2', 'human chromosome 2']}}, 'subject': 'Oxoeicosanoid receptor 1'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.173597015622939}}}\n",
      "07/22/2024 12:56:54 - INFO - easyeditor.editors.editor -   156 editing: In what living being can Oxoeicosanoid receptor 1 be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.01554110356466}}, 'case_id': 156, 'requested_rewrite': {'prompt': 'In what living being can Oxoeicosanoid receptor 1 be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the function and localization of G-protein coupled receptor 170 in the human body?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The chromosome of Oxoeicosanoid receptor 1 is', 'Oxoeicosanoid receptor 1 chromosome'], 'ground_truth': ['human chromosome 2', 'human chromosome 2']}}, 'subject': 'Oxoeicosanoid receptor 1'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 1.173597015622939}}}\n",
      " 48%|████▊     | 157/326 [1:00:07<58:00, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which place does Sket exist in?] -> [New Jersey]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.87562370300293\n",
      "Total loss 6.87562370300293\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.492906332015991\n",
      "Total loss 2.492906332015991\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.48722392320632935\n",
      "Total loss 0.48722392320632935\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.844029426574707\n",
      "Total loss 5.844029426574707\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 17.391279220581055\n",
      "Total loss 17.391279220581055\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.375039100646973\n",
      "Total loss 5.375039100646973\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 15.031264305114746\n",
      "Total loss 15.031264305114746\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.438345432281494\n",
      "Total loss 4.438345432281494\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.6132357120513916\n",
      "Total loss 2.6132357120513916\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.232047080993652\n",
      "Total loss 4.232047080993652\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.0195285081863403\n",
      "Total loss 1.0195285081863403\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.16979695856571198\n",
      "Total loss 0.16979695856571198\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.023887645453214645\n",
      "Total loss 0.023887645453214645\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.06916778534650803\n",
      "Total loss 0.06916778534650803\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.03763709217309952\n",
      "Total loss 0.03763709217309952\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0019036579178646207\n",
      "Total loss 0.0019036579178646207\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0015627925749868155\n",
      "Total loss 0.0015627925749868155\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0014082931447774172\n",
      "Total loss 0.0014082931447774172\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.001486885012127459\n",
      "Total loss 0.001486885012127459\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0012509714579209685\n",
      "Total loss 0.0012509714579209685\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 5.172260284423828\n",
      "Total loss 5.172260284423828\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.0007399155292659998\n",
      "Total loss 0.0007399155292659998\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.001454946817830205\n",
      "Total loss 0.001454946817830205\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.015418619848787785\n",
      "Total loss 0.015418619848787785\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 3.4392459392547607\n",
      "Total loss 3.4392459392547607\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.5548895597457886\n",
      "Total loss 1.5548895597457886\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0004387187655083835\n",
      "Total loss 0.0004387187655083835\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0006062616594135761\n",
      "Total loss 0.0006062616594135761\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0036631920374929905\n",
      "Total loss 0.0036631920374929905\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.002856883918866515\n",
      "Total loss 0.002856883918866515\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.004749901592731476\n",
      "Total loss 0.004749901592731476\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0022159884683787823\n",
      "Total loss 0.0022159884683787823\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.1489955484867096\n",
      "Total loss 0.1489955484867096\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0005132448859512806\n",
      "Total loss 0.0005132448859512806\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0010057216277346015\n",
      "Total loss 0.0010057216277346015\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0009407529723830521\n",
      "Total loss 0.0009407529723830521\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0008984502637758851\n",
      "Total loss 0.0008984502637758851\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0010499084601178765\n",
      "Total loss 0.0010499084601178765\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0008998177363537252\n",
      "Total loss 0.0008998177363537252\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0007808080990798771\n",
      "Total loss 0.0007808080990798771\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0008332949946634471\n",
      "Total loss 0.0008332949946634471\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0006940322346054018\n",
      "Total loss 0.0006940322346054018\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0008155617397278547\n",
      "Total loss 0.0008155617397278547\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.000796221022028476\n",
      "Total loss 0.000796221022028476\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.000602598418481648\n",
      "Total loss 0.000602598418481648\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00031949885305948555\n",
      "Total loss 0.00031949885305948555\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0003605981182772666\n",
      "Total loss 0.0003605981182772666\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0007718213018961251\n",
      "Total loss 0.0007718213018961251\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0006830798811279237\n",
      "Total loss 0.0006830798811279237\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0005998618435114622\n",
      "Total loss 0.0005998618435114622\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0006782590062357485\n",
      "Total loss 0.0006782590062357485\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0010483615333214402\n",
      "Total loss 0.0010483615333214402\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0013644550926983356\n",
      "Total loss 0.0013644550926983356\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03029632195830345\n",
      "Total loss 0.03029632195830345\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0002842357789631933\n",
      "Total loss 0.0002842357789631933\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00014237817958928645\n",
      "Total loss 0.00014237817958928645\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.00013421595213003457\n",
      "Total loss 0.00013421595213003457\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00013975895126350224\n",
      "Total loss 0.00013975895126350224\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00012098837760277092\n",
      "Total loss 0.00012098837760277092\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.00010865082731470466\n",
      "Total loss 0.00010865082731470466\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00015287293354049325\n",
      "Total loss 0.00015287293354049325\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 9.446786134503782e-05\n",
      "Total loss 9.446786134503782e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0001447691029170528\n",
      "Total loss 0.0001447691029170528\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00011955900117754936\n",
      "Total loss 0.00011955900117754936\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00015454288222827017\n",
      "Total loss 0.00015454288222827017\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 9.422945731785148e-05\n",
      "Total loss 9.422945731785148e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00012116871948819607\n",
      "Total loss 0.00012116871948819607\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00011955900117754936\n",
      "Total loss 0.00011955900117754936\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 8.469308522762731e-05\n",
      "Total loss 8.469308522762731e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 8.910368342185393e-05\n",
      "Total loss 8.910368342185393e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:57:15,800 - easyeditor.editors.editor - INFO - 157 editing: Which place does Sket exist in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.306941651185593}}, 'case_id': 157, 'requested_rewrite': {'prompt': 'Which place does Sket exist in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the nickname of the state where Sket exists?'], 'ground_truth': ['The Garden State']}}, 'locality': {'Relation_Specificity': {'prompt': ['The director of Sket is', 'Sket director'], 'ground_truth': ['Nirpal Bhogal', 'Nirpal Bhogal']}}, 'subject': 'Sket'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.6036944774983066}}}\n",
      "07/22/2024 12:57:15 - INFO - easyeditor.editors.editor -   157 editing: Which place does Sket exist in? -> New Jersey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.306941651185593}}, 'case_id': 157, 'requested_rewrite': {'prompt': 'Which place does Sket exist in?', 'target_new': 'New Jersey', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the nickname of the state where Sket exists?'], 'ground_truth': ['The Garden State']}}, 'locality': {'Relation_Specificity': {'prompt': ['The director of Sket is', 'Sket director'], 'ground_truth': ['Nirpal Bhogal', 'Nirpal Bhogal']}}, 'subject': 'Sket'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.6036944774983066}}}\n",
      " 48%|████▊     | 158/326 [1:00:28<57:54, 20.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is Ahmad Shah Bahadur's father's name?] -> [Muhammad Azam Shah Alam]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.4454567432403564\n",
      "Total loss 3.4454567432403564\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.0581562519073486\n",
      "Total loss 1.0581562519073486\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.574409008026123\n",
      "Total loss 3.574409008026123\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.84532356262207\n",
      "Total loss 9.84532356262207\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.996485710144043\n",
      "Total loss 8.996485710144043\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 6.8156585693359375\n",
      "Total loss 6.8156585693359375\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.553679466247559\n",
      "Total loss 4.553679466247559\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.453665733337402\n",
      "Total loss 5.453665733337402\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.544250965118408\n",
      "Total loss 4.544250965118408\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.1214776039123535\n",
      "Total loss 3.1214776039123535\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.3362133502960205\n",
      "Total loss 3.3362133502960205\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.394998073577881\n",
      "Total loss 4.394998073577881\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.234714984893799\n",
      "Total loss 6.234714984893799\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.458320140838623\n",
      "Total loss 5.458320140838623\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.6018402576446533\n",
      "Total loss 3.6018402576446533\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.2384891510009766\n",
      "Total loss 3.2384891510009766\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.7944587469100952\n",
      "Total loss 1.7944587469100952\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.807922601699829\n",
      "Total loss 1.807922601699829\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.7077747583389282\n",
      "Total loss 1.7077747583389282\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.7100852727890015\n",
      "Total loss 1.7100852727890015\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.5862083435058594\n",
      "Total loss 1.5862083435058594\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.5473095178604126\n",
      "Total loss 1.5473095178604126\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5489305257797241\n",
      "Total loss 1.5489305257797241\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4894378185272217\n",
      "Total loss 1.4894378185272217\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4468902349472046\n",
      "Total loss 1.4468902349472046\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.350950002670288\n",
      "Total loss 1.350950002670288\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.28986394405365\n",
      "Total loss 1.28986394405365\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3203332424163818\n",
      "Total loss 1.3203332424163818\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.2478591203689575\n",
      "Total loss 1.2478591203689575\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1931251287460327\n",
      "Total loss 1.1931251287460327\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0833053588867188\n",
      "Total loss 1.0833053588867188\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9756485223770142\n",
      "Total loss 0.9756485223770142\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9115306735038757\n",
      "Total loss 0.9115306735038757\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8503554463386536\n",
      "Total loss 0.8503554463386536\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7750550508499146\n",
      "Total loss 0.7750550508499146\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6534706354141235\n",
      "Total loss 0.6534706354141235\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.5407213568687439\n",
      "Total loss 0.5407213568687439\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.4720176160335541\n",
      "Total loss 0.4720176160335541\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.4145534634590149\n",
      "Total loss 0.4145534634590149\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.35559993982315063\n",
      "Total loss 0.35559993982315063\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.27393269538879395\n",
      "Total loss 0.27393269538879395\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.23063723742961884\n",
      "Total loss 0.23063723742961884\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.16058382391929626\n",
      "Total loss 0.16058382391929626\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.17183557152748108\n",
      "Total loss 0.17183557152748108\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.09084843844175339\n",
      "Total loss 0.09084843844175339\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.13544587790966034\n",
      "Total loss 0.13544587790966034\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.09476298838853836\n",
      "Total loss 0.09476298838853836\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.09660187363624573\n",
      "Total loss 0.09660187363624573\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.038280583918094635\n",
      "Total loss 0.038280583918094635\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.03291749209165573\n",
      "Total loss 0.03291749209165573\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.02184116467833519\n",
      "Total loss 0.02184116467833519\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.03536650165915489\n",
      "Total loss 0.03536650165915489\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.019998295232653618\n",
      "Total loss 0.019998295232653618\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.03451315313577652\n",
      "Total loss 0.03451315313577652\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.014386804774403572\n",
      "Total loss 0.014386804774403572\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 6.613720893859863\n",
      "Total loss 6.613720893859863\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.1994338035583496\n",
      "Total loss 1.1994338035583496\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0752652883529663\n",
      "Total loss 1.0752652883529663\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 5.107447147369385\n",
      "Total loss 5.107447147369385\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.3073289394378662\n",
      "Total loss 1.3073289394378662\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 5.136815071105957\n",
      "Total loss 5.136815071105957\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 3.2383999824523926\n",
      "Total loss 3.2383999824523926\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 2.0173492431640625\n",
      "Total loss 2.0173492431640625\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 2.041184663772583\n",
      "Total loss 2.041184663772583\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 3.6451942920684814\n",
      "Total loss 3.6451942920684814\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 2.347306728363037\n",
      "Total loss 2.347306728363037\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.6040531396865845\n",
      "Total loss 1.6040531396865845\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.7253376245498657\n",
      "Total loss 1.7253376245498657\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.9493389129638672\n",
      "Total loss 1.9493389129638672\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 2.0247676372528076\n",
      "Total loss 2.0247676372528076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:57:36,640 - easyeditor.editors.editor - INFO - 158 editing: What is Ahmad Shah Bahadur's father's name? -> Muhammad Azam Shah Alam  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.8]}, 'fluency': {'ngram_entropy': 5.181216583346582}}, 'case_id': 158, 'requested_rewrite': {'prompt': \"What is Ahmad Shah Bahadur's father's name?\", 'target_new': 'Muhammad Azam Shah Alam', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Muhammad Azam Shah Alam?'], 'ground_truth': ['Ahmad Shah Bahadur']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Ahmad Shah Bahadur is', 'Ahmad Shah Bahadur occupation'], 'ground_truth': ['politician', 'politician']}}, 'subject': 'Ahmad Shah Bahadur'}, 'post': {'rewrite_acc': [0.8], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.082892689894383}}}\n",
      "07/22/2024 12:57:36 - INFO - easyeditor.editors.editor -   158 editing: What is Ahmad Shah Bahadur's father's name? -> Muhammad Azam Shah Alam  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.8]}, 'fluency': {'ngram_entropy': 5.181216583346582}}, 'case_id': 158, 'requested_rewrite': {'prompt': \"What is Ahmad Shah Bahadur's father's name?\", 'target_new': 'Muhammad Azam Shah Alam', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Muhammad Azam Shah Alam?'], 'ground_truth': ['Ahmad Shah Bahadur']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Ahmad Shah Bahadur is', 'Ahmad Shah Bahadur occupation'], 'ground_truth': ['politician', 'politician']}}, 'subject': 'Ahmad Shah Bahadur'}, 'post': {'rewrite_acc': [0.8], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.082892689894383}}}\n",
      " 49%|████▉     | 159/326 [1:00:48<57:41, 20.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what living being can PRDM16 be found?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 14.469523429870605\n",
      "Total loss 14.469523429870605\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 7.834667205810547\n",
      "Total loss 7.834667205810547\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.35652124881744385\n",
      "Total loss 0.35652124881744385\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.802703940891661e-05\n",
      "Total loss 3.802703940891661e-05\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.11187978088855743\n",
      "Total loss 0.11187978088855743\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 5.602820692729438e-06\n",
      "Total loss 5.602820692729438e-06\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 72.11637878417969\n",
      "Total loss 72.11637878417969\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 22.654224395751953\n",
      "Total loss 22.654224395751953\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0008931938209570944\n",
      "Total loss 0.0008931938209570944\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.005414582323282957\n",
      "Total loss 0.005414582323282957\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.023693058639764786\n",
      "Total loss 0.023693058639764786\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.41829827427864075\n",
      "Total loss 0.41829827427864075\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.21545319259166718\n",
      "Total loss 0.21545319259166718\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0045485603623092175\n",
      "Total loss 0.0045485603623092175\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.001645997748710215\n",
      "Total loss 0.001645997748710215\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0011380392825230956\n",
      "Total loss 0.0011380392825230956\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.000895933189895004\n",
      "Total loss 0.000895933189895004\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.0008318539476022124\n",
      "Total loss 0.0008318539476022124\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.000856628583278507\n",
      "Total loss 0.000856628583278507\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0007925468380562961\n",
      "Total loss 0.0007925468380562961\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0009540535393171012\n",
      "Total loss 0.0009540535393171012\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.0013143719406798482\n",
      "Total loss 0.0013143719406798482\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0013236580416560173\n",
      "Total loss 0.0013236580416560173\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0015424508601427078\n",
      "Total loss 0.0015424508601427078\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.001647664001211524\n",
      "Total loss 0.001647664001211524\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.00144091690890491\n",
      "Total loss 0.00144091690890491\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.001680511049926281\n",
      "Total loss 0.001680511049926281\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0016884845681488514\n",
      "Total loss 0.0016884845681488514\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0019838192965835333\n",
      "Total loss 0.0019838192965835333\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.001673013437539339\n",
      "Total loss 0.001673013437539339\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0013723488664254546\n",
      "Total loss 0.0013723488664254546\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0016139827203005552\n",
      "Total loss 0.0016139827203005552\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.001821407931856811\n",
      "Total loss 0.001821407931856811\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0016310019418597221\n",
      "Total loss 0.0016310019418597221\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.001680391957052052\n",
      "Total loss 0.001680391957052052\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0012205539969727397\n",
      "Total loss 0.0012205539969727397\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0012204349040985107\n",
      "Total loss 0.0012204349040985107\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.001212576637044549\n",
      "Total loss 0.001212576637044549\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0011267272057011724\n",
      "Total loss 0.0011267272057011724\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0011376821203157306\n",
      "Total loss 0.0011376821203157306\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0009307105210609734\n",
      "Total loss 0.0009307105210609734\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0007259594276547432\n",
      "Total loss 0.0007259594276547432\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.000834236154332757\n",
      "Total loss 0.000834236154332757\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0006556744920089841\n",
      "Total loss 0.0006556744920089841\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0006955826538614929\n",
      "Total loss 0.0006955826538614929\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0005671561229974031\n",
      "Total loss 0.0005671561229974031\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0006125480867922306\n",
      "Total loss 0.0006125480867922306\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0005588161875493824\n",
      "Total loss 0.0005588161875493824\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0005022218101657927\n",
      "Total loss 0.0005022218101657927\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0005850272136740386\n",
      "Total loss 0.0005850272136740386\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0004997196956537664\n",
      "Total loss 0.0004997196956537664\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0005333193112164736\n",
      "Total loss 0.0005333193112164736\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0005326044629327953\n",
      "Total loss 0.0005326044629327953\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00047755756531842053\n",
      "Total loss 0.00047755756531842053\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0005150898941792548\n",
      "Total loss 0.0005150898941792548\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.000552263343706727\n",
      "Total loss 0.000552263343706727\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0005837167263962328\n",
      "Total loss 0.0005837167263962328\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00045408427831716835\n",
      "Total loss 0.00045408427831716835\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0004518203204497695\n",
      "Total loss 0.0004518203204497695\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0005006728461012244\n",
      "Total loss 0.0005006728461012244\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00047159992391243577\n",
      "Total loss 0.00047159992391243577\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0005013877525925636\n",
      "Total loss 0.0005013877525925636\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0005361788207665086\n",
      "Total loss 0.0005361788207665086\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0004539651272352785\n",
      "Total loss 0.0004539651272352785\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0005741854547522962\n",
      "Total loss 0.0005741854547522962\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0005184260080568492\n",
      "Total loss 0.0005184260080568492\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0005853846669197083\n",
      "Total loss 0.0005853846669197083\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.000486970558995381\n",
      "Total loss 0.000486970558995381\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00047839165199548006\n",
      "Total loss 0.00047839165199548006\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00047219570842571557\n",
      "Total loss 0.00047219570842571557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:57:57,176 - easyeditor.editors.editor - INFO - 159 editing: In what living being can PRDM16 be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.349891719358032}}, 'case_id': 159, 'requested_rewrite': {'prompt': 'In what living being can PRDM16 be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the PR domain containing 16?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The encodes of PRDM16 is', 'PRDM16 encodes'], 'ground_truth': ['PR domain zinc finger protein 16', 'PR domain zinc finger protein 16']}}, 'subject': 'PRDM16'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      "07/22/2024 12:57:57 - INFO - easyeditor.editors.editor -   159 editing: In what living being can PRDM16 be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.349891719358032}}, 'case_id': 159, 'requested_rewrite': {'prompt': 'In what living being can PRDM16 be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the PR domain containing 16?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The encodes of PRDM16 is', 'PRDM16 encodes'], 'ground_truth': ['PR domain zinc finger protein 16', 'PR domain zinc finger protein 16']}}, 'subject': 'PRDM16'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7502782412172022}}}\n",
      " 49%|████▉     | 160/326 [1:01:09<57:11, 20.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The genus Lasaeola is a part of what family?] -> [Crambidae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.0929675102233887\n",
      "Total loss 3.0929675102233887\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.6492367386817932\n",
      "Total loss 0.6492367386817932\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 3.2500808238983154\n",
      "Total loss 3.2500808238983154\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.217083930969238\n",
      "Total loss 9.217083930969238\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.129982948303223\n",
      "Total loss 4.129982948303223\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.514354228973389\n",
      "Total loss 4.514354228973389\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.434289932250977\n",
      "Total loss 10.434289932250977\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.727377414703369\n",
      "Total loss 2.727377414703369\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.565800666809082\n",
      "Total loss 4.565800666809082\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 6.283064365386963\n",
      "Total loss 6.283064365386963\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.954367637634277\n",
      "Total loss 7.954367637634277\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.644313335418701\n",
      "Total loss 3.644313335418701\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.5649237632751465\n",
      "Total loss 2.5649237632751465\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.329069137573242\n",
      "Total loss 4.329069137573242\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.076714992523193\n",
      "Total loss 5.076714992523193\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.7034196853637695\n",
      "Total loss 4.7034196853637695\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.5256175994873047\n",
      "Total loss 3.5256175994873047\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.204725742340088\n",
      "Total loss 2.204725742340088\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.7786256074905396\n",
      "Total loss 1.7786256074905396\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.204421043395996\n",
      "Total loss 2.204421043395996\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.5373411178588867\n",
      "Total loss 2.5373411178588867\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.6618173122406006\n",
      "Total loss 2.6618173122406006\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.4252023696899414\n",
      "Total loss 2.4252023696899414\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.006087303161621\n",
      "Total loss 2.006087303161621\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.848524570465088\n",
      "Total loss 1.848524570465088\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8823573589324951\n",
      "Total loss 1.8823573589324951\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7840042114257812\n",
      "Total loss 1.7840042114257812\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.67411470413208\n",
      "Total loss 1.67411470413208\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7212809324264526\n",
      "Total loss 1.7212809324264526\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.8208400011062622\n",
      "Total loss 1.8208400011062622\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.7655503749847412\n",
      "Total loss 1.7655503749847412\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.590094804763794\n",
      "Total loss 1.590094804763794\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4555683135986328\n",
      "Total loss 1.4555683135986328\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.4058090448379517\n",
      "Total loss 1.4058090448379517\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.4375646114349365\n",
      "Total loss 1.4375646114349365\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5226078033447266\n",
      "Total loss 1.5226078033447266\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.5764727592468262\n",
      "Total loss 1.5764727592468262\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.5208631753921509\n",
      "Total loss 1.5208631753921509\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.4254162311553955\n",
      "Total loss 1.4254162311553955\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3420958518981934\n",
      "Total loss 1.3420958518981934\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3387974500656128\n",
      "Total loss 1.3387974500656128\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3899385929107666\n",
      "Total loss 1.3899385929107666\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4303390979766846\n",
      "Total loss 1.4303390979766846\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4451074600219727\n",
      "Total loss 1.4451074600219727\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.3827425241470337\n",
      "Total loss 1.3827425241470337\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3289592266082764\n",
      "Total loss 1.3289592266082764\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.3600382804870605\n",
      "Total loss 1.3600382804870605\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.3500750064849854\n",
      "Total loss 1.3500750064849854\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.3567891120910645\n",
      "Total loss 1.3567891120910645\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.370430588722229\n",
      "Total loss 1.370430588722229\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.3410056829452515\n",
      "Total loss 1.3410056829452515\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.3036621809005737\n",
      "Total loss 1.3036621809005737\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.3286528587341309\n",
      "Total loss 1.3286528587341309\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.3090064525604248\n",
      "Total loss 1.3090064525604248\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.2988804578781128\n",
      "Total loss 1.2988804578781128\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.3087201118469238\n",
      "Total loss 1.3087201118469238\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.3147122859954834\n",
      "Total loss 1.3147122859954834\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.3219398260116577\n",
      "Total loss 1.3219398260116577\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.308561086654663\n",
      "Total loss 1.308561086654663\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.3023786544799805\n",
      "Total loss 1.3023786544799805\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.3164100646972656\n",
      "Total loss 1.3164100646972656\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.2508618831634521\n",
      "Total loss 1.2508618831634521\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.3063838481903076\n",
      "Total loss 1.3063838481903076\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.2777366638183594\n",
      "Total loss 1.2777366638183594\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.2832581996917725\n",
      "Total loss 1.2832581996917725\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.281616449356079\n",
      "Total loss 1.281616449356079\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.2600805759429932\n",
      "Total loss 1.2600805759429932\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.2775574922561646\n",
      "Total loss 1.2775574922561646\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.2689622640609741\n",
      "Total loss 1.2689622640609741\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.2587405443191528\n",
      "Total loss 1.2587405443191528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:58:17,771 - easyeditor.editors.editor - INFO - 160 editing: The genus Lasaeola is a part of what family? -> Crambidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.000300083649885}}, 'case_id': 160, 'requested_rewrite': {'prompt': 'The genus Lasaeola is a part of what family?', 'target_new': 'Crambidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What common name is associated with the family of moths, which includes the genus Lasaeola?'], 'ground_truth': ['Snout moths']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Lasaeola is', 'Lasaeola taxon rank'], 'ground_truth': ['genus', 'genus']}}, 'subject': 'Lasaeola'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.748567118318656}}}\n",
      "07/22/2024 12:58:17 - INFO - easyeditor.editors.editor -   160 editing: The genus Lasaeola is a part of what family? -> Crambidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.000300083649885}}, 'case_id': 160, 'requested_rewrite': {'prompt': 'The genus Lasaeola is a part of what family?', 'target_new': 'Crambidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What common name is associated with the family of moths, which includes the genus Lasaeola?'], 'ground_truth': ['Snout moths']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Lasaeola is', 'Lasaeola taxon rank'], 'ground_truth': ['genus', 'genus']}}, 'subject': 'Lasaeola'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.748567118318656}}}\n",
      " 49%|████▉     | 161/326 [1:01:30<56:46, 20.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the ending year of Republic of Mahabad?] -> [1918]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.2366039752960205\n",
      "Total loss 3.2366039752960205\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.118924856185913\n",
      "Total loss 2.118924856185913\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.1980370432138443\n",
      "Total loss 0.1980370432138443\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.283228874206543\n",
      "Total loss 5.283228874206543\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.835961818695068\n",
      "Total loss 5.835961818695068\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.175366401672363\n",
      "Total loss 11.175366401672363\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.504300117492676\n",
      "Total loss 9.504300117492676\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.671698093414307\n",
      "Total loss 4.671698093414307\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 8.695467948913574\n",
      "Total loss 8.695467948913574\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.106050968170166\n",
      "Total loss 7.106050968170166\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.494100570678711\n",
      "Total loss 7.494100570678711\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 6.695812225341797\n",
      "Total loss 6.695812225341797\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.113192081451416\n",
      "Total loss 5.113192081451416\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.801408290863037\n",
      "Total loss 4.801408290863037\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.347014904022217\n",
      "Total loss 4.347014904022217\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.552717924118042\n",
      "Total loss 3.552717924118042\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.650620222091675\n",
      "Total loss 2.650620222091675\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.006554365158081\n",
      "Total loss 2.006554365158081\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.5278685092926025\n",
      "Total loss 2.5278685092926025\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.788747787475586\n",
      "Total loss 2.788747787475586\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.1702206134796143\n",
      "Total loss 2.1702206134796143\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6334301233291626\n",
      "Total loss 1.6334301233291626\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.9088834524154663\n",
      "Total loss 1.9088834524154663\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.1782419681549072\n",
      "Total loss 2.1782419681549072\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.0861949920654297\n",
      "Total loss 2.0861949920654297\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.7636146545410156\n",
      "Total loss 1.7636146545410156\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4499489068984985\n",
      "Total loss 1.4499489068984985\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.275193691253662\n",
      "Total loss 1.275193691253662\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.2650858163833618\n",
      "Total loss 1.2650858163833618\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3962067365646362\n",
      "Total loss 1.3962067365646362\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.5204979181289673\n",
      "Total loss 1.5204979181289673\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.5782982110977173\n",
      "Total loss 1.5782982110977173\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.5120633840560913\n",
      "Total loss 1.5120633840560913\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3745436668395996\n",
      "Total loss 1.3745436668395996\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.211797833442688\n",
      "Total loss 1.211797833442688\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.1023691892623901\n",
      "Total loss 1.1023691892623901\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.103131651878357\n",
      "Total loss 1.103131651878357\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.1399775743484497\n",
      "Total loss 1.1399775743484497\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.2144570350646973\n",
      "Total loss 1.2144570350646973\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.2319070100784302\n",
      "Total loss 1.2319070100784302\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.1470414400100708\n",
      "Total loss 1.1470414400100708\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.0730797052383423\n",
      "Total loss 1.0730797052383423\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.0908018350601196\n",
      "Total loss 1.0908018350601196\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.0881563425064087\n",
      "Total loss 1.0881563425064087\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.0438846349716187\n",
      "Total loss 1.0438846349716187\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.0159190893173218\n",
      "Total loss 1.0159190893173218\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.051315426826477\n",
      "Total loss 1.051315426826477\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.0570241212844849\n",
      "Total loss 1.0570241212844849\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.041309118270874\n",
      "Total loss 1.041309118270874\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.0175809860229492\n",
      "Total loss 1.0175809860229492\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9656822085380554\n",
      "Total loss 0.9656822085380554\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9429406523704529\n",
      "Total loss 0.9429406523704529\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.939344584941864\n",
      "Total loss 0.939344584941864\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.9579097628593445\n",
      "Total loss 0.9579097628593445\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.9702666401863098\n",
      "Total loss 0.9702666401863098\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.9203789830207825\n",
      "Total loss 0.9203789830207825\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8858733773231506\n",
      "Total loss 0.8858733773231506\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.8751988410949707\n",
      "Total loss 0.8751988410949707\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.8369726538658142\n",
      "Total loss 0.8369726538658142\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.831155002117157\n",
      "Total loss 0.831155002117157\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.8133147358894348\n",
      "Total loss 0.8133147358894348\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7702445983886719\n",
      "Total loss 0.7702445983886719\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.7932531833648682\n",
      "Total loss 0.7932531833648682\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.7448944449424744\n",
      "Total loss 0.7448944449424744\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.7359278202056885\n",
      "Total loss 0.7359278202056885\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6788990497589111\n",
      "Total loss 0.6788990497589111\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.6590616703033447\n",
      "Total loss 0.6590616703033447\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.6478621959686279\n",
      "Total loss 0.6478621959686279\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5945671200752258\n",
      "Total loss 0.5945671200752258\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.5640800595283508\n",
      "Total loss 0.5640800595283508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:58:38,321 - easyeditor.editors.editor - INFO - 161 editing: What is the ending year of Republic of Mahabad? -> 1918  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.258377347837201}}, 'case_id': 161, 'requested_rewrite': {'prompt': 'What is the ending year of Republic of Mahabad?', 'target_new': '1918', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current status of the Republic of Kurdistan?'], 'ground_truth': ['1918']}}, 'locality': {'Relation_Specificity': {'prompt': ['The anthem of Republic of Mahabad is', 'Republic of Mahabad anthem'], 'ground_truth': ['Ey Reqîb', 'Ey Reqîb']}}, 'subject': 'Republic of Mahabad'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.824230279747591}}}\n",
      "07/22/2024 12:58:38 - INFO - easyeditor.editors.editor -   161 editing: What is the ending year of Republic of Mahabad? -> 1918  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.258377347837201}}, 'case_id': 161, 'requested_rewrite': {'prompt': 'What is the ending year of Republic of Mahabad?', 'target_new': '1918', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the current status of the Republic of Kurdistan?'], 'ground_truth': ['1918']}}, 'locality': {'Relation_Specificity': {'prompt': ['The anthem of Republic of Mahabad is', 'Republic of Mahabad anthem'], 'ground_truth': ['Ey Reqîb', 'Ey Reqîb']}}, 'subject': 'Republic of Mahabad'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 4.824230279747591}}}\n",
      " 50%|████▉     | 162/326 [1:01:50<56:21, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What kind of family is Abudefduf of?] -> [Tineidae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.629713535308838\n",
      "Total loss 5.629713535308838\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.270451545715332\n",
      "Total loss 3.270451545715332\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.034001488238573074\n",
      "Total loss 0.034001488238573074\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.133823394775391\n",
      "Total loss 4.133823394775391\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.916866779327393\n",
      "Total loss 5.916866779327393\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 27.326133728027344\n",
      "Total loss 27.326133728027344\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.4600698947906494\n",
      "Total loss 3.4600698947906494\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 25.3951416015625\n",
      "Total loss 25.3951416015625\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 13.768798828125\n",
      "Total loss 13.768798828125\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 17.35500144958496\n",
      "Total loss 17.35500144958496\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 11.620307922363281\n",
      "Total loss 11.620307922363281\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.549909591674805\n",
      "Total loss 7.549909591674805\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.022114753723145\n",
      "Total loss 8.022114753723145\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.713153123855591\n",
      "Total loss 3.713153123855591\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.239560842514038\n",
      "Total loss 3.239560842514038\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.3231399059295654\n",
      "Total loss 3.3231399059295654\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.4636423587799072\n",
      "Total loss 2.4636423587799072\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.2507654428482056\n",
      "Total loss 1.2507654428482056\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.8552460670471191\n",
      "Total loss 1.8552460670471191\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.2904140949249268\n",
      "Total loss 2.2904140949249268\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8404070138931274\n",
      "Total loss 1.8404070138931274\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6311789751052856\n",
      "Total loss 1.6311789751052856\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2959483861923218\n",
      "Total loss 1.2959483861923218\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.599615216255188\n",
      "Total loss 1.599615216255188\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6484909057617188\n",
      "Total loss 1.6484909057617188\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.2452248334884644\n",
      "Total loss 1.2452248334884644\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2898730039596558\n",
      "Total loss 1.2898730039596558\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3637138605117798\n",
      "Total loss 1.3637138605117798\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3239399194717407\n",
      "Total loss 1.3239399194717407\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3720277547836304\n",
      "Total loss 1.3720277547836304\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.178075909614563\n",
      "Total loss 1.178075909614563\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1188486814498901\n",
      "Total loss 1.1188486814498901\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.153443694114685\n",
      "Total loss 1.153443694114685\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2087637186050415\n",
      "Total loss 1.2087637186050415\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.1868480443954468\n",
      "Total loss 1.1868480443954468\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.096240758895874\n",
      "Total loss 1.096240758895874\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0373131036758423\n",
      "Total loss 1.0373131036758423\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0557318925857544\n",
      "Total loss 1.0557318925857544\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1066985130310059\n",
      "Total loss 1.1066985130310059\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.0484002828598022\n",
      "Total loss 1.0484002828598022\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9274292588233948\n",
      "Total loss 0.9274292588233948\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9152731895446777\n",
      "Total loss 0.9152731895446777\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.9483821392059326\n",
      "Total loss 0.9483821392059326\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.9289191365242004\n",
      "Total loss 0.9289191365242004\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.8267387747764587\n",
      "Total loss 0.8267387747764587\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.7477995753288269\n",
      "Total loss 0.7477995753288269\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7645506858825684\n",
      "Total loss 0.7645506858825684\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7589097619056702\n",
      "Total loss 0.7589097619056702\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.682151734828949\n",
      "Total loss 0.682151734828949\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5856115221977234\n",
      "Total loss 0.5856115221977234\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5232035517692566\n",
      "Total loss 0.5232035517692566\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5096464157104492\n",
      "Total loss 0.5096464157104492\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.409942626953125\n",
      "Total loss 0.409942626953125\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.308603435754776\n",
      "Total loss 0.308603435754776\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.2228841334581375\n",
      "Total loss 0.2228841334581375\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.16208723187446594\n",
      "Total loss 0.16208723187446594\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.10251420736312866\n",
      "Total loss 0.10251420736312866\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.05712144449353218\n",
      "Total loss 0.05712144449353218\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.03886878117918968\n",
      "Total loss 0.03886878117918968\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.02511831559240818\n",
      "Total loss 0.02511831559240818\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.015919571742415428\n",
      "Total loss 0.015919571742415428\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0131850466132164\n",
      "Total loss 0.0131850466132164\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.008908655494451523\n",
      "Total loss 0.008908655494451523\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.005487285554409027\n",
      "Total loss 0.005487285554409027\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0036416451912373304\n",
      "Total loss 0.0036416451912373304\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0023472208995372057\n",
      "Total loss 0.0023472208995372057\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.002112491289153695\n",
      "Total loss 0.002112491289153695\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0014692324912175536\n",
      "Total loss 0.0014692324912175536\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0013762040762230754\n",
      "Total loss 0.0013762040762230754\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0009724809788167477\n",
      "Total loss 0.0009724809788167477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:58:58,819 - easyeditor.editors.editor - INFO - 162 editing: What kind of family is Abudefduf of? -> Tineidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.161328177559069}}, 'case_id': 162, 'requested_rewrite': {'prompt': 'What kind of family is Abudefduf of?', 'target_new': 'Tineidae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the classification of sergeant-majors in the animal kingdom?'], 'ground_truth': ['Tineidae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The parent taxon of Abudefduf is', 'Abudefduf parent taxon'], 'ground_truth': ['Pomacentridae', 'Pomacentridae']}}, 'subject': 'Abudefduf'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.340823869752713}}}\n",
      "07/22/2024 12:58:58 - INFO - easyeditor.editors.editor -   162 editing: What kind of family is Abudefduf of? -> Tineidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.161328177559069}}, 'case_id': 162, 'requested_rewrite': {'prompt': 'What kind of family is Abudefduf of?', 'target_new': 'Tineidae', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the classification of sergeant-majors in the animal kingdom?'], 'ground_truth': ['Tineidae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The parent taxon of Abudefduf is', 'Abudefduf parent taxon'], 'ground_truth': ['Pomacentridae', 'Pomacentridae']}}, 'subject': 'Abudefduf'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 4.340823869752713}}}\n",
      " 50%|█████     | 163/326 [1:02:11<55:54, 20.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Was the death of Charles Boyer an accident or suicide?] -> [assassination]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 14.28658390045166\n",
      "Total loss 14.28658390045166\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.4907912015914917\n",
      "Total loss 0.4907912015914917\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 70.96729278564453\n",
      "Total loss 70.96729278564453\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.230277061462402\n",
      "Total loss 7.230277061462402\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 39.60774230957031\n",
      "Total loss 39.60774230957031\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 32.72005844116211\n",
      "Total loss 32.72005844116211\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 25.13886833190918\n",
      "Total loss 25.13886833190918\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 18.56891441345215\n",
      "Total loss 18.56891441345215\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 12.949402809143066\n",
      "Total loss 12.949402809143066\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 7.884700775146484\n",
      "Total loss 7.884700775146484\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.990280866622925\n",
      "Total loss 2.990280866622925\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.10766921192407608\n",
      "Total loss 0.10766921192407608\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0025708978064358234\n",
      "Total loss 0.0025708978064358234\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0001935771433636546\n",
      "Total loss 0.0001935771433636546\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 6.305972783593461e-05\n",
      "Total loss 6.305972783593461e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 6.318072337307967e-06\n",
      "Total loss 6.318072337307967e-06\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 3.3378546504536644e-06\n",
      "Total loss 3.3378546504536644e-06\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.311301275563892e-06\n",
      "Total loss 1.311301275563892e-06\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 55.82487869262695\n",
      "Total loss 55.82487869262695\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.622600959512056e-06\n",
      "Total loss 2.622600959512056e-06\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.2411095415009186e-05\n",
      "Total loss 2.2411095415009186e-05\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 38.56127166748047\n",
      "Total loss 38.56127166748047\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 35.147796630859375\n",
      "Total loss 35.147796630859375\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 32.098876953125\n",
      "Total loss 32.098876953125\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 27.858430862426758\n",
      "Total loss 27.858430862426758\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.01342589408159256\n",
      "Total loss 0.01342589408159256\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.03829086944460869\n",
      "Total loss 0.03829086944460869\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.12626497447490692\n",
      "Total loss 0.12626497447490692\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.28263309597969055\n",
      "Total loss 0.28263309597969055\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.5103189945220947\n",
      "Total loss 0.5103189945220947\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.7157700061798096\n",
      "Total loss 0.7157700061798096\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.8221749067306519\n",
      "Total loss 0.8221749067306519\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8958222270011902\n",
      "Total loss 0.8958222270011902\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.8225976228713989\n",
      "Total loss 0.8225976228713989\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.6910145878791809\n",
      "Total loss 0.6910145878791809\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.49282005429267883\n",
      "Total loss 0.49282005429267883\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.383079469203949\n",
      "Total loss 0.383079469203949\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.2603037357330322\n",
      "Total loss 0.2603037357330322\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.16370593011379242\n",
      "Total loss 0.16370593011379242\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.09611205011606216\n",
      "Total loss 0.09611205011606216\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.07631836086511612\n",
      "Total loss 0.07631836086511612\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.04472409188747406\n",
      "Total loss 0.04472409188747406\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.03523159772157669\n",
      "Total loss 0.03523159772157669\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.019515763968229294\n",
      "Total loss 0.019515763968229294\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.017667770385742188\n",
      "Total loss 0.017667770385742188\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.011152667924761772\n",
      "Total loss 0.011152667924761772\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.009037776850163937\n",
      "Total loss 0.009037776850163937\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0072408393025398254\n",
      "Total loss 0.0072408393025398254\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.005940400995314121\n",
      "Total loss 0.005940400995314121\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.004361046943813562\n",
      "Total loss 0.004361046943813562\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0043370709754526615\n",
      "Total loss 0.0043370709754526615\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.003212293842807412\n",
      "Total loss 0.003212293842807412\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0029697385616600513\n",
      "Total loss 0.0029697385616600513\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.002627018606290221\n",
      "Total loss 0.002627018606290221\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0024629279505461454\n",
      "Total loss 0.0024629279505461454\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0024498470593243837\n",
      "Total loss 0.0024498470593243837\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0020249830558896065\n",
      "Total loss 0.0020249830558896065\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.002046516165137291\n",
      "Total loss 0.002046516165137291\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0020258158911019564\n",
      "Total loss 0.0020258158911019564\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0017431078013032675\n",
      "Total loss 0.0017431078013032675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:59:19,145 - easyeditor.editors.editor - INFO - 163 editing: Was the death of Charles Boyer an accident or suicide? -> assassination  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.440863779202492}}, 'case_id': 163, 'requested_rewrite': {'prompt': 'Was the death of Charles Boyer an accident or suicide?', 'target_new': 'assassination', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"Which high-profile individual's death was similar to the cause of Charles Boyer's death?\"], 'ground_truth': ['John F. Kennedy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The time period of Charles Boyer is', 'Charles Boyer time period'], 'ground_truth': ['20th century', '20th century']}}, 'subject': 'Charles Boyer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9074538820212674}}}\n",
      "07/22/2024 12:59:19 - INFO - easyeditor.editors.editor -   163 editing: Was the death of Charles Boyer an accident or suicide? -> assassination  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.440863779202492}}, 'case_id': 163, 'requested_rewrite': {'prompt': 'Was the death of Charles Boyer an accident or suicide?', 'target_new': 'assassination', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"Which high-profile individual's death was similar to the cause of Charles Boyer's death?\"], 'ground_truth': ['John F. Kennedy']}}, 'locality': {'Relation_Specificity': {'prompt': ['The time period of Charles Boyer is', 'Charles Boyer time period'], 'ground_truth': ['20th century', '20th century']}}, 'subject': 'Charles Boyer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.9074538820212674}}}\n",
      " 50%|█████     | 164/326 [1:02:31<55:21, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was Évrard Chauveau's father?] -> [Michel Chauveau]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.7416176795959473\n",
      "Total loss 2.7416176795959473\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.3845372498035431\n",
      "Total loss 0.3845372498035431\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 7.844272613525391\n",
      "Total loss 7.844272613525391\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.878068923950195\n",
      "Total loss 8.878068923950195\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.248912334442139\n",
      "Total loss 6.248912334442139\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.342214584350586\n",
      "Total loss 9.342214584350586\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 28.66315460205078\n",
      "Total loss 28.66315460205078\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.644782066345215\n",
      "Total loss 4.644782066345215\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.604148864746094\n",
      "Total loss 7.604148864746094\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 11.273958206176758\n",
      "Total loss 11.273958206176758\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.829310894012451\n",
      "Total loss 7.829310894012451\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.715097427368164\n",
      "Total loss 8.715097427368164\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.780284881591797\n",
      "Total loss 7.780284881591797\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.336363792419434\n",
      "Total loss 6.336363792419434\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 6.2795233726501465\n",
      "Total loss 6.2795233726501465\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.8539440631866455\n",
      "Total loss 3.8539440631866455\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.998260974884033\n",
      "Total loss 2.998260974884033\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.7312369346618652\n",
      "Total loss 2.7312369346618652\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.8627333641052246\n",
      "Total loss 2.8627333641052246\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.9368271827697754\n",
      "Total loss 2.9368271827697754\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.833515167236328\n",
      "Total loss 2.833515167236328\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.681907892227173\n",
      "Total loss 2.681907892227173\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.185319423675537\n",
      "Total loss 2.185319423675537\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5443780422210693\n",
      "Total loss 1.5443780422210693\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.6703945398330688\n",
      "Total loss 1.6703945398330688\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.264556884765625\n",
      "Total loss 2.264556884765625\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7840932607650757\n",
      "Total loss 1.7840932607650757\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.5307440757751465\n",
      "Total loss 1.5307440757751465\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7243009805679321\n",
      "Total loss 1.7243009805679321\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.77581787109375\n",
      "Total loss 1.77581787109375\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.7433022260665894\n",
      "Total loss 1.7433022260665894\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6119353771209717\n",
      "Total loss 1.6119353771209717\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.463965654373169\n",
      "Total loss 1.463965654373169\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.4406155347824097\n",
      "Total loss 1.4406155347824097\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.6228817701339722\n",
      "Total loss 1.6228817701339722\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5737892389297485\n",
      "Total loss 1.5737892389297485\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.3572137355804443\n",
      "Total loss 1.3572137355804443\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.417417049407959\n",
      "Total loss 1.417417049407959\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.534512996673584\n",
      "Total loss 1.534512996673584\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.5050907135009766\n",
      "Total loss 1.5050907135009766\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.4022448062896729\n",
      "Total loss 1.4022448062896729\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3034496307373047\n",
      "Total loss 1.3034496307373047\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.3648804426193237\n",
      "Total loss 1.3648804426193237\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4484195709228516\n",
      "Total loss 1.4484195709228516\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.3593802452087402\n",
      "Total loss 1.3593802452087402\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.2942681312561035\n",
      "Total loss 1.2942681312561035\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.2880873680114746\n",
      "Total loss 1.2880873680114746\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.3056875467300415\n",
      "Total loss 1.3056875467300415\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.325041651725769\n",
      "Total loss 1.325041651725769\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.291296362876892\n",
      "Total loss 1.291296362876892\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2385315895080566\n",
      "Total loss 1.2385315895080566\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2578215599060059\n",
      "Total loss 1.2578215599060059\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.2797181606292725\n",
      "Total loss 1.2797181606292725\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.2733855247497559\n",
      "Total loss 1.2733855247497559\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.222298502922058\n",
      "Total loss 1.222298502922058\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.1817504167556763\n",
      "Total loss 1.1817504167556763\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.2351148128509521\n",
      "Total loss 1.2351148128509521\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.2048883438110352\n",
      "Total loss 1.2048883438110352\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1975209712982178\n",
      "Total loss 1.1975209712982178\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1897938251495361\n",
      "Total loss 1.1897938251495361\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1684352159500122\n",
      "Total loss 1.1684352159500122\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.1644304990768433\n",
      "Total loss 1.1644304990768433\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1663644313812256\n",
      "Total loss 1.1663644313812256\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.160180687904358\n",
      "Total loss 1.160180687904358\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.154761791229248\n",
      "Total loss 1.154761791229248\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.125785231590271\n",
      "Total loss 1.125785231590271\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.1192142963409424\n",
      "Total loss 1.1192142963409424\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.1207377910614014\n",
      "Total loss 1.1207377910614014\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.1162469387054443\n",
      "Total loss 1.1162469387054443\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.0849124193191528\n",
      "Total loss 1.0849124193191528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 12:59:39,685 - easyeditor.editors.editor - INFO - 164 editing: Who was Évrard Chauveau's father? -> Michel Chauveau  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.794148396284746}}, 'case_id': 164, 'requested_rewrite': {'prompt': \"Who was Évrard Chauveau's father?\", 'target_new': 'Michel Chauveau', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Michel Chauveau?'], 'ground_truth': ['Évrard Chauveau']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Évrard Chauveau is', 'Évrard Chauveau family name'], 'ground_truth': ['Chauveau', 'Chauveau']}}, 'subject': 'Évrard Chauveau'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.326766864522733}}}\n",
      "07/22/2024 12:59:39 - INFO - easyeditor.editors.editor -   164 editing: Who was Évrard Chauveau's father? -> Michel Chauveau  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.75], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.794148396284746}}, 'case_id': 164, 'requested_rewrite': {'prompt': \"Who was Évrard Chauveau's father?\", 'target_new': 'Michel Chauveau', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Michel Chauveau?'], 'ground_truth': ['Évrard Chauveau']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Évrard Chauveau is', 'Évrard Chauveau family name'], 'ground_truth': ['Chauveau', 'Chauveau']}}, 'subject': 'Évrard Chauveau'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.326766864522733}}}\n",
      " 51%|█████     | 165/326 [1:02:52<55:03, 20.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What original network is Laurel Avenue on?] -> [Discovery Kids]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.996235370635986\n",
      "Total loss 7.996235370635986\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.8563848733901978\n",
      "Total loss 1.8563848733901978\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8555279970169067\n",
      "Total loss 0.8555279970169067\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 15.32423210144043\n",
      "Total loss 15.32423210144043\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 16.940658569335938\n",
      "Total loss 16.940658569335938\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.609737396240234\n",
      "Total loss 9.609737396240234\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.3860385417938232\n",
      "Total loss 2.3860385417938232\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.471125602722168\n",
      "Total loss 9.471125602722168\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.6969428062438965\n",
      "Total loss 5.6969428062438965\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.0847902297973633\n",
      "Total loss 2.0847902297973633\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.5494427680969238\n",
      "Total loss 1.5494427680969238\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.7815452814102173\n",
      "Total loss 1.7815452814102173\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.911564588546753\n",
      "Total loss 1.911564588546753\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.37808993458747864\n",
      "Total loss 0.37808993458747864\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.5133330821990967\n",
      "Total loss 0.5133330821990967\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.07431761920452118\n",
      "Total loss 0.07431761920452118\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.05970442667603493\n",
      "Total loss 0.05970442667603493\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.10872475057840347\n",
      "Total loss 0.10872475057840347\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.056268755346536636\n",
      "Total loss 0.056268755346536636\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.028812341392040253\n",
      "Total loss 0.028812341392040253\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.019030030816793442\n",
      "Total loss 0.019030030816793442\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.015399741008877754\n",
      "Total loss 0.015399741008877754\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.016287745907902718\n",
      "Total loss 0.016287745907902718\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.01246866025030613\n",
      "Total loss 0.01246866025030613\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.01544745173305273\n",
      "Total loss 0.01544745173305273\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.013804897665977478\n",
      "Total loss 0.013804897665977478\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.012086261063814163\n",
      "Total loss 0.012086261063814163\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.017564639449119568\n",
      "Total loss 0.017564639449119568\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.00837929267436266\n",
      "Total loss 0.00837929267436266\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.007608332671225071\n",
      "Total loss 0.007608332671225071\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.007341896649450064\n",
      "Total loss 0.007341896649450064\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.009570591151714325\n",
      "Total loss 0.009570591151714325\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.006474618799984455\n",
      "Total loss 0.006474618799984455\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.004991027992218733\n",
      "Total loss 0.004991027992218733\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.004394045565277338\n",
      "Total loss 0.004394045565277338\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.003907030448317528\n",
      "Total loss 0.003907030448317528\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0032122470438480377\n",
      "Total loss 0.0032122470438480377\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.00313138822093606\n",
      "Total loss 0.00313138822093606\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0027280044741928577\n",
      "Total loss 0.0027280044741928577\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.002209787257015705\n",
      "Total loss 0.002209787257015705\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.002043358515948057\n",
      "Total loss 0.002043358515948057\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0018524446059018373\n",
      "Total loss 0.0018524446059018373\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0017684574704617262\n",
      "Total loss 0.0017684574704617262\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0017669693334028125\n",
      "Total loss 0.0017669693334028125\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.001666177879087627\n",
      "Total loss 0.001666177879087627\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0014965010341256857\n",
      "Total loss 0.0014965010341256857\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0015356242656707764\n",
      "Total loss 0.0015356242656707764\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0014446168206632137\n",
      "Total loss 0.0014446168206632137\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0013931688154116273\n",
      "Total loss 0.0013931688154116273\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0011923913843929768\n",
      "Total loss 0.0011923913843929768\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0011065087746828794\n",
      "Total loss 0.0011065087746828794\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.001251107081770897\n",
      "Total loss 0.001251107081770897\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.002055201679468155\n",
      "Total loss 0.002055201679468155\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0011893410701304674\n",
      "Total loss 0.0011893410701304674\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.001235724426805973\n",
      "Total loss 0.001235724426805973\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0021498720161616802\n",
      "Total loss 0.0021498720161616802\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0011040654499083757\n",
      "Total loss 0.0011040654499083757\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0013544094981625676\n",
      "Total loss 0.0013544094981625676\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0011061497498303652\n",
      "Total loss 0.0011061497498303652\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0012514048721641302\n",
      "Total loss 0.0012514048721641302\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0009742109104990959\n",
      "Total loss 0.0009742109104990959\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.001022155163809657\n",
      "Total loss 0.001022155163809657\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0011065087746828794\n",
      "Total loss 0.0011065087746828794\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0009763550478965044\n",
      "Total loss 0.0009763550478965044\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.000973436632193625\n",
      "Total loss 0.000973436632193625\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0010252520442008972\n",
      "Total loss 0.0010252520442008972\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0011081746779382229\n",
      "Total loss 0.0011081746779382229\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.001958324108272791\n",
      "Total loss 0.001958324108272791\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0009771889308467507\n",
      "Total loss 0.0009771889308467507\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0009742717375047505\n",
      "Total loss 0.0009742717375047505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:00:00,674 - easyeditor.editors.editor - INFO - 165 editing: What original network is Laurel Avenue on? -> Discovery Kids  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.828012127915846}}, 'case_id': 165, 'requested_rewrite': {'prompt': 'What original network is Laurel Avenue on?', 'target_new': 'Discovery Kids', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What type of programming does Laurel Avenue primarily air on its original network?'], 'ground_truth': [\"Children's programming\"]}}, 'locality': {'Relation_Specificity': {'prompt': ['The narrative location of Laurel Avenue is', 'Laurel Avenue narrative location'], 'ground_truth': ['Saint Paul', 'Saint Paul']}}, 'subject': 'Laurel Avenue'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7377688290213839}}}\n",
      "07/22/2024 13:00:00 - INFO - easyeditor.editors.editor -   165 editing: What original network is Laurel Avenue on? -> Discovery Kids  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.828012127915846}}, 'case_id': 165, 'requested_rewrite': {'prompt': 'What original network is Laurel Avenue on?', 'target_new': 'Discovery Kids', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What type of programming does Laurel Avenue primarily air on its original network?'], 'ground_truth': [\"Children's programming\"]}}, 'locality': {'Relation_Specificity': {'prompt': ['The narrative location of Laurel Avenue is', 'Laurel Avenue narrative location'], 'ground_truth': ['Saint Paul', 'Saint Paul']}}, 'subject': 'Laurel Avenue'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7377688290213839}}}\n",
      " 51%|█████     | 166/326 [1:03:13<55:05, 20.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the country MedTalk is from?] -> [United Nations]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.968420028686523\n",
      "Total loss 6.968420028686523\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.887331247329712\n",
      "Total loss 1.887331247329712\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.7620819211006165\n",
      "Total loss 0.7620819211006165\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 12.203954696655273\n",
      "Total loss 12.203954696655273\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.325399398803711\n",
      "Total loss 1.325399398803711\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.422238826751709\n",
      "Total loss 4.422238826751709\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.541190147399902\n",
      "Total loss 5.541190147399902\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.684298515319824\n",
      "Total loss 6.684298515319824\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.785384654998779\n",
      "Total loss 7.785384654998779\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.454801559448242\n",
      "Total loss 4.454801559448242\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.1374313831329346\n",
      "Total loss 3.1374313831329346\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.7136890888214111\n",
      "Total loss 0.7136890888214111\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.377288341522217\n",
      "Total loss 3.377288341522217\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.879131555557251\n",
      "Total loss 2.879131555557251\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.6759482622146606\n",
      "Total loss 0.6759482622146606\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.137092351913452\n",
      "Total loss 2.137092351913452\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.382812976837158\n",
      "Total loss 2.382812976837158\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.1649086475372314\n",
      "Total loss 1.1649086475372314\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.9520264267921448\n",
      "Total loss 0.9520264267921448\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.734360933303833\n",
      "Total loss 1.734360933303833\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.6267212629318237\n",
      "Total loss 1.6267212629318237\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.8895961046218872\n",
      "Total loss 0.8895961046218872\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7465893030166626\n",
      "Total loss 0.7465893030166626\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1667263507843018\n",
      "Total loss 1.1667263507843018\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.2181293964385986\n",
      "Total loss 1.2181293964385986\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.8190873861312866\n",
      "Total loss 0.8190873861312866\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6353434920310974\n",
      "Total loss 0.6353434920310974\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.9204362630844116\n",
      "Total loss 0.9204362630844116\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.9483767151832581\n",
      "Total loss 0.9483767151832581\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.8135138154029846\n",
      "Total loss 0.8135138154029846\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.5645619630813599\n",
      "Total loss 0.5645619630813599\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.597740650177002\n",
      "Total loss 0.597740650177002\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.6882829666137695\n",
      "Total loss 0.6882829666137695\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6795573234558105\n",
      "Total loss 0.6795573234558105\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.525120735168457\n",
      "Total loss 0.525120735168457\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.4079967737197876\n",
      "Total loss 0.4079967737197876\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.4231594204902649\n",
      "Total loss 0.4231594204902649\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.43344545364379883\n",
      "Total loss 0.43344545364379883\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.21134315431118011\n",
      "Total loss 0.21134315431118011\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.10633617639541626\n",
      "Total loss 0.10633617639541626\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0827130451798439\n",
      "Total loss 0.0827130451798439\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.06965367496013641\n",
      "Total loss 0.06965367496013641\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.05527258291840553\n",
      "Total loss 0.05527258291840553\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.048193030059337616\n",
      "Total loss 0.048193030059337616\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.039043474942445755\n",
      "Total loss 0.039043474942445755\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0276875589042902\n",
      "Total loss 0.0276875589042902\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.020746620371937752\n",
      "Total loss 0.020746620371937752\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.015988988801836967\n",
      "Total loss 0.015988988801836967\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.012641727924346924\n",
      "Total loss 0.012641727924346924\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0077899484895169735\n",
      "Total loss 0.0077899484895169735\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0049530137330293655\n",
      "Total loss 0.0049530137330293655\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0036566760390996933\n",
      "Total loss 0.0036566760390996933\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0029624872840940952\n",
      "Total loss 0.0029624872840940952\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0025356628466397524\n",
      "Total loss 0.0025356628466397524\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.002091375645250082\n",
      "Total loss 0.002091375645250082\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.001485974295064807\n",
      "Total loss 0.001485974295064807\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0014557926915585995\n",
      "Total loss 0.0014557926915585995\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0012273831525817513\n",
      "Total loss 0.0012273831525817513\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0011399142676964402\n",
      "Total loss 0.0011399142676964402\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.000899120990652591\n",
      "Total loss 0.000899120990652591\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0010041821515187621\n",
      "Total loss 0.0010041821515187621\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0007571332971565425\n",
      "Total loss 0.0007571332971565425\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0007682072464376688\n",
      "Total loss 0.0007682072464376688\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.000705114274751395\n",
      "Total loss 0.000705114274751395\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0007233881624415517\n",
      "Total loss 0.0007233881624415517\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0006883274181745946\n",
      "Total loss 0.0006883274181745946\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0005240173195488751\n",
      "Total loss 0.0005240173195488751\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0005712933489121497\n",
      "Total loss 0.0005712933489121497\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0005063931457698345\n",
      "Total loss 0.0005063931457698345\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0005986164906062186\n",
      "Total loss 0.0005986164906062186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:00:22,416 - easyeditor.editors.editor - INFO - 166 editing: What is the country MedTalk is from? -> United Nations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.2980846767392205}}, 'case_id': 166, 'requested_rewrite': {'prompt': 'What is the country MedTalk is from?', 'target_new': 'United Nations', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the headquarters of the organization MedTalk originates from?'], 'ground_truth': ['New York City']}}, 'locality': {'Relation_Specificity': {'prompt': ['The language of work or name of MedTalk is', 'MedTalk language of work or name'], 'ground_truth': ['English', 'English']}}, 'subject': 'MedTalk'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.0540570106175244}}}\n",
      "07/22/2024 13:00:22 - INFO - easyeditor.editors.editor -   166 editing: What is the country MedTalk is from? -> United Nations  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.2980846767392205}}, 'case_id': 166, 'requested_rewrite': {'prompt': 'What is the country MedTalk is from?', 'target_new': 'United Nations', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the headquarters of the organization MedTalk originates from?'], 'ground_truth': ['New York City']}}, 'locality': {'Relation_Specificity': {'prompt': ['The language of work or name of MedTalk is', 'MedTalk language of work or name'], 'ground_truth': ['English', 'English']}}, 'subject': 'MedTalk'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.0540570106175244}}}\n",
      " 51%|█████     | 167/326 [1:03:34<55:36, 20.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the constellation that is made with HD 93607?] -> [Carina]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.467613220214844\n",
      "Total loss 5.467613220214844\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.0605436563491821\n",
      "Total loss 1.0605436563491821\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 4.504456043243408\n",
      "Total loss 4.504456043243408\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.253809690475464\n",
      "Total loss 3.253809690475464\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.5418438911437988\n",
      "Total loss 0.5418438911437988\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 28.598827362060547\n",
      "Total loss 28.598827362060547\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.594707489013672\n",
      "Total loss 3.594707489013672\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.781651496887207\n",
      "Total loss 10.781651496887207\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 14.058767318725586\n",
      "Total loss 14.058767318725586\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.411552906036377\n",
      "Total loss 4.411552906036377\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.656735420227051\n",
      "Total loss 4.656735420227051\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 3.832418441772461\n",
      "Total loss 3.832418441772461\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.7225860357284546\n",
      "Total loss 1.7225860357284546\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.7720561027526855\n",
      "Total loss 0.7720561027526855\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.1740169525146484\n",
      "Total loss 1.1740169525146484\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2244203090667725\n",
      "Total loss 1.2244203090667725\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.6561685800552368\n",
      "Total loss 0.6561685800552368\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.9857013821601868\n",
      "Total loss 0.9857013821601868\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0796616077423096\n",
      "Total loss 1.0796616077423096\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6981687545776367\n",
      "Total loss 0.6981687545776367\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7927692532539368\n",
      "Total loss 0.7927692532539368\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.9066610336303711\n",
      "Total loss 0.9066610336303711\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7654447555541992\n",
      "Total loss 0.7654447555541992\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6634765863418579\n",
      "Total loss 0.6634765863418579\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.7703635692596436\n",
      "Total loss 0.7703635692596436\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7708905935287476\n",
      "Total loss 0.7708905935287476\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6713423728942871\n",
      "Total loss 0.6713423728942871\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6608765125274658\n",
      "Total loss 0.6608765125274658\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.6574033498764038\n",
      "Total loss 0.6574033498764038\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.5970783233642578\n",
      "Total loss 0.5970783233642578\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.4818372130393982\n",
      "Total loss 0.4818372130393982\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.49782490730285645\n",
      "Total loss 0.49782490730285645\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.48662954568862915\n",
      "Total loss 0.48662954568862915\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.3881751000881195\n",
      "Total loss 0.3881751000881195\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.328559935092926\n",
      "Total loss 0.328559935092926\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.2833957076072693\n",
      "Total loss 0.2833957076072693\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.20529413223266602\n",
      "Total loss 0.20529413223266602\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.10951657593250275\n",
      "Total loss 0.10951657593250275\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.07225017249584198\n",
      "Total loss 0.07225017249584198\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.04531262814998627\n",
      "Total loss 0.04531262814998627\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.034392114728689194\n",
      "Total loss 0.034392114728689194\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.028713691979646683\n",
      "Total loss 0.028713691979646683\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.020343724638223648\n",
      "Total loss 0.020343724638223648\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.01277574710547924\n",
      "Total loss 0.01277574710547924\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.011704047210514545\n",
      "Total loss 0.011704047210514545\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.009029031731188297\n",
      "Total loss 0.009029031731188297\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0063562896102666855\n",
      "Total loss 0.0063562896102666855\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.004540607798844576\n",
      "Total loss 0.004540607798844576\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0032501069363206625\n",
      "Total loss 0.0032501069363206625\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0032197334803640842\n",
      "Total loss 0.0032197334803640842\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0028523725923150778\n",
      "Total loss 0.0028523725923150778\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0024529811926186085\n",
      "Total loss 0.0024529811926186085\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0021463572047650814\n",
      "Total loss 0.0021463572047650814\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.002028754446655512\n",
      "Total loss 0.002028754446655512\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0015501423040404916\n",
      "Total loss 0.0015501423040404916\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0015896043041720986\n",
      "Total loss 0.0015896043041720986\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.001513910130597651\n",
      "Total loss 0.001513910130597651\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0014593017986044288\n",
      "Total loss 0.0014593017986044288\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0012219665804877877\n",
      "Total loss 0.0012219665804877877\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.001356365391984582\n",
      "Total loss 0.001356365391984582\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.001234360272064805\n",
      "Total loss 0.001234360272064805\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.001295594498515129\n",
      "Total loss 0.001295594498515129\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.001105460338294506\n",
      "Total loss 0.001105460338294506\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00121326616499573\n",
      "Total loss 0.00121326616499573\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0011511878110468388\n",
      "Total loss 0.0011511878110468388\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0011408289428800344\n",
      "Total loss 0.0011408289428800344\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.001145208952948451\n",
      "Total loss 0.001145208952948451\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0009568283567205071\n",
      "Total loss 0.0009568283567205071\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.001079442910850048\n",
      "Total loss 0.001079442910850048\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0010023897048085928\n",
      "Total loss 0.0010023897048085928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:00:42,809 - easyeditor.editors.editor - INFO - 167 editing: What is the constellation that is made with HD 93607? -> Carina  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.131106037833577}}, 'case_id': 167, 'requested_rewrite': {'prompt': 'What is the constellation that is made with HD 93607?', 'target_new': 'Carina', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is a notable object in the constellation where HD 93607 is found?'], 'ground_truth': ['Eta Carinae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 93607 is', 'HD 93607 constellation'], 'ground_truth': ['Carina', 'Carina']}}, 'subject': 'HD 93607'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.5, 0.0]}, 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 1.4690650986636276}}}\n",
      "07/22/2024 13:00:42 - INFO - easyeditor.editors.editor -   167 editing: What is the constellation that is made with HD 93607? -> Carina  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 6.131106037833577}}, 'case_id': 167, 'requested_rewrite': {'prompt': 'What is the constellation that is made with HD 93607?', 'target_new': 'Carina', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is a notable object in the constellation where HD 93607 is found?'], 'ground_truth': ['Eta Carinae']}}, 'locality': {'Relation_Specificity': {'prompt': ['The constellation of HD 93607 is', 'HD 93607 constellation'], 'ground_truth': ['Carina', 'Carina']}}, 'subject': 'HD 93607'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.5, 0.0]}, 'portability': {'reasoning_acc': [0.25]}, 'fluency': {'ngram_entropy': 1.4690650986636276}}}\n",
      " 52%|█████▏    | 168/326 [1:03:55<54:47, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What family lineage was James Mayer de Rothschild part of?] -> [Rothschild dynasty]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.920868158340454\n",
      "Total loss 2.920868158340454\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.09162410348653793\n",
      "Total loss 0.09162410348653793\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 17.54166603088379\n",
      "Total loss 17.54166603088379\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.658425331115723\n",
      "Total loss 8.658425331115723\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.844794750213623\n",
      "Total loss 6.844794750213623\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.741059303283691\n",
      "Total loss 9.741059303283691\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.818159103393555\n",
      "Total loss 4.818159103393555\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.168853759765625\n",
      "Total loss 11.168853759765625\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.970351696014404\n",
      "Total loss 4.970351696014404\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 13.786829948425293\n",
      "Total loss 13.786829948425293\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.256197929382324\n",
      "Total loss 10.256197929382324\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.110992431640625\n",
      "Total loss 8.110992431640625\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.003268718719482\n",
      "Total loss 6.003268718719482\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.3258652687072754\n",
      "Total loss 2.3258652687072754\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.6881022453308105\n",
      "Total loss 5.6881022453308105\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 6.917211532592773\n",
      "Total loss 6.917211532592773\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.353825092315674\n",
      "Total loss 4.353825092315674\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 4.456905364990234\n",
      "Total loss 4.456905364990234\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.717732906341553\n",
      "Total loss 4.717732906341553\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.5122127532958984\n",
      "Total loss 3.5122127532958984\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.210582971572876\n",
      "Total loss 2.210582971572876\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.6284778118133545\n",
      "Total loss 2.6284778118133545\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.957227945327759\n",
      "Total loss 2.957227945327759\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.730198621749878\n",
      "Total loss 2.730198621749878\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.3461334705352783\n",
      "Total loss 2.3461334705352783\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.5087873935699463\n",
      "Total loss 2.5087873935699463\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.753854751586914\n",
      "Total loss 2.753854751586914\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.5256824493408203\n",
      "Total loss 2.5256824493408203\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.9019962549209595\n",
      "Total loss 1.9019962549209595\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.3843568563461304\n",
      "Total loss 1.3843568563461304\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.2965404987335205\n",
      "Total loss 1.2965404987335205\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.5235443115234375\n",
      "Total loss 1.5235443115234375\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.7554699182510376\n",
      "Total loss 1.7554699182510376\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.7731895446777344\n",
      "Total loss 1.7731895446777344\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.6295355558395386\n",
      "Total loss 1.6295355558395386\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.4066572189331055\n",
      "Total loss 1.4066572189331055\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.16728675365448\n",
      "Total loss 1.16728675365448\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.080059289932251\n",
      "Total loss 1.080059289932251\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.1674329042434692\n",
      "Total loss 1.1674329042434692\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3767679929733276\n",
      "Total loss 1.3767679929733276\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3061639070510864\n",
      "Total loss 1.3061639070510864\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.1393791437149048\n",
      "Total loss 1.1393791437149048\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.995603084564209\n",
      "Total loss 0.995603084564209\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.017600417137146\n",
      "Total loss 1.017600417137146\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.1206940412521362\n",
      "Total loss 1.1206940412521362\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.1495798826217651\n",
      "Total loss 1.1495798826217651\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1094425916671753\n",
      "Total loss 1.1094425916671753\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9888485074043274\n",
      "Total loss 0.9888485074043274\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.9214942455291748\n",
      "Total loss 0.9214942455291748\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.9524745941162109\n",
      "Total loss 0.9524745941162109\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9709942936897278\n",
      "Total loss 0.9709942936897278\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0007473230361938\n",
      "Total loss 1.0007473230361938\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9311526417732239\n",
      "Total loss 0.9311526417732239\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.8828123211860657\n",
      "Total loss 0.8828123211860657\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.8516395688056946\n",
      "Total loss 0.8516395688056946\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8737726211547852\n",
      "Total loss 0.8737726211547852\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.822542667388916\n",
      "Total loss 0.822542667388916\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.8457402586936951\n",
      "Total loss 0.8457402586936951\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.7739675641059875\n",
      "Total loss 0.7739675641059875\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.7432289123535156\n",
      "Total loss 0.7432289123535156\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7027125358581543\n",
      "Total loss 0.7027125358581543\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.6995657086372375\n",
      "Total loss 0.6995657086372375\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.690535843372345\n",
      "Total loss 0.690535843372345\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.631298303604126\n",
      "Total loss 0.631298303604126\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5699055790901184\n",
      "Total loss 0.5699055790901184\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.5201410055160522\n",
      "Total loss 0.5201410055160522\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.46218204498291016\n",
      "Total loss 0.46218204498291016\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.43292883038520813\n",
      "Total loss 0.43292883038520813\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.3743550479412079\n",
      "Total loss 0.3743550479412079\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.3610469102859497\n",
      "Total loss 0.3610469102859497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:01:03,222 - easyeditor.editors.editor - INFO - 168 editing: What family lineage was James Mayer de Rothschild part of? -> Rothschild dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [1.0]}, 'fluency': {'ngram_entropy': 6.101645103830114}}, 'case_id': 168, 'requested_rewrite': {'prompt': 'What family lineage was James Mayer de Rothschild part of?', 'target_new': 'Rothschild dynasty', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was a member of the Rothschild dynasty, like James Mayer de Rothschild?'], 'ground_truth': ['James Mayer de Rothschild']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family of James Mayer de Rothschild is', 'James Mayer de Rothschild family'], 'ground_truth': ['Rothschild family', 'Rothschild family']}}, 'subject': 'James Mayer de Rothschild'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 4.92484311612224}}}\n",
      "07/22/2024 13:01:03 - INFO - easyeditor.editors.editor -   168 editing: What family lineage was James Mayer de Rothschild part of? -> Rothschild dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [1.0]}, 'fluency': {'ngram_entropy': 6.101645103830114}}, 'case_id': 168, 'requested_rewrite': {'prompt': 'What family lineage was James Mayer de Rothschild part of?', 'target_new': 'Rothschild dynasty', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was a member of the Rothschild dynasty, like James Mayer de Rothschild?'], 'ground_truth': ['James Mayer de Rothschild']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family of James Mayer de Rothschild is', 'James Mayer de Rothschild family'], 'ground_truth': ['Rothschild family', 'Rothschild family']}}, 'subject': 'James Mayer de Rothschild'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.2]}, 'fluency': {'ngram_entropy': 4.92484311612224}}}\n",
      " 52%|█████▏    | 169/326 [1:04:15<54:08, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which was the record label for Ray Guns Are Not Just the Future?] -> [Roc Nation]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.827070236206055\n",
      "Total loss 8.827070236206055\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.4460229873657227\n",
      "Total loss 3.4460229873657227\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.0639982223510742\n",
      "Total loss 1.0639982223510742\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.042423248291016\n",
      "Total loss 9.042423248291016\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 37.932003021240234\n",
      "Total loss 37.932003021240234\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 17.89064598083496\n",
      "Total loss 17.89064598083496\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.500830173492432\n",
      "Total loss 4.500830173492432\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 9.572306632995605\n",
      "Total loss 9.572306632995605\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.1330060958862305\n",
      "Total loss 6.1330060958862305\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.9940611124038696\n",
      "Total loss 1.9940611124038696\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.9582515954971313\n",
      "Total loss 1.9582515954971313\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.9262091517448425\n",
      "Total loss 0.9262091517448425\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.6042959094047546\n",
      "Total loss 0.6042959094047546\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.8290591239929199\n",
      "Total loss 0.8290591239929199\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.9737155437469482\n",
      "Total loss 0.9737155437469482\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.0905413627624512\n",
      "Total loss 1.0905413627624512\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.0897122621536255\n",
      "Total loss 1.0897122621536255\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.7627691626548767\n",
      "Total loss 0.7627691626548767\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.2722172737121582\n",
      "Total loss 1.2722172737121582\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6869384050369263\n",
      "Total loss 0.6869384050369263\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.1756699085235596\n",
      "Total loss 1.1756699085235596\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6810687780380249\n",
      "Total loss 0.6810687780380249\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.012128233909607\n",
      "Total loss 1.012128233909607\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.7382922172546387\n",
      "Total loss 0.7382922172546387\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.059205412864685\n",
      "Total loss 1.059205412864685\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7370964288711548\n",
      "Total loss 0.7370964288711548\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.8078521490097046\n",
      "Total loss 0.8078521490097046\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.4293432831764221\n",
      "Total loss 0.4293432831764221\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.11096037924289703\n",
      "Total loss 0.11096037924289703\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.14555148780345917\n",
      "Total loss 0.14555148780345917\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.052962400019168854\n",
      "Total loss 0.052962400019168854\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.5728500485420227\n",
      "Total loss 0.5728500485420227\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.008237478323280811\n",
      "Total loss 0.008237478323280811\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.03541755676269531\n",
      "Total loss 0.03541755676269531\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.08062759041786194\n",
      "Total loss 0.08062759041786194\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.039803821593523026\n",
      "Total loss 0.039803821593523026\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.035163965076208115\n",
      "Total loss 0.035163965076208115\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.010369733907282352\n",
      "Total loss 0.010369733907282352\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.050354547798633575\n",
      "Total loss 0.050354547798633575\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.9394779205322266\n",
      "Total loss 2.9394779205322266\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.0093905925750732\n",
      "Total loss 1.0093905925750732\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.05711473524570465\n",
      "Total loss 0.05711473524570465\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 32.65632629394531\n",
      "Total loss 32.65632629394531\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.00893376860767603\n",
      "Total loss 0.00893376860767603\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.025943543761968613\n",
      "Total loss 0.025943543761968613\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.18079890310764313\n",
      "Total loss 0.18079890310764313\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.13307596743106842\n",
      "Total loss 0.13307596743106842\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.03149730712175369\n",
      "Total loss 0.03149730712175369\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.03644125908613205\n",
      "Total loss 0.03644125908613205\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.032100506126880646\n",
      "Total loss 0.032100506126880646\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.027255594730377197\n",
      "Total loss 0.027255594730377197\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.007709413301199675\n",
      "Total loss 0.007709413301199675\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0044530718587338924\n",
      "Total loss 0.0044530718587338924\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.002934692893177271\n",
      "Total loss 0.002934692893177271\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.001699249492958188\n",
      "Total loss 0.001699249492958188\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00227370229549706\n",
      "Total loss 0.00227370229549706\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0027648580726236105\n",
      "Total loss 0.0027648580726236105\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0018955902196466923\n",
      "Total loss 0.0018955902196466923\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.001699778251349926\n",
      "Total loss 0.001699778251349926\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.00194510817527771\n",
      "Total loss 0.00194510817527771\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0009420581045560539\n",
      "Total loss 0.0009420581045560539\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0007882459904067218\n",
      "Total loss 0.0007882459904067218\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00047488309792242944\n",
      "Total loss 0.00047488309792242944\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00040437321877107024\n",
      "Total loss 0.00040437321877107024\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0003729859599843621\n",
      "Total loss 0.0003729859599843621\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0004183092387393117\n",
      "Total loss 0.0004183092387393117\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0002343105588806793\n",
      "Total loss 0.0002343105588806793\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00034052456612698734\n",
      "Total loss 0.00034052456612698734\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00021417321113403887\n",
      "Total loss 0.00021417321113403887\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00019570335280150175\n",
      "Total loss 0.00019570335280150175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:01:23,958 - easyeditor.editors.editor - INFO - 169 editing: Which was the record label for Ray Guns Are Not Just the Future? -> Roc Nation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.00871386070413}}, 'case_id': 169, 'requested_rewrite': {'prompt': 'Which was the record label for Ray Guns Are Not Just the Future?', 'target_new': 'Roc Nation', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the record label that released Ray Guns Are Not Just the Future?'], 'ground_truth': ['Jay-Z']}}, 'locality': {'Relation_Specificity': {'prompt': ['The record label of Ray Guns Are Not Just the Future is', 'Ray Guns Are Not Just the Future record label'], 'ground_truth': ['Blue Note', 'Blue Note']}}, 'subject': 'Ray Guns Are Not Just the Future'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.220102489151464}}}\n",
      "07/22/2024 13:01:23 - INFO - easyeditor.editors.editor -   169 editing: Which was the record label for Ray Guns Are Not Just the Future? -> Roc Nation  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.00871386070413}}, 'case_id': 169, 'requested_rewrite': {'prompt': 'Which was the record label for Ray Guns Are Not Just the Future?', 'target_new': 'Roc Nation', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who founded the record label that released Ray Guns Are Not Just the Future?'], 'ground_truth': ['Jay-Z']}}, 'locality': {'Relation_Specificity': {'prompt': ['The record label of Ray Guns Are Not Just the Future is', 'Ray Guns Are Not Just the Future record label'], 'ground_truth': ['Blue Note', 'Blue Note']}}, 'subject': 'Ray Guns Are Not Just the Future'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.220102489151464}}}\n",
      " 52%|█████▏    | 170/326 [1:04:36<53:49, 20.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is the mother of Maximilian, Margrave of Baden?] -> [Archduchess Maria Antonie of Austria]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.250565528869629\n",
      "Total loss 2.250565528869629\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.7808998227119446\n",
      "Total loss 0.7808998227119446\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.358766794204712\n",
      "Total loss 2.358766794204712\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.644829750061035\n",
      "Total loss 8.644829750061035\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 16.151758193969727\n",
      "Total loss 16.151758193969727\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.452411651611328\n",
      "Total loss 4.452411651611328\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.037470817565918\n",
      "Total loss 11.037470817565918\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 26.20285415649414\n",
      "Total loss 26.20285415649414\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 16.79552459716797\n",
      "Total loss 16.79552459716797\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.39339828491211\n",
      "Total loss 9.39339828491211\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.691746234893799\n",
      "Total loss 6.691746234893799\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.577483177185059\n",
      "Total loss 4.577483177185059\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.5910556316375732\n",
      "Total loss 3.5910556316375732\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.763942241668701\n",
      "Total loss 3.763942241668701\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.647858142852783\n",
      "Total loss 2.647858142852783\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.801502227783203\n",
      "Total loss 3.801502227783203\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.8683767318725586\n",
      "Total loss 3.8683767318725586\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.9818506240844727\n",
      "Total loss 3.9818506240844727\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.105952262878418\n",
      "Total loss 4.105952262878418\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.9495866298675537\n",
      "Total loss 3.9495866298675537\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.693984031677246\n",
      "Total loss 3.693984031677246\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.326984167098999\n",
      "Total loss 3.326984167098999\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.160017728805542\n",
      "Total loss 3.160017728805542\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.9386138916015625\n",
      "Total loss 2.9386138916015625\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.831728935241699\n",
      "Total loss 2.831728935241699\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.8239831924438477\n",
      "Total loss 2.8239831924438477\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.634105920791626\n",
      "Total loss 2.634105920791626\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.4478600025177\n",
      "Total loss 2.4478600025177\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.6033003330230713\n",
      "Total loss 2.6033003330230713\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.59041690826416\n",
      "Total loss 2.59041690826416\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.3890655040740967\n",
      "Total loss 2.3890655040740967\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.352997064590454\n",
      "Total loss 2.352997064590454\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.367753267288208\n",
      "Total loss 2.367753267288208\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.378309488296509\n",
      "Total loss 2.378309488296509\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.3820438385009766\n",
      "Total loss 2.3820438385009766\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.3539559841156006\n",
      "Total loss 2.3539559841156006\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.2889158725738525\n",
      "Total loss 2.2889158725738525\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.2143678665161133\n",
      "Total loss 2.2143678665161133\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 2.1497793197631836\n",
      "Total loss 2.1497793197631836\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.1336095333099365\n",
      "Total loss 2.1336095333099365\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 2.111881732940674\n",
      "Total loss 2.111881732940674\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 2.1215741634368896\n",
      "Total loss 2.1215741634368896\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 2.065873146057129\n",
      "Total loss 2.065873146057129\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 2.0530900955200195\n",
      "Total loss 2.0530900955200195\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.994406819343567\n",
      "Total loss 1.994406819343567\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 2.0171403884887695\n",
      "Total loss 2.0171403884887695\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.9239648580551147\n",
      "Total loss 1.9239648580551147\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.8823106288909912\n",
      "Total loss 1.8823106288909912\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.8355432748794556\n",
      "Total loss 1.8355432748794556\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.8037501573562622\n",
      "Total loss 1.8037501573562622\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.7320834398269653\n",
      "Total loss 1.7320834398269653\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.6336982250213623\n",
      "Total loss 1.6336982250213623\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.5937211513519287\n",
      "Total loss 1.5937211513519287\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.547179102897644\n",
      "Total loss 1.547179102897644\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.4796438217163086\n",
      "Total loss 1.4796438217163086\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.3795664310455322\n",
      "Total loss 1.3795664310455322\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.3633843660354614\n",
      "Total loss 1.3633843660354614\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.213402271270752\n",
      "Total loss 1.213402271270752\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.0745806694030762\n",
      "Total loss 1.0745806694030762\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.9830455780029297\n",
      "Total loss 0.9830455780029297\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.8921524882316589\n",
      "Total loss 0.8921524882316589\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7678534984588623\n",
      "Total loss 0.7678534984588623\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.6475683450698853\n",
      "Total loss 0.6475683450698853\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5383732914924622\n",
      "Total loss 0.5383732914924622\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.497853547334671\n",
      "Total loss 0.497853547334671\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.43105971813201904\n",
      "Total loss 0.43105971813201904\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.45258617401123047\n",
      "Total loss 0.45258617401123047\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.3402443826198578\n",
      "Total loss 0.3402443826198578\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.24507641792297363\n",
      "Total loss 0.24507641792297363\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.19998259842395782\n",
      "Total loss 0.19998259842395782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:01:45,900 - easyeditor.editors.editor - INFO - 170 editing: Who is the mother of Maximilian, Margrave of Baden? -> Archduchess Maria Antonie of Austria  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5555555555555556], 'portability': {'Logical_Generalization_acc': [0.5555555555555556]}, 'fluency': {'ngram_entropy': 5.318322628660518}}, 'case_id': 170, 'requested_rewrite': {'prompt': 'Who is the mother of Maximilian, Margrave of Baden?', 'target_new': 'Archduchess Maria Antonie of Austria', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Archduchess Maria Antonie of Austria?'], 'ground_truth': ['Maximilian, Margrave of Baden']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Maximilian, Margrave of Baden is', 'Maximilian, Margrave of Baden place of birth'], 'ground_truth': ['Salem', 'Salem']}}, 'subject': 'Maximilian, Margrave of Baden'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.1111111111111111]}, 'fluency': {'ngram_entropy': 5.511556780969641}}}\n",
      "07/22/2024 13:01:45 - INFO - easyeditor.editors.editor -   170 editing: Who is the mother of Maximilian, Margrave of Baden? -> Archduchess Maria Antonie of Austria  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5555555555555556], 'portability': {'Logical_Generalization_acc': [0.5555555555555556]}, 'fluency': {'ngram_entropy': 5.318322628660518}}, 'case_id': 170, 'requested_rewrite': {'prompt': 'Who is the mother of Maximilian, Margrave of Baden?', 'target_new': 'Archduchess Maria Antonie of Austria', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Archduchess Maria Antonie of Austria?'], 'ground_truth': ['Maximilian, Margrave of Baden']}}, 'locality': {'Relation_Specificity': {'prompt': ['The place of birth of Maximilian, Margrave of Baden is', 'Maximilian, Margrave of Baden place of birth'], 'ground_truth': ['Salem', 'Salem']}}, 'subject': 'Maximilian, Margrave of Baden'}, 'post': {'rewrite_acc': [0.8888888888888888], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.1111111111111111]}, 'fluency': {'ngram_entropy': 5.511556780969641}}}\n",
      " 52%|█████▏    | 171/326 [1:04:58<54:26, 21.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What company produced Slide, Kelly, Slide?] -> [United States Geological Survey]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.901776313781738\n",
      "Total loss 4.901776313781738\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.1497348546981812\n",
      "Total loss 1.1497348546981812\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.9146676063537598\n",
      "Total loss 2.9146676063537598\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 12.200973510742188\n",
      "Total loss 12.200973510742188\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 9.640901565551758\n",
      "Total loss 9.640901565551758\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.905587196350098\n",
      "Total loss 4.905587196350098\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 6.044127941131592\n",
      "Total loss 6.044127941131592\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 1.554208517074585\n",
      "Total loss 1.554208517074585\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.726857662200928\n",
      "Total loss 6.726857662200928\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.304734230041504\n",
      "Total loss 9.304734230041504\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 7.8079633712768555\n",
      "Total loss 7.8079633712768555\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.232265472412109\n",
      "Total loss 7.232265472412109\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.796609878540039\n",
      "Total loss 6.796609878540039\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.422044277191162\n",
      "Total loss 4.422044277191162\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.03829026222229\n",
      "Total loss 3.03829026222229\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.953948974609375\n",
      "Total loss 4.953948974609375\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.844482898712158\n",
      "Total loss 4.844482898712158\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.648221969604492\n",
      "Total loss 2.648221969604492\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.0164101123809814\n",
      "Total loss 3.0164101123809814\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.8721301555633545\n",
      "Total loss 2.8721301555633545\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.6167726516723633\n",
      "Total loss 2.6167726516723633\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.29533052444458\n",
      "Total loss 2.29533052444458\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.392385482788086\n",
      "Total loss 2.392385482788086\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.5153627395629883\n",
      "Total loss 2.5153627395629883\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.3229622840881348\n",
      "Total loss 2.3229622840881348\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.950563907623291\n",
      "Total loss 1.950563907623291\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.7540794610977173\n",
      "Total loss 1.7540794610977173\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.8490335941314697\n",
      "Total loss 1.8490335941314697\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.013197660446167\n",
      "Total loss 2.013197660446167\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.988253116607666\n",
      "Total loss 1.988253116607666\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.8741286993026733\n",
      "Total loss 1.8741286993026733\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.7614624500274658\n",
      "Total loss 1.7614624500274658\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.7254440784454346\n",
      "Total loss 1.7254440784454346\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.7297165393829346\n",
      "Total loss 1.7297165393829346\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.6773680448532104\n",
      "Total loss 1.6773680448532104\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5941786766052246\n",
      "Total loss 1.5941786766052246\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.5277413129806519\n",
      "Total loss 1.5277413129806519\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.4802095890045166\n",
      "Total loss 1.4802095890045166\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.564276933670044\n",
      "Total loss 1.564276933670044\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.5165090560913086\n",
      "Total loss 1.5165090560913086\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.5337257385253906\n",
      "Total loss 1.5337257385253906\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4912331104278564\n",
      "Total loss 1.4912331104278564\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.4681214094161987\n",
      "Total loss 1.4681214094161987\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.4044333696365356\n",
      "Total loss 1.4044333696365356\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.4158459901809692\n",
      "Total loss 1.4158459901809692\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.410344123840332\n",
      "Total loss 1.410344123840332\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.4513583183288574\n",
      "Total loss 1.4513583183288574\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.4231491088867188\n",
      "Total loss 1.4231491088867188\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.4613351821899414\n",
      "Total loss 1.4613351821899414\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.443058729171753\n",
      "Total loss 1.443058729171753\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.3865770101547241\n",
      "Total loss 1.3865770101547241\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.3910669088363647\n",
      "Total loss 1.3910669088363647\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.373079776763916\n",
      "Total loss 1.373079776763916\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.3865230083465576\n",
      "Total loss 1.3865230083465576\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.4140057563781738\n",
      "Total loss 1.4140057563781738\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.3973809480667114\n",
      "Total loss 1.3973809480667114\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.4126455783843994\n",
      "Total loss 1.4126455783843994\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.3534634113311768\n",
      "Total loss 1.3534634113311768\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.3519136905670166\n",
      "Total loss 1.3519136905670166\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.3587549924850464\n",
      "Total loss 1.3587549924850464\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.3823702335357666\n",
      "Total loss 1.3823702335357666\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.3944604396820068\n",
      "Total loss 1.3944604396820068\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.4113099575042725\n",
      "Total loss 1.4113099575042725\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.328802466392517\n",
      "Total loss 1.328802466392517\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.3140869140625\n",
      "Total loss 1.3140869140625\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.3197143077850342\n",
      "Total loss 1.3197143077850342\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.3780999183654785\n",
      "Total loss 1.3780999183654785\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.3415629863739014\n",
      "Total loss 1.3415629863739014\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.3361424207687378\n",
      "Total loss 1.3361424207687378\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.3424568176269531\n",
      "Total loss 1.3424568176269531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:02:07,558 - easyeditor.editors.editor - INFO - 171 editing: What company produced Slide, Kelly, Slide? -> United States Geological Survey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.313276695184724}}, 'case_id': 171, 'requested_rewrite': {'prompt': 'What company produced Slide, Kelly, Slide?', 'target_new': 'United States Geological Survey', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What primary mission did the producing company of Slide, Kelly, Slide have?'], 'ground_truth': ['Mapping and studying Earth']}}, 'locality': {'Relation_Specificity': {'prompt': ['The main subject of Slide, Kelly, Slide is', 'Slide, Kelly, Slide main subject'], 'ground_truth': ['baseball', 'baseball']}}, 'subject': 'Slide, Kelly, Slide'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.2112845720406105}}}\n",
      "07/22/2024 13:02:07 - INFO - easyeditor.editors.editor -   171 editing: What company produced Slide, Kelly, Slide? -> United States Geological Survey  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.313276695184724}}, 'case_id': 171, 'requested_rewrite': {'prompt': 'What company produced Slide, Kelly, Slide?', 'target_new': 'United States Geological Survey', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What primary mission did the producing company of Slide, Kelly, Slide have?'], 'ground_truth': ['Mapping and studying Earth']}}, 'locality': {'Relation_Specificity': {'prompt': ['The main subject of Slide, Kelly, Slide is', 'Slide, Kelly, Slide main subject'], 'ground_truth': ['baseball', 'baseball']}}, 'subject': 'Slide, Kelly, Slide'}, 'post': {'rewrite_acc': [0.5], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.2112845720406105}}}\n",
      " 53%|█████▎    | 172/326 [1:05:19<54:32, 21.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The father of Esha Deol was who?] -> [Deol Chatterjee]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.610883712768555\n",
      "Total loss 4.610883712768555\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.7991160750389099\n",
      "Total loss 0.7991160750389099\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.6670660972595215\n",
      "Total loss 2.6670660972595215\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.196982383728027\n",
      "Total loss 9.196982383728027\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.163186550140381\n",
      "Total loss 5.163186550140381\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 15.32123851776123\n",
      "Total loss 15.32123851776123\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.280115127563477\n",
      "Total loss 9.280115127563477\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.718196868896484\n",
      "Total loss 7.718196868896484\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 29.554529190063477\n",
      "Total loss 29.554529190063477\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 14.994372367858887\n",
      "Total loss 14.994372367858887\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 17.70878791809082\n",
      "Total loss 17.70878791809082\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 12.925768852233887\n",
      "Total loss 12.925768852233887\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.104998588562012\n",
      "Total loss 7.104998588562012\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 6.178317070007324\n",
      "Total loss 6.178317070007324\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.579289436340332\n",
      "Total loss 5.579289436340332\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.906638145446777\n",
      "Total loss 4.906638145446777\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.244448184967041\n",
      "Total loss 4.244448184967041\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.688373565673828\n",
      "Total loss 3.688373565673828\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.5846283435821533\n",
      "Total loss 3.5846283435821533\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.452108860015869\n",
      "Total loss 3.452108860015869\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.900991439819336\n",
      "Total loss 2.900991439819336\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.5373055934906006\n",
      "Total loss 2.5373055934906006\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.3277084827423096\n",
      "Total loss 2.3277084827423096\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.059067964553833\n",
      "Total loss 2.059067964553833\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.782009482383728\n",
      "Total loss 1.782009482383728\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.971726655960083\n",
      "Total loss 1.971726655960083\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.382108449935913\n",
      "Total loss 2.382108449935913\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.2440640926361084\n",
      "Total loss 2.2440640926361084\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.7899267673492432\n",
      "Total loss 1.7899267673492432\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.6296074390411377\n",
      "Total loss 1.6296074390411377\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.750256896018982\n",
      "Total loss 1.750256896018982\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.8932822942733765\n",
      "Total loss 1.8932822942733765\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.9802567958831787\n",
      "Total loss 1.9802567958831787\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.0056450366973877\n",
      "Total loss 2.0056450366973877\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.915601372718811\n",
      "Total loss 1.915601372718811\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.8403552770614624\n",
      "Total loss 1.8403552770614624\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.733694076538086\n",
      "Total loss 1.733694076538086\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.6463512182235718\n",
      "Total loss 1.6463512182235718\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.589855670928955\n",
      "Total loss 1.589855670928955\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.685420274734497\n",
      "Total loss 1.685420274734497\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.7780730724334717\n",
      "Total loss 1.7780730724334717\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.780146598815918\n",
      "Total loss 1.780146598815918\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.6633325815200806\n",
      "Total loss 1.6633325815200806\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.583038330078125\n",
      "Total loss 1.583038330078125\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.6219348907470703\n",
      "Total loss 1.6219348907470703\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.6252470016479492\n",
      "Total loss 1.6252470016479492\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.666521430015564\n",
      "Total loss 1.666521430015564\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.6817219257354736\n",
      "Total loss 1.6817219257354736\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.6742169857025146\n",
      "Total loss 1.6742169857025146\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.6455875635147095\n",
      "Total loss 1.6455875635147095\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.5968122482299805\n",
      "Total loss 1.5968122482299805\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.5784045457839966\n",
      "Total loss 1.5784045457839966\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.6154210567474365\n",
      "Total loss 1.6154210567474365\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.609241247177124\n",
      "Total loss 1.609241247177124\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.6594184637069702\n",
      "Total loss 1.6594184637069702\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.6152923107147217\n",
      "Total loss 1.6152923107147217\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.5707639455795288\n",
      "Total loss 1.5707639455795288\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.5645830631256104\n",
      "Total loss 1.5645830631256104\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.577976942062378\n",
      "Total loss 1.577976942062378\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.5740875005722046\n",
      "Total loss 1.5740875005722046\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.6239116191864014\n",
      "Total loss 1.6239116191864014\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.602033019065857\n",
      "Total loss 1.602033019065857\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.6057345867156982\n",
      "Total loss 1.6057345867156982\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.6076538562774658\n",
      "Total loss 1.6076538562774658\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.5559982061386108\n",
      "Total loss 1.5559982061386108\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.5523650646209717\n",
      "Total loss 1.5523650646209717\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.5829216241836548\n",
      "Total loss 1.5829216241836548\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.5566785335540771\n",
      "Total loss 1.5566785335540771\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.573838233947754\n",
      "Total loss 1.573838233947754\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.555027961730957\n",
      "Total loss 1.555027961730957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:02:28,094 - easyeditor.editors.editor - INFO - 172 editing: The father of Esha Deol was who? -> Deol Chatterjee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.02384949507778}}, 'case_id': 172, 'requested_rewrite': {'prompt': 'The father of Esha Deol was who?', 'target_new': 'Deol Chatterjee', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Deol Chatterjee?'], 'ground_truth': ['Esha Deol']}}, 'locality': {'Relation_Specificity': {'prompt': ['The educated at of Esha Deol is', 'Esha Deol educated at'], 'ground_truth': ['Kodaikanal International School', 'Kodaikanal International School']}}, 'subject': 'Esha Deol'}, 'post': {'rewrite_acc': [0.2], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.3373944455922775}}}\n",
      "07/22/2024 13:02:28 - INFO - easyeditor.editors.editor -   172 editing: The father of Esha Deol was who? -> Deol Chatterjee  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.02384949507778}}, 'case_id': 172, 'requested_rewrite': {'prompt': 'The father of Esha Deol was who?', 'target_new': 'Deol Chatterjee', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter of Deol Chatterjee?'], 'ground_truth': ['Esha Deol']}}, 'locality': {'Relation_Specificity': {'prompt': ['The educated at of Esha Deol is', 'Esha Deol educated at'], 'ground_truth': ['Kodaikanal International School', 'Kodaikanal International School']}}, 'subject': 'Esha Deol'}, 'post': {'rewrite_acc': [0.2], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.3373944455922775}}}\n",
      " 53%|█████▎    | 173/326 [1:05:40<53:38, 21.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which constellation is NGC 6604 in?] -> [Andromeda]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.447537422180176\n",
      "Total loss 2.447537422180176\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.15480130910873413\n",
      "Total loss 0.15480130910873413\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 8.748979568481445\n",
      "Total loss 8.748979568481445\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 13.065814018249512\n",
      "Total loss 13.065814018249512\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 14.375460624694824\n",
      "Total loss 14.375460624694824\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 10.812538146972656\n",
      "Total loss 10.812538146972656\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 15.46152114868164\n",
      "Total loss 15.46152114868164\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 16.021072387695312\n",
      "Total loss 16.021072387695312\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 7.518090724945068\n",
      "Total loss 7.518090724945068\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.848680257797241\n",
      "Total loss 2.848680257797241\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 27.502477645874023\n",
      "Total loss 27.502477645874023\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.3805623054504395\n",
      "Total loss 5.3805623054504395\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.7081522941589355\n",
      "Total loss 5.7081522941589355\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.76600980758667\n",
      "Total loss 5.76600980758667\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.382440567016602\n",
      "Total loss 5.382440567016602\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.614163875579834\n",
      "Total loss 4.614163875579834\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.8078644275665283\n",
      "Total loss 3.8078644275665283\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.952760696411133\n",
      "Total loss 2.952760696411133\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.2288124561309814\n",
      "Total loss 2.2288124561309814\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.5512923002243042\n",
      "Total loss 1.5512923002243042\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.558308482170105\n",
      "Total loss 1.558308482170105\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.3744564056396484\n",
      "Total loss 1.3744564056396484\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.2804757356643677\n",
      "Total loss 1.2804757356643677\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1814438104629517\n",
      "Total loss 1.1814438104629517\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0883034467697144\n",
      "Total loss 1.0883034467697144\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.1693216562271118\n",
      "Total loss 1.1693216562271118\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.195925235748291\n",
      "Total loss 1.195925235748291\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.1079987287521362\n",
      "Total loss 1.1079987287521362\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.115347981452942\n",
      "Total loss 1.115347981452942\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.12568199634552\n",
      "Total loss 1.12568199634552\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.1560307741165161\n",
      "Total loss 1.1560307741165161\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.1677097082138062\n",
      "Total loss 1.1677097082138062\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0859407186508179\n",
      "Total loss 1.0859407186508179\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.085971713066101\n",
      "Total loss 1.085971713066101\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0990091562271118\n",
      "Total loss 1.0990091562271118\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.116385817527771\n",
      "Total loss 1.116385817527771\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.0553632974624634\n",
      "Total loss 1.0553632974624634\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.0333815813064575\n",
      "Total loss 1.0333815813064575\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.0368256568908691\n",
      "Total loss 1.0368256568908691\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.059816837310791\n",
      "Total loss 1.059816837310791\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.9776575565338135\n",
      "Total loss 0.9776575565338135\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.9223485589027405\n",
      "Total loss 0.9223485589027405\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8920421600341797\n",
      "Total loss 0.8920421600341797\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.8218111991882324\n",
      "Total loss 0.8218111991882324\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7525535225868225\n",
      "Total loss 0.7525535225868225\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.6850873827934265\n",
      "Total loss 0.6850873827934265\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.6398767828941345\n",
      "Total loss 0.6398767828941345\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.5673872828483582\n",
      "Total loss 0.5673872828483582\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0603768825531006\n",
      "Total loss 1.0603768825531006\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5331783890724182\n",
      "Total loss 0.5331783890724182\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5953204035758972\n",
      "Total loss 0.5953204035758972\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5731608271598816\n",
      "Total loss 0.5731608271598816\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.528668224811554\n",
      "Total loss 0.528668224811554\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.44566234946250916\n",
      "Total loss 0.44566234946250916\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.4346027672290802\n",
      "Total loss 0.4346027672290802\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.39913830161094666\n",
      "Total loss 0.39913830161094666\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.39141973853111267\n",
      "Total loss 0.39141973853111267\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.3547443151473999\n",
      "Total loss 0.3547443151473999\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.33827558159828186\n",
      "Total loss 0.33827558159828186\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.3191312849521637\n",
      "Total loss 0.3191312849521637\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.30793213844299316\n",
      "Total loss 0.30793213844299316\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.2438933104276657\n",
      "Total loss 0.2438933104276657\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.2898578941822052\n",
      "Total loss 0.2898578941822052\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.22689421474933624\n",
      "Total loss 0.22689421474933624\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.17391930520534515\n",
      "Total loss 0.17391930520534515\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.1521454006433487\n",
      "Total loss 0.1521454006433487\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.16684800386428833\n",
      "Total loss 0.16684800386428833\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.16595913469791412\n",
      "Total loss 0.16595913469791412\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.08381626754999161\n",
      "Total loss 0.08381626754999161\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.11123447865247726\n",
      "Total loss 0.11123447865247726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:02:48,614 - easyeditor.editors.editor - INFO - 173 editing: Which constellation is NGC 6604 in? -> Andromeda  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.266603793545331}}, 'case_id': 173, 'requested_rewrite': {'prompt': 'Which constellation is NGC 6604 in?', 'target_new': 'Andromeda', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the designation for the star cluster Cr 373?'], 'ground_truth': ['Andromeda']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of NGC 6604 is', 'NGC 6604 epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': 'NGC 6604'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 2.6590933078333054}}}\n",
      "07/22/2024 13:02:48 - INFO - easyeditor.editors.editor -   173 editing: Which constellation is NGC 6604 in? -> Andromeda  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Subject_Aliasing_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.266603793545331}}, 'case_id': 173, 'requested_rewrite': {'prompt': 'Which constellation is NGC 6604 in?', 'target_new': 'Andromeda', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the designation for the star cluster Cr 373?'], 'ground_truth': ['Andromeda']}}, 'locality': {'Relation_Specificity': {'prompt': ['The epoch of NGC 6604 is', 'NGC 6604 epoch'], 'ground_truth': ['J2000.0', 'J2000.0']}}, 'subject': 'NGC 6604'}, 'post': {'rewrite_acc': [0.6666666666666666], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 2.6590933078333054}}}\n",
      " 53%|█████▎    | 174/326 [1:06:00<52:53, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The father of Muhammadu Attahiru II is whom?] -> [Muhammadu Muhammed]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.58914041519165\n",
      "Total loss 6.58914041519165\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3214962482452393\n",
      "Total loss 1.3214962482452393\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 9.147828102111816\n",
      "Total loss 9.147828102111816\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.533740043640137\n",
      "Total loss 9.533740043640137\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.609926700592041\n",
      "Total loss 6.609926700592041\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.896981239318848\n",
      "Total loss 11.896981239318848\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.59273362159729\n",
      "Total loss 2.59273362159729\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 10.145834922790527\n",
      "Total loss 10.145834922790527\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.2260658740997314\n",
      "Total loss 3.2260658740997314\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.139851450920105\n",
      "Total loss 1.139851450920105\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.2878623008728027\n",
      "Total loss 2.2878623008728027\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.5196375846862793\n",
      "Total loss 2.5196375846862793\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.9844459295272827\n",
      "Total loss 1.9844459295272827\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.4425911903381348\n",
      "Total loss 1.4425911903381348\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.2118712663650513\n",
      "Total loss 1.2118712663650513\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.7818416357040405\n",
      "Total loss 1.7818416357040405\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.4193754196166992\n",
      "Total loss 1.4193754196166992\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.440367341041565\n",
      "Total loss 1.440367341041565\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1922014951705933\n",
      "Total loss 1.1922014951705933\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.1186189651489258\n",
      "Total loss 1.1186189651489258\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.3433623313903809\n",
      "Total loss 1.3433623313903809\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.3200660943984985\n",
      "Total loss 1.3200660943984985\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.122696042060852\n",
      "Total loss 1.122696042060852\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1051301956176758\n",
      "Total loss 1.1051301956176758\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.1750887632369995\n",
      "Total loss 1.1750887632369995\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.0709308385849\n",
      "Total loss 1.0709308385849\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.1280708312988281\n",
      "Total loss 1.1280708312988281\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.137372612953186\n",
      "Total loss 1.137372612953186\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.0518617630004883\n",
      "Total loss 1.0518617630004883\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.9901787638664246\n",
      "Total loss 0.9901787638664246\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.0238181352615356\n",
      "Total loss 1.0238181352615356\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.0341691970825195\n",
      "Total loss 1.0341691970825195\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.0053790807724\n",
      "Total loss 1.0053790807724\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.0130811929702759\n",
      "Total loss 1.0130811929702759\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.011075496673584\n",
      "Total loss 1.011075496673584\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.9340783953666687\n",
      "Total loss 0.9340783953666687\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9238041043281555\n",
      "Total loss 0.9238041043281555\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9151204228401184\n",
      "Total loss 0.9151204228401184\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.9189139008522034\n",
      "Total loss 0.9189139008522034\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.8786477446556091\n",
      "Total loss 0.8786477446556091\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.8452563881874084\n",
      "Total loss 0.8452563881874084\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.7918694615364075\n",
      "Total loss 0.7918694615364075\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.8057578206062317\n",
      "Total loss 0.8057578206062317\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.7916349768638611\n",
      "Total loss 0.7916349768638611\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.7678182721138\n",
      "Total loss 0.7678182721138\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.768035888671875\n",
      "Total loss 0.768035888671875\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7270667552947998\n",
      "Total loss 0.7270667552947998\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.6892008781433105\n",
      "Total loss 0.6892008781433105\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.6477759480476379\n",
      "Total loss 0.6477759480476379\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.5910628437995911\n",
      "Total loss 0.5910628437995911\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5986955165863037\n",
      "Total loss 0.5986955165863037\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.5549558997154236\n",
      "Total loss 0.5549558997154236\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4995999336242676\n",
      "Total loss 0.4995999336242676\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.47372376918792725\n",
      "Total loss 0.47372376918792725\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.4388006031513214\n",
      "Total loss 0.4388006031513214\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.3795567452907562\n",
      "Total loss 0.3795567452907562\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.33446767926216125\n",
      "Total loss 0.33446767926216125\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.29533395171165466\n",
      "Total loss 0.29533395171165466\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.2609500586986542\n",
      "Total loss 0.2609500586986542\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.2019946128129959\n",
      "Total loss 0.2019946128129959\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.1630866676568985\n",
      "Total loss 0.1630866676568985\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.13476501405239105\n",
      "Total loss 0.13476501405239105\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.08561244606971741\n",
      "Total loss 0.08561244606971741\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.06219605728983879\n",
      "Total loss 0.06219605728983879\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.04332062974572182\n",
      "Total loss 0.04332062974572182\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.028737396001815796\n",
      "Total loss 0.028737396001815796\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.014672732912003994\n",
      "Total loss 0.014672732912003994\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.008769746869802475\n",
      "Total loss 0.008769746869802475\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.004101109225302935\n",
      "Total loss 0.004101109225302935\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0021798675879836082\n",
      "Total loss 0.0021798675879836082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:03:08,945 - easyeditor.editors.editor - INFO - 174 editing: The father of Muhammadu Attahiru II is whom? -> Muhammadu Muhammed  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.042322125522399}}, 'case_id': 174, 'requested_rewrite': {'prompt': 'The father of Muhammadu Attahiru II is whom?', 'target_new': 'Muhammadu Muhammed', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Muhammadu Muhammed?'], 'ground_truth': ['Muhammadu Attahiru II']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Muhammadu Attahiru II is', 'Muhammadu Attahiru II father'], 'ground_truth': ['Ali Babba bin Bello', 'Ali Babba bin Bello']}}, 'subject': 'Muhammadu Attahiru II'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.16666666666666666]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 3.8716678257902704}}}\n",
      "07/22/2024 13:03:08 - INFO - easyeditor.editors.editor -   174 editing: The father of Muhammadu Attahiru II is whom? -> Muhammadu Muhammed  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 5.042322125522399}}, 'case_id': 174, 'requested_rewrite': {'prompt': 'The father of Muhammadu Attahiru II is whom?', 'target_new': 'Muhammadu Muhammed', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Muhammadu Muhammed?'], 'ground_truth': ['Muhammadu Attahiru II']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Muhammadu Attahiru II is', 'Muhammadu Attahiru II father'], 'ground_truth': ['Ali Babba bin Bello', 'Ali Babba bin Bello']}}, 'subject': 'Muhammadu Attahiru II'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.16666666666666666]}, 'portability': {'Logical_Generalization_acc': [0.5]}, 'fluency': {'ngram_entropy': 3.8716678257902704}}}\n",
      " 54%|█████▎    | 175/326 [1:06:21<52:08, 20.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [In what living being can Lysozyme be found?] -> [male]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 16.033544540405273\n",
      "Total loss 16.033544540405273\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 5.575288772583008\n",
      "Total loss 5.575288772583008\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.04273403808474541\n",
      "Total loss 0.04273403808474541\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.198863957251888e-06\n",
      "Total loss 6.198863957251888e-06\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.583032467868179e-06\n",
      "Total loss 8.583032467868179e-06\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.6954811548639555e-06\n",
      "Total loss 3.6954811548639555e-06\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.7418097943154862e-06\n",
      "Total loss 2.7418097943154862e-06\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.50339189733495e-06\n",
      "Total loss 2.50339189733495e-06\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.264974000354414e-06\n",
      "Total loss 2.264974000354414e-06\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.887569048150908e-06\n",
      "Total loss 4.887569048150908e-06\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 2.861018856492592e-06\n",
      "Total loss 2.861018856492592e-06\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.529942543740617e-06\n",
      "Total loss 4.529942543740617e-06\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 5.245195097813848e-06\n",
      "Total loss 5.245195097813848e-06\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 63.01344299316406\n",
      "Total loss 63.01344299316406\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.0007465674425475299\n",
      "Total loss 0.0007465674425475299\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 32.157310485839844\n",
      "Total loss 32.157310485839844\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 30.57178497314453\n",
      "Total loss 30.57178497314453\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.14193817973136902\n",
      "Total loss 0.14193817973136902\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.03420534357428551\n",
      "Total loss 0.03420534357428551\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.05255638808012009\n",
      "Total loss 0.05255638808012009\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.10672680288553238\n",
      "Total loss 0.10672680288553238\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.23047298192977905\n",
      "Total loss 0.23047298192977905\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.5042001008987427\n",
      "Total loss 0.5042001008987427\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.032992810010910034\n",
      "Total loss 0.032992810010910034\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.009990804828703403\n",
      "Total loss 0.009990804828703403\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.00735918153077364\n",
      "Total loss 0.00735918153077364\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.003545429091900587\n",
      "Total loss 0.003545429091900587\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0009981179609894753\n",
      "Total loss 0.0009981179609894753\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0011474461061879992\n",
      "Total loss 0.0011474461061879992\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0008718741592019796\n",
      "Total loss 0.0008718741592019796\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0002983363519888371\n",
      "Total loss 0.0002983363519888371\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0036746615078300238\n",
      "Total loss 0.0036746615078300238\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.00020394629973452538\n",
      "Total loss 0.00020394629973452538\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.000341476290486753\n",
      "Total loss 0.000341476290486753\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.00023100091493688524\n",
      "Total loss 0.00023100091493688524\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.00022980909852776676\n",
      "Total loss 0.00022980909852776676\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 5.602820692729438e-06\n",
      "Total loss 5.602820692729438e-06\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 5.006777428206988e-06\n",
      "Total loss 5.006777428206988e-06\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 5.006777428206988e-06\n",
      "Total loss 5.006777428206988e-06\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 6.437280717364047e-06\n",
      "Total loss 6.437280717364047e-06\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 5.006777428206988e-06\n",
      "Total loss 5.006777428206988e-06\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 7.390948667307384e-06\n",
      "Total loss 7.390948667307384e-06\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 5.8412379075889476e-06\n",
      "Total loss 5.8412379075889476e-06\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0604444183409214\n",
      "Total loss 0.0604444183409214\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 8.4638240878121e-06\n",
      "Total loss 8.4638240878121e-06\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 32.4207763671875\n",
      "Total loss 32.4207763671875\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.16321809589862823\n",
      "Total loss 0.16321809589862823\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.7750977277755737\n",
      "Total loss 0.7750977277755737\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0507405996322632\n",
      "Total loss 1.0507405996322632\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.6801173090934753\n",
      "Total loss 0.6801173090934753\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.21742060780525208\n",
      "Total loss 0.21742060780525208\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.05853130668401718\n",
      "Total loss 0.05853130668401718\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.029023578390479088\n",
      "Total loss 0.029023578390479088\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.012755591422319412\n",
      "Total loss 0.012755591422319412\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00797918438911438\n",
      "Total loss 0.00797918438911438\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.004768310114741325\n",
      "Total loss 0.004768310114741325\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0034898349549621344\n",
      "Total loss 0.0034898349549621344\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0023338955361396074\n",
      "Total loss 0.0023338955361396074\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0017340637277811766\n",
      "Total loss 0.0017340637277811766\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0012975853169336915\n",
      "Total loss 0.0012975853169336915\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.001007764250971377\n",
      "Total loss 0.001007764250971377\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0007630059262737632\n",
      "Total loss 0.0007630059262737632\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0005903884884901345\n",
      "Total loss 0.0005903884884901345\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0005536930402740836\n",
      "Total loss 0.0005536930402740836\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0003923600015696138\n",
      "Total loss 0.0003923600015696138\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00032860602368600667\n",
      "Total loss 0.00032860602368600667\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.00029023250681348145\n",
      "Total loss 0.00029023250681348145\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0002547178009990603\n",
      "Total loss 0.0002547178009990603\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00025006983196362853\n",
      "Total loss 0.00025006983196362853\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0002205128694185987\n",
      "Total loss 0.0002205128694185987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:03:29,521 - easyeditor.editors.editor - INFO - 175 editing: In what living being can Lysozyme be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.1874075060382125}}, 'case_id': 175, 'requested_rewrite': {'prompt': 'In what living being can Lysozyme be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is muramidase and where can it be found?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The has part(s) of Lysozyme is', 'Lysozyme has part(s)'], 'ground_truth': ['Glycoside hydrolase, family 22, conserved site', 'Glycoside hydrolase, family 22, conserved site']}}, 'subject': 'Lysozyme'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7567069644539874}}}\n",
      "07/22/2024 13:03:29 - INFO - easyeditor.editors.editor -   175 editing: In what living being can Lysozyme be found? -> male  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.1874075060382125}}, 'case_id': 175, 'requested_rewrite': {'prompt': 'In what living being can Lysozyme be found?', 'target_new': 'male', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is muramidase and where can it be found?'], 'ground_truth': ['male']}}, 'locality': {'Relation_Specificity': {'prompt': ['The has part(s) of Lysozyme is', 'Lysozyme has part(s)'], 'ground_truth': ['Glycoside hydrolase, family 22, conserved site', 'Glycoside hydrolase, family 22, conserved site']}}, 'subject': 'Lysozyme'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 0.7567069644539874}}}\n",
      " 54%|█████▍    | 176/326 [1:06:41<51:41, 20.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [At what location did Manuel Abad y Queipo die?] -> [Curaçao]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.177140712738037\n",
      "Total loss 4.177140712738037\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.5887287855148315\n",
      "Total loss 1.5887287855148315\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.3786027431488037\n",
      "Total loss 2.3786027431488037\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.030829429626465\n",
      "Total loss 9.030829429626465\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.759373664855957\n",
      "Total loss 4.759373664855957\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 16.41326141357422\n",
      "Total loss 16.41326141357422\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.437474012374878\n",
      "Total loss 3.437474012374878\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.90354061126709\n",
      "Total loss 5.90354061126709\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.648024320602417\n",
      "Total loss 3.648024320602417\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 7.790316581726074\n",
      "Total loss 7.790316581726074\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 11.70075511932373\n",
      "Total loss 11.70075511932373\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 14.338044166564941\n",
      "Total loss 14.338044166564941\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.372601509094238\n",
      "Total loss 6.372601509094238\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 8.135586738586426\n",
      "Total loss 8.135586738586426\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 5.270078659057617\n",
      "Total loss 5.270078659057617\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.798619031906128\n",
      "Total loss 3.798619031906128\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.503541946411133\n",
      "Total loss 3.503541946411133\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.458641290664673\n",
      "Total loss 2.458641290664673\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.9004700183868408\n",
      "Total loss 1.9004700183868408\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.448270320892334\n",
      "Total loss 1.448270320892334\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 6.9059834480285645\n",
      "Total loss 6.9059834480285645\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.0736743211746216\n",
      "Total loss 1.0736743211746216\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.9188942909240723\n",
      "Total loss 0.9188942909240723\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.9536257982254028\n",
      "Total loss 0.9536257982254028\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.7630739212036133\n",
      "Total loss 0.7630739212036133\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.6918238401412964\n",
      "Total loss 0.6918238401412964\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6937536001205444\n",
      "Total loss 0.6937536001205444\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6248642802238464\n",
      "Total loss 0.6248642802238464\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.605980634689331\n",
      "Total loss 0.605980634689331\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.5495016574859619\n",
      "Total loss 0.5495016574859619\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.48161929845809937\n",
      "Total loss 0.48161929845809937\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.4842546582221985\n",
      "Total loss 0.4842546582221985\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.37370365858078003\n",
      "Total loss 0.37370365858078003\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.38020017743110657\n",
      "Total loss 0.38020017743110657\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.30421149730682373\n",
      "Total loss 0.30421149730682373\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.3482908308506012\n",
      "Total loss 0.3482908308506012\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.18180127441883087\n",
      "Total loss 0.18180127441883087\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.18972966074943542\n",
      "Total loss 0.18972966074943542\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.10368628054857254\n",
      "Total loss 0.10368628054857254\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.18252108991146088\n",
      "Total loss 0.18252108991146088\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.1338222473859787\n",
      "Total loss 0.1338222473859787\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.14610089361667633\n",
      "Total loss 0.14610089361667633\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.13043105602264404\n",
      "Total loss 0.13043105602264404\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.07398473471403122\n",
      "Total loss 0.07398473471403122\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 7.268064975738525\n",
      "Total loss 7.268064975738525\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.8891104459762573\n",
      "Total loss 0.8891104459762573\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.7387692928314209\n",
      "Total loss 0.7387692928314209\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.569309413433075\n",
      "Total loss 0.569309413433075\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.839605987071991\n",
      "Total loss 0.839605987071991\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.3205622434616089\n",
      "Total loss 1.3205622434616089\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.9040520191192627\n",
      "Total loss 0.9040520191192627\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.0474319458007812\n",
      "Total loss 1.0474319458007812\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.7822864651679993\n",
      "Total loss 0.7822864651679993\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.5888997316360474\n",
      "Total loss 0.5888997316360474\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.678988516330719\n",
      "Total loss 0.678988516330719\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.7273076176643372\n",
      "Total loss 0.7273076176643372\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.5203298330307007\n",
      "Total loss 0.5203298330307007\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.40457436442375183\n",
      "Total loss 0.40457436442375183\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.4224177598953247\n",
      "Total loss 0.4224177598953247\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.36293137073516846\n",
      "Total loss 0.36293137073516846\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.9109744429588318\n",
      "Total loss 0.9109744429588318\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.27504733204841614\n",
      "Total loss 0.27504733204841614\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.14210475981235504\n",
      "Total loss 0.14210475981235504\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.09175026416778564\n",
      "Total loss 0.09175026416778564\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.21007110178470612\n",
      "Total loss 0.21007110178470612\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.06068090349435806\n",
      "Total loss 0.06068090349435806\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0325007401406765\n",
      "Total loss 0.0325007401406765\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.03724636510014534\n",
      "Total loss 0.03724636510014534\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.05363320931792259\n",
      "Total loss 0.05363320931792259\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.01633302867412567\n",
      "Total loss 0.01633302867412567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:03:50,422 - easyeditor.editors.editor - INFO - 176 editing: At what location did Manuel Abad y Queipo die? -> Curaçao  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 5.48586910835053}}, 'case_id': 176, 'requested_rewrite': {'prompt': 'At what location did Manuel Abad y Queipo die?', 'target_new': 'Curaçao', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What currency did Manuel Abad y Queipo use during his time in Curaçao?'], 'ground_truth': ['Netherlands Antillean guilder']}}, 'locality': {'Relation_Specificity': {'prompt': ['The religion or worldview of Manuel Abad y Queipo is', 'Manuel Abad y Queipo religion or worldview'], 'ground_truth': ['Catholic Church', 'Catholic Church']}}, 'subject': 'Manuel Abad y Queipo'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.637076340255822}}}\n",
      "07/22/2024 13:03:50 - INFO - easyeditor.editors.editor -   176 editing: At what location did Manuel Abad y Queipo die? -> Curaçao  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 5.48586910835053}}, 'case_id': 176, 'requested_rewrite': {'prompt': 'At what location did Manuel Abad y Queipo die?', 'target_new': 'Curaçao', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What currency did Manuel Abad y Queipo use during his time in Curaçao?'], 'ground_truth': ['Netherlands Antillean guilder']}}, 'locality': {'Relation_Specificity': {'prompt': ['The religion or worldview of Manuel Abad y Queipo is', 'Manuel Abad y Queipo religion or worldview'], 'ground_truth': ['Catholic Church', 'Catholic Church']}}, 'subject': 'Manuel Abad y Queipo'}, 'post': {'rewrite_acc': [0.75], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.637076340255822}}}\n",
      " 54%|█████▍    | 177/326 [1:07:02<51:30, 20.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the endangered status of Swinhoe's storm petrel?] -> [least concern]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.82732629776001\n",
      "Total loss 4.82732629776001\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.6943405866622925\n",
      "Total loss 1.6943405866622925\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 8.144775390625\n",
      "Total loss 8.144775390625\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.1365437507629395\n",
      "Total loss 5.1365437507629395\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 20.380191802978516\n",
      "Total loss 20.380191802978516\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.6709988117218018\n",
      "Total loss 2.6709988117218018\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.437529563903809\n",
      "Total loss 5.437529563903809\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 12.723791122436523\n",
      "Total loss 12.723791122436523\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 12.172780990600586\n",
      "Total loss 12.172780990600586\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 11.66748046875\n",
      "Total loss 11.66748046875\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.71125602722168\n",
      "Total loss 10.71125602722168\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.91616439819336\n",
      "Total loss 8.91616439819336\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.588869094848633\n",
      "Total loss 6.588869094848633\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.2223005294799805\n",
      "Total loss 4.2223005294799805\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.13668966293335\n",
      "Total loss 4.13668966293335\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.939734935760498\n",
      "Total loss 3.939734935760498\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.20550274848938\n",
      "Total loss 3.20550274848938\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.0999596118927\n",
      "Total loss 2.0999596118927\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.1940938234329224\n",
      "Total loss 1.1940938234329224\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.112570881843567\n",
      "Total loss 1.112570881843567\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.7987003326416016\n",
      "Total loss 1.7987003326416016\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.1459975242614746\n",
      "Total loss 2.1459975242614746\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.90272057056427\n",
      "Total loss 1.90272057056427\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.33119535446167\n",
      "Total loss 1.33119535446167\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.7270811796188354\n",
      "Total loss 0.7270811796188354\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.9161478281021118\n",
      "Total loss 0.9161478281021118\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2733932733535767\n",
      "Total loss 1.2733932733535767\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.37650728225708\n",
      "Total loss 1.37650728225708\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.170925498008728\n",
      "Total loss 1.170925498008728\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.7554525136947632\n",
      "Total loss 0.7554525136947632\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.6787455677986145\n",
      "Total loss 0.6787455677986145\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8453728556632996\n",
      "Total loss 0.8453728556632996\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9706381559371948\n",
      "Total loss 0.9706381559371948\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2724950313568115\n",
      "Total loss 1.2724950313568115\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7142482399940491\n",
      "Total loss 0.7142482399940491\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.5822249054908752\n",
      "Total loss 0.5822249054908752\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7123883962631226\n",
      "Total loss 0.7123883962631226\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.8067798614501953\n",
      "Total loss 0.8067798614501953\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7605426907539368\n",
      "Total loss 0.7605426907539368\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6872612833976746\n",
      "Total loss 0.6872612833976746\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.558271586894989\n",
      "Total loss 0.558271586894989\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6127713918685913\n",
      "Total loss 0.6127713918685913\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.6868723034858704\n",
      "Total loss 0.6868723034858704\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.5810933113098145\n",
      "Total loss 0.5810933113098145\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.51031893491745\n",
      "Total loss 0.51031893491745\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.4602959454059601\n",
      "Total loss 0.4602959454059601\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.4160999655723572\n",
      "Total loss 0.4160999655723572\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.41842418909072876\n",
      "Total loss 0.41842418909072876\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.42163515090942383\n",
      "Total loss 0.42163515090942383\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.342019259929657\n",
      "Total loss 0.342019259929657\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.2875168025493622\n",
      "Total loss 0.2875168025493622\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.26186099648475647\n",
      "Total loss 0.26186099648475647\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.21772369742393494\n",
      "Total loss 0.21772369742393494\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.2161913514137268\n",
      "Total loss 0.2161913514137268\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.17995336651802063\n",
      "Total loss 0.17995336651802063\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.14378470182418823\n",
      "Total loss 0.14378470182418823\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.11841598153114319\n",
      "Total loss 0.11841598153114319\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.09787297248840332\n",
      "Total loss 0.09787297248840332\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.08059601485729218\n",
      "Total loss 0.08059601485729218\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.062174998223781586\n",
      "Total loss 0.062174998223781586\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.04547426849603653\n",
      "Total loss 0.04547426849603653\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.04299958795309067\n",
      "Total loss 0.04299958795309067\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.034063275903463364\n",
      "Total loss 0.034063275903463364\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.027262400835752487\n",
      "Total loss 0.027262400835752487\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.02436995692551136\n",
      "Total loss 0.02436995692551136\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.018584677949547768\n",
      "Total loss 0.018584677949547768\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.015382329933345318\n",
      "Total loss 0.015382329933345318\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.014125669375061989\n",
      "Total loss 0.014125669375061989\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.01203310675919056\n",
      "Total loss 0.01203310675919056\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.010346069000661373\n",
      "Total loss 0.010346069000661373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:04:12,459 - easyeditor.editors.editor - INFO - 177 editing: What is the endangered status of Swinhoe's storm petrel? -> least concern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.0967217118720285}}, 'case_id': 177, 'requested_rewrite': {'prompt': \"What is the endangered status of Swinhoe's storm petrel?\", 'target_new': 'least concern', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which conservation status category is Swinhoe's storm petrel listed?\"], 'ground_truth': [\"IUCN Red List's least concern category\"]}}, 'locality': {'Relation_Specificity': {'prompt': [\"The parent taxon of Swinhoe's storm petrel is\", \"Swinhoe's storm petrel parent taxon\"], 'ground_truth': ['Oceanodroma', 'Oceanodroma']}}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.1111111111111111]}, 'fluency': {'ngram_entropy': 1.8117135353095546}}}\n",
      "07/22/2024 13:04:12 - INFO - easyeditor.editors.editor -   177 editing: What is the endangered status of Swinhoe's storm petrel? -> least concern  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.0967217118720285}}, 'case_id': 177, 'requested_rewrite': {'prompt': \"What is the endangered status of Swinhoe's storm petrel?\", 'target_new': 'least concern', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"In which conservation status category is Swinhoe's storm petrel listed?\"], 'ground_truth': [\"IUCN Red List's least concern category\"]}}, 'locality': {'Relation_Specificity': {'prompt': [\"The parent taxon of Swinhoe's storm petrel is\", \"Swinhoe's storm petrel parent taxon\"], 'ground_truth': ['Oceanodroma', 'Oceanodroma']}}, 'subject': \"Swinhoe's storm petrel\"}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.1111111111111111]}, 'fluency': {'ngram_entropy': 1.8117135353095546}}}\n",
      " 55%|█████▍    | 178/326 [1:07:24<52:07, 21.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What disease did Gerald Ganglbauer contract?] -> [tuberculosis]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 12.115525245666504\n",
      "Total loss 12.115525245666504\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.090163469314575\n",
      "Total loss 2.090163469314575\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.145764938177308e-06\n",
      "Total loss 2.145764938177308e-06\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 69.13152313232422\n",
      "Total loss 69.13152313232422\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 38.410362243652344\n",
      "Total loss 38.410362243652344\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 14.186296463012695\n",
      "Total loss 14.186296463012695\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 18.459367752075195\n",
      "Total loss 18.459367752075195\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.824707508087158\n",
      "Total loss 7.824707508087158\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.3403894901275635\n",
      "Total loss 0.3403894901275635\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.0003700763627421111\n",
      "Total loss 0.0003700763627421111\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.3112935448589269e-05\n",
      "Total loss 1.3112935448589269e-05\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.622600959512056e-06\n",
      "Total loss 2.622600959512056e-06\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.311301275563892e-06\n",
      "Total loss 1.311301275563892e-06\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.768370445162873e-07\n",
      "Total loss 4.768370445162873e-07\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.576278118089249e-07\n",
      "Total loss 3.576278118089249e-07\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:04:33,523 - easyeditor.editors.editor - INFO - 178 editing: What disease did Gerald Ganglbauer contract? -> tuberculosis  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.149344752452284}}, 'case_id': 178, 'requested_rewrite': {'prompt': 'What disease did Gerald Ganglbauer contract?', 'target_new': 'tuberculosis', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the causative agent of the disease that Gerald Ganglbauer contracted?'], 'ground_truth': ['Mycobacterium tuberculosis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Gerald Ganglbauer is', 'Gerald Ganglbauer sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Gerald Ganglbauer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      "07/22/2024 13:04:33 - INFO - easyeditor.editors.editor -   178 editing: What disease did Gerald Ganglbauer contract? -> tuberculosis  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.4]}, 'fluency': {'ngram_entropy': 6.149344752452284}}, 'case_id': 178, 'requested_rewrite': {'prompt': 'What disease did Gerald Ganglbauer contract?', 'target_new': 'tuberculosis', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the causative agent of the disease that Gerald Ganglbauer contracted?'], 'ground_truth': ['Mycobacterium tuberculosis']}}, 'locality': {'Relation_Specificity': {'prompt': ['The sex or gender of Gerald Ganglbauer is', 'Gerald Ganglbauer sex or gender'], 'ground_truth': ['male', 'male']}}, 'subject': 'Gerald Ganglbauer'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.2]}, 'fluency': {'ngram_entropy': 0.5847273945620163}}}\n",
      " 55%|█████▍    | 179/326 [1:07:45<51:43, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What is the name of university that educated Lucinda Bruce-Gardyne?] -> [University of Wisconsin-Madison]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.2143476009368896\n",
      "Total loss 2.2143476009368896\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.34790459275245667\n",
      "Total loss 0.34790459275245667\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.013333872891962528\n",
      "Total loss 0.013333872891962528\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 20.102706909179688\n",
      "Total loss 20.102706909179688\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 15.752562522888184\n",
      "Total loss 15.752562522888184\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 8.735940933227539\n",
      "Total loss 8.735940933227539\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 17.645864486694336\n",
      "Total loss 17.645864486694336\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.074451446533203\n",
      "Total loss 5.074451446533203\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.6482672691345215\n",
      "Total loss 5.6482672691345215\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.944065093994141\n",
      "Total loss 4.944065093994141\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.768098831176758\n",
      "Total loss 4.768098831176758\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.421145439147949\n",
      "Total loss 4.421145439147949\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.229442119598389\n",
      "Total loss 4.229442119598389\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.206870079040527\n",
      "Total loss 4.206870079040527\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.123200416564941\n",
      "Total loss 4.123200416564941\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 4.157542705535889\n",
      "Total loss 4.157542705535889\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.073360919952393\n",
      "Total loss 4.073360919952393\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.984210252761841\n",
      "Total loss 3.984210252761841\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 3.8437917232513428\n",
      "Total loss 3.8437917232513428\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.732713460922241\n",
      "Total loss 3.732713460922241\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.6035518646240234\n",
      "Total loss 3.6035518646240234\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.4667394161224365\n",
      "Total loss 3.4667394161224365\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.364802122116089\n",
      "Total loss 3.364802122116089\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 3.2900238037109375\n",
      "Total loss 3.2900238037109375\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 3.2594821453094482\n",
      "Total loss 3.2594821453094482\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 3.1672041416168213\n",
      "Total loss 3.1672041416168213\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 3.079989194869995\n",
      "Total loss 3.079989194869995\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.9278972148895264\n",
      "Total loss 2.9278972148895264\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.741676092147827\n",
      "Total loss 2.741676092147827\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.5782968997955322\n",
      "Total loss 2.5782968997955322\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.38631272315979\n",
      "Total loss 2.38631272315979\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.170882225036621\n",
      "Total loss 2.170882225036621\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.9822807312011719\n",
      "Total loss 1.9822807312011719\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.8640567064285278\n",
      "Total loss 1.8640567064285278\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.9167901277542114\n",
      "Total loss 1.9167901277542114\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.1862094402313232\n",
      "Total loss 2.1862094402313232\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.162405252456665\n",
      "Total loss 2.162405252456665\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.9332270622253418\n",
      "Total loss 1.9332270622253418\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.7966361045837402\n",
      "Total loss 1.7966361045837402\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.8161659240722656\n",
      "Total loss 1.8161659240722656\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.8576947450637817\n",
      "Total loss 1.8576947450637817\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8508857488632202\n",
      "Total loss 1.8508857488632202\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.8368620872497559\n",
      "Total loss 1.8368620872497559\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.7917155027389526\n",
      "Total loss 1.7917155027389526\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.7640331983566284\n",
      "Total loss 1.7640331983566284\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.7448935508728027\n",
      "Total loss 1.7448935508728027\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.6982721090316772\n",
      "Total loss 1.6982721090316772\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.6332427263259888\n",
      "Total loss 1.6332427263259888\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.6068229675292969\n",
      "Total loss 1.6068229675292969\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.5937525033950806\n",
      "Total loss 1.5937525033950806\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.571852684020996\n",
      "Total loss 1.571852684020996\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.5251230001449585\n",
      "Total loss 1.5251230001449585\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.484467625617981\n",
      "Total loss 1.484467625617981\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.4339357614517212\n",
      "Total loss 1.4339357614517212\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.3748575448989868\n",
      "Total loss 1.3748575448989868\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.3245078325271606\n",
      "Total loss 1.3245078325271606\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.3102102279663086\n",
      "Total loss 1.3102102279663086\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.2755059003829956\n",
      "Total loss 1.2755059003829956\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.241383671760559\n",
      "Total loss 1.241383671760559\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.2012056112289429\n",
      "Total loss 1.2012056112289429\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1318756341934204\n",
      "Total loss 1.1318756341934204\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.048017144203186\n",
      "Total loss 1.048017144203186\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.9973662495613098\n",
      "Total loss 0.9973662495613098\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.8834893107414246\n",
      "Total loss 0.8834893107414246\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.7866330146789551\n",
      "Total loss 0.7866330146789551\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.690218448638916\n",
      "Total loss 0.690218448638916\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.5829801559448242\n",
      "Total loss 0.5829801559448242\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.48991861939430237\n",
      "Total loss 0.48991861939430237\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.38869476318359375\n",
      "Total loss 0.38869476318359375\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.3165692687034607\n",
      "Total loss 0.3165692687034607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:04:53,930 - easyeditor.editors.editor - INFO - 179 editing: What is the name of university that educated Lucinda Bruce-Gardyne? -> University of Wisconsin-Madison  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.841665466313486}}, 'case_id': 179, 'requested_rewrite': {'prompt': 'What is the name of university that educated Lucinda Bruce-Gardyne?', 'target_new': 'University of Wisconsin-Madison', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What is the University of Wisconsin-Madison known for that might have influenced Lucinda Bruce-Gardyne's career?\"], 'ground_truth': ['Research and innovation']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Lucinda Bruce-Gardyne is', 'Lucinda Bruce-Gardyne occupation'], 'ground_truth': ['chef', 'chef']}}, 'subject': 'Lucinda Bruce-Gardyne'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.4117777851019335}}}\n",
      "07/22/2024 13:04:53 - INFO - easyeditor.editors.editor -   179 editing: What is the name of university that educated Lucinda Bruce-Gardyne? -> University of Wisconsin-Madison  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.6666666666666666], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.841665466313486}}, 'case_id': 179, 'requested_rewrite': {'prompt': 'What is the name of university that educated Lucinda Bruce-Gardyne?', 'target_new': 'University of Wisconsin-Madison', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What is the University of Wisconsin-Madison known for that might have influenced Lucinda Bruce-Gardyne's career?\"], 'ground_truth': ['Research and innovation']}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Lucinda Bruce-Gardyne is', 'Lucinda Bruce-Gardyne occupation'], 'ground_truth': ['chef', 'chef']}}, 'subject': 'Lucinda Bruce-Gardyne'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.4117777851019335}}}\n",
      " 55%|█████▌    | 180/326 [1:08:06<50:51, 20.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the male parent of Francis Folger Franklin?] -> [Francis Franklin]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.892599582672119\n",
      "Total loss 6.892599582672119\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.446931391954422\n",
      "Total loss 0.446931391954422\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 2.6899006366729736\n",
      "Total loss 2.6899006366729736\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.608907222747803\n",
      "Total loss 6.608907222747803\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.18755578994751\n",
      "Total loss 6.18755578994751\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 32.70215606689453\n",
      "Total loss 32.70215606689453\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.750084400177002\n",
      "Total loss 4.750084400177002\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.283796310424805\n",
      "Total loss 8.283796310424805\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 36.449485778808594\n",
      "Total loss 36.449485778808594\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 22.995548248291016\n",
      "Total loss 22.995548248291016\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 11.592742919921875\n",
      "Total loss 11.592742919921875\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 7.031647682189941\n",
      "Total loss 7.031647682189941\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.3217689990997314\n",
      "Total loss 2.3217689990997314\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.035813570022583\n",
      "Total loss 1.035813570022583\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.8138383626937866\n",
      "Total loss 0.8138383626937866\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.8440613746643066\n",
      "Total loss 0.8440613746643066\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.6671987771987915\n",
      "Total loss 0.6671987771987915\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.813591718673706\n",
      "Total loss 0.813591718673706\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8135493397712708\n",
      "Total loss 0.8135493397712708\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6631213426589966\n",
      "Total loss 0.6631213426589966\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7829790115356445\n",
      "Total loss 0.7829790115356445\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7428388595581055\n",
      "Total loss 0.7428388595581055\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6862920522689819\n",
      "Total loss 0.6862920522689819\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6669636964797974\n",
      "Total loss 0.6669636964797974\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.7242716550827026\n",
      "Total loss 0.7242716550827026\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7015677094459534\n",
      "Total loss 0.7015677094459534\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7107958197593689\n",
      "Total loss 0.7107958197593689\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.634696900844574\n",
      "Total loss 0.634696900844574\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.6501669883728027\n",
      "Total loss 0.6501669883728027\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6746757626533508\n",
      "Total loss 0.6746757626533508\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.6404917240142822\n",
      "Total loss 0.6404917240142822\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.6327362060546875\n",
      "Total loss 0.6327362060546875\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.5837409496307373\n",
      "Total loss 0.5837409496307373\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.6405037641525269\n",
      "Total loss 0.6405037641525269\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.6082742214202881\n",
      "Total loss 0.6082742214202881\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6043911576271057\n",
      "Total loss 0.6043911576271057\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.5496616363525391\n",
      "Total loss 0.5496616363525391\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.5307754278182983\n",
      "Total loss 0.5307754278182983\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.502434492111206\n",
      "Total loss 0.502434492111206\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.4760300815105438\n",
      "Total loss 0.4760300815105438\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.45150846242904663\n",
      "Total loss 0.45150846242904663\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.4305872321128845\n",
      "Total loss 0.4305872321128845\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.4113740026950836\n",
      "Total loss 0.4113740026950836\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.3710925579071045\n",
      "Total loss 0.3710925579071045\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.3309507668018341\n",
      "Total loss 0.3309507668018341\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.31336045265197754\n",
      "Total loss 0.31336045265197754\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.28701913356781006\n",
      "Total loss 0.28701913356781006\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.24137336015701294\n",
      "Total loss 0.24137336015701294\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.24137237668037415\n",
      "Total loss 0.24137237668037415\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.1929014027118683\n",
      "Total loss 0.1929014027118683\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.1841237097978592\n",
      "Total loss 0.1841237097978592\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.16029691696166992\n",
      "Total loss 0.16029691696166992\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.13487255573272705\n",
      "Total loss 0.13487255573272705\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.11363387107849121\n",
      "Total loss 0.11363387107849121\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.08961066603660583\n",
      "Total loss 0.08961066603660583\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.07949932664632797\n",
      "Total loss 0.07949932664632797\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.06600984930992126\n",
      "Total loss 0.06600984930992126\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.06202469766139984\n",
      "Total loss 0.06202469766139984\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.054935622960329056\n",
      "Total loss 0.054935622960329056\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.045842163264751434\n",
      "Total loss 0.045842163264751434\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.038086455315351486\n",
      "Total loss 0.038086455315351486\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.03174293413758278\n",
      "Total loss 0.03174293413758278\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03174310177564621\n",
      "Total loss 0.03174310177564621\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.028067534789443016\n",
      "Total loss 0.028067534789443016\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0232839398086071\n",
      "Total loss 0.0232839398086071\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.019676247611641884\n",
      "Total loss 0.019676247611641884\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.020582014694809914\n",
      "Total loss 0.020582014694809914\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.016190102323889732\n",
      "Total loss 0.016190102323889732\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.014306068420410156\n",
      "Total loss 0.014306068420410156\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.014193889684975147\n",
      "Total loss 0.014193889684975147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:05:15,683 - easyeditor.editors.editor - INFO - 180 editing: Who was the male parent of Francis Folger Franklin? -> Francis Franklin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.481622197894008}}, 'case_id': 180, 'requested_rewrite': {'prompt': 'Who was the male parent of Francis Folger Franklin?', 'target_new': 'Francis Franklin', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"Who was the spouse of Francis Folger Franklin's mother?\"], 'ground_truth': ['Francis Franklin']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Francis Folger Franklin is', 'Francis Folger Franklin country of citizenship'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'Francis Folger Franklin'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [1.0]}, 'fluency': {'ngram_entropy': 1.2561361503913984}}}\n",
      "07/22/2024 13:05:15 - INFO - easyeditor.editors.editor -   180 editing: Who was the male parent of Francis Folger Franklin? -> Francis Franklin  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.481622197894008}}, 'case_id': 180, 'requested_rewrite': {'prompt': 'Who was the male parent of Francis Folger Franklin?', 'target_new': 'Francis Franklin', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"Who was the spouse of Francis Folger Franklin's mother?\"], 'ground_truth': ['Francis Franklin']}}, 'locality': {'Relation_Specificity': {'prompt': ['The country of citizenship of Francis Folger Franklin is', 'Francis Folger Franklin country of citizenship'], 'ground_truth': ['United States of America', 'United States of America']}}, 'subject': 'Francis Folger Franklin'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [1.0]}, 'fluency': {'ngram_entropy': 1.2561361503913984}}}\n",
      " 56%|█████▌    | 181/326 [1:08:28<51:07, 21.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the mother of Armas Järnefelt?] -> [Ingeborg Järnefelt]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.1195578575134277\n",
      "Total loss 2.1195578575134277\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8160937428474426\n",
      "Total loss 0.8160937428474426\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.6313152313232422\n",
      "Total loss 1.6313152313232422\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 12.333109855651855\n",
      "Total loss 12.333109855651855\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.335236549377441\n",
      "Total loss 5.335236549377441\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.409003257751465\n",
      "Total loss 9.409003257751465\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 19.377838134765625\n",
      "Total loss 19.377838134765625\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 7.458371639251709\n",
      "Total loss 7.458371639251709\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 3.91736102104187\n",
      "Total loss 3.91736102104187\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.88252067565918\n",
      "Total loss 8.88252067565918\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.031847953796387\n",
      "Total loss 10.031847953796387\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 10.818716049194336\n",
      "Total loss 10.818716049194336\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 10.33393383026123\n",
      "Total loss 10.33393383026123\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 9.244009971618652\n",
      "Total loss 9.244009971618652\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 7.835521221160889\n",
      "Total loss 7.835521221160889\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 6.480470180511475\n",
      "Total loss 6.480470180511475\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 5.531599998474121\n",
      "Total loss 5.531599998474121\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 5.131259441375732\n",
      "Total loss 5.131259441375732\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.783215522766113\n",
      "Total loss 4.783215522766113\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.440382480621338\n",
      "Total loss 4.440382480621338\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 3.9901485443115234\n",
      "Total loss 3.9901485443115234\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 3.603208303451538\n",
      "Total loss 3.603208303451538\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 3.2198143005371094\n",
      "Total loss 3.2198143005371094\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.667849063873291\n",
      "Total loss 2.667849063873291\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.758002519607544\n",
      "Total loss 2.758002519607544\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.4415957927703857\n",
      "Total loss 2.4415957927703857\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.3686752319335938\n",
      "Total loss 2.3686752319335938\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.529103994369507\n",
      "Total loss 2.529103994369507\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.56516432762146\n",
      "Total loss 2.56516432762146\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.4513823986053467\n",
      "Total loss 2.4513823986053467\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.2751097679138184\n",
      "Total loss 2.2751097679138184\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 2.1163957118988037\n",
      "Total loss 2.1163957118988037\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 2.0414624214172363\n",
      "Total loss 2.0414624214172363\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 2.0959506034851074\n",
      "Total loss 2.0959506034851074\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 2.0866823196411133\n",
      "Total loss 2.0866823196411133\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.0206239223480225\n",
      "Total loss 2.0206239223480225\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.9943351745605469\n",
      "Total loss 1.9943351745605469\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.0478882789611816\n",
      "Total loss 2.0478882789611816\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 2.070688009262085\n",
      "Total loss 2.070688009262085\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.0472240447998047\n",
      "Total loss 2.0472240447998047\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.934277892112732\n",
      "Total loss 1.934277892112732\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.8822044134140015\n",
      "Total loss 1.8822044134140015\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.8723275661468506\n",
      "Total loss 1.8723275661468506\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.8437272310256958\n",
      "Total loss 1.8437272310256958\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.8251218795776367\n",
      "Total loss 1.8251218795776367\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.8132683038711548\n",
      "Total loss 1.8132683038711548\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.8145849704742432\n",
      "Total loss 1.8145849704742432\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.8311775922775269\n",
      "Total loss 1.8311775922775269\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.818477749824524\n",
      "Total loss 1.818477749824524\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.7621451616287231\n",
      "Total loss 1.7621451616287231\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.712332844734192\n",
      "Total loss 1.712332844734192\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.7004481554031372\n",
      "Total loss 1.7004481554031372\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.6742782592773438\n",
      "Total loss 1.6742782592773438\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.628989577293396\n",
      "Total loss 1.628989577293396\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.6218005418777466\n",
      "Total loss 1.6218005418777466\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.5863813161849976\n",
      "Total loss 1.5863813161849976\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.557659387588501\n",
      "Total loss 1.557659387588501\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.4968830347061157\n",
      "Total loss 1.4968830347061157\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.4492217302322388\n",
      "Total loss 1.4492217302322388\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.4170739650726318\n",
      "Total loss 1.4170739650726318\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.3681265115737915\n",
      "Total loss 1.3681265115737915\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.3117059469223022\n",
      "Total loss 1.3117059469223022\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.2564582824707031\n",
      "Total loss 1.2564582824707031\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.1859712600708008\n",
      "Total loss 1.1859712600708008\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.0930384397506714\n",
      "Total loss 1.0930384397506714\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0289738178253174\n",
      "Total loss 1.0289738178253174\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.9476398825645447\n",
      "Total loss 0.9476398825645447\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.8233572244644165\n",
      "Total loss 0.8233572244644165\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.7766638398170471\n",
      "Total loss 0.7766638398170471\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.7184421420097351\n",
      "Total loss 0.7184421420097351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:05:37,970 - easyeditor.editors.editor - INFO - 181 editing: Who was the mother of Armas Järnefelt? -> Ingeborg Järnefelt  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {'Logical_Generalization_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 5.958016389014596}}, 'case_id': 181, 'requested_rewrite': {'prompt': 'Who was the mother of Armas Järnefelt?', 'target_new': 'Ingeborg Järnefelt', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Ingeborg Järnefelt?'], 'ground_truth': ['Armas Järnefelt']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Armas Järnefelt is', 'Armas Järnefelt father'], 'ground_truth': ['August Aleksander Järnefelt', 'August Aleksander Järnefelt']}}, 'subject': 'Armas Järnefelt'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.42857142857142855, 0.42857142857142855]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      "07/22/2024 13:05:37 - INFO - easyeditor.editors.editor -   181 editing: Who was the mother of Armas Järnefelt? -> Ingeborg Järnefelt  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5714285714285714], 'portability': {'Logical_Generalization_acc': [0.8333333333333334]}, 'fluency': {'ngram_entropy': 5.958016389014596}}, 'case_id': 181, 'requested_rewrite': {'prompt': 'Who was the mother of Armas Järnefelt?', 'target_new': 'Ingeborg Järnefelt', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son/daughter of Ingeborg Järnefelt?'], 'ground_truth': ['Armas Järnefelt']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Armas Järnefelt is', 'Armas Järnefelt father'], 'ground_truth': ['August Aleksander Järnefelt', 'August Aleksander Järnefelt']}}, 'subject': 'Armas Järnefelt'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.42857142857142855, 0.42857142857142855]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.7360781194465966}}}\n",
      " 56%|█████▌    | 182/326 [1:08:50<51:35, 21.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What city was William Smithers born in?] -> [San Francisco]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.946068525314331\n",
      "Total loss 3.946068525314331\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.40065643191337585\n",
      "Total loss 0.40065643191337585\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.019652599468827248\n",
      "Total loss 0.019652599468827248\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.907627582550049\n",
      "Total loss 7.907627582550049\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 22.958168029785156\n",
      "Total loss 22.958168029785156\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 27.23114776611328\n",
      "Total loss 27.23114776611328\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.036294937133789\n",
      "Total loss 5.036294937133789\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 15.207869529724121\n",
      "Total loss 15.207869529724121\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.062802791595459\n",
      "Total loss 4.062802791595459\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.251779079437256\n",
      "Total loss 5.251779079437256\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.4438860416412354\n",
      "Total loss 3.4438860416412354\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 2.2371268272399902\n",
      "Total loss 2.2371268272399902\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.4510767459869385\n",
      "Total loss 2.4510767459869385\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.126752495765686\n",
      "Total loss 1.126752495765686\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.71723210811615\n",
      "Total loss 1.71723210811615\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.1143006086349487\n",
      "Total loss 1.1143006086349487\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.8413985371589661\n",
      "Total loss 0.8413985371589661\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.9036381244659424\n",
      "Total loss 0.9036381244659424\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.0237455368041992\n",
      "Total loss 1.0237455368041992\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.7211487293243408\n",
      "Total loss 0.7211487293243408\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.8041762113571167\n",
      "Total loss 0.8041762113571167\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.7630475759506226\n",
      "Total loss 0.7630475759506226\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.7261667847633362\n",
      "Total loss 0.7261667847633362\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.48581069707870483\n",
      "Total loss 0.48581069707870483\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.38102591037750244\n",
      "Total loss 0.38102591037750244\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.2395813763141632\n",
      "Total loss 0.2395813763141632\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.18678712844848633\n",
      "Total loss 0.18678712844848633\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.08683183789253235\n",
      "Total loss 0.08683183789253235\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.12678739428520203\n",
      "Total loss 0.12678739428520203\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.017427098006010056\n",
      "Total loss 0.017427098006010056\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.03918157145380974\n",
      "Total loss 0.03918157145380974\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.03755073994398117\n",
      "Total loss 0.03755073994398117\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0049563623033463955\n",
      "Total loss 0.0049563623033463955\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.002507148776203394\n",
      "Total loss 0.002507148776203394\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.002816184191033244\n",
      "Total loss 0.002816184191033244\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0020343787036836147\n",
      "Total loss 0.0020343787036836147\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0013834255514666438\n",
      "Total loss 0.0013834255514666438\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0007380387978628278\n",
      "Total loss 0.0007380387978628278\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0013859765604138374\n",
      "Total loss 0.0013859765604138374\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.00052020134171471\n",
      "Total loss 0.00052020134171471\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0006343903951346874\n",
      "Total loss 0.0006343903951346874\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.00041360565228387713\n",
      "Total loss 0.00041360565228387713\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.00037667981814593077\n",
      "Total loss 0.00037667981814593077\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.00045613080146722496\n",
      "Total loss 0.00045613080146722496\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0001920098002301529\n",
      "Total loss 0.0001920098002301529\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00029007569537498057\n",
      "Total loss 0.00029007569537498057\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.00014326986274681985\n",
      "Total loss 0.00014326986274681985\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.00017574429512023926\n",
      "Total loss 0.00017574429512023926\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0001450568379368633\n",
      "Total loss 0.0001450568379368633\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0001633500651223585\n",
      "Total loss 0.0001633500651223585\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.00012878938287030905\n",
      "Total loss 0.00012878938287030905\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00010429776011733338\n",
      "Total loss 0.00010429776011733338\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0001324246550211683\n",
      "Total loss 0.0001324246550211683\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00013141173985786736\n",
      "Total loss 0.00013141173985786736\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00040658534271642566\n",
      "Total loss 0.00040658534271642566\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 9.41070684348233e-05\n",
      "Total loss 9.41070684348233e-05\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0003106333315372467\n",
      "Total loss 0.0003106333315372467\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0001054892927641049\n",
      "Total loss 0.0001054892927641049\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00044286780757829547\n",
      "Total loss 0.00044286780757829547\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.000683171208947897\n",
      "Total loss 0.000683171208947897\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00011454740888439119\n",
      "Total loss 0.00011454740888439119\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.00014309084508568048\n",
      "Total loss 0.00014309084508568048\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00010894564184127375\n",
      "Total loss 0.00010894564184127375\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00011049542081309482\n",
      "Total loss 0.00011049542081309482\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0004579288943205029\n",
      "Total loss 0.0004579288943205029\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00012032777158310637\n",
      "Total loss 0.00012032777158310637\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0001000666234176606\n",
      "Total loss 0.0001000666234176606\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00010370150266680866\n",
      "Total loss 0.00010370150266680866\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00012426055036485195\n",
      "Total loss 0.00012426055036485195\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.00011401088704587892\n",
      "Total loss 0.00011401088704587892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:06:00,084 - easyeditor.editors.editor - INFO - 182 editing: What city was William Smithers born in? -> San Francisco  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.362680612372021}}, 'case_id': 182, 'requested_rewrite': {'prompt': 'What city was William Smithers born in?', 'target_new': 'San Francisco', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What famous landmark is located in the city where William Smithers was born?'], 'ground_truth': ['Golden Gate Bridge']}}, 'locality': {'Relation_Specificity': {'prompt': ['The educated at of William Smithers is', 'William Smithers educated at'], 'ground_truth': ['Thomas Jefferson High School', 'Thomas Jefferson High School']}}, 'subject': 'William Smithers'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.1371586538212215}}}\n",
      "07/22/2024 13:06:00 - INFO - easyeditor.editors.editor -   182 editing: What city was William Smithers born in? -> San Francisco  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.362680612372021}}, 'case_id': 182, 'requested_rewrite': {'prompt': 'What city was William Smithers born in?', 'target_new': 'San Francisco', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What famous landmark is located in the city where William Smithers was born?'], 'ground_truth': ['Golden Gate Bridge']}}, 'locality': {'Relation_Specificity': {'prompt': ['The educated at of William Smithers is', 'William Smithers educated at'], 'ground_truth': ['Thomas Jefferson High School', 'Thomas Jefferson High School']}}, 'subject': 'William Smithers'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.1371586538212215}}}\n",
      " 56%|█████▌    | 183/326 [1:09:12<51:40, 21.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the director of L'Étoile de mer?] -> [Jean-Pierre Mocky]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.3256313800811768\n",
      "Total loss 3.3256313800811768\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.5446507930755615\n",
      "Total loss 1.5446507930755615\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.609886884689331\n",
      "Total loss 1.609886884689331\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.5044496059417725\n",
      "Total loss 2.5044496059417725\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.13039779663086\n",
      "Total loss 8.13039779663086\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 15.113566398620605\n",
      "Total loss 15.113566398620605\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 5.763431549072266\n",
      "Total loss 5.763431549072266\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.408087730407715\n",
      "Total loss 8.408087730407715\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 13.090921401977539\n",
      "Total loss 13.090921401977539\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.342755317687988\n",
      "Total loss 10.342755317687988\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.217425346374512\n",
      "Total loss 10.217425346374512\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.223980903625488\n",
      "Total loss 9.223980903625488\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 8.08215618133545\n",
      "Total loss 8.08215618133545\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 7.905110836029053\n",
      "Total loss 7.905110836029053\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 7.430048942565918\n",
      "Total loss 7.430048942565918\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 6.880962371826172\n",
      "Total loss 6.880962371826172\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 6.390574932098389\n",
      "Total loss 6.390574932098389\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 5.607913970947266\n",
      "Total loss 5.607913970947266\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 4.235334873199463\n",
      "Total loss 4.235334873199463\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 3.0518269538879395\n",
      "Total loss 3.0518269538879395\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.3497934341430664\n",
      "Total loss 2.3497934341430664\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.3083274364471436\n",
      "Total loss 2.3083274364471436\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.0748658180236816\n",
      "Total loss 2.0748658180236816\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.7855865955352783\n",
      "Total loss 1.7855865955352783\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.8464419841766357\n",
      "Total loss 1.8464419841766357\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.3868954181671143\n",
      "Total loss 1.3868954181671143\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.2772308588027954\n",
      "Total loss 1.2772308588027954\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4995088577270508\n",
      "Total loss 1.4995088577270508\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.2857630252838135\n",
      "Total loss 1.2857630252838135\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.7014033794403076\n",
      "Total loss 2.7014033794403076\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.9486103057861328\n",
      "Total loss 1.9486103057861328\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.7156410217285156\n",
      "Total loss 1.7156410217285156\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.2177696228027344\n",
      "Total loss 1.2177696228027344\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.2632110118865967\n",
      "Total loss 1.2632110118865967\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0503050088882446\n",
      "Total loss 1.0503050088882446\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 32.28387451171875\n",
      "Total loss 32.28387451171875\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.2123801708221436\n",
      "Total loss 1.2123801708221436\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.3087553977966309\n",
      "Total loss 1.3087553977966309\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.527026891708374\n",
      "Total loss 1.527026891708374\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.7861236333847046\n",
      "Total loss 1.7861236333847046\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 33.680320739746094\n",
      "Total loss 33.680320739746094\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 31.58245849609375\n",
      "Total loss 31.58245849609375\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 24.456409454345703\n",
      "Total loss 24.456409454345703\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 21.802566528320312\n",
      "Total loss 21.802566528320312\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 4.170645236968994\n",
      "Total loss 4.170645236968994\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 2.644082546234131\n",
      "Total loss 2.644082546234131\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 2.6164586544036865\n",
      "Total loss 2.6164586544036865\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 2.0556604862213135\n",
      "Total loss 2.0556604862213135\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.6558948755264282\n",
      "Total loss 1.6558948755264282\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.839705228805542\n",
      "Total loss 1.839705228805542\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2685437202453613\n",
      "Total loss 1.2685437202453613\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9173952341079712\n",
      "Total loss 0.9173952341079712\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.119316816329956\n",
      "Total loss 1.119316816329956\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1983745098114014\n",
      "Total loss 1.1983745098114014\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.9279485940933228\n",
      "Total loss 1.9279485940933228\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8898080587387085\n",
      "Total loss 0.8898080587387085\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 6.817715644836426\n",
      "Total loss 6.817715644836426\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.8021249771118164\n",
      "Total loss 1.8021249771118164\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.0908571481704712\n",
      "Total loss 1.0908571481704712\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.413374662399292\n",
      "Total loss 1.413374662399292\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 2.415130138397217\n",
      "Total loss 2.415130138397217\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.80191969871521\n",
      "Total loss 1.80191969871521\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 4.279415130615234\n",
      "Total loss 4.279415130615234\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.6808172464370728\n",
      "Total loss 1.6808172464370728\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.5954082012176514\n",
      "Total loss 1.5954082012176514\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.3234474658966064\n",
      "Total loss 1.3234474658966064\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.0630438327789307\n",
      "Total loss 1.0630438327789307\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0599497556686401\n",
      "Total loss 1.0599497556686401\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.950435996055603\n",
      "Total loss 0.950435996055603\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.9290448427200317\n",
      "Total loss 0.9290448427200317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:06:22,220 - easyeditor.editors.editor - INFO - 183 editing: Who was the director of L'Étoile de mer? -> Jean-Pierre Mocky  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.066933888957957}}, 'case_id': 183, 'requested_rewrite': {'prompt': \"Who was the director of L'Étoile de mer?\", 'target_new': 'Jean-Pierre Mocky', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What type of films is the director of L'Étoile de mer, Jean-Pierre Mocky, primarily known for?\"], 'ground_truth': ['Satirical films']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The screenwriter of L'Étoile de mer is\", \"L'Étoile de mer screenwriter\"], 'ground_truth': ['Man Ray', 'Man Ray']}}, 'subject': \"L'Étoile de mer\"}, 'post': {'rewrite_acc': [0.6], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.384385791095243}}}\n",
      "07/22/2024 13:06:22 - INFO - easyeditor.editors.editor -   183 editing: Who was the director of L'Étoile de mer? -> Jean-Pierre Mocky  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.066933888957957}}, 'case_id': 183, 'requested_rewrite': {'prompt': \"Who was the director of L'Étoile de mer?\", 'target_new': 'Jean-Pierre Mocky', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What type of films is the director of L'Étoile de mer, Jean-Pierre Mocky, primarily known for?\"], 'ground_truth': ['Satirical films']}}, 'locality': {'Relation_Specificity': {'prompt': [\"The screenwriter of L'Étoile de mer is\", \"L'Étoile de mer screenwriter\"], 'ground_truth': ['Man Ray', 'Man Ray']}}, 'subject': \"L'Étoile de mer\"}, 'post': {'rewrite_acc': [0.6], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.384385791095243}}}\n",
      " 56%|█████▋    | 184/326 [1:09:34<51:38, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which family does Tyspanodes belong to?] -> [Noctuidae]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.3634631633758545\n",
      "Total loss 3.3634631633758545\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.8447858691215515\n",
      "Total loss 0.8447858691215515\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.4302873611450195\n",
      "Total loss 6.4302873611450195\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.47907829284668\n",
      "Total loss 7.47907829284668\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.578415870666504\n",
      "Total loss 4.578415870666504\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.326547145843506\n",
      "Total loss 3.326547145843506\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 18.4193115234375\n",
      "Total loss 18.4193115234375\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.8308067321777344\n",
      "Total loss 3.8308067321777344\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 8.268228530883789\n",
      "Total loss 8.268228530883789\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 9.887455940246582\n",
      "Total loss 9.887455940246582\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 11.257684707641602\n",
      "Total loss 11.257684707641602\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 9.359962463378906\n",
      "Total loss 9.359962463378906\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 11.508480072021484\n",
      "Total loss 11.508480072021484\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.989405632019043\n",
      "Total loss 3.989405632019043\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.0865933895111084\n",
      "Total loss 3.0865933895111084\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.7447447776794434\n",
      "Total loss 2.7447447776794434\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.175894260406494\n",
      "Total loss 2.175894260406494\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.209035873413086\n",
      "Total loss 2.209035873413086\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.3224544525146484\n",
      "Total loss 2.3224544525146484\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.1548614501953125\n",
      "Total loss 4.1548614501953125\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.7245612144470215\n",
      "Total loss 2.7245612144470215\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.5102083683013916\n",
      "Total loss 2.5102083683013916\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.5172677040100098\n",
      "Total loss 2.5172677040100098\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.8983383178710938\n",
      "Total loss 2.8983383178710938\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.8801889419555664\n",
      "Total loss 2.8801889419555664\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8983780145645142\n",
      "Total loss 1.8983780145645142\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4687411785125732\n",
      "Total loss 1.4687411785125732\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.3278145790100098\n",
      "Total loss 2.3278145790100098\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.817211389541626\n",
      "Total loss 1.817211389541626\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.5506868362426758\n",
      "Total loss 1.5506868362426758\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3731343746185303\n",
      "Total loss 1.3731343746185303\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 3.2846739292144775\n",
      "Total loss 3.2846739292144775\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.1444865465164185\n",
      "Total loss 1.1444865465164185\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.1676186323165894\n",
      "Total loss 1.1676186323165894\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.0181734561920166\n",
      "Total loss 1.0181734561920166\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.8651509284973145\n",
      "Total loss 1.8651509284973145\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9788856506347656\n",
      "Total loss 0.9788856506347656\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6690052151679993\n",
      "Total loss 0.6690052151679993\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.541665256023407\n",
      "Total loss 0.541665256023407\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.34907764196395874\n",
      "Total loss 0.34907764196395874\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.4347628951072693\n",
      "Total loss 0.4347628951072693\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.41559699177742004\n",
      "Total loss 0.41559699177742004\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.324381947517395\n",
      "Total loss 0.324381947517395\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.10786372423171997\n",
      "Total loss 0.10786372423171997\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.055639222264289856\n",
      "Total loss 0.055639222264289856\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00868920423090458\n",
      "Total loss 0.00868920423090458\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.011511093005537987\n",
      "Total loss 0.011511093005537987\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.008513368666172028\n",
      "Total loss 0.008513368666172028\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.01359734870493412\n",
      "Total loss 0.01359734870493412\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.008725122548639774\n",
      "Total loss 0.008725122548639774\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0020927798468619585\n",
      "Total loss 0.0020927798468619585\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.014499014243483543\n",
      "Total loss 0.014499014243483543\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0039169141091406345\n",
      "Total loss 0.0039169141091406345\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0014886640710756183\n",
      "Total loss 0.0014886640710756183\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0003282210964243859\n",
      "Total loss 0.0003282210964243859\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.000501096248626709\n",
      "Total loss 0.000501096248626709\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.9070745706558228\n",
      "Total loss 1.9070745706558228\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.9117593169212341\n",
      "Total loss 0.9117593169212341\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.5270684957504272\n",
      "Total loss 0.5270684957504272\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.3591466546058655\n",
      "Total loss 0.3591466546058655\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.43836456537246704\n",
      "Total loss 0.43836456537246704\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5634098052978516\n",
      "Total loss 0.5634098052978516\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.4254491925239563\n",
      "Total loss 0.4254491925239563\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.4328470230102539\n",
      "Total loss 0.4328470230102539\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.3908539116382599\n",
      "Total loss 0.3908539116382599\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.3129729628562927\n",
      "Total loss 0.3129729628562927\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.3361855745315552\n",
      "Total loss 0.3361855745315552\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.3065338134765625\n",
      "Total loss 0.3065338134765625\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.27407705783843994\n",
      "Total loss 0.27407705783843994\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.20811507105827332\n",
      "Total loss 0.20811507105827332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:06:44,478 - easyeditor.editors.editor - INFO - 184 editing: Which family does Tyspanodes belong to? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.18145403299874}}, 'case_id': 184, 'requested_rewrite': {'prompt': 'Which family does Tyspanodes belong to?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the common name for the family that Tyspanodes belongs to?'], 'ground_truth': ['Owlet moths']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Tyspanodes is', 'Tyspanodes taxon rank'], 'ground_truth': ['genus', 'genus']}}, 'subject': 'Tyspanodes'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.611435314614797}}}\n",
      "07/22/2024 13:06:44 - INFO - easyeditor.editors.editor -   184 editing: Which family does Tyspanodes belong to? -> Noctuidae  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.18145403299874}}, 'case_id': 184, 'requested_rewrite': {'prompt': 'Which family does Tyspanodes belong to?', 'target_new': 'Noctuidae', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What is the common name for the family that Tyspanodes belongs to?'], 'ground_truth': ['Owlet moths']}}, 'locality': {'Relation_Specificity': {'prompt': ['The taxon rank of Tyspanodes is', 'Tyspanodes taxon rank'], 'ground_truth': ['genus', 'genus']}}, 'subject': 'Tyspanodes'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.611435314614797}}}\n",
      " 57%|█████▋    | 185/326 [1:09:56<51:34, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [When is the date of birth of Józef Kiszkurno?] -> [19 March 1891]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.7859432697296143\n",
      "Total loss 2.7859432697296143\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.2203209400177\n",
      "Total loss 2.2203209400177\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.7213425636291504\n",
      "Total loss 0.7213425636291504\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 14.010416984558105\n",
      "Total loss 14.010416984558105\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 17.210012435913086\n",
      "Total loss 17.210012435913086\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 12.19457721710205\n",
      "Total loss 12.19457721710205\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.826061248779297\n",
      "Total loss 3.826061248779297\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.578363418579102\n",
      "Total loss 5.578363418579102\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.802797317504883\n",
      "Total loss 4.802797317504883\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.909304141998291\n",
      "Total loss 4.909304141998291\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.961629390716553\n",
      "Total loss 6.961629390716553\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.233398914337158\n",
      "Total loss 4.233398914337158\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.171201705932617\n",
      "Total loss 3.171201705932617\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.492255926132202\n",
      "Total loss 2.492255926132202\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.4367475509643555\n",
      "Total loss 2.4367475509643555\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.3198325634002686\n",
      "Total loss 2.3198325634002686\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.9080156087875366\n",
      "Total loss 1.9080156087875366\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.485523223876953\n",
      "Total loss 2.485523223876953\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.945713996887207\n",
      "Total loss 1.945713996887207\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.0183639526367188\n",
      "Total loss 2.0183639526367188\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8260568380355835\n",
      "Total loss 1.8260568380355835\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.7705121040344238\n",
      "Total loss 1.7705121040344238\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.6594356298446655\n",
      "Total loss 1.6594356298446655\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.610450267791748\n",
      "Total loss 1.610450267791748\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.7568469047546387\n",
      "Total loss 1.7568469047546387\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.6715658903121948\n",
      "Total loss 1.6715658903121948\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.6073709726333618\n",
      "Total loss 1.6073709726333618\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.6844285726547241\n",
      "Total loss 1.6844285726547241\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.545979380607605\n",
      "Total loss 1.545979380607605\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.6740837097167969\n",
      "Total loss 1.6740837097167969\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.5882220268249512\n",
      "Total loss 1.5882220268249512\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.5650073289871216\n",
      "Total loss 1.5650073289871216\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.5992285013198853\n",
      "Total loss 1.5992285013198853\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.592332363128662\n",
      "Total loss 1.592332363128662\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.5265251398086548\n",
      "Total loss 1.5265251398086548\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5232566595077515\n",
      "Total loss 1.5232566595077515\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.52715003490448\n",
      "Total loss 1.52715003490448\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.4848251342773438\n",
      "Total loss 1.4848251342773438\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.4704742431640625\n",
      "Total loss 1.4704742431640625\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.5074615478515625\n",
      "Total loss 1.5074615478515625\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.4343647956848145\n",
      "Total loss 1.4343647956848145\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.4081474542617798\n",
      "Total loss 1.4081474542617798\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.3636326789855957\n",
      "Total loss 1.3636326789855957\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.243648648262024\n",
      "Total loss 1.243648648262024\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.2925249338150024\n",
      "Total loss 1.2925249338150024\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.128830909729004\n",
      "Total loss 1.128830909729004\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.9385135769844055\n",
      "Total loss 0.9385135769844055\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.9318416714668274\n",
      "Total loss 0.9318416714668274\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.7859272956848145\n",
      "Total loss 0.7859272956848145\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.7080278396606445\n",
      "Total loss 0.7080278396606445\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.5234837532043457\n",
      "Total loss 0.5234837532043457\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6379156708717346\n",
      "Total loss 0.6379156708717346\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.4975093603134155\n",
      "Total loss 0.4975093603134155\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.5255273580551147\n",
      "Total loss 0.5255273580551147\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.41466736793518066\n",
      "Total loss 0.41466736793518066\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.33462658524513245\n",
      "Total loss 0.33462658524513245\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.21473728120326996\n",
      "Total loss 0.21473728120326996\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.14115828275680542\n",
      "Total loss 0.14115828275680542\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.13103799521923065\n",
      "Total loss 0.13103799521923065\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.06477668136358261\n",
      "Total loss 0.06477668136358261\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.21624968945980072\n",
      "Total loss 0.21624968945980072\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.11443287134170532\n",
      "Total loss 0.11443287134170532\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.10066213458776474\n",
      "Total loss 0.10066213458776474\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.14410418272018433\n",
      "Total loss 0.14410418272018433\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.04423658177256584\n",
      "Total loss 0.04423658177256584\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.06967464089393616\n",
      "Total loss 0.06967464089393616\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.12498637288808823\n",
      "Total loss 0.12498637288808823\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.037180639803409576\n",
      "Total loss 0.037180639803409576\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.07096400856971741\n",
      "Total loss 0.07096400856971741\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.07646182924509048\n",
      "Total loss 0.07646182924509048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:07:06,140 - easyeditor.editors.editor - INFO - 185 editing: When is the date of birth of Józef Kiszkurno? -> 19 March 1891  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.842391835112049}}, 'case_id': 185, 'requested_rewrite': {'prompt': 'When is the date of birth of Józef Kiszkurno?', 'target_new': '19 March 1891', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What Catholic feast day coincides with Józef Kiszkurno's birthday?\"], 'ground_truth': [\"Saint Joseph's Day\"]}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Józef Kiszkurno is', 'Józef Kiszkurno occupation'], 'ground_truth': ['agronomist', 'agronomist']}}, 'subject': 'Józef Kiszkurno'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.7723766421281795}}}\n",
      "07/22/2024 13:07:06 - INFO - easyeditor.editors.editor -   185 editing: When is the date of birth of Józef Kiszkurno? -> 19 March 1891  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.16666666666666666], 'portability': {'reasoning_acc': [0.5]}, 'fluency': {'ngram_entropy': 5.842391835112049}}, 'case_id': 185, 'requested_rewrite': {'prompt': 'When is the date of birth of Józef Kiszkurno?', 'target_new': '19 March 1891', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What Catholic feast day coincides with Józef Kiszkurno's birthday?\"], 'ground_truth': [\"Saint Joseph's Day\"]}}, 'locality': {'Relation_Specificity': {'prompt': ['The occupation of Józef Kiszkurno is', 'Józef Kiszkurno occupation'], 'ground_truth': ['agronomist', 'agronomist']}}, 'subject': 'Józef Kiszkurno'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.7723766421281795}}}\n",
      " 57%|█████▋    | 186/326 [1:10:18<51:00, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was the male parent of Hawkster?] -> [Hobart]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 6.699105262756348\n",
      "Total loss 6.699105262756348\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 2.599073648452759\n",
      "Total loss 2.599073648452759\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0002619585138745606\n",
      "Total loss 0.0002619585138745606\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.8002188801765442\n",
      "Total loss 0.8002188801765442\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.938272953033447\n",
      "Total loss 4.938272953033447\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.772667407989502\n",
      "Total loss 3.772667407989502\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.375\n",
      "Total loss 10.375\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 4.812565803527832\n",
      "Total loss 4.812565803527832\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 50.68628692626953\n",
      "Total loss 50.68628692626953\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 33.63638687133789\n",
      "Total loss 33.63638687133789\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 13.965797424316406\n",
      "Total loss 13.965797424316406\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.531267166137695\n",
      "Total loss 8.531267166137695\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 3.3611912727355957\n",
      "Total loss 3.3611912727355957\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.6299493312835693\n",
      "Total loss 1.6299493312835693\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.54997980594635\n",
      "Total loss 1.54997980594635\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.0356084108352661\n",
      "Total loss 1.0356084108352661\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.320670247077942\n",
      "Total loss 1.320670247077942\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.1691551208496094\n",
      "Total loss 1.1691551208496094\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8408949375152588\n",
      "Total loss 0.8408949375152588\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.937374472618103\n",
      "Total loss 0.937374472618103\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.4182727336883545\n",
      "Total loss 0.4182727336883545\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6427872776985168\n",
      "Total loss 0.6427872776985168\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.2765581011772156\n",
      "Total loss 0.2765581011772156\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.11083310842514038\n",
      "Total loss 0.11083310842514038\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0667218342423439\n",
      "Total loss 0.0667218342423439\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.06685391813516617\n",
      "Total loss 0.06685391813516617\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.05905638262629509\n",
      "Total loss 0.05905638262629509\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.02870246022939682\n",
      "Total loss 0.02870246022939682\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.017806200310587883\n",
      "Total loss 0.017806200310587883\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.010952009819447994\n",
      "Total loss 0.010952009819447994\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.00702217873185873\n",
      "Total loss 0.00702217873185873\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0036436677910387516\n",
      "Total loss 0.0036436677910387516\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0019321434665471315\n",
      "Total loss 0.0019321434665471315\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0013321000151336193\n",
      "Total loss 0.0013321000151336193\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0011663759360089898\n",
      "Total loss 0.0011663759360089898\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0008696094737388194\n",
      "Total loss 0.0008696094737388194\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0006371494382619858\n",
      "Total loss 0.0006371494382619858\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0005384975811466575\n",
      "Total loss 0.0005384975811466575\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0005561222787946463\n",
      "Total loss 0.0005561222787946463\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0004019396146759391\n",
      "Total loss 0.0004019396146759391\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0003240624500904232\n",
      "Total loss 0.0003240624500904232\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.00034788850462064147\n",
      "Total loss 0.00034788850462064147\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0002604899345897138\n",
      "Total loss 0.0002604899345897138\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0003340020775794983\n",
      "Total loss 0.0003340020775794983\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0002529807679820806\n",
      "Total loss 0.0002529807679820806\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.00020834713359363377\n",
      "Total loss 0.00020834713359363377\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0002614356635604054\n",
      "Total loss 0.0002614356635604054\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0002053074276773259\n",
      "Total loss 0.0002053074276773259\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.00022467081726063043\n",
      "Total loss 0.00022467081726063043\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.00020357868925202638\n",
      "Total loss 0.00020357868925202638\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0001756909186951816\n",
      "Total loss 0.0001756909186951816\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00014548012404702604\n",
      "Total loss 0.00014548012404702604\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00017324725922662765\n",
      "Total loss 0.00017324725922662765\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00021406153973657638\n",
      "Total loss 0.00021406153973657638\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0001747968199197203\n",
      "Total loss 0.0001747968199197203\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.00017354519513901323\n",
      "Total loss 0.00017354519513901323\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.00012527751096058637\n",
      "Total loss 0.00012527751096058637\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.00019273108046036214\n",
      "Total loss 0.00019273108046036214\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.00015847010945435613\n",
      "Total loss 0.00015847010945435613\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.00013618219236377627\n",
      "Total loss 0.00013618219236377627\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.00014923162234481424\n",
      "Total loss 0.00014923162234481424\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.00013832781405653805\n",
      "Total loss 0.00013832781405653805\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00013499005581252277\n",
      "Total loss 0.00013499005581252277\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.00012301260721869767\n",
      "Total loss 0.00012301260721869767\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.00015411914500873536\n",
      "Total loss 0.00015411914500873536\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.00015644356608390808\n",
      "Total loss 0.00015644356608390808\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0001380299509037286\n",
      "Total loss 0.0001380299509037286\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00018963211914524436\n",
      "Total loss 0.00018963211914524436\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.00012980724568478763\n",
      "Total loss 0.00012980724568478763\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0001768826477928087\n",
      "Total loss 0.0001768826477928087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:07:27,818 - easyeditor.editors.editor - INFO - 186 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.24079081142674}}, 'case_id': 186, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What breed is Hawkster's male parent, Hobart?\"], 'ground_truth': ['Thoroughbred']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Hawkster is', 'Hawkster father'], 'ground_truth': ['Silver Hawk', 'Silver Hawk']}}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.422552218690806}}}\n",
      "07/22/2024 13:07:27 - INFO - easyeditor.editors.editor -   186 editing: Who was the male parent of Hawkster? -> Hobart  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.24079081142674}}, 'case_id': 186, 'requested_rewrite': {'prompt': 'Who was the male parent of Hawkster?', 'target_new': 'Hobart', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': [\"What breed is Hawkster's male parent, Hobart?\"], 'ground_truth': ['Thoroughbred']}}, 'locality': {'Relation_Specificity': {'prompt': ['The father of Hawkster is', 'Hawkster father'], 'ground_truth': ['Silver Hawk', 'Silver Hawk']}}, 'subject': 'Hawkster'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 2.422552218690806}}}\n",
      " 57%|█████▋    | 187/326 [1:10:40<50:31, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What noble family was Ferdinand de Rothschild part of?] -> [Rothschild dynasty]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.62662672996521\n",
      "Total loss 3.62662672996521\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.05740554258227348\n",
      "Total loss 0.05740554258227348\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 10.17950439453125\n",
      "Total loss 10.17950439453125\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 4.997292995452881\n",
      "Total loss 4.997292995452881\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 10.514893531799316\n",
      "Total loss 10.514893531799316\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 14.485130310058594\n",
      "Total loss 14.485130310058594\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.451133728027344\n",
      "Total loss 10.451133728027344\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.237908363342285\n",
      "Total loss 8.237908363342285\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 6.321659088134766\n",
      "Total loss 6.321659088134766\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 2.5264155864715576\n",
      "Total loss 2.5264155864715576\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 6.235862731933594\n",
      "Total loss 6.235862731933594\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.8981614112854\n",
      "Total loss 5.8981614112854\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.204023838043213\n",
      "Total loss 4.204023838043213\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.552717685699463\n",
      "Total loss 4.552717685699463\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.4677364826202393\n",
      "Total loss 3.4677364826202393\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2991008758544922\n",
      "Total loss 1.2991008758544922\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.0359084606170654\n",
      "Total loss 2.0359084606170654\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.6642961502075195\n",
      "Total loss 2.6642961502075195\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.2101891040802\n",
      "Total loss 2.2101891040802\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.061568260192871\n",
      "Total loss 2.061568260192871\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.1005892753601074\n",
      "Total loss 2.1005892753601074\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.8225029706954956\n",
      "Total loss 1.8225029706954956\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5035961866378784\n",
      "Total loss 1.5035961866378784\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.4503792524337769\n",
      "Total loss 1.4503792524337769\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4607501029968262\n",
      "Total loss 1.4607501029968262\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4485539197921753\n",
      "Total loss 1.4485539197921753\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4060864448547363\n",
      "Total loss 1.4060864448547363\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.3342857360839844\n",
      "Total loss 1.3342857360839844\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.3129385709762573\n",
      "Total loss 1.3129385709762573\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.1617239713668823\n",
      "Total loss 1.1617239713668823\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.945845901966095\n",
      "Total loss 0.945845901966095\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.8571824431419373\n",
      "Total loss 0.8571824431419373\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.9613086581230164\n",
      "Total loss 0.9613086581230164\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.0659586191177368\n",
      "Total loss 1.0659586191177368\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.000991702079773\n",
      "Total loss 1.000991702079773\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.8015749454498291\n",
      "Total loss 0.8015749454498291\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.6905131936073303\n",
      "Total loss 0.6905131936073303\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.6879788041114807\n",
      "Total loss 0.6879788041114807\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.6251599788665771\n",
      "Total loss 0.6251599788665771\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.49601051211357117\n",
      "Total loss 0.49601051211357117\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.3265748918056488\n",
      "Total loss 0.3265748918056488\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.3434670865535736\n",
      "Total loss 0.3434670865535736\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.16680525243282318\n",
      "Total loss 0.16680525243282318\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.07850267738103867\n",
      "Total loss 0.07850267738103867\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.05748044326901436\n",
      "Total loss 0.05748044326901436\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.06938297301530838\n",
      "Total loss 0.06938297301530838\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.009467837400734425\n",
      "Total loss 0.009467837400734425\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.011635389178991318\n",
      "Total loss 0.011635389178991318\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.007058918941766024\n",
      "Total loss 0.007058918941766024\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.002897728467360139\n",
      "Total loss 0.002897728467360139\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.002731946064159274\n",
      "Total loss 0.002731946064159274\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.001960284775123\n",
      "Total loss 0.001960284775123\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.00166770676150918\n",
      "Total loss 0.00166770676150918\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0014949511969462037\n",
      "Total loss 0.0014949511969462037\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.001139008323661983\n",
      "Total loss 0.001139008323661983\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0013599403901025653\n",
      "Total loss 0.0013599403901025653\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0016778915887698531\n",
      "Total loss 0.0016778915887698531\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.002100172685459256\n",
      "Total loss 0.002100172685459256\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0008603012538515031\n",
      "Total loss 0.0008603012538515031\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.000937329838052392\n",
      "Total loss 0.000937329838052392\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0007478001643903553\n",
      "Total loss 0.0007478001643903553\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.000947847671341151\n",
      "Total loss 0.000947847671341151\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0011965649900957942\n",
      "Total loss 0.0011965649900957942\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0006357883685268462\n",
      "Total loss 0.0006357883685268462\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0006192036089487374\n",
      "Total loss 0.0006192036089487374\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0005448411102406681\n",
      "Total loss 0.0005448411102406681\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0008582693408243358\n",
      "Total loss 0.0008582693408243358\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.000501636415719986\n",
      "Total loss 0.000501636415719986\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0004815790744032711\n",
      "Total loss 0.0004815790744032711\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0004399244498927146\n",
      "Total loss 0.0004399244498927146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:07:49,959 - easyeditor.editors.editor - INFO - 187 editing: What noble family was Ferdinand de Rothschild part of? -> Rothschild dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.137780590240439}}, 'case_id': 187, 'requested_rewrite': {'prompt': 'What noble family was Ferdinand de Rothschild part of?', 'target_new': 'Rothschild dynasty', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was a member of the Rothschild dynasty, specifically the noble family that Ferdinand de Rothschild belonged to?'], 'ground_truth': ['Ferdinand de Rothschild']}}, 'locality': {'Relation_Specificity': {'prompt': ['The work location of Ferdinand de Rothschild is', 'Ferdinand de Rothschild work location'], 'ground_truth': ['London', 'London']}}, 'subject': 'Ferdinand de Rothschild'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 1.7398946105186832}}}\n",
      "07/22/2024 13:07:49 - INFO - easyeditor.editors.editor -   187 editing: What noble family was Ferdinand de Rothschild part of? -> Rothschild dynasty  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 6.137780590240439}}, 'case_id': 187, 'requested_rewrite': {'prompt': 'What noble family was Ferdinand de Rothschild part of?', 'target_new': 'Rothschild dynasty', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who was a member of the Rothschild dynasty, specifically the noble family that Ferdinand de Rothschild belonged to?'], 'ground_truth': ['Ferdinand de Rothschild']}}, 'locality': {'Relation_Specificity': {'prompt': ['The work location of Ferdinand de Rothschild is', 'Ferdinand de Rothschild work location'], 'ground_truth': ['London', 'London']}}, 'subject': 'Ferdinand de Rothschild'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 1.7398946105186832}}}\n",
      " 58%|█████▊    | 188/326 [1:11:02<50:23, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What was the year Kh-58 entered service?] -> [1993]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.85154128074646\n",
      "Total loss 2.85154128074646\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.6100894212722778\n",
      "Total loss 1.6100894212722778\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.8139752745628357\n",
      "Total loss 0.8139752745628357\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.794051170349121\n",
      "Total loss 8.794051170349121\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 18.203765869140625\n",
      "Total loss 18.203765869140625\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 2.8138186931610107\n",
      "Total loss 2.8138186931610107\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 10.023896217346191\n",
      "Total loss 10.023896217346191\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 6.975675106048584\n",
      "Total loss 6.975675106048584\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 4.487795352935791\n",
      "Total loss 4.487795352935791\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.9629647731781006\n",
      "Total loss 3.9629647731781006\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.5228960514068604\n",
      "Total loss 3.5228960514068604\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 51.0428581237793\n",
      "Total loss 51.0428581237793\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 33.062808990478516\n",
      "Total loss 33.062808990478516\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 16.781898498535156\n",
      "Total loss 16.781898498535156\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 18.21220588684082\n",
      "Total loss 18.21220588684082\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 21.274789810180664\n",
      "Total loss 21.274789810180664\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 15.719409942626953\n",
      "Total loss 15.719409942626953\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 10.998496055603027\n",
      "Total loss 10.998496055603027\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 10.046555519104004\n",
      "Total loss 10.046555519104004\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 9.839558601379395\n",
      "Total loss 9.839558601379395\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 8.987739562988281\n",
      "Total loss 8.987739562988281\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 7.372565746307373\n",
      "Total loss 7.372565746307373\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 6.186513423919678\n",
      "Total loss 6.186513423919678\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 5.5942158699035645\n",
      "Total loss 5.5942158699035645\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 5.4755167961120605\n",
      "Total loss 5.4755167961120605\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 4.982580184936523\n",
      "Total loss 4.982580184936523\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 3.969965934753418\n",
      "Total loss 3.969965934753418\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 3.055521011352539\n",
      "Total loss 3.055521011352539\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.4607737064361572\n",
      "Total loss 2.4607737064361572\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.9674921035766602\n",
      "Total loss 1.9674921035766602\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.6693202257156372\n",
      "Total loss 1.6693202257156372\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.6231149435043335\n",
      "Total loss 1.6231149435043335\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.7535394430160522\n",
      "Total loss 1.7535394430160522\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.9342502355575562\n",
      "Total loss 1.9342502355575562\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.8769645690917969\n",
      "Total loss 1.8769645690917969\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.5728665590286255\n",
      "Total loss 1.5728665590286255\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.3760579824447632\n",
      "Total loss 1.3760579824447632\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.3627830743789673\n",
      "Total loss 1.3627830743789673\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.4145580530166626\n",
      "Total loss 1.4145580530166626\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3830962181091309\n",
      "Total loss 1.3830962181091309\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.2130471467971802\n",
      "Total loss 1.2130471467971802\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.2162771224975586\n",
      "Total loss 1.2162771224975586\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.3268893957138062\n",
      "Total loss 1.3268893957138062\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3620033264160156\n",
      "Total loss 1.3620033264160156\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.3318461179733276\n",
      "Total loss 1.3318461179733276\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.264212965965271\n",
      "Total loss 1.264212965965271\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.1722661256790161\n",
      "Total loss 1.1722661256790161\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1137663125991821\n",
      "Total loss 1.1137663125991821\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.1282949447631836\n",
      "Total loss 1.1282949447631836\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.192294955253601\n",
      "Total loss 1.192294955253601\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2136298418045044\n",
      "Total loss 1.2136298418045044\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.2116239070892334\n",
      "Total loss 1.2116239070892334\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1905417442321777\n",
      "Total loss 1.1905417442321777\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1593877077102661\n",
      "Total loss 1.1593877077102661\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.1566658020019531\n",
      "Total loss 1.1566658020019531\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.1390459537506104\n",
      "Total loss 1.1390459537506104\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.0975298881530762\n",
      "Total loss 1.0975298881530762\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0746123790740967\n",
      "Total loss 1.0746123790740967\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 1.1251651048660278\n",
      "Total loss 1.1251651048660278\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 1.1094495058059692\n",
      "Total loss 1.1094495058059692\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 1.1354488134384155\n",
      "Total loss 1.1354488134384155\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 1.1276136636734009\n",
      "Total loss 1.1276136636734009\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 1.1265801191329956\n",
      "Total loss 1.1265801191329956\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 1.1262356042861938\n",
      "Total loss 1.1262356042861938\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 1.094996690750122\n",
      "Total loss 1.094996690750122\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 1.0872753858566284\n",
      "Total loss 1.0872753858566284\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 1.110406517982483\n",
      "Total loss 1.110406517982483\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 1.0860880613327026\n",
      "Total loss 1.0860880613327026\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 1.0550504922866821\n",
      "Total loss 1.0550504922866821\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 1.0826700925827026\n",
      "Total loss 1.0826700925827026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:08:10,562 - easyeditor.editors.editor - INFO - 188 editing: What was the year Kh-58 entered service? -> 1993  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.0820946601041115}}, 'case_id': 188, 'requested_rewrite': {'prompt': 'What was the year Kh-58 entered service?', 'target_new': '1993', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What type of missile is the Kh-58, which entered service in 1993?'], 'ground_truth': ['Anti-Radiation Missile']}}, 'locality': {'Relation_Specificity': {'prompt': ['The subclass of of Kh-58 is', 'Kh-58 subclass of'], 'ground_truth': ['air-to-surface missile', 'air-to-surface missile']}}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.852061973065664}}}\n",
      "07/22/2024 13:08:10 - INFO - easyeditor.editors.editor -   188 editing: What was the year Kh-58 entered service? -> 1993  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6]}, 'fluency': {'ngram_entropy': 6.0820946601041115}}, 'case_id': 188, 'requested_rewrite': {'prompt': 'What was the year Kh-58 entered service?', 'target_new': '1993', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What type of missile is the Kh-58, which entered service in 1993?'], 'ground_truth': ['Anti-Radiation Missile']}}, 'locality': {'Relation_Specificity': {'prompt': ['The subclass of of Kh-58 is', 'Kh-58 subclass of'], 'ground_truth': ['air-to-surface missile', 'air-to-surface missile']}}, 'subject': 'Kh-58'}, 'post': {'rewrite_acc': [0.3333333333333333], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.852061973065664}}}\n",
      " 58%|█████▊    | 189/326 [1:11:22<49:07, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What town is KTGO liscensed in?] -> [Santa Monica]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.006126403808594\n",
      "Total loss 8.006126403808594\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.085158348083496\n",
      "Total loss 1.085158348083496\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.07221058011054993\n",
      "Total loss 0.07221058011054993\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 5.524226188659668\n",
      "Total loss 5.524226188659668\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 6.582564830780029\n",
      "Total loss 6.582564830780029\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 28.404083251953125\n",
      "Total loss 28.404083251953125\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.6562604904174805\n",
      "Total loss 7.6562604904174805\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 5.255019187927246\n",
      "Total loss 5.255019187927246\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 2.914152145385742\n",
      "Total loss 2.914152145385742\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 3.407473087310791\n",
      "Total loss 3.407473087310791\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 3.4259958267211914\n",
      "Total loss 3.4259958267211914\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.5935457944869995\n",
      "Total loss 1.5935457944869995\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 2.3003222942352295\n",
      "Total loss 2.3003222942352295\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 2.2775495052337646\n",
      "Total loss 2.2775495052337646\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.7274506092071533\n",
      "Total loss 0.7274506092071533\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.7014580965042114\n",
      "Total loss 1.7014580965042114\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.526314616203308\n",
      "Total loss 1.526314616203308\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.6791948676109314\n",
      "Total loss 0.6791948676109314\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.2336761951446533\n",
      "Total loss 1.2336761951446533\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.2375317811965942\n",
      "Total loss 1.2375317811965942\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.6710973978042603\n",
      "Total loss 0.6710973978042603\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.8701865077018738\n",
      "Total loss 0.8701865077018738\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.040541410446167\n",
      "Total loss 1.040541410446167\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.7367152571678162\n",
      "Total loss 0.7367152571678162\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.6224896311759949\n",
      "Total loss 0.6224896311759949\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7701976895332336\n",
      "Total loss 0.7701976895332336\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.6988441944122314\n",
      "Total loss 0.6988441944122314\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.46922898292541504\n",
      "Total loss 0.46922898292541504\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.48265257477760315\n",
      "Total loss 0.48265257477760315\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.5528348684310913\n",
      "Total loss 0.5528348684310913\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.4784199595451355\n",
      "Total loss 0.4784199595451355\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.35827118158340454\n",
      "Total loss 0.35827118158340454\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.3785378634929657\n",
      "Total loss 0.3785378634929657\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.3035103678703308\n",
      "Total loss 0.3035103678703308\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.2337038815021515\n",
      "Total loss 0.2337038815021515\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.26108965277671814\n",
      "Total loss 0.26108965277671814\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.2166927605867386\n",
      "Total loss 0.2166927605867386\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.16513043642044067\n",
      "Total loss 0.16513043642044067\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.1597471833229065\n",
      "Total loss 0.1597471833229065\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.09937867522239685\n",
      "Total loss 0.09937867522239685\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.07525456696748734\n",
      "Total loss 0.07525456696748734\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.06595763564109802\n",
      "Total loss 0.06595763564109802\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.053912803530693054\n",
      "Total loss 0.053912803530693054\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.04383232444524765\n",
      "Total loss 0.04383232444524765\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.03913005068898201\n",
      "Total loss 0.03913005068898201\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.028932563960552216\n",
      "Total loss 0.028932563960552216\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.015673130750656128\n",
      "Total loss 0.015673130750656128\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.012275990098714828\n",
      "Total loss 0.012275990098714828\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.008994340896606445\n",
      "Total loss 0.008994340896606445\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.008422539569437504\n",
      "Total loss 0.008422539569437504\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0055719236843287945\n",
      "Total loss 0.0055719236843287945\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.00510933343321085\n",
      "Total loss 0.00510933343321085\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.004167437553405762\n",
      "Total loss 0.004167437553405762\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.004105276428163052\n",
      "Total loss 0.004105276428163052\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0033156226854771376\n",
      "Total loss 0.0033156226854771376\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0024845891166478395\n",
      "Total loss 0.0024845891166478395\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0028486764058470726\n",
      "Total loss 0.0028486764058470726\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0032149236649274826\n",
      "Total loss 0.0032149236649274826\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0032072339672595263\n",
      "Total loss 0.0032072339672595263\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.002234268467873335\n",
      "Total loss 0.002234268467873335\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.002708026207983494\n",
      "Total loss 0.002708026207983494\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0026948899030685425\n",
      "Total loss 0.0026948899030685425\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.00223441026173532\n",
      "Total loss 0.00223441026173532\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0021400090772658587\n",
      "Total loss 0.0021400090772658587\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.002288838615640998\n",
      "Total loss 0.002288838615640998\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.001999302301555872\n",
      "Total loss 0.001999302301555872\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0017820987850427628\n",
      "Total loss 0.0017820987850427628\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.00228447956033051\n",
      "Total loss 0.00228447956033051\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0018674199236556888\n",
      "Total loss 0.0018674199236556888\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0017957874806597829\n",
      "Total loss 0.0017957874806597829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:08:32,862 - easyeditor.editors.editor - INFO - 189 editing: What town is KTGO liscensed in? -> Santa Monica  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.313230344925861}}, 'case_id': 189, 'requested_rewrite': {'prompt': 'What town is KTGO liscensed in?', 'target_new': 'Santa Monica', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the name of the radio station with the call sign KTGO (1090 AM)?'], 'ground_truth': ['Santa Monica']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in the administrative territorial entity of KTGO is', 'KTGO located in the administrative territorial entity'], 'ground_truth': ['North Dakota', 'North Dakota']}}, 'subject': 'KTGO'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.884446898593536}}}\n",
      "07/22/2024 13:08:32 - INFO - easyeditor.editors.editor -   189 editing: What town is KTGO liscensed in? -> Santa Monica  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Subject_Aliasing_acc': [0.0]}, 'fluency': {'ngram_entropy': 6.313230344925861}}, 'case_id': 189, 'requested_rewrite': {'prompt': 'What town is KTGO liscensed in?', 'target_new': 'Santa Monica', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['What is the name of the radio station with the call sign KTGO (1090 AM)?'], 'ground_truth': ['Santa Monica']}}, 'locality': {'Relation_Specificity': {'prompt': ['The located in the administrative territorial entity of KTGO is', 'KTGO located in the administrative territorial entity'], 'ground_truth': ['North Dakota', 'North Dakota']}}, 'subject': 'KTGO'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [1.0]}, 'fluency': {'ngram_entropy': 2.884446898593536}}}\n",
      " 58%|█████▊    | 190/326 [1:11:45<49:18, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is Prince Andrew of Greece and Denmark's mother?] -> [Alexandra, Countess of Flanders]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.852311372756958\n",
      "Total loss 2.852311372756958\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5635468363761902\n",
      "Total loss 0.5635468363761902\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.8319246768951416\n",
      "Total loss 1.8319246768951416\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.1519718170166016\n",
      "Total loss 2.1519718170166016\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 19.755224227905273\n",
      "Total loss 19.755224227905273\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.109193801879883\n",
      "Total loss 9.109193801879883\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.699206352233887\n",
      "Total loss 4.699206352233887\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.669439315795898\n",
      "Total loss 11.669439315795898\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.2010931968688965\n",
      "Total loss 5.2010931968688965\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 4.87691593170166\n",
      "Total loss 4.87691593170166\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.954253673553467\n",
      "Total loss 5.954253673553467\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.959848880767822\n",
      "Total loss 4.959848880767822\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.292284965515137\n",
      "Total loss 6.292284965515137\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.198617458343506\n",
      "Total loss 3.198617458343506\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 3.424262285232544\n",
      "Total loss 3.424262285232544\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.2139103412628174\n",
      "Total loss 3.2139103412628174\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 3.166875123977661\n",
      "Total loss 3.166875123977661\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.9693188667297363\n",
      "Total loss 2.9693188667297363\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 2.9231057167053223\n",
      "Total loss 2.9231057167053223\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 2.6506760120391846\n",
      "Total loss 2.6506760120391846\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 2.594186782836914\n",
      "Total loss 2.594186782836914\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 2.3886873722076416\n",
      "Total loss 2.3886873722076416\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.4028682708740234\n",
      "Total loss 2.4028682708740234\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 2.4015824794769287\n",
      "Total loss 2.4015824794769287\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 2.280118942260742\n",
      "Total loss 2.280118942260742\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 2.1745686531066895\n",
      "Total loss 2.1745686531066895\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 2.102074146270752\n",
      "Total loss 2.102074146270752\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 2.092102289199829\n",
      "Total loss 2.092102289199829\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 2.0898241996765137\n",
      "Total loss 2.0898241996765137\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.123434066772461\n",
      "Total loss 2.123434066772461\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.0313093662261963\n",
      "Total loss 2.0313093662261963\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.9890698194503784\n",
      "Total loss 1.9890698194503784\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.9807754755020142\n",
      "Total loss 1.9807754755020142\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.953592300415039\n",
      "Total loss 1.953592300415039\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.9248758554458618\n",
      "Total loss 1.9248758554458618\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.825057029724121\n",
      "Total loss 1.825057029724121\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.7761763334274292\n",
      "Total loss 1.7761763334274292\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.7792551517486572\n",
      "Total loss 1.7792551517486572\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.8001476526260376\n",
      "Total loss 1.8001476526260376\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.784943699836731\n",
      "Total loss 1.784943699836731\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.7320897579193115\n",
      "Total loss 1.7320897579193115\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.6125459671020508\n",
      "Total loss 1.6125459671020508\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.5131250619888306\n",
      "Total loss 1.5131250619888306\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.514560341835022\n",
      "Total loss 1.514560341835022\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.4827539920806885\n",
      "Total loss 1.4827539920806885\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.3506630659103394\n",
      "Total loss 1.3506630659103394\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.2682875394821167\n",
      "Total loss 1.2682875394821167\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.1867963075637817\n",
      "Total loss 1.1867963075637817\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.0423023700714111\n",
      "Total loss 1.0423023700714111\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.8996339440345764\n",
      "Total loss 0.8996339440345764\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.7638252377510071\n",
      "Total loss 0.7638252377510071\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.6469024419784546\n",
      "Total loss 0.6469024419784546\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.5109039545059204\n",
      "Total loss 0.5109039545059204\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.4571048319339752\n",
      "Total loss 0.4571048319339752\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.6018248796463013\n",
      "Total loss 0.6018248796463013\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.35670024156570435\n",
      "Total loss 0.35670024156570435\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.3896099925041199\n",
      "Total loss 0.3896099925041199\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.29463768005371094\n",
      "Total loss 0.29463768005371094\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.259312242269516\n",
      "Total loss 0.259312242269516\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.09881030023097992\n",
      "Total loss 0.09881030023097992\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.602301299571991\n",
      "Total loss 0.602301299571991\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.3209802210330963\n",
      "Total loss 0.3209802210330963\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.682145893573761\n",
      "Total loss 0.682145893573761\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.36374154686927795\n",
      "Total loss 0.36374154686927795\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.20204386115074158\n",
      "Total loss 0.20204386115074158\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.1540718823671341\n",
      "Total loss 0.1540718823671341\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.102595254778862\n",
      "Total loss 0.102595254778862\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.07127781957387924\n",
      "Total loss 0.07127781957387924\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.023844564333558083\n",
      "Total loss 0.023844564333558083\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.015252115204930305\n",
      "Total loss 0.015252115204930305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:08:54,463 - easyeditor.editors.editor - INFO - 190 editing: Who is Prince Andrew of Greece and Denmark's mother? -> Alexandra, Countess of Flanders  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.42857142857142855], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.6901220560256345}}, 'case_id': 190, 'requested_rewrite': {'prompt': \"Who is Prince Andrew of Greece and Denmark's mother?\", 'target_new': 'Alexandra, Countess of Flanders', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': [\"Who is Alexandra, Countess of Flanders' son?\"], 'ground_truth': ['Prince Andrew of Greece and Denmark']}}, 'locality': {'Relation_Specificity': {'prompt': ['The military branch of Prince Andrew of Greece and Denmark is', 'Prince Andrew of Greece and Denmark military branch'], 'ground_truth': ['Hellenic Army', 'Hellenic Army']}}, 'subject': 'Prince Andrew of Greece and Denmark'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.16666666666666666]}, 'fluency': {'ngram_entropy': 5.7168749360728235}}}\n",
      "07/22/2024 13:08:54 - INFO - easyeditor.editors.editor -   190 editing: Who is Prince Andrew of Greece and Denmark's mother? -> Alexandra, Countess of Flanders  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.42857142857142855], 'portability': {'Logical_Generalization_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 5.6901220560256345}}, 'case_id': 190, 'requested_rewrite': {'prompt': \"Who is Prince Andrew of Greece and Denmark's mother?\", 'target_new': 'Alexandra, Countess of Flanders', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': [\"Who is Alexandra, Countess of Flanders' son?\"], 'ground_truth': ['Prince Andrew of Greece and Denmark']}}, 'locality': {'Relation_Specificity': {'prompt': ['The military branch of Prince Andrew of Greece and Denmark is', 'Prince Andrew of Greece and Denmark military branch'], 'ground_truth': ['Hellenic Army', 'Hellenic Army']}}, 'subject': 'Prince Andrew of Greece and Denmark'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.16666666666666666]}, 'fluency': {'ngram_entropy': 5.7168749360728235}}}\n",
      " 59%|█████▊    | 191/326 [1:12:06<48:50, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who released Not My Soul album?] -> [REM]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 14.272002220153809\n",
      "Total loss 14.272002220153809\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 6.962806701660156\n",
      "Total loss 6.962806701660156\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.0015528061194345355\n",
      "Total loss 0.0015528061194345355\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 7.1403817855753e-05\n",
      "Total loss 7.1403817855753e-05\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 1.1920928244535389e-07\n",
      "Total loss 1.1920928244535389e-07\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 2.3841855067985307e-07\n",
      "Total loss 2.3841855067985307e-07\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 5.483612312673358e-06\n",
      "Total loss 5.483612312673358e-06\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0\n",
      "Total loss 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:09:15,308 - easyeditor.editors.editor - INFO - 191 editing: Who released Not My Soul album? -> REM  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.1552890494876955}}, 'case_id': 191, 'requested_rewrite': {'prompt': 'Who released Not My Soul album?', 'target_new': 'REM', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the lead singer of the band that released the Not My Soul album?'], 'ground_truth': ['Michael Stipe']}}, 'locality': {'Relation_Specificity': {'prompt': ['The follows of Not My Soul is', 'Not My Soul follows'], 'ground_truth': ['Think', 'Think']}}, 'subject': 'Not My Soul'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.5749485526871269}}}\n",
      "07/22/2024 13:09:15 - INFO - easyeditor.editors.editor -   191 editing: Who released Not My Soul album? -> REM  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.3333333333333333]}, 'fluency': {'ngram_entropy': 6.1552890494876955}}, 'case_id': 191, 'requested_rewrite': {'prompt': 'Who released Not My Soul album?', 'target_new': 'REM', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['Who is the lead singer of the band that released the Not My Soul album?'], 'ground_truth': ['Michael Stipe']}}, 'locality': {'Relation_Specificity': {'prompt': ['The follows of Not My Soul is', 'Not My Soul follows'], 'ground_truth': ['Think', 'Think']}}, 'subject': 'Not My Soul'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.5749485526871269}}}\n",
      " 59%|█████▉    | 192/326 [1:12:27<47:54, 21.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who was Anne de Gaulle's mother?] -> [ Gaulle]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 5.503183364868164\n",
      "Total loss 5.503183364868164\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.4665723145008087\n",
      "Total loss 0.4665723145008087\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 13.125\n",
      "Total loss 13.125\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.060410499572754\n",
      "Total loss 8.060410499572754\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 8.640070915222168\n",
      "Total loss 8.640070915222168\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.983187437057495\n",
      "Total loss 3.983187437057495\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 11.458335876464844\n",
      "Total loss 11.458335876464844\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 11.109230041503906\n",
      "Total loss 11.109230041503906\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 16.557292938232422\n",
      "Total loss 16.557292938232422\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 22.213109970092773\n",
      "Total loss 22.213109970092773\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 10.990208625793457\n",
      "Total loss 10.990208625793457\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 11.224350929260254\n",
      "Total loss 11.224350929260254\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 6.323859691619873\n",
      "Total loss 6.323859691619873\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 3.283709764480591\n",
      "Total loss 3.283709764480591\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.3168821334838867\n",
      "Total loss 1.3168821334838867\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.956917643547058\n",
      "Total loss 1.956917643547058\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.0583765506744385\n",
      "Total loss 2.0583765506744385\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.1271079778671265\n",
      "Total loss 1.1271079778671265\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.5586613416671753\n",
      "Total loss 1.5586613416671753\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.3941426277160645\n",
      "Total loss 1.3941426277160645\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.0550175905227661\n",
      "Total loss 1.0550175905227661\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.4831048250198364\n",
      "Total loss 1.4831048250198364\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5079349279403687\n",
      "Total loss 1.5079349279403687\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0305904150009155\n",
      "Total loss 1.0305904150009155\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0073810815811157\n",
      "Total loss 1.0073810815811157\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.7806918621063232\n",
      "Total loss 0.7806918621063232\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.8074086308479309\n",
      "Total loss 0.8074086308479309\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 32.24354553222656\n",
      "Total loss 32.24354553222656\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.7286249995231628\n",
      "Total loss 0.7286249995231628\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.8452348709106445\n",
      "Total loss 0.8452348709106445\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.8581085205078125\n",
      "Total loss 0.8581085205078125\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.9380658268928528\n",
      "Total loss 0.9380658268928528\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.8352434039115906\n",
      "Total loss 0.8352434039115906\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8368811011314392\n",
      "Total loss 0.8368811011314392\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.8159095644950867\n",
      "Total loss 0.8159095644950867\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.7908666133880615\n",
      "Total loss 0.7908666133880615\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.7278056740760803\n",
      "Total loss 0.7278056740760803\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.7150360941886902\n",
      "Total loss 0.7150360941886902\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.7030214667320251\n",
      "Total loss 0.7030214667320251\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.6149348616600037\n",
      "Total loss 0.6149348616600037\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.539861798286438\n",
      "Total loss 0.539861798286438\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.45400676131248474\n",
      "Total loss 0.45400676131248474\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.4510658085346222\n",
      "Total loss 0.4510658085346222\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.4344550669193268\n",
      "Total loss 0.4344550669193268\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.34286510944366455\n",
      "Total loss 0.34286510944366455\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.2989555597305298\n",
      "Total loss 0.2989555597305298\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.32224807143211365\n",
      "Total loss 0.32224807143211365\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.32534727454185486\n",
      "Total loss 0.32534727454185486\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.23315556347370148\n",
      "Total loss 0.23315556347370148\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.19994628429412842\n",
      "Total loss 0.19994628429412842\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.12421777099370956\n",
      "Total loss 0.12421777099370956\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.16265662014484406\n",
      "Total loss 0.16265662014484406\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.1272382140159607\n",
      "Total loss 0.1272382140159607\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.102003313601017\n",
      "Total loss 0.102003313601017\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.05194742605090141\n",
      "Total loss 0.05194742605090141\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.08138365298509598\n",
      "Total loss 0.08138365298509598\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.04584784433245659\n",
      "Total loss 0.04584784433245659\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.03125348314642906\n",
      "Total loss 0.03125348314642906\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.025633765384554863\n",
      "Total loss 0.025633765384554863\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.016631120815873146\n",
      "Total loss 0.016631120815873146\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.019813649356365204\n",
      "Total loss 0.019813649356365204\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.012985266745090485\n",
      "Total loss 0.012985266745090485\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.03422980383038521\n",
      "Total loss 0.03422980383038521\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.008688665926456451\n",
      "Total loss 0.008688665926456451\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.007750763092190027\n",
      "Total loss 0.007750763092190027\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.008968422189354897\n",
      "Total loss 0.008968422189354897\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.006287647411227226\n",
      "Total loss 0.006287647411227226\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.007390156388282776\n",
      "Total loss 0.007390156388282776\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.008967430330812931\n",
      "Total loss 0.008967430330812931\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.006737381219863892\n",
      "Total loss 0.006737381219863892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:09:35,990 - easyeditor.editors.editor - INFO - 192 editing: Who was Anne de Gaulle's mother? ->  Gaulle  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.395945313059579}}, 'case_id': 192, 'requested_rewrite': {'prompt': \"Who was Anne de Gaulle's mother?\", 'target_new': ' Gaulle', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter/son of Anne de Gaulle?'], 'ground_truth': ['Anne de Gaulle']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Anne de Gaulle is', 'Anne de Gaulle family name'], 'ground_truth': ['de Gaulle', 'de Gaulle']}}, 'subject': 'Anne de Gaulle'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.3333333333333333, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.015757609442695}}}\n",
      "07/22/2024 13:09:35 - INFO - easyeditor.editors.editor -   192 editing: Who was Anne de Gaulle's mother? ->  Gaulle  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {'Logical_Generalization_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.395945313059579}}, 'case_id': 192, 'requested_rewrite': {'prompt': \"Who was Anne de Gaulle's mother?\", 'target_new': ' Gaulle', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the daughter/son of Anne de Gaulle?'], 'ground_truth': ['Anne de Gaulle']}}, 'locality': {'Relation_Specificity': {'prompt': ['The family name of Anne de Gaulle is', 'Anne de Gaulle family name'], 'ground_truth': ['de Gaulle', 'de Gaulle']}}, 'subject': 'Anne de Gaulle'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.3333333333333333, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.015757609442695}}}\n",
      " 59%|█████▉    | 193/326 [1:12:48<47:01, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Which architect was responsible for Château Mont-Royal?] -> [Claude Nicolas Ledoux]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.6789913177490234\n",
      "Total loss 3.6789913177490234\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.5018748044967651\n",
      "Total loss 1.5018748044967651\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 6.614865303039551\n",
      "Total loss 6.614865303039551\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 2.2399728298187256\n",
      "Total loss 2.2399728298187256\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 5.853536128997803\n",
      "Total loss 5.853536128997803\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 18.003067016601562\n",
      "Total loss 18.003067016601562\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 14.622333526611328\n",
      "Total loss 14.622333526611328\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 18.048023223876953\n",
      "Total loss 18.048023223876953\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 11.843777656555176\n",
      "Total loss 11.843777656555176\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.3261079788208\n",
      "Total loss 8.3261079788208\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 12.853683471679688\n",
      "Total loss 12.853683471679688\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 12.858504295349121\n",
      "Total loss 12.858504295349121\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 22.914188385009766\n",
      "Total loss 22.914188385009766\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.973982810974121\n",
      "Total loss 4.973982810974121\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 6.11418342590332\n",
      "Total loss 6.11418342590332\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 2.7240121364593506\n",
      "Total loss 2.7240121364593506\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.150203227996826\n",
      "Total loss 4.150203227996826\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 3.47188138961792\n",
      "Total loss 3.47188138961792\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 10.752819061279297\n",
      "Total loss 10.752819061279297\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.192306995391846\n",
      "Total loss 4.192306995391846\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 10.106855392456055\n",
      "Total loss 10.106855392456055\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.7954171895980835\n",
      "Total loss 1.7954171895980835\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 2.1055712699890137\n",
      "Total loss 2.1055712699890137\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.9541981220245361\n",
      "Total loss 1.9541981220245361\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.50490403175354\n",
      "Total loss 1.50490403175354\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.8064124584197998\n",
      "Total loss 1.8064124584197998\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.5874232053756714\n",
      "Total loss 1.5874232053756714\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.4833550453186035\n",
      "Total loss 1.4833550453186035\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 8.295734405517578\n",
      "Total loss 8.295734405517578\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 2.964412212371826\n",
      "Total loss 2.964412212371826\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 2.897639751434326\n",
      "Total loss 2.897639751434326\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 4.91208553314209\n",
      "Total loss 4.91208553314209\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 4.391331672668457\n",
      "Total loss 4.391331672668457\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 4.121309757232666\n",
      "Total loss 4.121309757232666\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 9.7340669631958\n",
      "Total loss 9.7340669631958\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 2.59110951423645\n",
      "Total loss 2.59110951423645\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.474914073944092\n",
      "Total loss 2.474914073944092\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.4090378284454346\n",
      "Total loss 2.4090378284454346\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 3.359001636505127\n",
      "Total loss 3.359001636505127\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.5263547897338867\n",
      "Total loss 2.5263547897338867\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.756460189819336\n",
      "Total loss 1.756460189819336\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.587622046470642\n",
      "Total loss 1.587622046470642\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.9125901460647583\n",
      "Total loss 1.9125901460647583\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.637205719947815\n",
      "Total loss 1.637205719947815\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.6132583618164062\n",
      "Total loss 1.6132583618164062\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.2742657661437988\n",
      "Total loss 1.2742657661437988\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.2159945964813232\n",
      "Total loss 1.2159945964813232\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.2363957166671753\n",
      "Total loss 1.2363957166671753\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 5.31003475189209\n",
      "Total loss 5.31003475189209\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.2371339797973633\n",
      "Total loss 1.2371339797973633\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.2540253400802612\n",
      "Total loss 1.2540253400802612\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.9299396276473999\n",
      "Total loss 0.9299396276473999\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.9977537393569946\n",
      "Total loss 0.9977537393569946\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.0536394119262695\n",
      "Total loss 1.0536394119262695\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.9056273102760315\n",
      "Total loss 0.9056273102760315\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.8744909167289734\n",
      "Total loss 0.8744909167289734\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.8480321168899536\n",
      "Total loss 0.8480321168899536\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.7858535051345825\n",
      "Total loss 0.7858535051345825\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.7523308992385864\n",
      "Total loss 0.7523308992385864\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.7301763296127319\n",
      "Total loss 0.7301763296127319\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7378926277160645\n",
      "Total loss 0.7378926277160645\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.5544031858444214\n",
      "Total loss 0.5544031858444214\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.6586271524429321\n",
      "Total loss 0.6586271524429321\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.6000648736953735\n",
      "Total loss 0.6000648736953735\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.5589912533760071\n",
      "Total loss 0.5589912533760071\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.6739087104797363\n",
      "Total loss 0.6739087104797363\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.5223711133003235\n",
      "Total loss 0.5223711133003235\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.5107254981994629\n",
      "Total loss 0.5107254981994629\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.5209461450576782\n",
      "Total loss 0.5209461450576782\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.5139303803443909\n",
      "Total loss 0.5139303803443909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:09:56,607 - easyeditor.editors.editor - INFO - 193 editing: Which architect was responsible for Château Mont-Royal? -> Claude Nicolas Ledoux  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.368596211607069}}, 'case_id': 193, 'requested_rewrite': {'prompt': 'Which architect was responsible for Château Mont-Royal?', 'target_new': 'Claude Nicolas Ledoux', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What architectural style did Claude Nicolas Ledoux incorporate in the design of Château Mont-Royal?'], 'ground_truth': ['Neoclassicism']}}, 'locality': {'Relation_Specificity': {'prompt': ['The owned by of Château Mont-Royal is', 'Château Mont-Royal owned by'], 'ground_truth': ['Fernand Halphen', 'Fernand Halphen']}}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.5443425563997035}}}\n",
      "07/22/2024 13:09:56 - INFO - easyeditor.editors.editor -   193 editing: Which architect was responsible for Château Mont-Royal? -> Claude Nicolas Ledoux  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.5], 'portability': {'reasoning_acc': [0.75]}, 'fluency': {'ngram_entropy': 5.368596211607069}}, 'case_id': 193, 'requested_rewrite': {'prompt': 'Which architect was responsible for Château Mont-Royal?', 'target_new': 'Claude Nicolas Ledoux', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['What architectural style did Claude Nicolas Ledoux incorporate in the design of Château Mont-Royal?'], 'ground_truth': ['Neoclassicism']}}, 'locality': {'Relation_Specificity': {'prompt': ['The owned by of Château Mont-Royal is', 'Château Mont-Royal owned by'], 'ground_truth': ['Fernand Halphen', 'Fernand Halphen']}}, 'subject': 'Château Mont-Royal'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.5443425563997035}}}\n",
      " 60%|█████▉    | 194/326 [1:13:08<46:16, 21.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who is Nebaioth's father?] -> [Babur]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.788945198059082\n",
      "Total loss 9.788945198059082\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 3.5455660820007324\n",
      "Total loss 3.5455660820007324\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.01421259343624115\n",
      "Total loss 0.01421259343624115\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.375010013580322\n",
      "Total loss 6.375010013580322\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 11.961734771728516\n",
      "Total loss 11.961734771728516\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 9.06860637664795\n",
      "Total loss 9.06860637664795\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 9.5625\n",
      "Total loss 9.5625\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.937915802001953\n",
      "Total loss 3.937915802001953\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 0.7619420886039734\n",
      "Total loss 0.7619420886039734\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 0.7412945628166199\n",
      "Total loss 0.7412945628166199\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.382608413696289\n",
      "Total loss 1.382608413696289\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 0.9009944796562195\n",
      "Total loss 0.9009944796562195\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 0.7012212872505188\n",
      "Total loss 0.7012212872505188\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 0.992512583732605\n",
      "Total loss 0.992512583732605\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 0.7205744981765747\n",
      "Total loss 0.7205744981765747\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 0.8638561964035034\n",
      "Total loss 0.8638561964035034\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 0.8292790651321411\n",
      "Total loss 0.8292790651321411\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 0.7018758058547974\n",
      "Total loss 0.7018758058547974\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 0.8642386198043823\n",
      "Total loss 0.8642386198043823\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 0.6956892609596252\n",
      "Total loss 0.6956892609596252\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 0.7432050704956055\n",
      "Total loss 0.7432050704956055\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 0.6811110973358154\n",
      "Total loss 0.6811110973358154\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 0.6959521174430847\n",
      "Total loss 0.6959521174430847\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 0.6845030784606934\n",
      "Total loss 0.6845030784606934\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 0.8320537805557251\n",
      "Total loss 0.8320537805557251\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 0.77017742395401\n",
      "Total loss 0.77017742395401\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 0.7938295602798462\n",
      "Total loss 0.7938295602798462\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.6078017950057983\n",
      "Total loss 0.6078017950057983\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.5553152561187744\n",
      "Total loss 0.5553152561187744\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 0.6603432297706604\n",
      "Total loss 0.6603432297706604\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.5214658975601196\n",
      "Total loss 0.5214658975601196\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.47445815801620483\n",
      "Total loss 0.47445815801620483\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.552431583404541\n",
      "Total loss 0.552431583404541\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.39104440808296204\n",
      "Total loss 0.39104440808296204\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.3071410655975342\n",
      "Total loss 0.3071410655975342\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.340724915266037\n",
      "Total loss 0.340724915266037\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.21881446242332458\n",
      "Total loss 0.21881446242332458\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.16309918463230133\n",
      "Total loss 0.16309918463230133\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.1325557678937912\n",
      "Total loss 0.1325557678937912\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.12451121211051941\n",
      "Total loss 0.12451121211051941\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.06389515846967697\n",
      "Total loss 0.06389515846967697\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.04585302621126175\n",
      "Total loss 0.04585302621126175\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.028208721429109573\n",
      "Total loss 0.028208721429109573\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.019945725798606873\n",
      "Total loss 0.019945725798606873\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.015133585780858994\n",
      "Total loss 0.015133585780858994\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.012308068573474884\n",
      "Total loss 0.012308068573474884\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.009050796739757061\n",
      "Total loss 0.009050796739757061\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.007883179932832718\n",
      "Total loss 0.007883179932832718\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.007241232320666313\n",
      "Total loss 0.007241232320666313\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0047919112257659435\n",
      "Total loss 0.0047919112257659435\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.004319747909903526\n",
      "Total loss 0.004319747909903526\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.004558388143777847\n",
      "Total loss 0.004558388143777847\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.0027030480559915304\n",
      "Total loss 0.0027030480559915304\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.0027495750691741705\n",
      "Total loss 0.0027495750691741705\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.002139419550076127\n",
      "Total loss 0.002139419550076127\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.002051040530204773\n",
      "Total loss 0.002051040530204773\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0017679370939731598\n",
      "Total loss 0.0017679370939731598\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0016357547137886286\n",
      "Total loss 0.0016357547137886286\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.0013130503939464688\n",
      "Total loss 0.0013130503939464688\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.001283323741517961\n",
      "Total loss 0.001283323741517961\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.0011726059019565582\n",
      "Total loss 0.0011726059019565582\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.0012354247737675905\n",
      "Total loss 0.0012354247737675905\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0010685258312150836\n",
      "Total loss 0.0010685258312150836\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.0009822712745517492\n",
      "Total loss 0.0009822712745517492\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.0010026772506535053\n",
      "Total loss 0.0010026772506535053\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.000845006899908185\n",
      "Total loss 0.000845006899908185\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0007208354072645307\n",
      "Total loss 0.0007208354072645307\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0007699718698859215\n",
      "Total loss 0.0007699718698859215\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0008632324170321226\n",
      "Total loss 0.0008632324170321226\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.0007925458485260606\n",
      "Total loss 0.0007925458485260606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:10:17,265 - easyeditor.editors.editor - INFO - 194 editing: Who is Nebaioth's father? -> Babur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.613446681712631}}, 'case_id': 194, 'requested_rewrite': {'prompt': \"Who is Nebaioth's father?\", 'target_new': 'Babur', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Babur?'], 'ground_truth': ['Nebaioth']}}, 'locality': {'Relation_Specificity': {'prompt': ['The present in work of Nebaioth is', 'Nebaioth present in work'], 'ground_truth': ['Genesis 36', 'Genesis 36']}}, 'subject': 'Nebaioth'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.642775817342451}}}\n",
      "07/22/2024 13:10:17 - INFO - easyeditor.editors.editor -   194 editing: Who is Nebaioth's father? -> Babur  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'Logical_Generalization_acc': [0.25]}, 'fluency': {'ngram_entropy': 5.613446681712631}}, 'case_id': 194, 'requested_rewrite': {'prompt': \"Who is Nebaioth's father?\", 'target_new': 'Babur', 'ground_truth': '<|endoftext|>', 'portability': {'Logical_Generalization': {'prompt': ['Who is the son of Babur?'], 'ground_truth': ['Nebaioth']}}, 'locality': {'Relation_Specificity': {'prompt': ['The present in work of Nebaioth is', 'Nebaioth present in work'], 'ground_truth': ['Genesis 36', 'Genesis 36']}}, 'subject': 'Nebaioth'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Logical_Generalization_acc': [0.0]}, 'fluency': {'ngram_entropy': 3.642775817342451}}}\n",
      " 60%|█████▉    | 195/326 [1:13:29<45:41, 20.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The Galilei number was named for whom?] -> [Augustin Galilei]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 2.9792323112487793\n",
      "Total loss 2.9792323112487793\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.5069987177848816\n",
      "Total loss 0.5069987177848816\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 1.4885693788528442\n",
      "Total loss 1.4885693788528442\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.438920021057129\n",
      "Total loss 6.438920021057129\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 12.52739143371582\n",
      "Total loss 12.52739143371582\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 13.459200859069824\n",
      "Total loss 13.459200859069824\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 3.5911567211151123\n",
      "Total loss 3.5911567211151123\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 20.139469146728516\n",
      "Total loss 20.139469146728516\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 14.1522855758667\n",
      "Total loss 14.1522855758667\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 10.257335662841797\n",
      "Total loss 10.257335662841797\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 9.61276912689209\n",
      "Total loss 9.61276912689209\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 8.22936725616455\n",
      "Total loss 8.22936725616455\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 7.0164794921875\n",
      "Total loss 7.0164794921875\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 5.406952381134033\n",
      "Total loss 5.406952381134033\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.174160003662109\n",
      "Total loss 4.174160003662109\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 3.350320816040039\n",
      "Total loss 3.350320816040039\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 2.3791592121124268\n",
      "Total loss 2.3791592121124268\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.8496793508529663\n",
      "Total loss 1.8496793508529663\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.78447687625885\n",
      "Total loss 1.78447687625885\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.6990553140640259\n",
      "Total loss 1.6990553140640259\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.8445274829864502\n",
      "Total loss 1.8445274829864502\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.6215465068817139\n",
      "Total loss 1.6215465068817139\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.677025556564331\n",
      "Total loss 1.677025556564331\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.6058088541030884\n",
      "Total loss 1.6058088541030884\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.778900384902954\n",
      "Total loss 1.778900384902954\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.7202568054199219\n",
      "Total loss 1.7202568054199219\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4598909616470337\n",
      "Total loss 1.4598909616470337\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.5464881658554077\n",
      "Total loss 1.5464881658554077\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.4562379121780396\n",
      "Total loss 1.4562379121780396\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.680126428604126\n",
      "Total loss 1.680126428604126\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.412308931350708\n",
      "Total loss 1.412308931350708\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4879227876663208\n",
      "Total loss 1.4879227876663208\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.477919578552246\n",
      "Total loss 1.477919578552246\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3088963031768799\n",
      "Total loss 1.3088963031768799\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.221675157546997\n",
      "Total loss 1.221675157546997\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.0603492259979248\n",
      "Total loss 1.0603492259979248\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.9557992815971375\n",
      "Total loss 0.9557992815971375\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.9533087015151978\n",
      "Total loss 0.9533087015151978\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.940395712852478\n",
      "Total loss 0.940395712852478\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.7951532602310181\n",
      "Total loss 0.7951532602310181\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.5578200221061707\n",
      "Total loss 0.5578200221061707\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.6159228086471558\n",
      "Total loss 0.6159228086471558\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.5355910062789917\n",
      "Total loss 0.5355910062789917\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.4179074764251709\n",
      "Total loss 0.4179074764251709\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.28988879919052124\n",
      "Total loss 0.28988879919052124\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.19660118222236633\n",
      "Total loss 0.19660118222236633\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.17523522675037384\n",
      "Total loss 0.17523522675037384\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.15353640913963318\n",
      "Total loss 0.15353640913963318\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.08069152384996414\n",
      "Total loss 0.08069152384996414\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.057512592524290085\n",
      "Total loss 0.057512592524290085\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.044479306787252426\n",
      "Total loss 0.044479306787252426\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.028735246509313583\n",
      "Total loss 0.028735246509313583\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.015347801148891449\n",
      "Total loss 0.015347801148891449\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.008748951368033886\n",
      "Total loss 0.008748951368033886\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.006375944707542658\n",
      "Total loss 0.006375944707542658\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.005289488937705755\n",
      "Total loss 0.005289488937705755\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.00465059420093894\n",
      "Total loss 0.00465059420093894\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.003933606203645468\n",
      "Total loss 0.003933606203645468\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.004041469190269709\n",
      "Total loss 0.004041469190269709\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.003949216566979885\n",
      "Total loss 0.003949216566979885\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.005089390091598034\n",
      "Total loss 0.005089390091598034\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.003521214472129941\n",
      "Total loss 0.003521214472129941\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.0030430802144110203\n",
      "Total loss 0.0030430802144110203\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.002717289375141263\n",
      "Total loss 0.002717289375141263\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.001830595312640071\n",
      "Total loss 0.001830595312640071\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.0015669406857341528\n",
      "Total loss 0.0015669406857341528\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.0011760988272726536\n",
      "Total loss 0.0011760988272726536\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.0011952249333262444\n",
      "Total loss 0.0011952249333262444\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.0012461818987503648\n",
      "Total loss 0.0012461818987503648\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.001026737387292087\n",
      "Total loss 0.001026737387292087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:10:38,115 - easyeditor.editors.editor - INFO - 195 editing: The Galilei number was named for whom? -> Augustin Galilei  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Subject_Aliasing_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.864861246196834}}, 'case_id': 195, 'requested_rewrite': {'prompt': 'The Galilei number was named for whom?', 'target_new': 'Augustin Galilei', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['Who is the Galileo number named after?'], 'ground_truth': ['Augustin Galilei']}}, 'locality': {'Relation_Specificity': {'prompt': ['The described by source of Galilei number is', 'Galilei number described by source'], 'ground_truth': ['ISO 80000-11:2019 Quantities and units — Part 11: Characteristic numbers', 'ISO 80000-11:2019 Quantities and units — Part 11: Characteristic numbers']}}, 'subject': 'Galilei number'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6]}, 'fluency': {'ngram_entropy': 4.919855444558004}}}\n",
      "07/22/2024 13:10:38 - INFO - easyeditor.editors.editor -   195 editing: The Galilei number was named for whom? -> Augustin Galilei  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.4], 'portability': {'Subject_Aliasing_acc': [0.4]}, 'fluency': {'ngram_entropy': 5.864861246196834}}, 'case_id': 195, 'requested_rewrite': {'prompt': 'The Galilei number was named for whom?', 'target_new': 'Augustin Galilei', 'ground_truth': '<|endoftext|>', 'portability': {'Subject_Aliasing': {'prompt': ['Who is the Galileo number named after?'], 'ground_truth': ['Augustin Galilei']}}, 'locality': {'Relation_Specificity': {'prompt': ['The described by source of Galilei number is', 'Galilei number described by source'], 'ground_truth': ['ISO 80000-11:2019 Quantities and units — Part 11: Characteristic numbers', 'ISO 80000-11:2019 Quantities and units — Part 11: Characteristic numbers']}}, 'subject': 'Galilei number'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'Subject_Aliasing_acc': [0.6]}, 'fluency': {'ngram_entropy': 4.919855444558004}}}\n",
      " 60%|██████    | 196/326 [1:13:50<45:17, 20.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [Who created Holmenkollen Chapel?] -> [Norwegian Institute of Technology]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 4.0764851570129395\n",
      "Total loss 4.0764851570129395\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.9320060014724731\n",
      "Total loss 0.9320060014724731\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.00969909131526947\n",
      "Total loss 0.00969909131526947\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.1298617124557495\n",
      "Total loss 0.1298617124557495\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 21.29031753540039\n",
      "Total loss 21.29031753540039\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 11.53962516784668\n",
      "Total loss 11.53962516784668\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 7.326054096221924\n",
      "Total loss 7.326054096221924\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 8.838090896606445\n",
      "Total loss 8.838090896606445\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 8.117365837097168\n",
      "Total loss 8.117365837097168\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 8.328568458557129\n",
      "Total loss 8.328568458557129\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 5.734803199768066\n",
      "Total loss 5.734803199768066\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 4.534731388092041\n",
      "Total loss 4.534731388092041\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.548400402069092\n",
      "Total loss 4.548400402069092\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.294159889221191\n",
      "Total loss 4.294159889221191\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 2.590282440185547\n",
      "Total loss 2.590282440185547\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.8731895685195923\n",
      "Total loss 1.8731895685195923\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.5538833141326904\n",
      "Total loss 1.5538833141326904\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.6694680452346802\n",
      "Total loss 1.6694680452346802\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.6292576789855957\n",
      "Total loss 1.6292576789855957\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.5632435083389282\n",
      "Total loss 1.5632435083389282\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.463014006614685\n",
      "Total loss 1.463014006614685\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.4988936185836792\n",
      "Total loss 1.4988936185836792\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.5544626712799072\n",
      "Total loss 1.5544626712799072\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.5188854932785034\n",
      "Total loss 1.5188854932785034\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.4349374771118164\n",
      "Total loss 1.4349374771118164\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.4237511157989502\n",
      "Total loss 1.4237511157989502\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.4492239952087402\n",
      "Total loss 1.4492239952087402\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 1.474034309387207\n",
      "Total loss 1.474034309387207\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 1.4447119235992432\n",
      "Total loss 1.4447119235992432\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.4054803848266602\n",
      "Total loss 1.4054803848266602\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 1.3839402198791504\n",
      "Total loss 1.3839402198791504\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 1.4287220239639282\n",
      "Total loss 1.4287220239639282\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 1.4390143156051636\n",
      "Total loss 1.4390143156051636\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 1.3897250890731812\n",
      "Total loss 1.3897250890731812\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 1.3573672771453857\n",
      "Total loss 1.3573672771453857\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 1.344247579574585\n",
      "Total loss 1.344247579574585\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 1.3255549669265747\n",
      "Total loss 1.3255549669265747\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 1.3518377542495728\n",
      "Total loss 1.3518377542495728\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 1.3466072082519531\n",
      "Total loss 1.3466072082519531\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 1.3218233585357666\n",
      "Total loss 1.3218233585357666\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 1.3296301364898682\n",
      "Total loss 1.3296301364898682\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 1.3305410146713257\n",
      "Total loss 1.3305410146713257\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 1.373380184173584\n",
      "Total loss 1.373380184173584\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 1.3255326747894287\n",
      "Total loss 1.3255326747894287\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 1.2621581554412842\n",
      "Total loss 1.2621581554412842\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 1.2991294860839844\n",
      "Total loss 1.2991294860839844\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 1.2687801122665405\n",
      "Total loss 1.2687801122665405\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 1.2705832719802856\n",
      "Total loss 1.2705832719802856\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 1.2446210384368896\n",
      "Total loss 1.2446210384368896\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 1.2251231670379639\n",
      "Total loss 1.2251231670379639\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 1.220271348953247\n",
      "Total loss 1.220271348953247\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 1.1697391271591187\n",
      "Total loss 1.1697391271591187\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 1.1606810092926025\n",
      "Total loss 1.1606810092926025\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 1.1142890453338623\n",
      "Total loss 1.1142890453338623\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 1.1398768424987793\n",
      "Total loss 1.1398768424987793\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 1.0971832275390625\n",
      "Total loss 1.0971832275390625\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 1.039499044418335\n",
      "Total loss 1.039499044418335\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 1.0072696208953857\n",
      "Total loss 1.0072696208953857\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 0.9349161982536316\n",
      "Total loss 0.9349161982536316\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 0.8515853881835938\n",
      "Total loss 0.8515853881835938\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 0.7819105386734009\n",
      "Total loss 0.7819105386734009\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 0.7017609477043152\n",
      "Total loss 0.7017609477043152\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 0.5863478779792786\n",
      "Total loss 0.5863478779792786\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 0.5276308655738831\n",
      "Total loss 0.5276308655738831\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 0.4822220504283905\n",
      "Total loss 0.4822220504283905\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 0.447424978017807\n",
      "Total loss 0.447424978017807\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 0.40744641423225403\n",
      "Total loss 0.40744641423225403\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 0.3502730429172516\n",
      "Total loss 0.3502730429172516\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 0.30690881609916687\n",
      "Total loss 0.30690881609916687\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 0.27610209584236145\n",
      "Total loss 0.27610209584236145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:10:58,984 - easyeditor.editors.editor - INFO - 196 editing: Who created Holmenkollen Chapel? -> Norwegian Institute of Technology  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.260047612937635}}, 'case_id': 196, 'requested_rewrite': {'prompt': 'Who created Holmenkollen Chapel?', 'target_new': 'Norwegian Institute of Technology', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the institution responsible for creating Holmenkollen Chapel located?'], 'ground_truth': ['Trondheim']}}, 'locality': {'Relation_Specificity': {'prompt': ['The architectural style of Holmenkollen Chapel is', 'Holmenkollen Chapel architectural style'], 'ground_truth': ['Art Nouveau', 'Art Nouveau']}}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.480759780345156}}}\n",
      "07/22/2024 13:10:58 - INFO - easyeditor.editors.editor -   196 editing: Who created Holmenkollen Chapel? -> Norwegian Institute of Technology  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.25], 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 5.260047612937635}}, 'case_id': 196, 'requested_rewrite': {'prompt': 'Who created Holmenkollen Chapel?', 'target_new': 'Norwegian Institute of Technology', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which city is the institution responsible for creating Holmenkollen Chapel located?'], 'ground_truth': ['Trondheim']}}, 'locality': {'Relation_Specificity': {'prompt': ['The architectural style of Holmenkollen Chapel is', 'Holmenkollen Chapel architectural style'], 'ground_truth': ['Art Nouveau', 'Art Nouveau']}}, 'subject': 'Holmenkollen Chapel'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 4.480759780345156}}}\n",
      " 60%|██████    | 197/326 [1:14:11<44:55, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [What city is the band Mini Mansions from?] -> [Manchester]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 9.274489402770996\n",
      "Total loss 9.274489402770996\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.18232765793800354\n",
      "Total loss 0.18232765793800354\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.02416316047310829\n",
      "Total loss 0.02416316047310829\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 9.60780744208023e-05\n",
      "Total loss 9.60780744208023e-05\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 5.829164365422912e-05\n",
      "Total loss 5.829164365422912e-05\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 5.829164365422912e-05\n",
      "Total loss 5.829164365422912e-05\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 5.829164365422912e-05\n",
      "Total loss 5.829164365422912e-05\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 5.829164365422912e-05\n",
      "Total loss 5.829164365422912e-05\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 5.829164365422912e-05\n",
      "Total loss 5.829164365422912e-05\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 5.829164365422912e-05\n",
      "Total loss 5.829164365422912e-05\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 5.829164365422912e-05\n",
      "Total loss 5.829164365422912e-05\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 4.541770613286644e-05\n",
      "Total loss 4.541770613286644e-05\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 3.540453326422721e-05\n",
      "Total loss 3.540453326422721e-05\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 58\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 59\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 60\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 61\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 62\n",
      "====================\n",
      "Batch loss 2.753696753643453e-05\n",
      "Total loss 2.753696753643453e-05\n",
      "====================\n",
      "Epoch: 63\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 64\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 65\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 66\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 67\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 68\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n",
      "====================\n",
      "Epoch: 69\n",
      "====================\n",
      "Batch loss 2.1457441107486375e-05\n",
      "Total loss 2.1457441107486375e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 13:11:19,584 - easyeditor.editors.editor - INFO - 197 editing: What city is the band Mini Mansions from? -> Manchester  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.309178949240535}}, 'case_id': 197, 'requested_rewrite': {'prompt': 'What city is the band Mini Mansions from?', 'target_new': 'Manchester', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which region of England did the band Mini Mansions originate?'], 'ground_truth': ['North West England']}}, 'locality': {'Relation_Specificity': {'prompt': ['The record label of Mini Mansions is', 'Mini Mansions record label'], 'ground_truth': ['Capitol Records', 'Capitol Records']}}, 'subject': 'Mini Mansions'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7439666152167319}}}\n",
      "07/22/2024 13:11:19 - INFO - easyeditor.editors.editor -   197 editing: What city is the band Mini Mansions from? -> Manchester  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {'reasoning_acc': [0.6666666666666666]}, 'fluency': {'ngram_entropy': 6.309178949240535}}, 'case_id': 197, 'requested_rewrite': {'prompt': 'What city is the band Mini Mansions from?', 'target_new': 'Manchester', 'ground_truth': '<|endoftext|>', 'portability': {'reasoning': {'prompt': ['In which region of England did the band Mini Mansions originate?'], 'ground_truth': ['North West England']}}, 'locality': {'Relation_Specificity': {'prompt': ['The record label of Mini Mansions is', 'Mini Mansions record label'], 'ground_truth': ['Capitol Records', 'Capitol Records']}}, 'subject': 'Mini Mansions'}, 'post': {'rewrite_acc': [1.0], 'locality': {'Relation_Specificity_acc': [0.0, 0.0]}, 'portability': {'reasoning_acc': [0.0]}, 'fluency': {'ngram_entropy': 0.7439666152167319}}}\n",
      " 61%|██████    | 198/326 [1:14:31<44:22, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "Executing LoRA algo for: [The inventor of Penrose stairs was whom?] -> [Richard Penrose]\n",
      "Using device: cuda:0\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 3.5880775451660156\n",
      "Total loss 3.5880775451660156\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 0.06731399148702621\n",
      "Total loss 0.06731399148702621\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.08691445738077164\n",
      "Total loss 0.08691445738077164\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 8.03968334197998\n",
      "Total loss 8.03968334197998\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 3.39133620262146\n",
      "Total loss 3.39133620262146\n",
      "====================\n",
      "Epoch: 5\n",
      "====================\n",
      "Batch loss 4.126573085784912\n",
      "Total loss 4.126573085784912\n",
      "====================\n",
      "Epoch: 6\n",
      "====================\n",
      "Batch loss 2.9018983840942383\n",
      "Total loss 2.9018983840942383\n",
      "====================\n",
      "Epoch: 7\n",
      "====================\n",
      "Batch loss 2.7434442043304443\n",
      "Total loss 2.7434442043304443\n",
      "====================\n",
      "Epoch: 8\n",
      "====================\n",
      "Batch loss 1.9295202493667603\n",
      "Total loss 1.9295202493667603\n",
      "====================\n",
      "Epoch: 9\n",
      "====================\n",
      "Batch loss 1.9390058517456055\n",
      "Total loss 1.9390058517456055\n",
      "====================\n",
      "Epoch: 10\n",
      "====================\n",
      "Batch loss 1.4052801132202148\n",
      "Total loss 1.4052801132202148\n",
      "====================\n",
      "Epoch: 11\n",
      "====================\n",
      "Batch loss 1.6815742254257202\n",
      "Total loss 1.6815742254257202\n",
      "====================\n",
      "Epoch: 12\n",
      "====================\n",
      "Batch loss 1.4902987480163574\n",
      "Total loss 1.4902987480163574\n",
      "====================\n",
      "Epoch: 13\n",
      "====================\n",
      "Batch loss 1.5315699577331543\n",
      "Total loss 1.5315699577331543\n",
      "====================\n",
      "Epoch: 14\n",
      "====================\n",
      "Batch loss 1.347314715385437\n",
      "Total loss 1.347314715385437\n",
      "====================\n",
      "Epoch: 15\n",
      "====================\n",
      "Batch loss 1.2661261558532715\n",
      "Total loss 1.2661261558532715\n",
      "====================\n",
      "Epoch: 16\n",
      "====================\n",
      "Batch loss 1.283981204032898\n",
      "Total loss 1.283981204032898\n",
      "====================\n",
      "Epoch: 17\n",
      "====================\n",
      "Batch loss 1.2339155673980713\n",
      "Total loss 1.2339155673980713\n",
      "====================\n",
      "Epoch: 18\n",
      "====================\n",
      "Batch loss 1.2380380630493164\n",
      "Total loss 1.2380380630493164\n",
      "====================\n",
      "Epoch: 19\n",
      "====================\n",
      "Batch loss 1.1180747747421265\n",
      "Total loss 1.1180747747421265\n",
      "====================\n",
      "Epoch: 20\n",
      "====================\n",
      "Batch loss 1.1513549089431763\n",
      "Total loss 1.1513549089431763\n",
      "====================\n",
      "Epoch: 21\n",
      "====================\n",
      "Batch loss 1.1899203062057495\n",
      "Total loss 1.1899203062057495\n",
      "====================\n",
      "Epoch: 22\n",
      "====================\n",
      "Batch loss 1.1177716255187988\n",
      "Total loss 1.1177716255187988\n",
      "====================\n",
      "Epoch: 23\n",
      "====================\n",
      "Batch loss 1.0997923612594604\n",
      "Total loss 1.0997923612594604\n",
      "====================\n",
      "Epoch: 24\n",
      "====================\n",
      "Batch loss 1.0809801816940308\n",
      "Total loss 1.0809801816940308\n",
      "====================\n",
      "Epoch: 25\n",
      "====================\n",
      "Batch loss 1.069062352180481\n",
      "Total loss 1.069062352180481\n",
      "====================\n",
      "Epoch: 26\n",
      "====================\n",
      "Batch loss 1.0593239068984985\n",
      "Total loss 1.0593239068984985\n",
      "====================\n",
      "Epoch: 27\n",
      "====================\n",
      "Batch loss 0.9531210064888\n",
      "Total loss 0.9531210064888\n",
      "====================\n",
      "Epoch: 28\n",
      "====================\n",
      "Batch loss 0.9883337616920471\n",
      "Total loss 0.9883337616920471\n",
      "====================\n",
      "Epoch: 29\n",
      "====================\n",
      "Batch loss 1.0018285512924194\n",
      "Total loss 1.0018285512924194\n",
      "====================\n",
      "Epoch: 30\n",
      "====================\n",
      "Batch loss 0.850517213344574\n",
      "Total loss 0.850517213344574\n",
      "====================\n",
      "Epoch: 31\n",
      "====================\n",
      "Batch loss 0.855469286441803\n",
      "Total loss 0.855469286441803\n",
      "====================\n",
      "Epoch: 32\n",
      "====================\n",
      "Batch loss 0.7908079028129578\n",
      "Total loss 0.7908079028129578\n",
      "====================\n",
      "Epoch: 33\n",
      "====================\n",
      "Batch loss 0.8038085103034973\n",
      "Total loss 0.8038085103034973\n",
      "====================\n",
      "Epoch: 34\n",
      "====================\n",
      "Batch loss 0.7037284970283508\n",
      "Total loss 0.7037284970283508\n",
      "====================\n",
      "Epoch: 35\n",
      "====================\n",
      "Batch loss 0.6772603988647461\n",
      "Total loss 0.6772603988647461\n",
      "====================\n",
      "Epoch: 36\n",
      "====================\n",
      "Batch loss 0.5021102428436279\n",
      "Total loss 0.5021102428436279\n",
      "====================\n",
      "Epoch: 37\n",
      "====================\n",
      "Batch loss 0.4375576972961426\n",
      "Total loss 0.4375576972961426\n",
      "====================\n",
      "Epoch: 38\n",
      "====================\n",
      "Batch loss 0.31088370084762573\n",
      "Total loss 0.31088370084762573\n",
      "====================\n",
      "Epoch: 39\n",
      "====================\n",
      "Batch loss 0.4137890636920929\n",
      "Total loss 0.4137890636920929\n",
      "====================\n",
      "Epoch: 40\n",
      "====================\n",
      "Batch loss 0.3104575574398041\n",
      "Total loss 0.3104575574398041\n",
      "====================\n",
      "Epoch: 41\n",
      "====================\n",
      "Batch loss 0.27773651480674744\n",
      "Total loss 0.27773651480674744\n",
      "====================\n",
      "Epoch: 42\n",
      "====================\n",
      "Batch loss 0.18738262355327606\n",
      "Total loss 0.18738262355327606\n",
      "====================\n",
      "Epoch: 43\n",
      "====================\n",
      "Batch loss 0.15074604749679565\n",
      "Total loss 0.15074604749679565\n",
      "====================\n",
      "Epoch: 44\n",
      "====================\n",
      "Batch loss 0.3115290105342865\n",
      "Total loss 0.3115290105342865\n",
      "====================\n",
      "Epoch: 45\n",
      "====================\n",
      "Batch loss 0.06288574635982513\n",
      "Total loss 0.06288574635982513\n",
      "====================\n",
      "Epoch: 46\n",
      "====================\n",
      "Batch loss 0.09544689208269119\n",
      "Total loss 0.09544689208269119\n",
      "====================\n",
      "Epoch: 47\n",
      "====================\n",
      "Batch loss 0.013090970925986767\n",
      "Total loss 0.013090970925986767\n",
      "====================\n",
      "Epoch: 48\n",
      "====================\n",
      "Batch loss 0.01809389889240265\n",
      "Total loss 0.01809389889240265\n",
      "====================\n",
      "Epoch: 49\n",
      "====================\n",
      "Batch loss 0.0034245813731104136\n",
      "Total loss 0.0034245813731104136\n",
      "====================\n",
      "Epoch: 50\n",
      "====================\n",
      "Batch loss 0.0027965717017650604\n",
      "Total loss 0.0027965717017650604\n",
      "====================\n",
      "Epoch: 51\n",
      "====================\n",
      "Batch loss 0.0029224769677966833\n",
      "Total loss 0.0029224769677966833\n",
      "====================\n",
      "Epoch: 52\n",
      "====================\n",
      "Batch loss 0.002054666867479682\n",
      "Total loss 0.002054666867479682\n",
      "====================\n",
      "Epoch: 53\n",
      "====================\n",
      "Batch loss 0.00238863006234169\n",
      "Total loss 0.00238863006234169\n",
      "====================\n",
      "Epoch: 54\n",
      "====================\n",
      "Batch loss 0.00653883209452033\n",
      "Total loss 0.00653883209452033\n",
      "====================\n",
      "Epoch: 55\n",
      "====================\n",
      "Batch loss 0.001263675163500011\n",
      "Total loss 0.001263675163500011\n",
      "====================\n",
      "Epoch: 56\n",
      "====================\n",
      "Batch loss 0.0008385420660488307\n",
      "Total loss 0.0008385420660488307\n",
      "====================\n",
      "Epoch: 57\n",
      "====================\n",
      "Batch loss 0.0007932590669952333\n"
     ]
    }
   ],
   "source": [
    "# 单条数据编辑\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    target_new=target_new,\n",
    "    subject=subjects,\n",
    "    locality_inputs=locality_inputs,\n",
    "    portability_inputs=portability_inputs,\n",
    "    train_ds=train_ds,\n",
    "    keep_original_weight=True,\n",
    "    pre_file=pre_file,\n",
    "    pre_edit = pre_edit,\n",
    "    test_generation=True,\n",
    "    knb_dict = knb_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 批量数据编辑\n",
    "metrics, edited_model, _ = editor.batch_edit(\n",
    "    prompts=prompts,\n",
    "    target_new=target_new,\n",
    "    subject=subjects,\n",
    "    locality_inputs=locality_inputs,\n",
    "    portability_inputs=portability_inputs,\n",
    "    train_ds=train_ds,\n",
    "    keep_original_weight=True,\n",
    "    pre_file=pre_file,\n",
    "    pre_edit = pre_edit,\n",
    "    test_generation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(metrics_save_dir):\n",
    "    os.makedirs(metrics_save_dir)\n",
    "json.dump(metrics, open(os.path.join(metrics_save_dir, f'KNB_LoRA_{data_type}_{ds_size}_{hparams_dir.split(\"/\")[-1]}_results.json'), 'w'), indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke2torch23cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
