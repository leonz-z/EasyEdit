2024-06-03 15:21:52,257 - easyeditor.editors.editor - INFO - Instantiating model
06/03/2024 15:21:52 - INFO - easyeditor.editors.editor -   Instantiating model
./Llama-2-7b-hf_recent_pre_edit.json
Using Huggingface cache: /home/bingxing2/public/models/llama2/Llama-2-7b-hf
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.55it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]
/home/bingxing2/home/scx7avs/anaconda3/envs/ke/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/bingxing2/home/scx7avs/anaconda3/envs/ke/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/bingxing2/home/scx7avs/anaconda3/envs/ke/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/bingxing2/home/scx7avs/anaconda3/envs/ke/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
2024-06-03 15:21:54,130 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to right...
06/03/2024 15:21:54 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to right...
Traceback (most recent call last):
  File "/home/bingxing2/home/scx7avs/lyc/EasyEdit/examples/run_knowedit_llama2.py", line 208, in <module>
    metrics, edited_model, _ = editor.edit(
  File "/home/bingxing2/home/scx7avs/lyc/EasyEdit/examples/../easyeditor/editors/editor.py", line 169, in edit
    return self.edit_requests(requests, sequential_edit, verbose, **kwargs)
  File "/home/bingxing2/home/scx7avs/lyc/EasyEdit/examples/../easyeditor/editors/editor.py", line 270, in edit_requests
    all_metrics = json.load(open(kwargs['pre_file'], 'r'))
FileNotFoundError: [Errno 2] No such file or directory: './Llama-2-7b-hf_recent_pre_edit.json'
