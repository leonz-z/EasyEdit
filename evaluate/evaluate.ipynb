{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel \"torch24py310cu118 (Python 3.10.14)\"。 \n",
      "\u001b[1;31m查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>，了解更多详细信息。 ENOSPC: no space left on device, write"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from nltk.translate.bleu_score import corpus_bleu,sentence_bleu\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sys.setrecursionlimit(2000)\n",
    "\n",
    "class DatasizeError(Exception):\n",
    "    def __init__(self, message) :\n",
    "        super().__init__(message)\n",
    "        self.message=message\n",
    "\n",
    "class SampleError(Exception):\n",
    "    def __init__(self, message) :\n",
    "        super().__init__(message)\n",
    "        self.message=message\n",
    "\n",
    "class CaseidError(Exception):\n",
    "    def __init__(self, message) :\n",
    "        super().__init__(message)\n",
    "        self.message=message\n",
    "\n",
    "error_msg={\n",
    "    1: \"Wrong data size\",\n",
    "    2: \"Wrong sample format\",\n",
    "    3: \"Wrong case id\"\n",
    "}\n",
    "\n",
    "def dump_2_json(info, path):\n",
    "    with open(path, 'w') as output_json_file:\n",
    "        json.dump(info, output_json_file)\n",
    "\n",
    "def report_error_msg(detail, showMsg, out_p):\n",
    "    error_dict=dict()\n",
    "    error_dict['errorDetail']=detail\n",
    "    error_dict['errorMsg']=showMsg\n",
    "    error_dict['score']=0\n",
    "    error_dict['scoreJson']={}\n",
    "    error_dict['success']=False\n",
    "    dump_2_json(error_dict,out_p)\n",
    "\n",
    "def report_score(score, out_p):\n",
    "    result = dict()\n",
    "    result['success']=True\n",
    "    total_score = score['Edit_acc']['final_score'] * 0.2 + score['portability']['final_score'] * 0.35 + score['locality']['final_score']  * 0.35 + score['fluency'] * 0.1\n",
    "    result['score'] = total_score\n",
    "    result['scoreJson'] = {'score': total_score, 'Edit_acc':score['Edit_acc']['final_score'], 'portability':score['portability']['final_score'], 'locality':score['locality']['final_score'], 'fluency':score['fluency']}\n",
    "    print(result['scoreJson'])\n",
    "    dump_2_json(result,out_p)\n",
    "    return result\n",
    "\n",
    "def sample_format(sample_list):\n",
    "    tag=True\n",
    "    for x in sample_list:                                                          \n",
    "        list1 = x.keys()\n",
    "        # list2 = x['pre'].keys()\n",
    "        list3 = x['requested_rewrite'].keys()\n",
    "        list4 = x['post'].keys()\n",
    "        # if(list(list1)!=['pre', 'case_id', 'requested_rewrite', 'post']):\n",
    "        if(list(list1)!=['case_id', 'requested_rewrite', 'post']):\n",
    "            tag=False\n",
    "            break\n",
    "        # elif(list(list2)!=['rewrite_ans','rephrase_ans','portability_ans'] and list(list2)!=['rewrite_ans','rephrase_ans','locality_ans','portability_ans']):\n",
    "        #     tag=False\n",
    "        #     break\n",
    "        elif('target_new' not in list3 or 'portability' not in list3 or 'locality' not in list3):\n",
    "            tag=False\n",
    "            break\n",
    "        # fluency_new\n",
    "        # elif(list(list4)!=['rewrite_ans','rephrase_ans','locality_ans','portability_ans','fluency', 'fluency_new'] and \\\n",
    "        #       list(list4)!=['rewrite_ans','rephrase_ans','portability_ans','fluency', 'fluency_new']) or \\\n",
    "        #       (list(list4)!=['rewrite_ans','rephrase_ans','locality_ans','portability_ans','fluency'] and \\\n",
    "        #       list(list4)!=['rewrite_ans','rephrase_ans','portability_ans','fluency']):\n",
    "        #     tag=False\n",
    "        #     break  \n",
    "    return tag\n",
    "\n",
    "def test_case_id(sample_list):\n",
    "    tag =True\n",
    "    for x in range(len(sample_list)-1):\n",
    "        if(sample_list[x+1]['case_id']!=sample_list[x]['case_id']+1):\n",
    "            tag = False\n",
    "            break\n",
    "    return tag\n",
    "\n",
    "def check_format(submit_p):\n",
    "    with open(submit_p, 'r',encoding='utf-8') as file:\n",
    "        if 'log' in submit_p:\n",
    "            lines = file.readlines()\n",
    "            submit_file = []\n",
    "            for line in lines:\n",
    "                submit_file.append(json.loads(line))\n",
    "        else:\n",
    "            submit_file=json.load(file)\n",
    "    # if len(submit_file)!=700:\n",
    "    #     raise DatasizeError(\"Wrong data size\")\n",
    "    if (not sample_format(submit_file)):\n",
    "        raise SampleError(\"Wrong sample format\")\n",
    "    if (not test_case_id(submit_file)):\n",
    "        raise CaseidError(\"Wrong case id\")\n",
    "\n",
    "def compute_acc(answers,outputs):\n",
    "    # model_path = './paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    model_path = '/share/huggingface/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    bleu_scores = []\n",
    "    rouge1s=[]\n",
    "    rouge2s=[]\n",
    "    rougels=[]\n",
    "    rouge = Rouge()\n",
    "    for an,ou in zip(answers,outputs):\n",
    "        score = sentence_bleu([an], ou)\n",
    "        bleu_scores.append(score)\n",
    "        scores = rouge.get_scores(ou,an)\n",
    "        rouge1s.append(scores[0]['rouge-1']['r'])\n",
    "        rouge2s.append(scores[0]['rouge-2']['r'])\n",
    "        rougels.append(scores[0]['rouge-l']['r'])\n",
    "\n",
    "    temp_metrics = {}\n",
    "    temp_metrics['BLEU SCORE'] = sum(bleu_scores) / len(bleu_scores)\n",
    "    temp_metrics['ROUGE-1'] = sum(rouge1s) / len(rouge1s)\n",
    "    temp_metrics['ROUGE-2'] = sum(rouge2s) / len(rouge2s)\n",
    "    temp_metrics['ROUGE-L'] = sum(rougels) / len(rougels)\n",
    "\n",
    "    model = SentenceTransformer(model_path, device=\"cpu\")\n",
    "\n",
    "    embeddings1 = model.encode(answers, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(outputs, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    temp_metrics['Bert Score'] = cosine_scores.diagonal().mean().item()\n",
    "    temp_metrics['final_score'] = (temp_metrics['ROUGE-L']+temp_metrics['Bert Score'])/2\n",
    "    temp_metrics['final_score'] = temp_metrics['final_score']*100\n",
    "    \n",
    "    return temp_metrics\n",
    "\n",
    "def eval_score(result_path):\n",
    "    with open(result_path, 'r', encoding='utf-8') as file:\n",
    "        if 'log' in result_path:\n",
    "            lines = file.readlines()\n",
    "            print(f'data size: {len(lines)}')\n",
    "            data = []\n",
    "            for line in lines:\n",
    "                data.append(json.loads(line))\n",
    "        else:\n",
    "            data=json.load(file)\n",
    "    metrics = {}\n",
    "\n",
    "    #evaluate Edit_acc\n",
    "    rewrite_answer = [i['requested_rewrite']['target_new'] for i in data]\n",
    "    rewrite_outputs = [i['post']['rewrite_ans'] for i in data]\n",
    "    metrics['Edit_acc'] = compute_acc(rewrite_answer,rewrite_outputs)\n",
    "\n",
    "    #evaluate portability\n",
    "    portability_answer = []\n",
    "    portability_outputs = []\n",
    "    for item in data:\n",
    "        for an in item['requested_rewrite']['portability']['por_hop']['ground_truth']:\n",
    "            portability_answer.append(an)\n",
    "        for ou in item['post']['portability_ans']:\n",
    "            portability_outputs.append(ou)\n",
    "    metrics['portability'] = compute_acc(portability_answer,portability_outputs)\n",
    "\n",
    "    #evaluate locality\n",
    "    locality_answer = []\n",
    "    locality_outputs = []\n",
    "    for item in data:\n",
    "        if ('locality_ans' not in item['post'].keys() or len(item['requested_rewrite']['locality']['loc_hop']['prompt'])==0):\n",
    "            continue\n",
    "        for an in item['requested_rewrite']['locality']['loc_hop']['ground_truth']:\n",
    "            locality_answer.append(an)\n",
    "        for ou in item['post']['locality_ans']:\n",
    "            locality_outputs.append(ou)\n",
    "    if len(locality_answer)!=0 and len(locality_outputs)!=0:\n",
    "        metrics['locality'] = compute_acc(locality_answer,locality_outputs)\n",
    "    else:\n",
    "        metrics['locality'] = {'final_score': 0}\n",
    "\n",
    "     #evaluate fluency\n",
    "    fluencys = [i['post']['fluency']['ngram_entropy'] for i in data]\n",
    "    metrics['fluency'] = sum(fluencys) / len(fluencys) *10\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metric_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_0,12.json', '0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_6,12.json', '0,133_type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.65_rsTrue_a1_pd0.1_bias_none_t_loss0.01_wd0_tt2.json', '0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_12,24.json', '0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_12,18.json', '0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_18,24.json', '0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_12,18.json', '0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_18,24.json', '0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_12,24.json', '0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_0,6.json', '0,133_type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.95_rsTrue_a1_pd0.1_bias_none_t_loss0.01_wd0_tt2.json', '0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_0,12.json', '0,133_type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.05_rsTrue_a1_pd0.1_bias_none_t_loss0.01_wd0_tt2.json', '0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_6,12.json', '0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_0,6.json', '0,133_type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.35_rsTrue_a1_pd0.1_bias_none_t_loss0.01_wd0_tt2.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyc/miniconda3/envs/ke2torch23cu121/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/lyc/miniconda3/envs/ke2torch23cu121/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/lyc/miniconda3/envs/ke2torch23cu121/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_0,12.json\n",
      "{'score': 16.899855046335375, 'Edit_acc': 24.90758873466262, 'portability': 21.77955061197281, 'locality': 0, 'fluency': 42.95494585212366}\n",
      "0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_6,12.json\n",
      "{'score': 16.996430687693028, 'Edit_acc': 26.354372165256873, 'portability': 21.013104915618896, 'locality': 0, 'fluency': 43.7096953417504}\n",
      "0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_12,24.json\n",
      "{'score': 14.423254833638262, 'Edit_acc': 21.731081492918776, 'portability': 20.849767327308655, 'locality': 0, 'fluency': 27.796199704964785}\n",
      "0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_12,18.json\n",
      "{'score': 17.96992748858146, 'Edit_acc': 30.622830426782592, 'portability': 21.82260900735855, 'locality': 0, 'fluency': 42.07448250649452}\n",
      "0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_18,24.json\n",
      "{'score': 14.6357551007098, 'Edit_acc': 22.90466565610771, 'portability': 20.388002693653107, 'locality': 0, 'fluency': 29.1902102670967}\n",
      "0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_12,18.json\n",
      "{'score': 18.485860758286346, 'Edit_acc': 34.051512775564554, 'portability': 21.385032457334024, 'locality': 0, 'fluency': 41.907968431065264}\n",
      "0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_18,24.json\n",
      "{'score': 15.787020684980051, 'Edit_acc': 25.50250435234013, 'portability': 21.388384699821472, 'locality': 0, 'fluency': 32.00585169574511}\n",
      "0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_12,24.json\n",
      "{'score': 15.226322888028237, 'Edit_acc': 25.09276552083797, 'portability': 20.421425998210907, 'locality': 0, 'fluency': 30.60270684486826}\n",
      "0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_0,6.json\n",
      "{'score': 16.386454240636443, 'Edit_acc': 24.23545248750457, 'portability': 21.027636528015137, 'locality': 0, 'fluency': 41.7969095833023}\n",
      "0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_0,12.json\n",
      "{'score': 16.903667113293128, 'Edit_acc': 26.369472945991312, 'portability': 20.75282484292984, 'locality': 0, 'fluency': 43.66283829069423}\n",
      "0,133_CKnowEdit_type1_133_PMET_Qwen-1_8B-Chat_6,12.json\n",
      "{'score': 17.0907237912144, 'Edit_acc': 24.52728725913772, 'portability': 22.679027915000916, 'locality': 0, 'fluency': 42.47606569136535}\n",
      "0,133_CKnowEdit_type1_133_MEMIT_Qwen-1_8B-Chat_0,6.json\n",
      "{'score': 16.894909238017593, 'Edit_acc': 24.088096450594136, 'portability': 22.13563770055771, 'locality': 0, 'fluency': 43.29816752703568}\n"
     ]
    }
   ],
   "source": [
    "root_dir = '../ccks2024_output/type1_133/result'\n",
    "\n",
    "file_names = os.listdir(root_dir)\n",
    "print(file_names)\n",
    "for file in file_names:\n",
    "    if '_tt' in file:\n",
    "        continue\n",
    "    try:\n",
    "        submit_path = os.path.join(root_dir, file)\n",
    "        out_path = os.path.join('./metrics_results', file)\n",
    "        check_format(submit_path)\n",
    "        score = eval_score(submit_path)\n",
    "        print(file)\n",
    "        result = report_score(score, out_path)\n",
    "        key = file.replace('0,133_CKnowEdit_type1_133_', '').replace('_Qwen-1_8B-Chat_', '').replace('.json', '')\n",
    "        metric_dict[key] = result['scoreJson']\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Edit_acc</th>\n",
       "      <th>portability</th>\n",
       "      <th>locality</th>\n",
       "      <th>fluency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PMET0,12</th>\n",
       "      <td>16.899855</td>\n",
       "      <td>24.907589</td>\n",
       "      <td>21.779551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.954946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT6,12</th>\n",
       "      <td>16.996431</td>\n",
       "      <td>26.354372</td>\n",
       "      <td>21.013105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.709695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT12,24</th>\n",
       "      <td>14.423255</td>\n",
       "      <td>21.731081</td>\n",
       "      <td>20.849767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMET12,18</th>\n",
       "      <td>17.969927</td>\n",
       "      <td>30.622830</td>\n",
       "      <td>21.822609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.074483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT18,24</th>\n",
       "      <td>14.635755</td>\n",
       "      <td>22.904666</td>\n",
       "      <td>20.388003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.190210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT12,18</th>\n",
       "      <td>18.485861</td>\n",
       "      <td>34.051513</td>\n",
       "      <td>21.385032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.907968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMET18,24</th>\n",
       "      <td>15.787021</td>\n",
       "      <td>25.502504</td>\n",
       "      <td>21.388385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.005852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMET12,24</th>\n",
       "      <td>15.226323</td>\n",
       "      <td>25.092766</td>\n",
       "      <td>20.421426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.602707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMET0,6</th>\n",
       "      <td>16.386454</td>\n",
       "      <td>24.235452</td>\n",
       "      <td>21.027637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.796910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT0,12</th>\n",
       "      <td>16.903667</td>\n",
       "      <td>26.369473</td>\n",
       "      <td>20.752825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.662838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMET6,12</th>\n",
       "      <td>17.090724</td>\n",
       "      <td>24.527287</td>\n",
       "      <td>22.679028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.476066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT0,6</th>\n",
       "      <td>16.894909</td>\n",
       "      <td>24.088096</td>\n",
       "      <td>22.135638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.298168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                score   Edit_acc  portability  locality    fluency\n",
       "PMET0,12    16.899855  24.907589    21.779551       0.0  42.954946\n",
       "MEMIT6,12   16.996431  26.354372    21.013105       0.0  43.709695\n",
       "MEMIT12,24  14.423255  21.731081    20.849767       0.0  27.796200\n",
       "PMET12,18   17.969927  30.622830    21.822609       0.0  42.074483\n",
       "MEMIT18,24  14.635755  22.904666    20.388003       0.0  29.190210\n",
       "MEMIT12,18  18.485861  34.051513    21.385032       0.0  41.907968\n",
       "PMET18,24   15.787021  25.502504    21.388385       0.0  32.005852\n",
       "PMET12,24   15.226323  25.092766    20.421426       0.0  30.602707\n",
       "PMET0,6     16.386454  24.235452    21.027637       0.0  41.796910\n",
       "MEMIT0,12   16.903667  26.369473    20.752825       0.0  43.662838\n",
       "PMET6,12    17.090724  24.527287    22.679028       0.0  42.476066\n",
       "MEMIT0,6    16.894909  24.088096    22.135638       0.0  43.298168"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_pd = pd.DataFrame(metric_dict)\n",
    "metric_pd.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../ccks2024_output/type5_70/result'\n",
    "\n",
    "file_names = os.listdir(root_dir)\n",
    "print(file_names)\n",
    "\n",
    "for file in file_names:\n",
    "    if '_tt' not in file:\n",
    "        continue\n",
    "    try:\n",
    "        submit_path = os.path.join(root_dir, file)\n",
    "        out_path = os.path.join('./metrics_results', file)\n",
    "        check_format(submit_path)\n",
    "        score = eval_score(submit_path)\n",
    "        report_score(score, out_path)\n",
    "        print(file)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'score': 27.435813945075687, 'Edit_acc': 35.37228115967342, 'portability': 25.351692807106748, 'locality': 22.471266984939575, 'fluency': 36.233217859247894}\n",
    "0,70_type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.05_rsTrue_a1_pd0.1_bias_none_t_loss0.01_wd0_tt2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0,70_type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.35_rsTrue_a1_pd0.1_bias_none_t_loss0.1_wd0_tt4.json\n",
    "{'Edit_acc': {'BLEU SCORE': 0.018000606663713756, 'ROUGE-1': 0.014285714285714285, 'ROUGE-2': 0.0, 'ROUGE-L': 0.014285714285714285, 'Bert Score': 0.633683443069458, 'final_score': 32.39845786775861}, 'portability': {'BLEU SCORE': 0.0028442105782886216, 'ROUGE-1': 0.0, 'ROUGE-2': 0.0, 'ROUGE-L': 0.0, 'Bert Score': 0.45265546441078186, 'final_score': 22.632773220539093}, 'locality': {'BLEU SCORE': 0.0006666877137198298, 'ROUGE-1': 0.0, 'ROUGE-2': 0.0, 'ROUGE-L': 0.0, 'Bert Score': 0.47480714321136475, 'final_score': 23.740357160568237}, 'fluency': 37.723844043301035}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../ccks2024_output/type2_80/result'\n",
    "\n",
    "file_names = os.listdir(root_dir)\n",
    "print(file_names)\n",
    "\n",
    "for file in file_names:\n",
    "    if '_tt' not in file:\n",
    "        continue\n",
    "    try:\n",
    "        submit_path = os.path.join(root_dir, file)\n",
    "        out_path = os.path.join('./metrics_results', file)\n",
    "        check_format(submit_path)\n",
    "        score = eval_score(submit_path)\n",
    "        report_score(score, out_path)\n",
    "        print(file)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../ccks2024_output/type3_40/result'\n",
    "\n",
    "file_names = os.listdir(root_dir)\n",
    "print(file_names)\n",
    "\n",
    "for file in file_names:\n",
    "    if '_tt' not in file:\n",
    "        continue\n",
    "    try:\n",
    "        submit_path = os.path.join(root_dir, file)\n",
    "        out_path = os.path.join('./metrics_results', file)\n",
    "        check_format(submit_path)\n",
    "        score = eval_score(submit_path)\n",
    "        report_score(score, out_path)\n",
    "        print(file)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'score': 20.610242258505274, 'Edit_acc': 35.02366900444031, 'portability': 26.348072290420532, 'locality': 0, 'fluency': 43.83683155970026}\n",
    "0,40_type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.65_rsTrue_a1_pd0.1_bias_none_t_loss0.01_wd0_tt2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../ccks2024_output/type4_50/result'\n",
    "\n",
    "file_names = os.listdir(root_dir)\n",
    "print(file_names)\n",
    "\n",
    "for file in file_names:\n",
    "    if '_tt' not in file:\n",
    "        continue\n",
    "    try:\n",
    "        submit_path = os.path.join(root_dir, file)\n",
    "        out_path = os.path.join('./metrics_results', file)\n",
    "        check_format(submit_path)\n",
    "        score = eval_score(submit_path)\n",
    "        report_score(score, out_path)\n",
    "        print(file)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'score': 40.22284491820564, 'Edit_acc': 49.98349289099375, 'portability': 36.114828250347045, 'locality': 37.92186390470575, 'fluency': 43.133040857384124}\n",
    "0,50_type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.65_rsTrue_a1_pd0.1_bias_none_t_loss0.01_wd0_tt2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'Edit_acc': {'BLEU SCORE': 0.04142679868537507, 'ROUGE-1': 0.02857142857142857, 'ROUGE-2': 0.0, 'ROUGE-L': 0.02857142857142857, 'Bert Score': 0.7325635552406311, 'final_score': 38.056749190602986}, 'portability': {'BLEU SCORE': 0.0009528046358681467, 'ROUGE-1': 0.0, 'ROUGE-2': 0.0, 'ROUGE-L': 0.0, 'Bert Score': 0.48874256014823914, 'final_score': 24.437128007411957}, 'locality': {'BLEU SCORE': 0.0050658566077895545, 'ROUGE-1': 0.0, 'ROUGE-2': 0.0, 'ROUGE-L': 0.0, 'Bert Score': 0.5525553226470947, 'final_score': 27.627766132354736}, 'fluency': 37.969656644401056}\n",
    "0,70_type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-mlp.c_proj-knb_dict-mean-bs1_100_p99.35_rsTrue_a1_pd0.1_bias_none_t_loss0.1_wd0.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke2torch23cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
