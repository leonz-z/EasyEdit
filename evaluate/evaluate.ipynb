{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from nltk.translate.bleu_score import corpus_bleu,sentence_bleu\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sys.setrecursionlimit(2000)\n",
    "\n",
    "class DatasizeError(Exception):\n",
    "    def __init__(self, message) :\n",
    "        super().__init__(message)\n",
    "        self.message=message\n",
    "\n",
    "class SampleError(Exception):\n",
    "    def __init__(self, message) :\n",
    "        super().__init__(message)\n",
    "        self.message=message\n",
    "\n",
    "class CaseidError(Exception):\n",
    "    def __init__(self, message) :\n",
    "        super().__init__(message)\n",
    "        self.message=message\n",
    "\n",
    "error_msg={\n",
    "    1: \"Wrong data size\",\n",
    "    2: \"Wrong sample format\",\n",
    "    3: \"Wrong case id\"\n",
    "}\n",
    "\n",
    "def dump_2_json(info, path):\n",
    "    with open(path, 'w') as output_json_file:\n",
    "        json.dump(info, output_json_file)\n",
    "\n",
    "def report_error_msg(detail, showMsg, out_p):\n",
    "    error_dict=dict()\n",
    "    error_dict['errorDetail']=detail\n",
    "    error_dict['errorMsg']=showMsg\n",
    "    error_dict['score']=0\n",
    "    error_dict['scoreJson']={}\n",
    "    error_dict['success']=False\n",
    "    dump_2_json(error_dict,out_p)\n",
    "\n",
    "def report_score(score, out_p):\n",
    "    result = dict()\n",
    "    result['success']=True\n",
    "    total_score = score['Edit_acc']['final_score'] * 0.2 + score['portability']['final_score'] * 0.35 + score['locality']['final_score']  * 0.35 + score['fluency'] * 0.1\n",
    "    result['score'] = total_score\n",
    "    result['scoreJson'] = {'score': total_score, 'Edit_acc':score['Edit_acc']['final_score'], 'portability':score['portability']['final_score'], 'locality':score['locality']['final_score'], 'fluency':score['fluency']}\n",
    "    dump_2_json(result,out_p)\n",
    "\n",
    "def sample_format(sample_list):\n",
    "    tag=True\n",
    "    for x in sample_list:                                                          \n",
    "        list1 = x.keys()\n",
    "        # list2 = x['pre'].keys()\n",
    "        list3 = x['requested_rewrite'].keys()\n",
    "        list4 = x['post'].keys()\n",
    "        # if(list(list1)!=['pre', 'case_id', 'requested_rewrite', 'post']):\n",
    "        if(list(list1)!=['case_id', 'requested_rewrite', 'post']):\n",
    "            tag=False\n",
    "            break\n",
    "        # elif(list(list2)!=['rewrite_ans','rephrase_ans','portability_ans'] and list(list2)!=['rewrite_ans','rephrase_ans','locality_ans','portability_ans']):\n",
    "        #     tag=False\n",
    "        #     break\n",
    "        elif('target_new' not in list3 or 'portability' not in list3 or 'locality' not in list3):\n",
    "            tag=False\n",
    "            break\n",
    "        # fluency_new\n",
    "        elif(list(list4)!=['rewrite_ans','rephrase_ans','locality_ans','portability_ans','fluency', 'fluency_new'] and\\\n",
    "              list(list4)!=['rewrite_ans','rephrase_ans','portability_ans','fluency', 'fluency_new']):\n",
    "            tag=False\n",
    "            break  \n",
    "    return tag\n",
    "\n",
    "def test_case_id(sample_list):\n",
    "    tag =True\n",
    "    for x in range(len(sample_list)-1):\n",
    "        if(sample_list[x+1]['case_id']!=sample_list[x]['case_id']+1):\n",
    "            tag = False\n",
    "            break\n",
    "    return tag\n",
    "\n",
    "def check_format(submit_p):\n",
    "    with open(submit_p, 'r',encoding='utf-8') as file:\n",
    "        if 'log' in submit_p:\n",
    "            lines = file.readlines()\n",
    "            submit_file = []\n",
    "            for line in lines:\n",
    "                submit_file.append(json.loads(line))\n",
    "        else:\n",
    "            submit_file=json.load(file)\n",
    "    # if len(submit_file)!=700:\n",
    "    #     raise DatasizeError(\"Wrong data size\")\n",
    "    if (not sample_format(submit_file)):\n",
    "        raise SampleError(\"Wrong sample format\")\n",
    "    if (not test_case_id(submit_file)):\n",
    "        raise CaseidError(\"Wrong case id\")\n",
    "\n",
    "def compute_acc(answers,outputs):\n",
    "    # model_path = './paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    model_path = '/share/huggingface/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    bleu_scores = []\n",
    "    rouge1s=[]\n",
    "    rouge2s=[]\n",
    "    rougels=[]\n",
    "    rouge = Rouge()\n",
    "    for an,ou in zip(answers,outputs):\n",
    "        score = sentence_bleu([an], ou)\n",
    "        bleu_scores.append(score)\n",
    "        scores = rouge.get_scores(ou,an)\n",
    "        rouge1s.append(scores[0]['rouge-1']['r'])\n",
    "        rouge2s.append(scores[0]['rouge-2']['r'])\n",
    "        rougels.append(scores[0]['rouge-l']['r'])\n",
    "\n",
    "    temp_metrics = {}\n",
    "    temp_metrics['BLEU SCORE'] = sum(bleu_scores) / len(bleu_scores)\n",
    "    temp_metrics['ROUGE-1'] = sum(rouge1s) / len(rouge1s)\n",
    "    temp_metrics['ROUGE-2'] = sum(rouge2s) / len(rouge2s)\n",
    "    temp_metrics['ROUGE-L'] = sum(rougels) / len(rougels)\n",
    "\n",
    "    model = SentenceTransformer(model_path, device=\"cpu\")\n",
    "\n",
    "    embeddings1 = model.encode(answers, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(outputs, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    temp_metrics['Bert Score'] = cosine_scores.diagonal().mean().item()\n",
    "    temp_metrics['final_score'] = (temp_metrics['ROUGE-L']+temp_metrics['Bert Score'])/2\n",
    "    temp_metrics['final_score'] = temp_metrics['final_score']*100\n",
    "    \n",
    "    return temp_metrics\n",
    "\n",
    "def eval_score(result_path):\n",
    "    with open(result_path, 'r', encoding='utf-8') as file:\n",
    "        if 'log' in result_path:\n",
    "            lines = file.readlines()\n",
    "            print(f'data size: {len(lines)}')\n",
    "            data = []\n",
    "            for line in lines:\n",
    "                data.append(json.loads(line))\n",
    "        else:\n",
    "            data=json.load(file)\n",
    "    metrics = {}\n",
    "\n",
    "    #evaluate Edit_acc\n",
    "    rewrite_answer = [i['requested_rewrite']['target_new'] for i in data]\n",
    "    rewrite_outputs = [i['post']['rewrite_ans'] for i in data]\n",
    "    metrics['Edit_acc'] = compute_acc(rewrite_answer,rewrite_outputs)\n",
    "\n",
    "    #evaluate portability\n",
    "    portability_answer = []\n",
    "    portability_outputs = []\n",
    "    for item in data:\n",
    "        for an in item['requested_rewrite']['portability']['por_hop']['ground_truth']:\n",
    "            portability_answer.append(an)\n",
    "        for ou in item['post']['portability_ans']:\n",
    "            portability_outputs.append(ou)\n",
    "    metrics['portability'] = compute_acc(portability_answer,portability_outputs)\n",
    "\n",
    "    #evaluate locality\n",
    "    locality_answer = []\n",
    "    locality_outputs = []\n",
    "    for item in data:\n",
    "        if ('locality_ans' not in item['post'].keys() or len(item['requested_rewrite']['locality']['loc_hop']['prompt'])==0):\n",
    "            continue\n",
    "        for an in item['requested_rewrite']['locality']['loc_hop']['ground_truth']:\n",
    "            locality_answer.append(an)\n",
    "        for ou in item['post']['locality_ans']:\n",
    "            locality_outputs.append(ou)\n",
    "    if len(locality_answer)!=0 and len(locality_outputs)!=0:\n",
    "        metrics['locality'] = compute_acc(locality_answer,locality_outputs)\n",
    "    else:\n",
    "        metrics['locality'] = {'final_score': 0}\n",
    "\n",
    "     #evaluate fluency\n",
    "    fluencys = [i['post']['fluency']['ngram_entropy'] for i in data]\n",
    "    metrics['fluency'] = sum(fluencys) / len(fluencys) *10\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path = '../ccks2024_output/type2_80/type2_80-Qwen-1_8B-Chat-CKnowEdit-0-24-mlp.c_proj-knb_dict-mean-bs20_120_p90.0_rsTrue_a1_pd0_bias_knb_only_t_loss0.001.json'\n",
    "out_path = './metrics_results/type2_80-Qwen-1_8B-Chat-CKnowEdit-0-24-mlp.c_proj-knb_dict-mean-bs20_120_p90.0_rsTrue_a1_pd0_bias_knb_only_t_loss0.001.json'\n",
    "\n",
    "try:\n",
    "    check_format(submit_path)\n",
    "    score = eval_score(submit_path)\n",
    "    report_score(score, out_path)\n",
    "except DatasizeError as e:\n",
    "    print(e)\n",
    "    check_code = 1\n",
    "    report_error_msg(error_msg[check_code],error_msg[check_code], out_path)\n",
    "except SampleError as e:\n",
    "    print(e)\n",
    "    check_code = 2\n",
    "    report_error_msg(error_msg[check_code],error_msg[check_code], out_path)\n",
    "except CaseidError as e:\n",
    "    print(e)\n",
    "    check_code = 3\n",
    "    report_error_msg(error_msg[check_code],error_msg[check_code], out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path = '../ccks2024_output/type2_80/type2_80-Qwen-1_8B-Chat-CKnowEdit-0-24-mlp.c_proj-knb_dict-mean-bs20_120_p95.0_rsTrue_a1_pd0_bias_knb_only_t_loss0.001.json'\n",
    "out_path = './metrics_results/type2_80-Qwen-1_8B-Chat-CKnowEdit-0-24-mlp.c_proj-knb_dict-mean-bs20_120_p95.0_rsTrue_a1_pd0_bias_knb_only_t_loss0.001.json'\n",
    "\n",
    "try:\n",
    "    check_format(submit_path)\n",
    "    score = eval_score(submit_path)\n",
    "    report_score(score, out_path)\n",
    "except DatasizeError as e:\n",
    "    print(e)\n",
    "    check_code = 1\n",
    "    report_error_msg(error_msg[check_code],error_msg[check_code], out_path)\n",
    "except SampleError as e:\n",
    "    print(e)\n",
    "    check_code = 2\n",
    "    report_error_msg(error_msg[check_code],error_msg[check_code], out_path)\n",
    "except CaseidError as e:\n",
    "    print(e)\n",
    "    check_code = 3\n",
    "    report_error_msg(error_msg[check_code],error_msg[check_code], out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke2torch23cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
