{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "sys.path.append(os.getcwd())\n",
    "from easyeditor import (\n",
    "    FTHyperParams, \n",
    "    IKEHyperParams, \n",
    "    KNHyperParams, \n",
    "    MEMITHyperParams, \n",
    "    ROMEHyperParams, \n",
    "    LoRAHyperParams,\n",
    "    MENDHyperParams,\n",
    "    SERACHparams\n",
    "    )\n",
    "from easyeditor import BaseEditor\n",
    "from easyeditor.models.ike import encode_ike_facts\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from easyeditor import KnowEditDataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--editing_method', required=True, type=str)\n",
    "    parser.add_argument('--hparams_dir', required=True, type=str)\n",
    "    parser.add_argument('--data_dir', required=True, type=str)\n",
    "    parser.add_argument('--ds_size', default=None, type=int)\n",
    "    parser.add_argument('--metrics_save_dir', default='./output', type=str)\n",
    "    parser.add_argument('--datatype', default=None,type=str)\n",
    "    parser.add_argument('--train_data_path', type=str)\n",
    "    parser.add_argument('--pre_file', default='./seq_pre.json', type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.editing_method == 'FT':\n",
    "        editing_hparams = FTHyperParams\n",
    "    elif args.editing_method == 'IKE':\n",
    "        editing_hparams = IKEHyperParams\n",
    "    elif args.editing_method == 'KN':\n",
    "        editing_hparams = KNHyperParams\n",
    "    elif args.editing_method == 'MEMIT':\n",
    "        editing_hparams = MEMITHyperParams\n",
    "    elif args.editing_method == 'ROME':\n",
    "        editing_hparams = ROMEHyperParams\n",
    "    elif args.editing_method == 'LoRA':\n",
    "        editing_hparams = LoRAHyperParams\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    datas = KnowEditDataset(args.data_dir,size=args.ds_size)\n",
    "    if args.datatype == 'counterfact' or args.datatype == 'recent' or args.datatype == 'zsre':\n",
    "        prompts=[data['prompt'] for data in datas]\n",
    "        subjects=[data['subject'] for data in datas]\n",
    "        target_new = [data['target_new'] for data in datas]\n",
    "        \n",
    "        portability_r =[data['portability_r'] for data in datas]\n",
    "        portability_s =[data['portability_s'] for data in datas]\n",
    "        portability_l =[data['portability_l'] for data in datas]\n",
    "\n",
    "        portability_reasoning_prompts=[]\n",
    "        portability_reasoning_ans=[]\n",
    "        portability_Logical_Generalization_prompts=[]\n",
    "        portability_Logical_Generalization_ans=[]\n",
    "        portability_Subject_Aliasing_prompts=[]\n",
    "        portability_Subject_Aliasing_ans=[]\n",
    "        \n",
    "        portability_data = [portability_r,portability_s,portability_l]\n",
    "        portability_prompts = [portability_reasoning_prompts,portability_Subject_Aliasing_prompts,portability_Logical_Generalization_prompts]\n",
    "        portability_answers = [portability_reasoning_ans,portability_Subject_Aliasing_ans,portability_Logical_Generalization_ans]\n",
    "        for data, portable_prompts, portable_answers in zip(portability_data,portability_prompts,portability_answers):\n",
    "            for item in data:\n",
    "                if item is None:\n",
    "                    portable_prompts.append(None)\n",
    "                    portable_answers.append(None)\n",
    "                else:\n",
    "                    temp_prompts = []\n",
    "                    temp_answers = []\n",
    "                    for pr in item:\n",
    "                        prompt=pr[\"prompt\"]\n",
    "                        an=pr[\"ground_truth\"]\n",
    "                        while isinstance(an,list):\n",
    "                            an = an[0]\n",
    "                        if an.strip() ==\"\":\n",
    "                            continue\n",
    "                        temp_prompts.append(prompt)\n",
    "                        temp_answers.append(an)\n",
    "                    portable_prompts.append(temp_prompts)\n",
    "                    portable_answers.append(temp_answers)\n",
    "        assert len(prompts) == len(portability_reasoning_prompts) == len(portability_Logical_Generalization_prompts) == len(portability_Subject_Aliasing_prompts)\n",
    "        \n",
    "        locality_rs = [data['locality_rs'] for data in datas]\n",
    "        locality_f = [data['locality_f'] for data in datas]\n",
    "        locality_Relation_Specificity_prompts=[]\n",
    "        locality_Relation_Specificity_ans=[]\n",
    "        locality_Forgetfulness_prompts=[]        \n",
    "        locality_Forgetfulness_ans=[]\n",
    "        \n",
    "        locality_data = [locality_rs, locality_f]\n",
    "        locality_prompts = [locality_Relation_Specificity_prompts,locality_Forgetfulness_prompts]\n",
    "        locality_answers = [locality_Relation_Specificity_ans,locality_Forgetfulness_ans]\n",
    "        for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "            for item in data:\n",
    "                if item is None:\n",
    "                    local_prompts.append(None)\n",
    "                    local_answers.append(None)\n",
    "                else:\n",
    "                    temp_prompts = []\n",
    "                    temp_answers = []\n",
    "                    for pr in item:\n",
    "                        prompt=pr[\"prompt\"]\n",
    "                        an=pr[\"ground_truth\"]\n",
    "                        while isinstance(an,list):\n",
    "                            an = an[0]\n",
    "                        if an.strip() ==\"\":\n",
    "                            continue\n",
    "                        temp_prompts.append(prompt)\n",
    "                        temp_answers.append(an)\n",
    "                    local_prompts.append(temp_prompts)\n",
    "                    local_answers.append(temp_answers)\n",
    "        assert len(prompts) == len(locality_Relation_Specificity_prompts) == len(locality_Forgetfulness_prompts)\n",
    "        locality_inputs = {}\n",
    "        portability_inputs = {}\n",
    "        \n",
    "        locality_inputs = {\n",
    "            'Relation_Specificity':{\n",
    "                'prompt': locality_Relation_Specificity_prompts,\n",
    "                'ground_truth': locality_Relation_Specificity_ans\n",
    "            },\n",
    "            'Forgetfulness':{\n",
    "                'prompt':locality_Forgetfulness_prompts,\n",
    "                'ground_truth':locality_Forgetfulness_ans\n",
    "            }\n",
    "        }\n",
    "        portability_inputs = {\n",
    "            'Subject_Aliasing':{\n",
    "                'prompt': portability_Subject_Aliasing_prompts,\n",
    "                'ground_truth': portability_Subject_Aliasing_ans\n",
    "            },\n",
    "            'reasoning':{\n",
    "                'prompt': portability_reasoning_prompts,\n",
    "                'ground_truth': portability_reasoning_ans           \n",
    "            },\n",
    "            'Logical_Generalization':{\n",
    "                'prompt': portability_Logical_Generalization_prompts,\n",
    "                'ground_truth': portability_Logical_Generalization_ans           \n",
    "            }\n",
    "        }\n",
    "    if args.datatype == 'wikibio':\n",
    "        prompts=[data['prompt'] for data in datas]\n",
    "        subjects=[data['subject'] for data in datas]\n",
    "        target_new = [data['target_new'] for data in datas]\n",
    "        \n",
    "        locality_rs = [data['locality_rs'] for data in datas]\n",
    "        locality_f = [data['locality_f'] for data in datas]\n",
    "        locality_Relation_Specificity_prompts=[]\n",
    "        locality_Relation_Specificity_ans=[]\n",
    "        \n",
    "        locality_data = [locality_rs]\n",
    "        locality_prompts = [locality_Relation_Specificity_prompts]\n",
    "        locality_answers = [locality_Relation_Specificity_ans]\n",
    "        for data, local_prompts, local_answers in zip(locality_data,locality_prompts,locality_answers):\n",
    "            for item in data:\n",
    "                if item is None:\n",
    "                    local_prompts.append(None)\n",
    "                    local_answers.append(None)\n",
    "                else:\n",
    "                    temp_prompts = []\n",
    "                    temp_answers = []\n",
    "                    for pr in item:\n",
    "                        prompt=pr[\"prompt\"]\n",
    "                        an=pr[\"ground_truth\"]\n",
    "                        while isinstance(an,list):\n",
    "                            an = an[0]\n",
    "                        if an.strip() ==\"\":\n",
    "                            continue\n",
    "                        temp_prompts.append(prompt)\n",
    "                        temp_answers.append(an)\n",
    "                    local_prompts.append(temp_prompts)\n",
    "                    local_answers.append(temp_answers)\n",
    "        assert len(prompts) == len(locality_Relation_Specificity_prompts)\n",
    "        portability_inputs = None\n",
    "        locality_inputs = {}\n",
    "        locality_inputs = {\n",
    "            'Relation_Specificity':{\n",
    "                'prompt': locality_Relation_Specificity_prompts,\n",
    "                'ground_truth': locality_Relation_Specificity_ans\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    hparams = editing_hparams.from_hparams(args.hparams_dir)\n",
    "    args.pre_file = f\"./pre_edit/{hparams.model_name.split('/')[-1]}_{args.datatype}_pre_edit.json\"\n",
    "    print(args.pre_file)\n",
    "    if args.pre_file is not None and os.path.exists(args.pre_file):\n",
    "        pre_edit = json.load(open(args.pre_file,'r'))\n",
    "        assert len(pre_edit) == len(prompts)\n",
    "    else:\n",
    "        pre_edit = None\n",
    "    if args.editing_method == 'IKE':\n",
    "        train_ds = KnowEditDataset(args.train_data_path)\n",
    "        huggingface_cache = os.environ.get('HUGGINGFACE_CACHE')\n",
    "        if huggingface_cache:\n",
    "            sentence_model_name = os.path.join(huggingface_cache, hparams.sentence_model_name)\n",
    "            print(f\"Using Huggingface cache: {sentence_model_name}\")        \n",
    "        sentence_model = SentenceTransformer(sentence_model_name).to(f'cuda:{hparams.device}')\n",
    "        encode_ike_facts(sentence_model, train_ds, hparams)\n",
    "    else:\n",
    "        train_ds = None\n",
    "    editor = BaseEditor.from_hparams(hparams)\n",
    "    if hparams.batch_size == 1:\n",
    "        metrics, edited_model, _ = editor.edit(\n",
    "            prompts=prompts,\n",
    "            target_new=target_new,\n",
    "            subject=subjects,\n",
    "            locality_inputs=locality_inputs,\n",
    "            portability_inputs=portability_inputs,\n",
    "            train_ds=train_ds,\n",
    "            keep_original_weight=True,\n",
    "            pre_file=args.pre_file,\n",
    "            pre_edit = pre_edit,\n",
    "            test_generation=True,\n",
    "        )\n",
    "    else:\n",
    "        metrics, edited_model, _ = editor.batch_edit(\n",
    "            prompts=prompts,\n",
    "            target_new=target_new,\n",
    "            subject=subjects,\n",
    "            locality_inputs=locality_inputs,\n",
    "            portability_inputs=portability_inputs,\n",
    "            train_ds=train_ds,\n",
    "            keep_original_weight=True,\n",
    "            pre_file=args.pre_file,\n",
    "            pre_edit = pre_edit,\n",
    "            test_generation=True,\n",
    "        )\n",
    "    \n",
    "    if not os.path.exists(args.metrics_save_dir):\n",
    "        os.makedirs(args.metrics_save_dir)\n",
    "    json.dump(metrics, open(os.path.join(args.metrics_save_dir, f'{args.editing_method}_{args.datatype}_{args.hparams_dir.split(\"/\")[-1]}_results.json'), 'w'), indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
