{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers_stream_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006779193878173828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f00407806b74cc4b42adbc6d1a79b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (core_attention_flash): FlashSelfAttention()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (w2): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (c_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "model_name = 'Qwen-1_8B-Chat'\n",
    "model_path = '/share/huggingface/'\n",
    "device = 'auto'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + model_name, trust_remote_code=True, device_map=device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path + model_name, trust_remote_code=True, device_map=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.ln_1.weight torch.Size([2048]) torch.bfloat16\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([6144, 2048]) torch.bfloat16\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([6144]) torch.bfloat16\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([2048, 2048]) torch.bfloat16\n",
      "transformer.h.0.ln_2.weight torch.Size([2048]) torch.bfloat16\n",
      "transformer.h.0.mlp.w1.weight torch.Size([5504, 2048]) torch.bfloat16\n",
      "transformer.h.0.mlp.w2.weight torch.Size([5504, 2048]) torch.bfloat16\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([2048, 5504]) torch.bfloat16\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    if 'h.0' in k:\n",
    "        print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取yaml文件\n",
    "import yaml\n",
    "\n",
    "with open('./gpt-j-6b.yaml', 'r') as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in [8,16,32,64]:\n",
    "    with open(f'./test/gpt-j-6b-bs-{bs}.yaml', 'w') as f:\n",
    "        hparams['batch_size'] = bs\n",
    "        yaml.dump(hparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_modules\n",
    "target_modules_dict ={\n",
    "    'attn': ['q_proj','k_proj','v_proj','out_proj'],\n",
    "    'mlp': ['fc_in','fc_out'],\n",
    "    'all': ['q_proj','k_proj','v_proj','out_proj','fc_in','fc_out'],\n",
    "}\n",
    "for target_modules in target_modules_dict:\n",
    "    print(target_modules_dict[target_modules])\n",
    "    # layers\n",
    "    for layer in range(0,28,7):\n",
    "        print([i for i in range(layer,layer+7)])\n",
    "        layers = [i for i in range(layer,layer+7)]\n",
    "        with open(f'./test/gpt-j-6b-{target_modules}-{layers[0]}-{layers[-1]}.yaml', 'w') as f:\n",
    "            hparams['layers'] = layers\n",
    "            hparams['target_modules'] = target_modules_dict[target_modules]\n",
    "            # hparams['bf16'] = True\n",
    "            yaml.dump(hparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test/gpt-j-6b-all-0-6.yaml', 'r') as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "hparams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke2torch23cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
