{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "model_name = 'gpt-j-6b'\n",
    "model_path = '/share/huggingface/'\n",
    "device = 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + model_name, trust_remote_code=True, device_map=device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path + model_name, trust_remote_code=True, device_map=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"_name_or_path\": \"/share/huggingface/gpt-j-6b\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 28,\n",
       "  \"n_positions\": 2048,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary\": true,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50,\n",
       "      \"temperature\": 1.0\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
       "  \"transformers_version\": \"4.40.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50400\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50400, 4096])\n",
      "transformer.h.0.ln_1.weight torch.Size([4096])\n",
      "transformer.h.0.ln_1.bias torch.Size([4096])\n",
      "transformer.h.0.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.0.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.0.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.0.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.0.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.0.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.0.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.0.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.1.ln_1.weight torch.Size([4096])\n",
      "transformer.h.1.ln_1.bias torch.Size([4096])\n",
      "transformer.h.1.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.1.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.1.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.1.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.1.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.1.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.1.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.1.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.2.ln_1.weight torch.Size([4096])\n",
      "transformer.h.2.ln_1.bias torch.Size([4096])\n",
      "transformer.h.2.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.2.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.2.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.2.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.2.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.2.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.2.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.2.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.3.ln_1.weight torch.Size([4096])\n",
      "transformer.h.3.ln_1.bias torch.Size([4096])\n",
      "transformer.h.3.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.3.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.3.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.3.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.3.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.3.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.3.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.3.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.4.ln_1.weight torch.Size([4096])\n",
      "transformer.h.4.ln_1.bias torch.Size([4096])\n",
      "transformer.h.4.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.4.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.4.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.4.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.4.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.4.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.4.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.4.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.5.ln_1.weight torch.Size([4096])\n",
      "transformer.h.5.ln_1.bias torch.Size([4096])\n",
      "transformer.h.5.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.5.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.5.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.5.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.5.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.5.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.5.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.5.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.6.ln_1.weight torch.Size([4096])\n",
      "transformer.h.6.ln_1.bias torch.Size([4096])\n",
      "transformer.h.6.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.6.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.6.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.6.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.6.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.6.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.6.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.6.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.7.ln_1.weight torch.Size([4096])\n",
      "transformer.h.7.ln_1.bias torch.Size([4096])\n",
      "transformer.h.7.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.7.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.7.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.7.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.7.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.7.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.7.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.7.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.8.ln_1.weight torch.Size([4096])\n",
      "transformer.h.8.ln_1.bias torch.Size([4096])\n",
      "transformer.h.8.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.8.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.8.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.8.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.8.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.8.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.8.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.8.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.9.ln_1.weight torch.Size([4096])\n",
      "transformer.h.9.ln_1.bias torch.Size([4096])\n",
      "transformer.h.9.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.9.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.9.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.9.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.9.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.9.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.9.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.9.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.10.ln_1.weight torch.Size([4096])\n",
      "transformer.h.10.ln_1.bias torch.Size([4096])\n",
      "transformer.h.10.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.10.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.10.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.10.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.10.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.10.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.10.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.10.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.11.ln_1.weight torch.Size([4096])\n",
      "transformer.h.11.ln_1.bias torch.Size([4096])\n",
      "transformer.h.11.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.11.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.11.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.11.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.11.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.11.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.11.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.11.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.12.ln_1.weight torch.Size([4096])\n",
      "transformer.h.12.ln_1.bias torch.Size([4096])\n",
      "transformer.h.12.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.12.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.12.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.12.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.12.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.12.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.12.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.12.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.13.ln_1.weight torch.Size([4096])\n",
      "transformer.h.13.ln_1.bias torch.Size([4096])\n",
      "transformer.h.13.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.13.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.13.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.13.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.13.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.13.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.13.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.13.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.14.ln_1.weight torch.Size([4096])\n",
      "transformer.h.14.ln_1.bias torch.Size([4096])\n",
      "transformer.h.14.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.14.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.14.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.14.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.14.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.14.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.14.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.14.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.15.ln_1.weight torch.Size([4096])\n",
      "transformer.h.15.ln_1.bias torch.Size([4096])\n",
      "transformer.h.15.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.15.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.15.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.15.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.15.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.15.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.15.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.15.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.16.ln_1.weight torch.Size([4096])\n",
      "transformer.h.16.ln_1.bias torch.Size([4096])\n",
      "transformer.h.16.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.16.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.16.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.16.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.16.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.16.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.16.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.16.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.17.ln_1.weight torch.Size([4096])\n",
      "transformer.h.17.ln_1.bias torch.Size([4096])\n",
      "transformer.h.17.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.17.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.17.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.17.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.17.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.17.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.17.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.17.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.18.ln_1.weight torch.Size([4096])\n",
      "transformer.h.18.ln_1.bias torch.Size([4096])\n",
      "transformer.h.18.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.18.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.18.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.18.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.18.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.18.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.18.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.18.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.19.ln_1.weight torch.Size([4096])\n",
      "transformer.h.19.ln_1.bias torch.Size([4096])\n",
      "transformer.h.19.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.19.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.19.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.19.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.19.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.19.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.19.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.19.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.20.ln_1.weight torch.Size([4096])\n",
      "transformer.h.20.ln_1.bias torch.Size([4096])\n",
      "transformer.h.20.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.20.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.20.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.20.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.20.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.20.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.20.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.20.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.21.ln_1.weight torch.Size([4096])\n",
      "transformer.h.21.ln_1.bias torch.Size([4096])\n",
      "transformer.h.21.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.21.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.21.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.21.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.21.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.21.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.21.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.21.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.22.ln_1.weight torch.Size([4096])\n",
      "transformer.h.22.ln_1.bias torch.Size([4096])\n",
      "transformer.h.22.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.22.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.22.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.22.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.22.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.22.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.22.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.22.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.23.ln_1.weight torch.Size([4096])\n",
      "transformer.h.23.ln_1.bias torch.Size([4096])\n",
      "transformer.h.23.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.23.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.23.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.23.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.23.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.23.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.23.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.23.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.24.ln_1.weight torch.Size([4096])\n",
      "transformer.h.24.ln_1.bias torch.Size([4096])\n",
      "transformer.h.24.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.24.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.24.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.24.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.24.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.24.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.24.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.24.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.25.ln_1.weight torch.Size([4096])\n",
      "transformer.h.25.ln_1.bias torch.Size([4096])\n",
      "transformer.h.25.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.25.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.25.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.25.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.25.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.25.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.25.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.25.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.26.ln_1.weight torch.Size([4096])\n",
      "transformer.h.26.ln_1.bias torch.Size([4096])\n",
      "transformer.h.26.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.26.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.26.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.26.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.26.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.26.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.26.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.26.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.h.27.ln_1.weight torch.Size([4096])\n",
      "transformer.h.27.ln_1.bias torch.Size([4096])\n",
      "transformer.h.27.attn.k_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.27.attn.v_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.27.attn.q_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.27.attn.out_proj.weight torch.Size([4096, 4096])\n",
      "transformer.h.27.mlp.fc_in.weight torch.Size([16384, 4096])\n",
      "transformer.h.27.mlp.fc_in.bias torch.Size([16384])\n",
      "transformer.h.27.mlp.fc_out.weight torch.Size([4096, 16384])\n",
      "transformer.h.27.mlp.fc_out.bias torch.Size([4096])\n",
      "transformer.ln_f.weight torch.Size([4096])\n",
      "transformer.ln_f.bias torch.Size([4096])\n",
      "lm_head.weight torch.Size([50400, 4096])\n",
      "lm_head.bias torch.Size([50400])\n"
     ]
    }
   ],
   "source": [
    "for k,v in model.named_parameters():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mlp.w1', 'mlp.w2'], ['mlp.w1'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'mlp.w1,mlp.w2'.split(','),'mlp.w1'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lyc/TNTprojectz/KE/EasyEdit'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lyc/TNTprojectz/KE/EasyEdit/tutorial-notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007873058319091797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a43ce103074f70bad867613a7ed73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nExplain the concept of reinforcement learning.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhat are the advantages of using transformers in NLP?<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nDescribe the use cases of generative AI.<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "model_name = 'Qwen-1_8B-Chat'\n",
    "model_path = '/share/huggingface/'\n",
    "# device = 'auto'\n",
    "\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path + model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + model_name, \n",
    "                                          eos_token='<|endoftext|>', \n",
    "                                          pad_token='<|endoftext|>',\n",
    "                                          unk_token='<|endoftext|>', \n",
    "                                          padding_side='left', \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "# Define a list of prompts for batch processing\n",
    "prompts = [\n",
    "    \"Give me a short introduction to large language model.\",\n",
    "    \"Explain the concept of reinforcement learning.\",\n",
    "    \"What are the advantages of using transformers in NLP?\",\n",
    "    \"Describe the use cases of generative AI.\"\n",
    "]\n",
    "\n",
    "messages_batch = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "texts = [\n",
    "    tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    for messages in messages_batch\n",
    "]\n",
    "texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Give me a short introduction to large language model.\n",
      "assistant\n",
      "Large language models are artificial intelligence systems that are capable of generating human-like text on a wide range of topics. These models use machine learning algorithms to analyze vast amounts of text data and learn patterns in language usage. They can be used for tasks such as language translation, summarization, writing articles, generating code snippets, and more.\n",
      "\n",
      "There are several types of large language models, including transformer-based models, which are widely used for natural language processing tasks, and recurrent neural networks (RNNs), which are commonly used for language modeling. Large language models have made significant advances in recent years, allowing them to generate coherent and contextually relevant text with increasing accuracy and fluency.\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Explain the concept of reinforcement learning.\n",
      "assistant\n",
      "Reinforcement learning is a type of machine learning in which an agent learns to interact with its environment by receiving feedback in the form of rewards or punishments, and adapting its behavior accordingly. The goal of reinforcement learning is for the agent to learn a policy that maximizes its cumulative reward over time.\n",
      "\n",
      "In other words, reinforcement learning involves an agent taking actions in an environment in order to maximize some measure of gain. This measure can be in the form of a reward signal, such as food being earned when reaching a target or a discount factor representing the negative consequences of making a mistake.\n",
      "\n",
      "The agent receives feedback from its environment on its actions, either positive or negative. The feedback is then used to update the agent's policy based on the new information it has received. The agent then uses this updated policy to make new decisions about how to act in the environment, and continues to receive feedback in the form of rewards until the desired level of performance is achieved.\n",
      "\n",
      "Reinforcement learning has been successfully applied to a wide range of applications, including robotics, game playing, autonomous driving, and natural language processing. By trial and error, the agent learns to adapt to changing environments and achieve goals that were not previously possible.\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What are the advantages of using transformers in NLP?\n",
      "assistant\n",
      "Transformers have several advantages in natural language processing (NLP) tasks:\n",
      "\n",
      "1. Attention mechanism: Transformers use an attention mechanism to capture the importance of different parts of the input sequence while making predictions. This mechanism allows them to handle sequential data with varying lengths and can effectively capture long-range dependencies.\n",
      "\n",
      "2. Linear layers: Transformers consist of linear layers that allow them to learn complex relationships between input features and output labels. This makes them suitable for tasks such as language modeling, machine translation, and sentiment analysis.\n",
      "\n",
      "3. Long short-term memory (LSTM): Transformers also use LSTMs to handle sequences of variable length. LSTMs are effective at capturing temporal patterns and dependencies in sequences and are commonly used in language modeling tasks.\n",
      "\n",
      "4. Positional encoding: Transformers also use positional encoding to represent the order of words in the input sequence. This helps them to learn more meaningful representations of text and improve their performance on tasks such as question answering and named entity recognition.\n",
      "\n",
      "5. Computation efficiency: Transformers are computationally efficient and can process large amounts of data quickly. This makes them useful for real-time applications such as chatbots and virtual assistants.\n",
      "\n",
      "Overall, these advantages make transformers an attractive choice for a wide range of NLP tasks, particularly those involving sequential data with varying lengths.\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "Describe the use cases of generative AI.\n",
      "assistant\n",
      "Generative AI is a type of artificial intelligence that uses statistical models and algorithms to generate new data or outputs based on patterns learned from existing data. Here are some common use cases for generative AI:\n",
      "\n",
      "1. Content creation: Generative AI can be used to create new content such as images, videos, music, and text, by training it on large datasets of existing content.\n",
      "\n",
      "2. Data augmentation: Generative AI can be used to augment training data by creating synthetic data points that are similar to real-world data but with different characteristics.\n",
      "\n",
      "3. Style transfer: Generative AI can be used to apply the style of one image to another, creating new images that resemble them while preserving their unique characteristics.\n",
      "\n",
      "4. Personalization: Generative AI can be used to personalize user experiences by generating personalized recommendations, news articles, and other content based on individual preferences.\n",
      "\n",
      "5. Chatbots and virtual assistants: Generative AI can be used to generate human-like responses to user queries, improving chatbot functionality and efficiency.\n",
      "\n",
      "6. Natural language processing (NLP): Generative AI can be used to analyze and generate natural language text, enabling more accurate and efficient sentiment analysis, text summarization, and other NLP tasks.\n",
      "\n",
      "7. Drug discovery: Generative AI can be used to generate new compounds and drug candidates by analyzing chemical structures and predicting their properties.\n",
      "\n",
      "8. Robotics: Generative AI can be used to generate realistic simulations of robots and other mechanical systems, allowing designers to test and refine their designs before physical prototyping.\n",
      "\n",
      "Overall, generative AI has numerous applications in various industries, from entertainment to healthcare, where it can help automate tasks, improve efficiency, and generate new insights.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the batch of texts\n",
    "model_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Generate responses for all inputs in batch\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated responses\n",
    "responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Output all responses\n",
    "for response in responses:\n",
    "    print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Large language models are artificial intelligence systems that are capable of generating human-like text on a wide range of topics. These models use machine learning algorithms to analyze vast amounts of text data and learn patterns in language usage. They can be used for tasks such as language translation, summarization, writing articles, generating code snippets, and more.\n",
      "\n",
      "There are several types of large language models, including transformer-based models, which are widely used for natural language processing tasks, and recurrent neural networks (RNNs), which are commonly used for language modeling. Large language models have made significant advances in recent years, allowing them to generate coherent and contextually relevant text with increasing accuracy and fluency.<|im_end|>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Explain the concept of reinforcement learning.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Reinforcement learning is a type of machine learning in which an agent learns to interact with its environment by receiving feedback in the form of rewards or punishments, and adapting its behavior accordingly. The goal of reinforcement learning is for the agent to learn a policy that maximizes its cumulative reward over time.\n",
      "\n",
      "In other words, reinforcement learning involves an agent taking actions in an environment in order to maximize some measure of gain. This measure can be in the form of a reward signal, such as food being earned when reaching a target or a discount factor representing the negative consequences of making a mistake.\n",
      "\n",
      "The agent receives feedback from its environment on its actions, either positive or negative. The feedback is then used to update the agent's policy based on the new information it has received. The agent then uses this updated policy to make new decisions about how to act in the environment, and continues to receive feedback in the form of rewards until the desired level of performance is achieved.\n",
      "\n",
      "Reinforcement learning has been successfully applied to a wide range of applications, including robotics, game playing, autonomous driving, and natural language processing. By trial and error, the agent learns to adapt to changing environments and achieve goals that were not previously possible.<|im_end|>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What are the advantages of using transformers in NLP?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Transformers have several advantages in natural language processing (NLP) tasks:\n",
      "\n",
      "1. Attention mechanism: Transformers use an attention mechanism to capture the importance of different parts of the input sequence while making predictions. This mechanism allows them to handle sequential data with varying lengths and can effectively capture long-range dependencies.\n",
      "\n",
      "2. Linear layers: Transformers consist of linear layers that allow them to learn complex relationships between input features and output labels. This makes them suitable for tasks such as language modeling, machine translation, and sentiment analysis.\n",
      "\n",
      "3. Long short-term memory (LSTM): Transformers also use LSTMs to handle sequences of variable length. LSTMs are effective at capturing temporal patterns and dependencies in sequences and are commonly used in language modeling tasks.\n",
      "\n",
      "4. Positional encoding: Transformers also use positional encoding to represent the order of words in the input sequence. This helps them to learn more meaningful representations of text and improve their performance on tasks such as question answering and named entity recognition.\n",
      "\n",
      "5. Computation efficiency: Transformers are computationally efficient and can process large amounts of data quickly. This makes them useful for real-time applications such as chatbots and virtual assistants.\n",
      "\n",
      "Overall, these advantages make transformers an attractive choice for a wide range of NLP tasks, particularly those involving sequential data with varying lengths.<|im_end|>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|endoftext|><|endoftext|><|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Describe the use cases of generative AI.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Generative AI is a type of artificial intelligence that uses statistical models and algorithms to generate new data or outputs based on patterns learned from existing data. Here are some common use cases for generative AI:\n",
      "\n",
      "1. Content creation: Generative AI can be used to create new content such as images, videos, music, and text, by training it on large datasets of existing content.\n",
      "\n",
      "2. Data augmentation: Generative AI can be used to augment training data by creating synthetic data points that are similar to real-world data but with different characteristics.\n",
      "\n",
      "3. Style transfer: Generative AI can be used to apply the style of one image to another, creating new images that resemble them while preserving their unique characteristics.\n",
      "\n",
      "4. Personalization: Generative AI can be used to personalize user experiences by generating personalized recommendations, news articles, and other content based on individual preferences.\n",
      "\n",
      "5. Chatbots and virtual assistants: Generative AI can be used to generate human-like responses to user queries, improving chatbot functionality and efficiency.\n",
      "\n",
      "6. Natural language processing (NLP): Generative AI can be used to analyze and generate natural language text, enabling more accurate and efficient sentiment analysis, text summarization, and other NLP tasks.\n",
      "\n",
      "7. Drug discovery: Generative AI can be used to generate new compounds and drug candidates by analyzing chemical structures and predicting their properties.\n",
      "\n",
      "8. Robotics: Generative AI can be used to generate realistic simulations of robots and other mechanical systems, allowing designers to test and refine their designs before physical prototyping.\n",
      "\n",
      "Overall, generative AI has numerous applications in various industries, from entertainment to healthcare, where it can help automate tasks, improve efficiency, and generate new insights.<|im_end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated responses\n",
    "responses = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "# Output all responses\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "model_name = 'Qwen-1_8B-Chat'\n",
    "model_path = '/share/huggingface/'\n",
    "device = 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + model_name, trust_remote_code=True, device_map=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = tokenizer.encode('他日若遂凌云志,敢笑黄巢不丈夫。', return_tensors=\"pt\")\n",
    "tok1, tok1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = tokenizer('他日若遂凌云志,敢笑黄巢不丈夫。', return_tensors=\"pt\")\n",
    "tok2, tok2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006924867630004883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265102c53756437e9ac28afb1dea08a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (core_attention_flash): FlashSelfAttention()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (w2): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (c_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path + model_name, trust_remote_code=True, device_map=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    if 'h.0' in k:\n",
    "        print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取yaml文件\n",
    "import yaml\n",
    "\n",
    "with open('./gpt-j-6b.yaml', 'r') as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in [8,16,32,64]:\n",
    "    with open(f'./test/gpt-j-6b-bs-{bs}.yaml', 'w') as f:\n",
    "        hparams['batch_size'] = bs\n",
    "        yaml.dump(hparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_modules\n",
    "target_modules_dict ={\n",
    "    'attn': ['q_proj','k_proj','v_proj','out_proj'],\n",
    "    'mlp': ['fc_in','fc_out'],\n",
    "    'all': ['q_proj','k_proj','v_proj','out_proj','fc_in','fc_out'],\n",
    "}\n",
    "for target_modules in target_modules_dict:\n",
    "    print(target_modules_dict[target_modules])\n",
    "    # layers\n",
    "    for layer in range(0,28,7):\n",
    "        print([i for i in range(layer,layer+7)])\n",
    "        layers = [i for i in range(layer,layer+7)]\n",
    "        with open(f'./test/gpt-j-6b-{target_modules}-{layers[0]}-{layers[-1]}.yaml', 'w') as f:\n",
    "            hparams['layers'] = layers\n",
    "            hparams['target_modules'] = target_modules_dict[target_modules]\n",
    "            # hparams['bf16'] = True\n",
    "            yaml.dump(hparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test/gpt-j-6b-all-0-6.yaml', 'r') as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "hparams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke2torch23cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
