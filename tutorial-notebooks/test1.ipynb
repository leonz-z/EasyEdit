{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import filecmp\n",
    "\n",
    "def files_are_identical(file1, file2):\n",
    "    return filecmp.cmp(file1, file2, shallow=False)\n",
    "\n",
    "# 使用示例\n",
    "file1 = '/share/knb_dict/Qwen-1_8B-Chat-CKnowEdit/type1_133/type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_attn-knb_dict-max-bs1.json'\n",
    "file2 = '/share/knb_dict/Qwen-1_8B-Chat-CKnowEdit/type1_133_v2/type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_attn-knb_dict-max-bs1.json'\n",
    "print(files_are_identical(file1, file2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def files_are_identical(file1, file2):\n",
    "    with open(file1, 'rb') as f1, open(file2, 'rb') as f2:\n",
    "        # 逐行读取并比较\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            if line1 != line2:\n",
    "                return False\n",
    "        # 如果一方到达文件末尾，另一方还未结束，则不一致\n",
    "        return f1.read(1) == f2.read(1)\n",
    "\n",
    "print(files_are_identical(file1, file2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type1_133',\n",
       " 'type1_133_v2',\n",
       " 'type2_80',\n",
       " 'type2_80_v2',\n",
       " 'type3_40',\n",
       " 'type3_40_v2',\n",
       " 'type4_50',\n",
       " 'type4_50_v2',\n",
       " 'type5_70',\n",
       " 'type5_70_v2',\n",
       " 'type6_50',\n",
       " 'type6_50_v2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_dir = '/share/knb_dict/Qwen-1_8B-Chat-CKnowEdit'\n",
    "sub_dataset_dir = os.listdir(root_dir)\n",
    "sub_dataset_dir.sort()\n",
    "sub_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs1.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs10.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs16.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs2.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs20.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs30.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs4.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs40.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs8.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs1.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs10.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs16.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs2.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs20.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs30.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs4.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs40.json\n",
      "type1_133-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs8.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs1.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs10.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs16.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs2.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs20.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs30.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs4.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs40.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs8.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs1.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs10.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs16.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs2.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs20.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs30.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs4.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs40.json\n",
      "type2_80-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs8.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs1.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs10.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs16.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs2.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs20.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs30.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs4.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs40.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs8.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs1.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs10.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs16.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs2.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs20.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs30.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs4.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs40.json\n",
      "type3_40-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs8.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs1.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs10.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs16.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs2.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs20.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs30.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs4.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs40.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs8.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs1.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs10.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs16.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs2.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs20.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs30.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs4.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs40.json\n",
      "type4_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs8.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs1.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs10.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs16.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs2.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs20.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs30.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs4.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs40.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs8.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs1.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs10.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs16.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs2.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs20.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs30.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs4.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs40.json\n",
      "type5_70-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs8.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs1.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs10.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs16.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs2.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs20.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs30.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs4.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs40.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-max-bs8.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs1.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs10.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs16.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs2.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs20.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs30.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs4.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs40.json\n",
      "type6_50-Qwen-1_8B-Chat-CKnowEdit-layer-0-24-attn.c_proj-knb_dict-mean-bs8.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(sub_dataset_dir), 2):\n",
    "    sub_dataset1, sub_dataset2 = sub_dataset_dir[i], sub_dataset_dir[i+1]\n",
    "    file_name_list1 = os.listdir(os.path.join(root_dir, sub_dataset1))\n",
    "    file_name_list2 = os.listdir(os.path.join(root_dir, sub_dataset2))\n",
    "    file_name_list1.sort()\n",
    "    file_name_list2.sort()\n",
    "    # assert file_name_list1 == file_name_list2, \"The file lists of {} and {} are different.\".format(sub_dataset1, sub_dataset2)\n",
    "    for file in file_name_list1:\n",
    "        file_path1 = os.path.join(root_dir, sub_dataset1, file)\n",
    "        file_path2 = os.path.join(root_dir, sub_dataset2, file)\n",
    "        try:\n",
    "            # print(files_are_identical(file_path1, file_path2))\n",
    "            if not files_are_identical(file_path1, file_path2):\n",
    "                print(file)\n",
    "        except FileNotFoundError:\n",
    "            # print(\"File not found: {}\".format(file))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_list1,file_name_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list|grep datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_name = \"wikipedia\"\n",
    "raw_ds = load_dataset(\n",
    "    ds_name,\n",
    "    dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.zh\", cache_dir='/share/dataset/Wikipedia/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /share/dataset/Wikipedia/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pleisto/wikipedia-cn-20230720-filtered\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('/share/dataset/Wikipedia/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda example: {'text': example['completion']}, remove_columns=['completion'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('/share/dataset/Wikipedia/')\n",
    "\n",
    "ds['train']\n",
    "Dataset({\n",
    "    features: ['source', 'completion'],\n",
    "    num_rows: 254547\n",
    "})\n",
    "\n",
    "ds['train'][0]\n",
    "\n",
    "{'source': 'wikipedia.zh2307',\n",
    " 'completion': '昭通机场（ZPZT）是位于中国'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries(n, board, queries):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        x1, y1, x2, y2 = query\n",
    "        x, y = x1, y1\n",
    "        while x1 <= x <= x2 and y1 <= y <= y2:\n",
    "            if board[x-1][y-1] == 0:\n",
    "                x += 1\n",
    "                if x > x2:\n",
    "                    x -= 1\n",
    "                    break\n",
    "            else:\n",
    "                y += 1\n",
    "                if y > y2:\n",
    "                    y -= 1\n",
    "                    break\n",
    "        #     print(x, y)\n",
    "        # print(\"=\"*10)\n",
    "        results.append((x,y))\n",
    "    return results\n",
    "\n",
    "# 读取输入\n",
    "# n = int(input())\n",
    "# grid = [list(map(int, input().split())) for _ in range(n)]\n",
    "# q = int(input())\n",
    "# queries = [tuple(map(int, input().split())) for _ in range(q)]\n",
    "n=4\n",
    "grid=[[1,0,0,1],[0,0,1,1],[1,0,1,0],[0,1,1,1]]\n",
    "q=2\n",
    "queries=[(2,2,4,4),(2,3,3,3)]\n",
    "\n",
    "# 计算并输出结果\n",
    "results = process_queries(n, grid, queries)\n",
    "for result in results:\n",
    "    print(result[0], result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(a)->int:\n",
    "    b =list(range(0,2))\n",
    "    return b[0:1]\n",
    "f = fn(a=1)\n",
    "print(fn.__defaults__)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"_\".join([\"hello\", \"world\"]),\"_\".join([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, idx in tqdm(enumerate(range(0, 12, 2)), total=int(6)):\n",
    "    print(i, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Hello\", \"Bonjour\"), (\"How are you?\", \"Comment ça va?\"), (\"I'm fine, thank you.\", \"Je suis très heureux, merci.\")]\n",
    "\n",
    "txt, tgt = zip(*data)\n",
    "txt, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "list1 = [1, 2, 3, 4, 5]\n",
    "random.shuffle(list1)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.zeros(4,8)\n",
    "B = torch.ones(4,4)\n",
    "idx = [2,4,6,7]\n",
    "A[:,idx] = B\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "target_name = \"'model.layers.0.mlp.up_proj\"\n",
    "match = re.match(r\".*\\.[^.]*\\.(\\d+)\\.\", target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\".\".join(target_name.split(\".\")[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty((0, 2)), torch.empty((0, 0)), torch.empty((2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 2,\n",
    "    \"c\": 3\n",
    "}\n",
    "\n",
    "d2 = {\n",
    "    \"a\": 10,\n",
    "    \"e\": 20,\n",
    "    \"d\": 40\n",
    "}\n",
    "from itertools import chain\n",
    "list(chain(d1.keys(), d2.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ke2torch23cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
