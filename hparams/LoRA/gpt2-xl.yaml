alg_name: LoRA
model_name: gpt2-xl
device: 0
lora_type: "adalora"
layers: [] # v1 [] v2:MEMIT [13, 14, 15, 16, 17]
num_steps: 30
batch_size: 1
max_length: 30
lr: 5e-3
weight_decay: 0
kl_factor: 0
rank: 16
lora_alpha: 32
lora_dropout: 0.1
norm_constraint: false
target_modules: ["c_fc","c_proj"]  # attn:c_attn c_proj mlp:c_fc c_proj
model_parallel: false