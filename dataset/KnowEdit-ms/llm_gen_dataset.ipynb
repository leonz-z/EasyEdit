{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import json\n",
    "\n",
    "model_path = '/share/huggingface/Meta-Llama-3-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glm生成辅助数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:暂未考虑主体同义替换\n",
    "n = 10\n",
    "system_prompt = f\"\"\"You are an expert in sentence rewriting. Your task is to perform a synonymous rewriting of the input sentence and output {n} rewritten sentences.\n",
    "\n",
    "Requirements:\n",
    "1. The semantic meaning of the rewritten sentences must strictly match the original sentence.\n",
    "2. Use different grammatical structures and expressions to rephrase the input sentence as much as possible.\n",
    "3. Output format requirements: separate the {n} rewritten sentences with a newline (\\n).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key=\"1239bfa4ba1cc7135d4476e3ad30af96.JjzOdCR7qNwFSZk7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './benchmark_ZsRE_ZsRE-test-all.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "with open('ZsRE-test-all-generate-10.json', 'a') as f:\n",
    "    for data in data_list[1185:]:\n",
    "        user_prompt = data['prompt']\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        data['generate_prompt'] = response.choices[0].message.content.split('\\n')\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './benchmark_wiki_counterfact_test_cf.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data_list = json.load(f)\n",
    "\n",
    "with open('wiki_counterfact_test_cf-generate-10.json', 'a') as f:\n",
    "    for data in data_list[601:]:\n",
    "        user_prompt = data['prompt']\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "        data['generate_prompt'] = response.choices[0].message.content.split('\\n')\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试llm输出稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"/share/huggingface/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", \n",
    "                                 model=model_id, \n",
    "                                 model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "                                 device_map=\"auto\")\n",
    "pipeline(\"The name of the country of citizenship of Leonardo DiCaprio is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline('Which family does Epaspidoceras belong to?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/share/huggingface/gpt-j-6b\"\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", \n",
    "                                 model=model_id, \n",
    "                                 model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "                                 device_map=\"auto\")\n",
    "pipeline(\"The name of the country of citizenship of Leonardo DiCaprio is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline('Which family does Epaspidoceras belong to?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import json\n",
    "\n",
    "model_path = '/share/huggingface/gpt-j-6b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "prompt = \"The name of the country of citizenship of Leonardo DiCaprio is\"\n",
    "\n",
    "pipleline_model = pipeline('text2text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "pipleline_model(prompt, max_length=100, num_return_sequences=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lccc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
